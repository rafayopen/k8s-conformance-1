I0429 23:02:47.034134      21 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-371654096
I0429 23:02:47.034401      21 e2e.go:92] Starting e2e run "4c6ae5b2-dbe3-43bd-9697-b2549e28748e" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1588201366 - Will randomize all specs
Will run 276 of 4731 specs

Apr 29 23:02:47.047: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:02:47.048: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr 29 23:02:47.059: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr 29 23:02:47.079: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr 29 23:02:47.079: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Apr 29 23:02:47.079: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr 29 23:02:47.085: INFO: 10 / 10 pods ready in namespace 'kube-system' in daemonset 'aws-node' (0 seconds elapsed)
Apr 29 23:02:47.085: INFO: 10 / 10 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Apr 29 23:02:47.085: INFO: e2e test version: v1.16.8
Apr 29 23:02:47.086: INFO: kube-apiserver version: v1.16.8-eks-e16311
Apr 29 23:02:47.086: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:02:47.091: INFO: Cluster IP family: ipv4
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:02:47.091: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename configmap
Apr 29 23:02:47.127: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Apr 29 23:02:47.137: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-388
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-7e4cfd21-7b51-4be7-be4d-a12d73f77ec5
STEP: Creating a pod to test consume configMaps
Apr 29 23:02:47.264: INFO: Waiting up to 5m0s for pod "pod-configmaps-be565afd-6763-41a6-87f4-47dd70b72176" in namespace "configmap-388" to be "success or failure"
Apr 29 23:02:47.266: INFO: Pod "pod-configmaps-be565afd-6763-41a6-87f4-47dd70b72176": Phase="Pending", Reason="", readiness=false. Elapsed: 1.897326ms
Apr 29 23:02:49.269: INFO: Pod "pod-configmaps-be565afd-6763-41a6-87f4-47dd70b72176": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004594892s
Apr 29 23:02:51.272: INFO: Pod "pod-configmaps-be565afd-6763-41a6-87f4-47dd70b72176": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007423592s
STEP: Saw pod success
Apr 29 23:02:51.272: INFO: Pod "pod-configmaps-be565afd-6763-41a6-87f4-47dd70b72176" satisfied condition "success or failure"
Apr 29 23:02:51.274: INFO: Trying to get logs from node ip-192-168-83-225.us-west-2.compute.internal pod pod-configmaps-be565afd-6763-41a6-87f4-47dd70b72176 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 23:02:51.293: INFO: Waiting for pod pod-configmaps-be565afd-6763-41a6-87f4-47dd70b72176 to disappear
Apr 29 23:02:51.295: INFO: Pod pod-configmaps-be565afd-6763-41a6-87f4-47dd70b72176 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:02:51.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-388" for this suite.
Apr 29 23:02:57.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:02:57.395: INFO: namespace configmap-388 deletion completed in 6.096582463s

• [SLOW TEST:10.304 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:02:57.395: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8955
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:02:57.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 version'
Apr 29 23:02:57.586: INFO: stderr: ""
Apr 29 23:02:57.586: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.8\", GitCommit:\"ec6eb119b81be488b030e849b9e64fda4caaf33c\", GitTreeState:\"clean\", BuildDate:\"2020-03-12T21:00:06Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16+\", GitVersion:\"v1.16.8-eks-e16311\", GitCommit:\"e163110a04dcb2f39c3325af96d019b4925419eb\", GitTreeState:\"clean\", BuildDate:\"2020-03-27T22:37:12Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:02:57.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8955" for this suite.
Apr 29 23:03:03.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:03:03.691: INFO: namespace kubectl-8955 deletion completed in 6.101105571s

• [SLOW TEST:6.296 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:03:03.692: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-2927
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:03:03.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-2927" for this suite.
Apr 29 23:03:09.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:03:10.008: INFO: namespace tables-2927 deletion completed in 6.164427104s

• [SLOW TEST:6.316 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:03:10.008: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9534
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:03:10.162: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c0813179-5072-4b16-be2c-cda68928697c" in namespace "security-context-test-9534" to be "success or failure"
Apr 29 23:03:10.164: INFO: Pod "alpine-nnp-false-c0813179-5072-4b16-be2c-cda68928697c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.885053ms
Apr 29 23:03:12.167: INFO: Pod "alpine-nnp-false-c0813179-5072-4b16-be2c-cda68928697c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004730017s
Apr 29 23:03:14.170: INFO: Pod "alpine-nnp-false-c0813179-5072-4b16-be2c-cda68928697c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007711868s
Apr 29 23:03:14.170: INFO: Pod "alpine-nnp-false-c0813179-5072-4b16-be2c-cda68928697c" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:03:14.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9534" for this suite.
Apr 29 23:03:20.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:03:20.280: INFO: namespace security-context-test-9534 deletion completed in 6.092947991s

• [SLOW TEST:10.272 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:03:20.281: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr 29 23:03:20.640: INFO: Pod name wrapped-volume-race-d5c0b91b-49b9-4b01-bff5-c02a2e86b69c: Found 0 pods out of 5
Apr 29 23:03:25.645: INFO: Pod name wrapped-volume-race-d5c0b91b-49b9-4b01-bff5-c02a2e86b69c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d5c0b91b-49b9-4b01-bff5-c02a2e86b69c in namespace emptydir-wrapper-483, will wait for the garbage collector to delete the pods
Apr 29 23:03:37.719: INFO: Deleting ReplicationController wrapped-volume-race-d5c0b91b-49b9-4b01-bff5-c02a2e86b69c took: 6.545704ms
Apr 29 23:03:38.019: INFO: Terminating ReplicationController wrapped-volume-race-d5c0b91b-49b9-4b01-bff5-c02a2e86b69c pods took: 300.157518ms
STEP: Creating RC which spawns configmap-volume pods
Apr 29 23:04:13.570: INFO: Pod name wrapped-volume-race-cb606299-74d5-4aa0-98ed-4a129418d4bf: Found 0 pods out of 5
Apr 29 23:04:18.575: INFO: Pod name wrapped-volume-race-cb606299-74d5-4aa0-98ed-4a129418d4bf: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-cb606299-74d5-4aa0-98ed-4a129418d4bf in namespace emptydir-wrapper-483, will wait for the garbage collector to delete the pods
Apr 29 23:04:28.649: INFO: Deleting ReplicationController wrapped-volume-race-cb606299-74d5-4aa0-98ed-4a129418d4bf took: 6.48538ms
Apr 29 23:04:28.949: INFO: Terminating ReplicationController wrapped-volume-race-cb606299-74d5-4aa0-98ed-4a129418d4bf pods took: 300.151121ms
STEP: Creating RC which spawns configmap-volume pods
Apr 29 23:05:11.765: INFO: Pod name wrapped-volume-race-95997c8f-74b6-4b56-9387-594614586d9c: Found 0 pods out of 5
Apr 29 23:05:16.769: INFO: Pod name wrapped-volume-race-95997c8f-74b6-4b56-9387-594614586d9c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-95997c8f-74b6-4b56-9387-594614586d9c in namespace emptydir-wrapper-483, will wait for the garbage collector to delete the pods
Apr 29 23:05:28.847: INFO: Deleting ReplicationController wrapped-volume-race-95997c8f-74b6-4b56-9387-594614586d9c took: 6.568798ms
Apr 29 23:05:29.147: INFO: Terminating ReplicationController wrapped-volume-race-95997c8f-74b6-4b56-9387-594614586d9c pods took: 300.143728ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:06:12.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-483" for this suite.
Apr 29 23:06:20.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:06:20.899: INFO: namespace emptydir-wrapper-483 deletion completed in 8.091385981s

• [SLOW TEST:180.618 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:06:20.899: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1659
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Apr 29 23:06:25.059: INFO: Pod pod-hostip-c98b38ec-81d2-4d97-a5c3-5ac452634b90 has hostIP: 192.168.82.130
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:06:25.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1659" for this suite.
Apr 29 23:06:53.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:06:53.160: INFO: namespace pods-1659 deletion completed in 28.097554385s

• [SLOW TEST:32.262 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:06:53.161: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6459
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Apr 29 23:06:59.824: INFO: Successfully updated pod "adopt-release-4hvjv"
STEP: Checking that the Job readopts the Pod
Apr 29 23:06:59.824: INFO: Waiting up to 15m0s for pod "adopt-release-4hvjv" in namespace "job-6459" to be "adopted"
Apr 29 23:06:59.826: INFO: Pod "adopt-release-4hvjv": Phase="Running", Reason="", readiness=true. Elapsed: 1.896778ms
Apr 29 23:07:01.830: INFO: Pod "adopt-release-4hvjv": Phase="Running", Reason="", readiness=true. Elapsed: 2.006475643s
Apr 29 23:07:01.830: INFO: Pod "adopt-release-4hvjv" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Apr 29 23:07:02.338: INFO: Successfully updated pod "adopt-release-4hvjv"
STEP: Checking that the Job releases the Pod
Apr 29 23:07:02.338: INFO: Waiting up to 15m0s for pod "adopt-release-4hvjv" in namespace "job-6459" to be "released"
Apr 29 23:07:02.340: INFO: Pod "adopt-release-4hvjv": Phase="Running", Reason="", readiness=true. Elapsed: 1.968616ms
Apr 29 23:07:04.343: INFO: Pod "adopt-release-4hvjv": Phase="Running", Reason="", readiness=true. Elapsed: 2.004727065s
Apr 29 23:07:04.343: INFO: Pod "adopt-release-4hvjv" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:07:04.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6459" for this suite.
Apr 29 23:07:54.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:07:54.440: INFO: namespace job-6459 deletion completed in 50.093593432s

• [SLOW TEST:61.280 seconds]
[sig-apps] Job
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:07:54.440: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3687
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 29 23:07:54.597: INFO: Waiting up to 5m0s for pod "pod-da7eebab-efc0-47ef-89a0-b9e7f227809c" in namespace "emptydir-3687" to be "success or failure"
Apr 29 23:07:54.599: INFO: Pod "pod-da7eebab-efc0-47ef-89a0-b9e7f227809c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.931345ms
Apr 29 23:07:56.602: INFO: Pod "pod-da7eebab-efc0-47ef-89a0-b9e7f227809c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004662309s
Apr 29 23:07:58.605: INFO: Pod "pod-da7eebab-efc0-47ef-89a0-b9e7f227809c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007499949s
STEP: Saw pod success
Apr 29 23:07:58.605: INFO: Pod "pod-da7eebab-efc0-47ef-89a0-b9e7f227809c" satisfied condition "success or failure"
Apr 29 23:07:58.607: INFO: Trying to get logs from node ip-192-168-144-27.us-west-2.compute.internal pod pod-da7eebab-efc0-47ef-89a0-b9e7f227809c container test-container: <nil>
STEP: delete the pod
Apr 29 23:07:58.629: INFO: Waiting for pod pod-da7eebab-efc0-47ef-89a0-b9e7f227809c to disappear
Apr 29 23:07:58.631: INFO: Pod pod-da7eebab-efc0-47ef-89a0-b9e7f227809c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:07:58.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3687" for this suite.
Apr 29 23:08:04.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:08:04.728: INFO: namespace emptydir-3687 deletion completed in 6.092911665s

• [SLOW TEST:10.287 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:08:04.728: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4253
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Apr 29 23:08:04.873: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 29 23:08:04.883: INFO: Waiting for terminating namespaces to be deleted...
Apr 29 23:08:04.886: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-144-27.us-west-2.compute.internal before test
Apr 29 23:08:04.891: INFO: aws-node-rccgv from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.891: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:08:04.891: INFO: kube-proxy-4xrfl from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.891: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:08:04.891: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-2ffxh from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:08:04.891: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:08:04.891: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:08:04.891: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-148-117.us-west-2.compute.internal before test
Apr 29 23:08:04.901: INFO: aws-node-wzq6b from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.901: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:08:04.901: INFO: kube-proxy-b4plm from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.901: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:08:04.901: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-69sgr from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:08:04.901: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:08:04.901: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:08:04.901: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-149-254.us-west-2.compute.internal before test
Apr 29 23:08:04.912: INFO: sonobuoy from sonobuoy started at 2020-04-29 23:02:27 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.912: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 29 23:08:04.912: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-xtlz4 from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:08:04.912: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:08:04.912: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:08:04.912: INFO: aws-node-kcbxd from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.912: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:08:04.912: INFO: kube-proxy-jvnfx from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.912: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:08:04.912: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-197-211.us-west-2.compute.internal before test
Apr 29 23:08:04.920: INFO: kube-proxy-xhpsv from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.920: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:08:04.920: INFO: aws-node-ffvhf from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.920: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:08:04.920: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-wlv6l from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:08:04.920: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:08:04.920: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:08:04.920: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-211-77.us-west-2.compute.internal before test
Apr 29 23:08:04.928: INFO: aws-node-xj5sd from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.928: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:08:04.928: INFO: kube-proxy-fck6k from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.928: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:08:04.928: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-xpbv9 from sonobuoy started at 2020-04-29 23:02:31 +0000 UTC (2 container statuses recorded)
Apr 29 23:08:04.928: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:08:04.928: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:08:04.928: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-218-192.us-west-2.compute.internal before test
Apr 29 23:08:04.936: INFO: kube-proxy-b6znl from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.936: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:08:04.936: INFO: sonobuoy-e2e-job-c0680a6adde246b1 from sonobuoy started at 2020-04-29 23:02:31 +0000 UTC (2 container statuses recorded)
Apr 29 23:08:04.936: INFO: 	Container e2e ready: true, restart count 0
Apr 29 23:08:04.936: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:08:04.936: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-8cg7t from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:08:04.936: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:08:04.936: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:08:04.936: INFO: aws-node-s9vwd from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.936: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:08:04.936: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-222-85.us-west-2.compute.internal before test
Apr 29 23:08:04.944: INFO: aws-node-l9blv from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.944: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:08:04.944: INFO: kube-proxy-fh79j from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.944: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:08:04.944: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-qh6r9 from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:08:04.944: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:08:04.944: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:08:04.944: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-72-199.us-west-2.compute.internal before test
Apr 29 23:08:04.954: INFO: aws-node-zldl6 from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.954: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:08:04.954: INFO: coredns-5c97f79574-9t82l from kube-system started at 2020-04-29 22:50:59 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.954: INFO: 	Container coredns ready: true, restart count 0
Apr 29 23:08:04.954: INFO: coredns-5c97f79574-wjwsw from kube-system started at 2020-04-29 22:50:59 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.954: INFO: 	Container coredns ready: true, restart count 0
Apr 29 23:08:04.954: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-sg2vq from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:08:04.954: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:08:04.954: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:08:04.954: INFO: kube-proxy-kklzp from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.954: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:08:04.954: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-82-130.us-west-2.compute.internal before test
Apr 29 23:08:04.962: INFO: kube-proxy-8pccx from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.962: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:08:04.962: INFO: aws-node-rbjxk from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.962: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:08:04.962: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-ptxhd from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:08:04.962: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:08:04.962: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:08:04.962: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-83-225.us-west-2.compute.internal before test
Apr 29 23:08:04.971: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-rmfrj from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:08:04.971: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:08:04.971: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:08:04.971: INFO: kube-proxy-sz6rb from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.971: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:08:04.971: INFO: aws-node-j8bsf from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:08:04.971: INFO: 	Container aws-node ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node ip-192-168-144-27.us-west-2.compute.internal
STEP: verifying the node has the label node ip-192-168-148-117.us-west-2.compute.internal
STEP: verifying the node has the label node ip-192-168-149-254.us-west-2.compute.internal
STEP: verifying the node has the label node ip-192-168-197-211.us-west-2.compute.internal
STEP: verifying the node has the label node ip-192-168-211-77.us-west-2.compute.internal
STEP: verifying the node has the label node ip-192-168-218-192.us-west-2.compute.internal
STEP: verifying the node has the label node ip-192-168-222-85.us-west-2.compute.internal
STEP: verifying the node has the label node ip-192-168-72-199.us-west-2.compute.internal
STEP: verifying the node has the label node ip-192-168-82-130.us-west-2.compute.internal
STEP: verifying the node has the label node ip-192-168-83-225.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod aws-node-ffvhf requesting resource cpu=10m on Node ip-192-168-197-211.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod aws-node-j8bsf requesting resource cpu=10m on Node ip-192-168-83-225.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod aws-node-kcbxd requesting resource cpu=10m on Node ip-192-168-149-254.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod aws-node-l9blv requesting resource cpu=10m on Node ip-192-168-222-85.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod aws-node-rbjxk requesting resource cpu=10m on Node ip-192-168-82-130.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod aws-node-rccgv requesting resource cpu=10m on Node ip-192-168-144-27.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod aws-node-s9vwd requesting resource cpu=10m on Node ip-192-168-218-192.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod aws-node-wzq6b requesting resource cpu=10m on Node ip-192-168-148-117.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod aws-node-xj5sd requesting resource cpu=10m on Node ip-192-168-211-77.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod aws-node-zldl6 requesting resource cpu=10m on Node ip-192-168-72-199.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod coredns-5c97f79574-9t82l requesting resource cpu=100m on Node ip-192-168-72-199.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod coredns-5c97f79574-wjwsw requesting resource cpu=100m on Node ip-192-168-72-199.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod kube-proxy-4xrfl requesting resource cpu=100m on Node ip-192-168-144-27.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod kube-proxy-8pccx requesting resource cpu=100m on Node ip-192-168-82-130.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod kube-proxy-b4plm requesting resource cpu=100m on Node ip-192-168-148-117.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod kube-proxy-b6znl requesting resource cpu=100m on Node ip-192-168-218-192.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod kube-proxy-fck6k requesting resource cpu=100m on Node ip-192-168-211-77.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod kube-proxy-fh79j requesting resource cpu=100m on Node ip-192-168-222-85.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod kube-proxy-jvnfx requesting resource cpu=100m on Node ip-192-168-149-254.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod kube-proxy-kklzp requesting resource cpu=100m on Node ip-192-168-72-199.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod kube-proxy-sz6rb requesting resource cpu=100m on Node ip-192-168-83-225.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod kube-proxy-xhpsv requesting resource cpu=100m on Node ip-192-168-197-211.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-192-168-149-254.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod sonobuoy-e2e-job-c0680a6adde246b1 requesting resource cpu=0m on Node ip-192-168-218-192.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-2ffxh requesting resource cpu=0m on Node ip-192-168-144-27.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-69sgr requesting resource cpu=0m on Node ip-192-168-148-117.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-8cg7t requesting resource cpu=0m on Node ip-192-168-218-192.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-ptxhd requesting resource cpu=0m on Node ip-192-168-82-130.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-qh6r9 requesting resource cpu=0m on Node ip-192-168-222-85.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-rmfrj requesting resource cpu=0m on Node ip-192-168-83-225.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-sg2vq requesting resource cpu=0m on Node ip-192-168-72-199.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-wlv6l requesting resource cpu=0m on Node ip-192-168-197-211.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-xpbv9 requesting resource cpu=0m on Node ip-192-168-211-77.us-west-2.compute.internal
Apr 29 23:08:05.061: INFO: Pod sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-xtlz4 requesting resource cpu=0m on Node ip-192-168-149-254.us-west-2.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
Apr 29 23:08:05.061: INFO: Creating a pod which consumes cpu=2667m on Node ip-192-168-82-130.us-west-2.compute.internal
Apr 29 23:08:05.069: INFO: Creating a pod which consumes cpu=2667m on Node ip-192-168-149-254.us-west-2.compute.internal
Apr 29 23:08:05.073: INFO: Creating a pod which consumes cpu=2667m on Node ip-192-168-197-211.us-west-2.compute.internal
Apr 29 23:08:05.078: INFO: Creating a pod which consumes cpu=2667m on Node ip-192-168-218-192.us-west-2.compute.internal
Apr 29 23:08:05.082: INFO: Creating a pod which consumes cpu=2527m on Node ip-192-168-72-199.us-west-2.compute.internal
Apr 29 23:08:05.087: INFO: Creating a pod which consumes cpu=2667m on Node ip-192-168-83-225.us-west-2.compute.internal
Apr 29 23:08:05.094: INFO: Creating a pod which consumes cpu=2667m on Node ip-192-168-144-27.us-west-2.compute.internal
Apr 29 23:08:05.100: INFO: Creating a pod which consumes cpu=2667m on Node ip-192-168-148-117.us-west-2.compute.internal
Apr 29 23:08:05.104: INFO: Creating a pod which consumes cpu=2667m on Node ip-192-168-211-77.us-west-2.compute.internal
Apr 29 23:08:05.110: INFO: Creating a pod which consumes cpu=2667m on Node ip-192-168-222-85.us-west-2.compute.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-06f126bb-25a8-4c9c-a541-798ad0a98974.160a6d0496e6c570], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4253/filler-pod-06f126bb-25a8-4c9c-a541-798ad0a98974 to ip-192-168-149-254.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-06f126bb-25a8-4c9c-a541-798ad0a98974.160a6d04c9afe747], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-06f126bb-25a8-4c9c-a541-798ad0a98974.160a6d052ff8f56b], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-06f126bb-25a8-4c9c-a541-798ad0a98974.160a6d05346ff751], Reason = [Created], Message = [Created container filler-pod-06f126bb-25a8-4c9c-a541-798ad0a98974]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-06f126bb-25a8-4c9c-a541-798ad0a98974.160a6d053aa9adc6], Reason = [Started], Message = [Started container filler-pod-06f126bb-25a8-4c9c-a541-798ad0a98974]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-123f78ec-f726-42fc-abcc-f533b84b110a.160a6d049796059f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4253/filler-pod-123f78ec-f726-42fc-abcc-f533b84b110a to ip-192-168-218-192.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-123f78ec-f726-42fc-abcc-f533b84b110a.160a6d04caa3935e], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-123f78ec-f726-42fc-abcc-f533b84b110a.160a6d04f739010c], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-123f78ec-f726-42fc-abcc-f533b84b110a.160a6d04faa2f61e], Reason = [Created], Message = [Created container filler-pod-123f78ec-f726-42fc-abcc-f533b84b110a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-123f78ec-f726-42fc-abcc-f533b84b110a.160a6d0501ac6c5f], Reason = [Started], Message = [Started container filler-pod-123f78ec-f726-42fc-abcc-f533b84b110a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-51d0892c-355e-4fb3-84ae-355451494ae7.160a6d0499564d82], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4253/filler-pod-51d0892c-355e-4fb3-84ae-355451494ae7 to ip-192-168-148-117.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-51d0892c-355e-4fb3-84ae-355451494ae7.160a6d04cb039641], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-51d0892c-355e-4fb3-84ae-355451494ae7.160a6d05323bbaf8], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-51d0892c-355e-4fb3-84ae-355451494ae7.160a6d053625b4dc], Reason = [Created], Message = [Created container filler-pod-51d0892c-355e-4fb3-84ae-355451494ae7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-51d0892c-355e-4fb3-84ae-355451494ae7.160a6d053d6be427], Reason = [Started], Message = [Started container filler-pod-51d0892c-355e-4fb3-84ae-355451494ae7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-59ac1fba-3e11-436f-8eca-781aa46ab048.160a6d04975429c3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4253/filler-pod-59ac1fba-3e11-436f-8eca-781aa46ab048 to ip-192-168-197-211.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-59ac1fba-3e11-436f-8eca-781aa46ab048.160a6d04c9614111], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-59ac1fba-3e11-436f-8eca-781aa46ab048.160a6d052ec1c601], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-59ac1fba-3e11-436f-8eca-781aa46ab048.160a6d0532d6e537], Reason = [Created], Message = [Created container filler-pod-59ac1fba-3e11-436f-8eca-781aa46ab048]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-59ac1fba-3e11-436f-8eca-781aa46ab048.160a6d053a1cc157], Reason = [Started], Message = [Started container filler-pod-59ac1fba-3e11-436f-8eca-781aa46ab048]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-775844ea-b429-4d92-abe4-a0bdc44798f9.160a6d04983ec3db], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4253/filler-pod-775844ea-b429-4d92-abe4-a0bdc44798f9 to ip-192-168-83-225.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-775844ea-b429-4d92-abe4-a0bdc44798f9.160a6d04cc7d9075], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-775844ea-b429-4d92-abe4-a0bdc44798f9.160a6d052980cb8c], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-775844ea-b429-4d92-abe4-a0bdc44798f9.160a6d052d864f82], Reason = [Created], Message = [Created container filler-pod-775844ea-b429-4d92-abe4-a0bdc44798f9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-775844ea-b429-4d92-abe4-a0bdc44798f9.160a6d05351c9c56], Reason = [Started], Message = [Started container filler-pod-775844ea-b429-4d92-abe4-a0bdc44798f9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e062b1d-de0d-4d48-8fe8-394fdb992821.160a6d0496b1f507], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4253/filler-pod-7e062b1d-de0d-4d48-8fe8-394fdb992821 to ip-192-168-82-130.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e062b1d-de0d-4d48-8fe8-394fdb992821.160a6d04cc5ea8e9], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e062b1d-de0d-4d48-8fe8-394fdb992821.160a6d04d078d91e], Reason = [Created], Message = [Created container filler-pod-7e062b1d-de0d-4d48-8fe8-394fdb992821]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7e062b1d-de0d-4d48-8fe8-394fdb992821.160a6d04d710f4f4], Reason = [Started], Message = [Started container filler-pod-7e062b1d-de0d-4d48-8fe8-394fdb992821]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b636b55e-2610-4c35-b525-cb6864e20052.160a6d0499207e6b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4253/filler-pod-b636b55e-2610-4c35-b525-cb6864e20052 to ip-192-168-144-27.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b636b55e-2610-4c35-b525-cb6864e20052.160a6d04cc3f2218], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b636b55e-2610-4c35-b525-cb6864e20052.160a6d04f72d6091], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b636b55e-2610-4c35-b525-cb6864e20052.160a6d04fad3078a], Reason = [Created], Message = [Created container filler-pod-b636b55e-2610-4c35-b525-cb6864e20052]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b636b55e-2610-4c35-b525-cb6864e20052.160a6d05021e86fd], Reason = [Started], Message = [Started container filler-pod-b636b55e-2610-4c35-b525-cb6864e20052]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e18b0cde-1127-453a-b68b-fabd5f37e8bd.160a6d0497ef9510], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4253/filler-pod-e18b0cde-1127-453a-b68b-fabd5f37e8bd to ip-192-168-72-199.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e18b0cde-1127-453a-b68b-fabd5f37e8bd.160a6d04ce9e045c], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e18b0cde-1127-453a-b68b-fabd5f37e8bd.160a6d052f12cba9], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e18b0cde-1127-453a-b68b-fabd5f37e8bd.160a6d05341abd20], Reason = [Created], Message = [Created container filler-pod-e18b0cde-1127-453a-b68b-fabd5f37e8bd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e18b0cde-1127-453a-b68b-fabd5f37e8bd.160a6d053c282845], Reason = [Started], Message = [Started container filler-pod-e18b0cde-1127-453a-b68b-fabd5f37e8bd]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecaeb1d0-2e70-4753-8b94-88f2857a51af.160a6d04996526c1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4253/filler-pod-ecaeb1d0-2e70-4753-8b94-88f2857a51af to ip-192-168-211-77.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecaeb1d0-2e70-4753-8b94-88f2857a51af.160a6d04d8150976], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecaeb1d0-2e70-4753-8b94-88f2857a51af.160a6d053854b9d1], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecaeb1d0-2e70-4753-8b94-88f2857a51af.160a6d053ceeb7a3], Reason = [Created], Message = [Created container filler-pod-ecaeb1d0-2e70-4753-8b94-88f2857a51af]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecaeb1d0-2e70-4753-8b94-88f2857a51af.160a6d054476eb27], Reason = [Started], Message = [Started container filler-pod-ecaeb1d0-2e70-4753-8b94-88f2857a51af]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f6496344-105a-4122-a22d-067cab956753.160a6d0499757552], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4253/filler-pod-f6496344-105a-4122-a22d-067cab956753 to ip-192-168-222-85.us-west-2.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f6496344-105a-4122-a22d-067cab956753.160a6d04cc456c2e], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f6496344-105a-4122-a22d-067cab956753.160a6d0535360391], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f6496344-105a-4122-a22d-067cab956753.160a6d053abec70f], Reason = [Created], Message = [Created container filler-pod-f6496344-105a-4122-a22d-067cab956753]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f6496344-105a-4122-a22d-067cab956753.160a6d0541d9f70e], Reason = [Started], Message = [Started container filler-pod-f6496344-105a-4122-a22d-067cab956753]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.160a6d0589b556f2], Reason = [FailedScheduling], Message = [0/10 nodes are available: 10 Insufficient cpu.]
STEP: removing the label node off the node ip-192-168-144-27.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-192-168-148-117.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-192-168-211-77.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-192-168-222-85.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-192-168-83-225.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-192-168-149-254.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-192-168-197-211.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-192-168-218-192.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-192-168-72-199.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-192-168-82-130.us-west-2.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:08:10.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4253" for this suite.
Apr 29 23:08:16.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:08:16.391: INFO: namespace sched-pred-4253 deletion completed in 6.139893623s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:11.663 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:08:16.391: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4999
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr 29 23:08:16.542: INFO: Waiting up to 5m0s for pod "pod-9baba04f-c268-465c-b506-8a063d182bcb" in namespace "emptydir-4999" to be "success or failure"
Apr 29 23:08:16.544: INFO: Pod "pod-9baba04f-c268-465c-b506-8a063d182bcb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.981047ms
Apr 29 23:08:18.548: INFO: Pod "pod-9baba04f-c268-465c-b506-8a063d182bcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005392558s
STEP: Saw pod success
Apr 29 23:08:18.548: INFO: Pod "pod-9baba04f-c268-465c-b506-8a063d182bcb" satisfied condition "success or failure"
Apr 29 23:08:18.550: INFO: Trying to get logs from node ip-192-168-144-27.us-west-2.compute.internal pod pod-9baba04f-c268-465c-b506-8a063d182bcb container test-container: <nil>
STEP: delete the pod
Apr 29 23:08:18.567: INFO: Waiting for pod pod-9baba04f-c268-465c-b506-8a063d182bcb to disappear
Apr 29 23:08:18.568: INFO: Pod pod-9baba04f-c268-465c-b506-8a063d182bcb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:08:18.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4999" for this suite.
Apr 29 23:08:24.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:08:24.664: INFO: namespace emptydir-4999 deletion completed in 6.092742635s

• [SLOW TEST:8.273 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:08:24.665: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7206
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:08:28.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7206" for this suite.
Apr 29 23:09:12.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:09:12.927: INFO: namespace kubelet-test-7206 deletion completed in 44.093915858s

• [SLOW TEST:48.262 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:09:12.927: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7172
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-343e65d4-9ff8-45ac-86ad-a92209f7b176
STEP: Creating secret with name s-test-opt-upd-c66bfa5d-0a9f-44de-b83a-b3430d9ebe2f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-343e65d4-9ff8-45ac-86ad-a92209f7b176
STEP: Updating secret s-test-opt-upd-c66bfa5d-0a9f-44de-b83a-b3430d9ebe2f
STEP: Creating secret with name s-test-opt-create-bacd9c9f-9dd4-476f-92e4-e59eea75a799
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:09:21.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7172" for this suite.
Apr 29 23:09:33.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:09:33.303: INFO: namespace projected-7172 deletion completed in 12.093353817s

• [SLOW TEST:20.376 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:09:33.303: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5630
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5630.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-5630.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5630.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5630.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5630.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-5630.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5630.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-5630.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5630.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5630.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-5630.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-5630.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-5630.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-5630.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-5630.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-5630.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-5630.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5630.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 23:09:45.480: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-5630.svc.cluster.local from pod dns-5630/dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434: the server could not find the requested resource (get pods dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434)
Apr 29 23:09:45.483: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5630.svc.cluster.local from pod dns-5630/dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434: the server could not find the requested resource (get pods dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434)
Apr 29 23:09:45.485: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-5630.svc.cluster.local from pod dns-5630/dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434: the server could not find the requested resource (get pods dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434)
Apr 29 23:09:45.488: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-5630.svc.cluster.local from pod dns-5630/dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434: the server could not find the requested resource (get pods dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434)
Apr 29 23:09:45.495: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-5630.svc.cluster.local from pod dns-5630/dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434: the server could not find the requested resource (get pods dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434)
Apr 29 23:09:45.497: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-5630.svc.cluster.local from pod dns-5630/dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434: the server could not find the requested resource (get pods dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434)
Apr 29 23:09:45.499: INFO: Unable to read jessie_udp@dns-test-service-2.dns-5630.svc.cluster.local from pod dns-5630/dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434: the server could not find the requested resource (get pods dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434)
Apr 29 23:09:45.501: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-5630.svc.cluster.local from pod dns-5630/dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434: the server could not find the requested resource (get pods dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434)
Apr 29 23:09:45.507: INFO: Lookups using dns-5630/dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-5630.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-5630.svc.cluster.local wheezy_udp@dns-test-service-2.dns-5630.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-5630.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-5630.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-5630.svc.cluster.local jessie_udp@dns-test-service-2.dns-5630.svc.cluster.local jessie_tcp@dns-test-service-2.dns-5630.svc.cluster.local]

Apr 29 23:09:50.537: INFO: DNS probes using dns-5630/dns-test-d7cdc590-adea-4a20-8a37-63e7eaa3c434 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:09:50.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5630" for this suite.
Apr 29 23:09:56.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:09:56.668: INFO: namespace dns-5630 deletion completed in 6.094957724s

• [SLOW TEST:23.365 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:09:56.668: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-6527
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 29 23:09:56.811: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 29 23:10:22.928: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.92.178:8080/dial?request=hostName&protocol=http&host=192.168.136.176&port=8080&tries=1'] Namespace:pod-network-test-6527 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:10:22.928: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:10:23.026: INFO: Waiting for endpoints: map[]
Apr 29 23:10:23.029: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.92.178:8080/dial?request=hostName&protocol=http&host=192.168.216.250&port=8080&tries=1'] Namespace:pod-network-test-6527 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:10:23.029: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:10:23.121: INFO: Waiting for endpoints: map[]
Apr 29 23:10:23.123: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.92.178:8080/dial?request=hostName&protocol=http&host=192.168.211.104&port=8080&tries=1'] Namespace:pod-network-test-6527 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:10:23.123: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:10:23.213: INFO: Waiting for endpoints: map[]
Apr 29 23:10:23.215: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.92.178:8080/dial?request=hostName&protocol=http&host=192.168.144.175&port=8080&tries=1'] Namespace:pod-network-test-6527 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:10:23.215: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:10:23.301: INFO: Waiting for endpoints: map[]
Apr 29 23:10:23.304: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.92.178:8080/dial?request=hostName&protocol=http&host=192.168.131.165&port=8080&tries=1'] Namespace:pod-network-test-6527 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:10:23.304: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:10:23.389: INFO: Waiting for endpoints: map[]
Apr 29 23:10:23.392: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.92.178:8080/dial?request=hostName&protocol=http&host=192.168.86.98&port=8080&tries=1'] Namespace:pod-network-test-6527 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:10:23.392: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:10:23.477: INFO: Waiting for endpoints: map[]
Apr 29 23:10:23.479: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.92.178:8080/dial?request=hostName&protocol=http&host=192.168.73.40&port=8080&tries=1'] Namespace:pod-network-test-6527 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:10:23.479: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:10:23.577: INFO: Waiting for endpoints: map[]
Apr 29 23:10:23.579: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.92.178:8080/dial?request=hostName&protocol=http&host=192.168.67.134&port=8080&tries=1'] Namespace:pod-network-test-6527 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:10:23.579: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:10:23.673: INFO: Waiting for endpoints: map[]
Apr 29 23:10:23.675: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.92.178:8080/dial?request=hostName&protocol=http&host=192.168.199.248&port=8080&tries=1'] Namespace:pod-network-test-6527 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:10:23.675: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:10:23.760: INFO: Waiting for endpoints: map[]
Apr 29 23:10:23.762: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.92.178:8080/dial?request=hostName&protocol=http&host=192.168.201.122&port=8080&tries=1'] Namespace:pod-network-test-6527 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:10:23.762: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:10:23.869: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:10:23.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6527" for this suite.
Apr 29 23:10:35.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:10:35.967: INFO: namespace pod-network-test-6527 deletion completed in 12.094387609s

• [SLOW TEST:39.298 seconds]
[sig-network] Networking
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:10:35.967: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7014
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Apr 29 23:10:36.110: INFO: PodSpec: initContainers in spec.initContainers
Apr 29 23:11:21.215: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b2b2ad59-df1b-4359-9241-c2476e421164", GenerateName:"", Namespace:"init-container-7014", SelfLink:"/api/v1/namespaces/init-container-7014/pods/pod-init-b2b2ad59-df1b-4359-9241-c2476e421164", UID:"d13f8932-8e83-4f15-987b-e8e953e567a2", ResourceVersion:"6678", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63723798636, loc:(*time.Location)(0x789e8e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"110316704"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-brls9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002a4b600), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-brls9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-brls9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-brls9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002f76fa8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-192-168-148-117.us-west-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0024614a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002f77020)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002f77040)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002f77048), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002f7704c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723798636, loc:(*time.Location)(0x789e8e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723798636, loc:(*time.Location)(0x789e8e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723798636, loc:(*time.Location)(0x789e8e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723798636, loc:(*time.Location)(0x789e8e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.148.117", PodIP:"192.168.147.152", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.147.152"}}, StartTime:(*v1.Time)(0xc002cf8a80), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0027ea460)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0027ea4d0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://08c4faf9cef1dd84ff2c4edda49c416275be97f6868b4831632510c7db4c25ec", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002cf8ae0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002cf8ac0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc002f770c4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:11:21.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7014" for this suite.
Apr 29 23:11:49.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:11:49.319: INFO: namespace init-container-7014 deletion completed in 28.093884698s

• [SLOW TEST:73.353 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:11:49.320: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9508
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-cb95b290-6ba9-4722-bdce-fd4aab356aa2
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:11:51.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9508" for this suite.
Apr 29 23:12:03.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:12:03.615: INFO: namespace configmap-9508 deletion completed in 12.10101854s

• [SLOW TEST:14.295 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:12:03.615: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9388
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:12:03.759: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:12:04.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9388" for this suite.
Apr 29 23:12:10.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:12:10.962: INFO: namespace custom-resource-definition-9388 deletion completed in 6.173013786s

• [SLOW TEST:7.347 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:12:10.962: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-355
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 29 23:12:11.134: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f376060-0ce9-45e7-98d0-e99e032364a2" in namespace "projected-355" to be "success or failure"
Apr 29 23:12:11.137: INFO: Pod "downwardapi-volume-3f376060-0ce9-45e7-98d0-e99e032364a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.757018ms
Apr 29 23:12:13.140: INFO: Pod "downwardapi-volume-3f376060-0ce9-45e7-98d0-e99e032364a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005633352s
Apr 29 23:12:15.143: INFO: Pod "downwardapi-volume-3f376060-0ce9-45e7-98d0-e99e032364a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008261625s
STEP: Saw pod success
Apr 29 23:12:15.143: INFO: Pod "downwardapi-volume-3f376060-0ce9-45e7-98d0-e99e032364a2" satisfied condition "success or failure"
Apr 29 23:12:15.145: INFO: Trying to get logs from node ip-192-168-222-85.us-west-2.compute.internal pod downwardapi-volume-3f376060-0ce9-45e7-98d0-e99e032364a2 container client-container: <nil>
STEP: delete the pod
Apr 29 23:12:15.162: INFO: Waiting for pod downwardapi-volume-3f376060-0ce9-45e7-98d0-e99e032364a2 to disappear
Apr 29 23:12:15.164: INFO: Pod downwardapi-volume-3f376060-0ce9-45e7-98d0-e99e032364a2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:12:15.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-355" for this suite.
Apr 29 23:12:21.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:12:21.259: INFO: namespace projected-355 deletion completed in 6.092316758s

• [SLOW TEST:10.297 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:12:21.260: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8181
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 23:12:22.169: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 29 23:12:24.176: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723798742, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723798742, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723798742, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723798742, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 23:12:27.192: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
Apr 29 23:12:27.217: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:12:27.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8181" for this suite.
Apr 29 23:12:33.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:12:33.459: INFO: namespace webhook-8181 deletion completed in 6.09237663s
STEP: Destroying namespace "webhook-8181-markers" for this suite.
Apr 29 23:12:39.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:12:39.551: INFO: namespace webhook-8181-markers deletion completed in 6.091979226s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.309 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:12:39.569: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3573
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr 29 23:12:39.723: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3573 /api/v1/namespaces/watch-3573/configmaps/e2e-watch-test-watch-closed 96a03b0a-cbce-44b2-864a-7eb0738d60a8 7059 0 2020-04-29 23:12:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 29 23:12:39.723: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3573 /api/v1/namespaces/watch-3573/configmaps/e2e-watch-test-watch-closed 96a03b0a-cbce-44b2-864a-7eb0738d60a8 7060 0 2020-04-29 23:12:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr 29 23:12:39.740: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3573 /api/v1/namespaces/watch-3573/configmaps/e2e-watch-test-watch-closed 96a03b0a-cbce-44b2-864a-7eb0738d60a8 7061 0 2020-04-29 23:12:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 29 23:12:39.740: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3573 /api/v1/namespaces/watch-3573/configmaps/e2e-watch-test-watch-closed 96a03b0a-cbce-44b2-864a-7eb0738d60a8 7062 0 2020-04-29 23:12:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:12:39.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3573" for this suite.
Apr 29 23:12:45.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:12:45.839: INFO: namespace watch-3573 deletion completed in 6.094846606s

• [SLOW TEST:6.270 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:12:45.839: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6205
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 29 23:12:45.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6205'
Apr 29 23:12:46.118: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 29 23:12:46.118: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Apr 29 23:12:48.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete deployment e2e-test-httpd-deployment --namespace=kubectl-6205'
Apr 29 23:12:48.188: INFO: stderr: ""
Apr 29 23:12:48.188: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:12:48.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6205" for this suite.
Apr 29 23:12:54.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:12:54.289: INFO: namespace kubectl-6205 deletion completed in 6.096141942s

• [SLOW TEST:8.450 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:12:54.289: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-526
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Apr 29 23:12:56.955: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-526 pod-service-account-a18a98d0-de56-463c-bb99-4718ea386193 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Apr 29 23:12:57.106: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-526 pod-service-account-a18a98d0-de56-463c-bb99-4718ea386193 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Apr 29 23:12:57.250: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-526 pod-service-account-a18a98d0-de56-463c-bb99-4718ea386193 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:12:57.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-526" for this suite.
Apr 29 23:13:03.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:13:03.492: INFO: namespace svcaccounts-526 deletion completed in 6.100887595s

• [SLOW TEST:9.202 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:13:03.492: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2583
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Apr 29 23:13:03.667: INFO: Found 0 stateful pods, waiting for 3
Apr 29 23:13:13.670: INFO: Found 2 stateful pods, waiting for 3
Apr 29 23:13:23.670: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 23:13:23.670: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 23:13:23.670: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Apr 29 23:13:33.670: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 23:13:33.670: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 23:13:33.670: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Apr 29 23:13:33.692: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr 29 23:13:43.718: INFO: Updating stateful set ss2
Apr 29 23:13:43.722: INFO: Waiting for Pod statefulset-2583/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Apr 29 23:13:53.748: INFO: Found 2 stateful pods, waiting for 3
Apr 29 23:14:03.751: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 23:14:03.751: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 23:14:03.751: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Apr 29 23:14:13.751: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 23:14:13.751: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 23:14:13.751: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr 29 23:14:13.772: INFO: Updating stateful set ss2
Apr 29 23:14:13.776: INFO: Waiting for Pod statefulset-2583/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 29 23:14:23.796: INFO: Updating stateful set ss2
Apr 29 23:14:23.800: INFO: Waiting for StatefulSet statefulset-2583/ss2 to complete update
Apr 29 23:14:23.800: INFO: Waiting for Pod statefulset-2583/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Apr 29 23:14:33.805: INFO: Deleting all statefulset in ns statefulset-2583
Apr 29 23:14:33.807: INFO: Scaling statefulset ss2 to 0
Apr 29 23:14:53.818: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 23:14:53.820: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:14:53.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2583" for this suite.
Apr 29 23:14:59.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:14:59.926: INFO: namespace statefulset-2583 deletion completed in 6.092051445s

• [SLOW TEST:116.434 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:14:59.926: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 29 23:15:00.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-6685'
Apr 29 23:15:00.131: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 29 23:15:00.131: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Apr 29 23:15:00.134: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Apr 29 23:15:00.143: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Apr 29 23:15:00.150: INFO: scanned /root for discovery docs: <nil>
Apr 29 23:15:00.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-6685'
Apr 29 23:15:15.874: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr 29 23:15:15.874: INFO: stdout: "Created e2e-test-httpd-rc-dc2860d16404108f5efd9ef24f48fddc\nScaling up e2e-test-httpd-rc-dc2860d16404108f5efd9ef24f48fddc from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-dc2860d16404108f5efd9ef24f48fddc up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-dc2860d16404108f5efd9ef24f48fddc to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Apr 29 23:15:15.874: INFO: stdout: "Created e2e-test-httpd-rc-dc2860d16404108f5efd9ef24f48fddc\nScaling up e2e-test-httpd-rc-dc2860d16404108f5efd9ef24f48fddc from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-dc2860d16404108f5efd9ef24f48fddc up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-dc2860d16404108f5efd9ef24f48fddc to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Apr 29 23:15:15.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-6685'
Apr 29 23:15:15.925: INFO: stderr: ""
Apr 29 23:15:15.925: INFO: stdout: "e2e-test-httpd-rc-dc2860d16404108f5efd9ef24f48fddc-dfrrb "
Apr 29 23:15:15.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods e2e-test-httpd-rc-dc2860d16404108f5efd9ef24f48fddc-dfrrb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6685'
Apr 29 23:15:15.972: INFO: stderr: ""
Apr 29 23:15:15.972: INFO: stdout: "true"
Apr 29 23:15:15.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods e2e-test-httpd-rc-dc2860d16404108f5efd9ef24f48fddc-dfrrb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6685'
Apr 29 23:15:16.022: INFO: stderr: ""
Apr 29 23:15:16.022: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Apr 29 23:15:16.022: INFO: e2e-test-httpd-rc-dc2860d16404108f5efd9ef24f48fddc-dfrrb is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Apr 29 23:15:16.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete rc e2e-test-httpd-rc --namespace=kubectl-6685'
Apr 29 23:15:16.085: INFO: stderr: ""
Apr 29 23:15:16.085: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:15:16.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6685" for this suite.
Apr 29 23:15:22.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:15:22.182: INFO: namespace kubectl-6685 deletion completed in 6.093282786s

• [SLOW TEST:22.256 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:15:22.182: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7956
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 29 23:15:28.856: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2fc04ab1-5fe4-432c-a381-af807cd425fe"
Apr 29 23:15:28.856: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2fc04ab1-5fe4-432c-a381-af807cd425fe" in namespace "pods-7956" to be "terminated due to deadline exceeded"
Apr 29 23:15:28.858: INFO: Pod "pod-update-activedeadlineseconds-2fc04ab1-5fe4-432c-a381-af807cd425fe": Phase="Running", Reason="", readiness=true. Elapsed: 2.197423ms
Apr 29 23:15:30.861: INFO: Pod "pod-update-activedeadlineseconds-2fc04ab1-5fe4-432c-a381-af807cd425fe": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.00502992s
Apr 29 23:15:30.861: INFO: Pod "pod-update-activedeadlineseconds-2fc04ab1-5fe4-432c-a381-af807cd425fe" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:15:30.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7956" for this suite.
Apr 29 23:15:36.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:15:36.959: INFO: namespace pods-7956 deletion completed in 6.094212477s

• [SLOW TEST:14.777 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:15:36.959: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8876
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8876
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 29 23:15:37.101: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 29 23:16:01.234: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.129.158:8080/dial?request=hostName&protocol=udp&host=192.168.208.105&port=8081&tries=1'] Namespace:pod-network-test-8876 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:16:01.234: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:16:01.332: INFO: Waiting for endpoints: map[]
Apr 29 23:16:01.336: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.129.158:8080/dial?request=hostName&protocol=udp&host=192.168.81.209&port=8081&tries=1'] Namespace:pod-network-test-8876 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:16:01.336: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:16:01.439: INFO: Waiting for endpoints: map[]
Apr 29 23:16:01.442: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.129.158:8080/dial?request=hostName&protocol=udp&host=192.168.143.63&port=8081&tries=1'] Namespace:pod-network-test-8876 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:16:01.442: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:16:01.527: INFO: Waiting for endpoints: map[]
Apr 29 23:16:01.530: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.129.158:8080/dial?request=hostName&protocol=udp&host=192.168.205.98&port=8081&tries=1'] Namespace:pod-network-test-8876 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:16:01.530: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:16:01.618: INFO: Waiting for endpoints: map[]
Apr 29 23:16:01.621: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.129.158:8080/dial?request=hostName&protocol=udp&host=192.168.79.233&port=8081&tries=1'] Namespace:pod-network-test-8876 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:16:01.621: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:16:01.722: INFO: Waiting for endpoints: map[]
Apr 29 23:16:01.725: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.129.158:8080/dial?request=hostName&protocol=udp&host=192.168.74.58&port=8081&tries=1'] Namespace:pod-network-test-8876 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:16:01.725: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:16:01.815: INFO: Waiting for endpoints: map[]
Apr 29 23:16:01.818: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.129.158:8080/dial?request=hostName&protocol=udp&host=192.168.137.52&port=8081&tries=1'] Namespace:pod-network-test-8876 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:16:01.818: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:16:01.907: INFO: Waiting for endpoints: map[]
Apr 29 23:16:01.910: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.129.158:8080/dial?request=hostName&protocol=udp&host=192.168.156.112&port=8081&tries=1'] Namespace:pod-network-test-8876 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:16:01.910: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:16:01.998: INFO: Waiting for endpoints: map[]
Apr 29 23:16:02.004: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.129.158:8080/dial?request=hostName&protocol=udp&host=192.168.193.43&port=8081&tries=1'] Namespace:pod-network-test-8876 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:16:02.004: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:16:02.100: INFO: Waiting for endpoints: map[]
Apr 29 23:16:02.102: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.129.158:8080/dial?request=hostName&protocol=udp&host=192.168.222.112&port=8081&tries=1'] Namespace:pod-network-test-8876 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:16:02.102: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:16:02.186: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:16:02.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8876" for this suite.
Apr 29 23:16:14.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:16:14.283: INFO: namespace pod-network-test-8876 deletion completed in 12.092450585s

• [SLOW TEST:37.324 seconds]
[sig-network] Networking
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:16:14.283: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7145
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-23c2dbe3-b34d-4a51-aca5-262fae7d3ef6 in namespace container-probe-7145
Apr 29 23:16:16.437: INFO: Started pod liveness-23c2dbe3-b34d-4a51-aca5-262fae7d3ef6 in namespace container-probe-7145
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 23:16:16.439: INFO: Initial restart count of pod liveness-23c2dbe3-b34d-4a51-aca5-262fae7d3ef6 is 0
Apr 29 23:16:26.456: INFO: Restart count of pod container-probe-7145/liveness-23c2dbe3-b34d-4a51-aca5-262fae7d3ef6 is now 1 (10.016730958s elapsed)
Apr 29 23:16:46.485: INFO: Restart count of pod container-probe-7145/liveness-23c2dbe3-b34d-4a51-aca5-262fae7d3ef6 is now 2 (30.045676055s elapsed)
Apr 29 23:17:06.515: INFO: Restart count of pod container-probe-7145/liveness-23c2dbe3-b34d-4a51-aca5-262fae7d3ef6 is now 3 (50.075264358s elapsed)
Apr 29 23:17:28.547: INFO: Restart count of pod container-probe-7145/liveness-23c2dbe3-b34d-4a51-aca5-262fae7d3ef6 is now 4 (1m12.107257923s elapsed)
Apr 29 23:18:28.636: INFO: Restart count of pod container-probe-7145/liveness-23c2dbe3-b34d-4a51-aca5-262fae7d3ef6 is now 5 (2m12.196381188s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:18:28.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7145" for this suite.
Apr 29 23:18:34.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:18:34.743: INFO: namespace container-probe-7145 deletion completed in 6.094237225s

• [SLOW TEST:140.460 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:18:34.743: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 23:18:35.224: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 23:18:38.244: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:18:38.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7310" for this suite.
Apr 29 23:18:44.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:18:44.388: INFO: namespace webhook-7310 deletion completed in 6.088495447s
STEP: Destroying namespace "webhook-7310-markers" for this suite.
Apr 29 23:18:50.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:18:50.482: INFO: namespace webhook-7310-markers deletion completed in 6.094397609s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.757 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:18:50.500: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7144
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Apr 29 23:19:00.664: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0429 23:19:00.664241      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:19:00.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7144" for this suite.
Apr 29 23:19:06.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:19:06.927: INFO: namespace gc-7144 deletion completed in 6.26025184s

• [SLOW TEST:16.427 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:19:06.928: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1011
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 29 23:19:07.145: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d8ba9a98-71b0-44ec-b139-fb369513a267" in namespace "downward-api-1011" to be "success or failure"
Apr 29 23:19:07.151: INFO: Pod "downwardapi-volume-d8ba9a98-71b0-44ec-b139-fb369513a267": Phase="Pending", Reason="", readiness=false. Elapsed: 6.268797ms
Apr 29 23:19:09.154: INFO: Pod "downwardapi-volume-d8ba9a98-71b0-44ec-b139-fb369513a267": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009109684s
Apr 29 23:19:11.157: INFO: Pod "downwardapi-volume-d8ba9a98-71b0-44ec-b139-fb369513a267": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011578458s
STEP: Saw pod success
Apr 29 23:19:11.157: INFO: Pod "downwardapi-volume-d8ba9a98-71b0-44ec-b139-fb369513a267" satisfied condition "success or failure"
Apr 29 23:19:11.159: INFO: Trying to get logs from node ip-192-168-149-254.us-west-2.compute.internal pod downwardapi-volume-d8ba9a98-71b0-44ec-b139-fb369513a267 container client-container: <nil>
STEP: delete the pod
Apr 29 23:19:11.180: INFO: Waiting for pod downwardapi-volume-d8ba9a98-71b0-44ec-b139-fb369513a267 to disappear
Apr 29 23:19:11.182: INFO: Pod downwardapi-volume-d8ba9a98-71b0-44ec-b139-fb369513a267 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:19:11.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1011" for this suite.
Apr 29 23:19:17.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:19:17.278: INFO: namespace downward-api-1011 deletion completed in 6.092631939s

• [SLOW TEST:10.351 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:19:17.279: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6000
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-69cbc871-ed58-45f8-a6a7-6c31bbf4b45a
STEP: Creating a pod to test consume configMaps
Apr 29 23:19:17.432: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-04743a3e-7316-4ae1-9545-ee4e053974b3" in namespace "projected-6000" to be "success or failure"
Apr 29 23:19:17.434: INFO: Pod "pod-projected-configmaps-04743a3e-7316-4ae1-9545-ee4e053974b3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.823023ms
Apr 29 23:19:19.437: INFO: Pod "pod-projected-configmaps-04743a3e-7316-4ae1-9545-ee4e053974b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004616721s
Apr 29 23:19:21.439: INFO: Pod "pod-projected-configmaps-04743a3e-7316-4ae1-9545-ee4e053974b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00712948s
STEP: Saw pod success
Apr 29 23:19:21.439: INFO: Pod "pod-projected-configmaps-04743a3e-7316-4ae1-9545-ee4e053974b3" satisfied condition "success or failure"
Apr 29 23:19:21.441: INFO: Trying to get logs from node ip-192-168-72-199.us-west-2.compute.internal pod pod-projected-configmaps-04743a3e-7316-4ae1-9545-ee4e053974b3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 23:19:21.462: INFO: Waiting for pod pod-projected-configmaps-04743a3e-7316-4ae1-9545-ee4e053974b3 to disappear
Apr 29 23:19:21.463: INFO: Pod pod-projected-configmaps-04743a3e-7316-4ae1-9545-ee4e053974b3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:19:21.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6000" for this suite.
Apr 29 23:19:27.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:19:27.557: INFO: namespace projected-6000 deletion completed in 6.090193725s

• [SLOW TEST:10.278 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:19:27.557: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2108
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-2108
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 29 23:19:27.702: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 29 23:19:49.843: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.72.224:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2108 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:19:49.843: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:19:49.952: INFO: Found all expected endpoints: [netserver-0]
Apr 29 23:19:49.955: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.86.98:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2108 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:19:49.955: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:19:50.057: INFO: Found all expected endpoints: [netserver-1]
Apr 29 23:19:50.059: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.204.57:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2108 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:19:50.059: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:19:50.144: INFO: Found all expected endpoints: [netserver-2]
Apr 29 23:19:50.146: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.193.126:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2108 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:19:50.146: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:19:50.244: INFO: Found all expected endpoints: [netserver-3]
Apr 29 23:19:50.246: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.205.50:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2108 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:19:50.246: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:19:50.343: INFO: Found all expected endpoints: [netserver-4]
Apr 29 23:19:50.345: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.129.158:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2108 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:19:50.345: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:19:50.437: INFO: Found all expected endpoints: [netserver-5]
Apr 29 23:19:50.440: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.134.114:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2108 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:19:50.440: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:19:50.534: INFO: Found all expected endpoints: [netserver-6]
Apr 29 23:19:50.536: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.138.98:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2108 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:19:50.536: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:19:50.620: INFO: Found all expected endpoints: [netserver-7]
Apr 29 23:19:50.623: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.87.127:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2108 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:19:50.623: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:19:50.713: INFO: Found all expected endpoints: [netserver-8]
Apr 29 23:19:50.716: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.196.177:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2108 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:19:50.716: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:19:50.800: INFO: Found all expected endpoints: [netserver-9]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:19:50.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2108" for this suite.
Apr 29 23:20:02.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:20:02.921: INFO: namespace pod-network-test-2108 deletion completed in 12.116904353s

• [SLOW TEST:35.364 seconds]
[sig-network] Networking
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:20:02.921: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1181
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-b85da85f-5e26-4ec1-9516-b9ad4df0e4c3
STEP: Creating a pod to test consume configMaps
Apr 29 23:20:03.085: INFO: Waiting up to 5m0s for pod "pod-configmaps-05145c9e-57e4-4d2e-8c24-899342bb7432" in namespace "configmap-1181" to be "success or failure"
Apr 29 23:20:03.087: INFO: Pod "pod-configmaps-05145c9e-57e4-4d2e-8c24-899342bb7432": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069573ms
Apr 29 23:20:05.090: INFO: Pod "pod-configmaps-05145c9e-57e4-4d2e-8c24-899342bb7432": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004886209s
Apr 29 23:20:07.092: INFO: Pod "pod-configmaps-05145c9e-57e4-4d2e-8c24-899342bb7432": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007721665s
STEP: Saw pod success
Apr 29 23:20:07.092: INFO: Pod "pod-configmaps-05145c9e-57e4-4d2e-8c24-899342bb7432" satisfied condition "success or failure"
Apr 29 23:20:07.094: INFO: Trying to get logs from node ip-192-168-144-27.us-west-2.compute.internal pod pod-configmaps-05145c9e-57e4-4d2e-8c24-899342bb7432 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 23:20:07.116: INFO: Waiting for pod pod-configmaps-05145c9e-57e4-4d2e-8c24-899342bb7432 to disappear
Apr 29 23:20:07.119: INFO: Pod pod-configmaps-05145c9e-57e4-4d2e-8c24-899342bb7432 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:20:07.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1181" for this suite.
Apr 29 23:20:13.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:20:13.222: INFO: namespace configmap-1181 deletion completed in 6.099747227s

• [SLOW TEST:10.302 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:20:13.223: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-b11b147d-9c2f-4d38-92e0-b316ae0a3ae3
STEP: Creating a pod to test consume configMaps
Apr 29 23:20:13.383: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9c3f5d38-640f-440d-88d2-3b7550e313a9" in namespace "projected-7846" to be "success or failure"
Apr 29 23:20:13.385: INFO: Pod "pod-projected-configmaps-9c3f5d38-640f-440d-88d2-3b7550e313a9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.8909ms
Apr 29 23:20:15.387: INFO: Pod "pod-projected-configmaps-9c3f5d38-640f-440d-88d2-3b7550e313a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.004551619s
Apr 29 23:20:17.390: INFO: Pod "pod-projected-configmaps-9c3f5d38-640f-440d-88d2-3b7550e313a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007404069s
STEP: Saw pod success
Apr 29 23:20:17.390: INFO: Pod "pod-projected-configmaps-9c3f5d38-640f-440d-88d2-3b7550e313a9" satisfied condition "success or failure"
Apr 29 23:20:17.392: INFO: Trying to get logs from node ip-192-168-197-211.us-west-2.compute.internal pod pod-projected-configmaps-9c3f5d38-640f-440d-88d2-3b7550e313a9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 23:20:17.411: INFO: Waiting for pod pod-projected-configmaps-9c3f5d38-640f-440d-88d2-3b7550e313a9 to disappear
Apr 29 23:20:17.413: INFO: Pod pod-projected-configmaps-9c3f5d38-640f-440d-88d2-3b7550e313a9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:20:17.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7846" for this suite.
Apr 29 23:20:23.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:20:23.509: INFO: namespace projected-7846 deletion completed in 6.092498967s

• [SLOW TEST:10.286 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:20:23.509: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-3283
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Apr 29 23:20:24.171: INFO: created pod pod-service-account-defaultsa
Apr 29 23:20:24.171: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr 29 23:20:24.175: INFO: created pod pod-service-account-mountsa
Apr 29 23:20:24.175: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr 29 23:20:24.179: INFO: created pod pod-service-account-nomountsa
Apr 29 23:20:24.179: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr 29 23:20:24.184: INFO: created pod pod-service-account-defaultsa-mountspec
Apr 29 23:20:24.184: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr 29 23:20:24.189: INFO: created pod pod-service-account-mountsa-mountspec
Apr 29 23:20:24.189: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr 29 23:20:24.193: INFO: created pod pod-service-account-nomountsa-mountspec
Apr 29 23:20:24.193: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr 29 23:20:24.197: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr 29 23:20:24.197: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr 29 23:20:24.201: INFO: created pod pod-service-account-mountsa-nomountspec
Apr 29 23:20:24.202: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr 29 23:20:24.206: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr 29 23:20:24.206: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:20:24.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3283" for this suite.
Apr 29 23:20:30.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:20:30.304: INFO: namespace svcaccounts-3283 deletion completed in 6.094124715s

• [SLOW TEST:6.795 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:20:30.304: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4343
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-4343
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4343 to expose endpoints map[]
Apr 29 23:20:30.460: INFO: Get endpoints failed (3.401427ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Apr 29 23:20:31.464: INFO: successfully validated that service endpoint-test2 in namespace services-4343 exposes endpoints map[] (1.007648525s elapsed)
STEP: Creating pod pod1 in namespace services-4343
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4343 to expose endpoints map[pod1:[80]]
Apr 29 23:20:33.492: INFO: successfully validated that service endpoint-test2 in namespace services-4343 exposes endpoints map[pod1:[80]] (2.018959852s elapsed)
STEP: Creating pod pod2 in namespace services-4343
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4343 to expose endpoints map[pod1:[80] pod2:[80]]
Apr 29 23:20:35.521: INFO: successfully validated that service endpoint-test2 in namespace services-4343 exposes endpoints map[pod1:[80] pod2:[80]] (2.024276664s elapsed)
STEP: Deleting pod pod1 in namespace services-4343
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4343 to expose endpoints map[pod2:[80]]
Apr 29 23:20:36.543: INFO: successfully validated that service endpoint-test2 in namespace services-4343 exposes endpoints map[pod2:[80]] (1.0161423s elapsed)
STEP: Deleting pod pod2 in namespace services-4343
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4343 to expose endpoints map[]
Apr 29 23:20:37.556: INFO: successfully validated that service endpoint-test2 in namespace services-4343 exposes endpoints map[] (1.007764551s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:20:37.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4343" for this suite.
Apr 29 23:20:43.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:20:43.676: INFO: namespace services-4343 deletion completed in 6.093115871s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:13.372 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:20:43.676: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6164
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:20:43.819: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:20:44.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6164" for this suite.
Apr 29 23:20:50.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:20:50.446: INFO: namespace custom-resource-definition-6164 deletion completed in 6.092180851s

• [SLOW TEST:6.770 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:20:50.447: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-9321
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Apr 29 23:20:50.589: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 29 23:21:50.600: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:21:50.603: INFO: Starting informer...
STEP: Starting pod...
Apr 29 23:21:50.815: INFO: Pod is running on ip-192-168-148-117.us-west-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Apr 29 23:21:50.825: INFO: Pod wasn't evicted. Proceeding
Apr 29 23:21:50.825: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Apr 29 23:23:05.833: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:23:05.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9321" for this suite.
Apr 29 23:23:33.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:23:33.937: INFO: namespace taint-single-pod-9321 deletion completed in 28.099251585s

• [SLOW TEST:163.490 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:23:33.937: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9601
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-ceacdff1-caa0-4981-8ed4-a6048b59b441
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:23:34.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9601" for this suite.
Apr 29 23:23:40.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:23:40.180: INFO: namespace secrets-9601 deletion completed in 6.094957708s

• [SLOW TEST:6.243 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:23:40.180: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2017
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 29 23:23:40.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-2017'
Apr 29 23:23:40.458: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 29 23:23:40.458: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Apr 29 23:23:44.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete deployment e2e-test-httpd-deployment --namespace=kubectl-2017'
Apr 29 23:23:44.524: INFO: stderr: ""
Apr 29 23:23:44.524: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:23:44.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2017" for this suite.
Apr 29 23:23:50.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:23:50.621: INFO: namespace kubectl-2017 deletion completed in 6.092700744s

• [SLOW TEST:10.440 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:23:50.621: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-970
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 23:23:51.233: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 29 23:23:53.240: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723799431, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723799431, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723799431, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723799431, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 23:23:56.253: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:23:56.257: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5844-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:23:56.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-970" for this suite.
Apr 29 23:24:02.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:24:02.945: INFO: namespace webhook-970 deletion completed in 6.099807986s
STEP: Destroying namespace "webhook-970-markers" for this suite.
Apr 29 23:24:08.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:24:09.088: INFO: namespace webhook-970-markers deletion completed in 6.143166682s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.495 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:24:09.116: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5526
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Apr 29 23:24:09.275: INFO: Waiting up to 5m0s for pod "var-expansion-bdad03f8-fc6a-44aa-bbb8-fbea4a6c3975" in namespace "var-expansion-5526" to be "success or failure"
Apr 29 23:24:09.277: INFO: Pod "var-expansion-bdad03f8-fc6a-44aa-bbb8-fbea4a6c3975": Phase="Pending", Reason="", readiness=false. Elapsed: 1.948875ms
Apr 29 23:24:11.280: INFO: Pod "var-expansion-bdad03f8-fc6a-44aa-bbb8-fbea4a6c3975": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004990627s
Apr 29 23:24:13.283: INFO: Pod "var-expansion-bdad03f8-fc6a-44aa-bbb8-fbea4a6c3975": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007920436s
STEP: Saw pod success
Apr 29 23:24:13.283: INFO: Pod "var-expansion-bdad03f8-fc6a-44aa-bbb8-fbea4a6c3975" satisfied condition "success or failure"
Apr 29 23:24:13.285: INFO: Trying to get logs from node ip-192-168-83-225.us-west-2.compute.internal pod var-expansion-bdad03f8-fc6a-44aa-bbb8-fbea4a6c3975 container dapi-container: <nil>
STEP: delete the pod
Apr 29 23:24:13.308: INFO: Waiting for pod var-expansion-bdad03f8-fc6a-44aa-bbb8-fbea4a6c3975 to disappear
Apr 29 23:24:13.310: INFO: Pod var-expansion-bdad03f8-fc6a-44aa-bbb8-fbea4a6c3975 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:24:13.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5526" for this suite.
Apr 29 23:24:19.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:24:19.604: INFO: namespace var-expansion-5526 deletion completed in 6.290375039s

• [SLOW TEST:10.488 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:24:19.605: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9896
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 29 23:24:19.945: INFO: Waiting up to 5m0s for pod "downwardapi-volume-069ac9e1-cbb3-462d-8236-0794260a4943" in namespace "downward-api-9896" to be "success or failure"
Apr 29 23:24:19.977: INFO: Pod "downwardapi-volume-069ac9e1-cbb3-462d-8236-0794260a4943": Phase="Pending", Reason="", readiness=false. Elapsed: 32.423385ms
Apr 29 23:24:22.002: INFO: Pod "downwardapi-volume-069ac9e1-cbb3-462d-8236-0794260a4943": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.056949226s
STEP: Saw pod success
Apr 29 23:24:22.002: INFO: Pod "downwardapi-volume-069ac9e1-cbb3-462d-8236-0794260a4943" satisfied condition "success or failure"
Apr 29 23:24:22.026: INFO: Trying to get logs from node ip-192-168-83-225.us-west-2.compute.internal pod downwardapi-volume-069ac9e1-cbb3-462d-8236-0794260a4943 container client-container: <nil>
STEP: delete the pod
Apr 29 23:24:22.130: INFO: Waiting for pod downwardapi-volume-069ac9e1-cbb3-462d-8236-0794260a4943 to disappear
Apr 29 23:24:22.148: INFO: Pod downwardapi-volume-069ac9e1-cbb3-462d-8236-0794260a4943 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:24:22.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9896" for this suite.
Apr 29 23:24:28.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:24:28.299: INFO: namespace downward-api-9896 deletion completed in 6.147260997s

• [SLOW TEST:8.694 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:24:28.299: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9467
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 23:24:28.664: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 29 23:24:30.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723799468, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723799468, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723799468, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723799468, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 23:24:33.689: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:24:33.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9467" for this suite.
Apr 29 23:24:39.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:24:39.796: INFO: namespace webhook-9467 deletion completed in 6.096650056s
STEP: Destroying namespace "webhook-9467-markers" for this suite.
Apr 29 23:24:45.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:24:45.892: INFO: namespace webhook-9467-markers deletion completed in 6.095950834s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.610 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:24:45.909: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3809
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:24:46.092: INFO: Create a RollingUpdate DaemonSet
Apr 29 23:24:46.097: INFO: Check that daemon pods launch on every node of the cluster
Apr 29 23:24:46.102: INFO: Number of nodes with available pods: 0
Apr 29 23:24:46.102: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:24:47.109: INFO: Number of nodes with available pods: 0
Apr 29 23:24:47.109: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:24:48.109: INFO: Number of nodes with available pods: 3
Apr 29 23:24:48.109: INFO: Node ip-192-168-149-254.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:24:49.109: INFO: Number of nodes with available pods: 6
Apr 29 23:24:49.109: INFO: Node ip-192-168-149-254.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:24:50.109: INFO: Number of nodes with available pods: 6
Apr 29 23:24:50.109: INFO: Node ip-192-168-149-254.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:24:51.109: INFO: Number of nodes with available pods: 6
Apr 29 23:24:51.109: INFO: Node ip-192-168-149-254.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:24:52.109: INFO: Number of nodes with available pods: 8
Apr 29 23:24:52.109: INFO: Node ip-192-168-218-192.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:24:53.109: INFO: Number of nodes with available pods: 8
Apr 29 23:24:53.109: INFO: Node ip-192-168-218-192.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:24:54.109: INFO: Number of nodes with available pods: 9
Apr 29 23:24:54.109: INFO: Node ip-192-168-222-85.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:24:55.109: INFO: Number of nodes with available pods: 10
Apr 29 23:24:55.109: INFO: Number of running nodes: 10, number of available pods: 10
Apr 29 23:24:55.109: INFO: Update the DaemonSet to trigger a rollout
Apr 29 23:24:55.119: INFO: Updating DaemonSet daemon-set
Apr 29 23:25:02.150: INFO: Roll back the DaemonSet before rollout is complete
Apr 29 23:25:02.166: INFO: Updating DaemonSet daemon-set
Apr 29 23:25:02.166: INFO: Make sure DaemonSet rollback is complete
Apr 29 23:25:02.837: INFO: Wrong image for pod: daemon-set-gb5rs. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 29 23:25:02.837: INFO: Pod daemon-set-gb5rs is not available
Apr 29 23:25:03.879: INFO: Wrong image for pod: daemon-set-gb5rs. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 29 23:25:03.879: INFO: Pod daemon-set-gb5rs is not available
Apr 29 23:25:04.844: INFO: Pod daemon-set-xk6t7 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3809, will wait for the garbage collector to delete the pods
Apr 29 23:25:05.012: INFO: Deleting DaemonSet.extensions daemon-set took: 54.975265ms
Apr 29 23:25:05.313: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.233721ms
Apr 29 23:25:12.215: INFO: Number of nodes with available pods: 0
Apr 29 23:25:12.215: INFO: Number of running nodes: 0, number of available pods: 0
Apr 29 23:25:12.222: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3809/daemonsets","resourceVersion":"10801"},"items":null}

Apr 29 23:25:12.225: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3809/pods","resourceVersion":"10801"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:25:12.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3809" for this suite.
Apr 29 23:25:18.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:25:18.350: INFO: namespace daemonsets-3809 deletion completed in 6.095905004s

• [SLOW TEST:32.441 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:25:18.350: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9358
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:25:18.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9358" for this suite.
Apr 29 23:25:24.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:25:24.595: INFO: namespace services-9358 deletion completed in 6.095026655s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.244 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:25:24.595: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3808
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:25:37.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3808" for this suite.
Apr 29 23:25:43.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:25:43.884: INFO: namespace resourcequota-3808 deletion completed in 6.095607223s

• [SLOW TEST:19.289 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:25:43.884: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2326
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:25:44.026: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 29 23:25:45.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-2326 create -f -'
Apr 29 23:25:45.888: INFO: stderr: ""
Apr 29 23:25:45.888: INFO: stdout: "e2e-test-crd-publish-openapi-732-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 29 23:25:45.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-2326 delete e2e-test-crd-publish-openapi-732-crds test-cr'
Apr 29 23:25:45.945: INFO: stderr: ""
Apr 29 23:25:45.945: INFO: stdout: "e2e-test-crd-publish-openapi-732-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Apr 29 23:25:45.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-2326 apply -f -'
Apr 29 23:25:46.103: INFO: stderr: ""
Apr 29 23:25:46.103: INFO: stdout: "e2e-test-crd-publish-openapi-732-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 29 23:25:46.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-2326 delete e2e-test-crd-publish-openapi-732-crds test-cr'
Apr 29 23:25:46.159: INFO: stderr: ""
Apr 29 23:25:46.159: INFO: stdout: "e2e-test-crd-publish-openapi-732-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr 29 23:25:46.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 explain e2e-test-crd-publish-openapi-732-crds'
Apr 29 23:25:46.263: INFO: stderr: ""
Apr 29 23:25:46.263: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-732-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:25:48.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2326" for this suite.
Apr 29 23:25:54.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:25:54.957: INFO: namespace crd-publish-openapi-2326 deletion completed in 6.09319046s

• [SLOW TEST:11.073 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:25:54.957: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6773
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Apr 29 23:25:55.101: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 29 23:25:55.111: INFO: Waiting for terminating namespaces to be deleted...
Apr 29 23:25:55.113: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-144-27.us-west-2.compute.internal before test
Apr 29 23:25:55.124: INFO: aws-node-rccgv from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.124: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:25:55.124: INFO: kube-proxy-4xrfl from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.124: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:25:55.124: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-2ffxh from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:25:55.124: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:25:55.124: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:25:55.124: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-148-117.us-west-2.compute.internal before test
Apr 29 23:25:55.135: INFO: aws-node-wzq6b from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.135: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:25:55.135: INFO: kube-proxy-b4plm from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.135: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:25:55.135: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-69sgr from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:25:55.135: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:25:55.135: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:25:55.135: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-149-254.us-west-2.compute.internal before test
Apr 29 23:25:55.145: INFO: aws-node-kcbxd from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.145: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:25:55.145: INFO: kube-proxy-jvnfx from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.145: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:25:55.145: INFO: sonobuoy from sonobuoy started at 2020-04-29 23:02:27 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.145: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 29 23:25:55.145: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-xtlz4 from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:25:55.145: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:25:55.145: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:25:55.145: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-197-211.us-west-2.compute.internal before test
Apr 29 23:25:55.153: INFO: kube-proxy-xhpsv from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.153: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:25:55.153: INFO: aws-node-ffvhf from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.153: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:25:55.153: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-wlv6l from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:25:55.153: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:25:55.153: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:25:55.153: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-211-77.us-west-2.compute.internal before test
Apr 29 23:25:55.161: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-xpbv9 from sonobuoy started at 2020-04-29 23:02:31 +0000 UTC (2 container statuses recorded)
Apr 29 23:25:55.161: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:25:55.161: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:25:55.161: INFO: aws-node-xj5sd from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.161: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:25:55.161: INFO: kube-proxy-fck6k from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.161: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:25:55.161: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-218-192.us-west-2.compute.internal before test
Apr 29 23:25:55.169: INFO: aws-node-s9vwd from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.169: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:25:55.169: INFO: kube-proxy-b6znl from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.169: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:25:55.169: INFO: sonobuoy-e2e-job-c0680a6adde246b1 from sonobuoy started at 2020-04-29 23:02:31 +0000 UTC (2 container statuses recorded)
Apr 29 23:25:55.169: INFO: 	Container e2e ready: true, restart count 0
Apr 29 23:25:55.169: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:25:55.169: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-8cg7t from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:25:55.169: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:25:55.169: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:25:55.169: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-222-85.us-west-2.compute.internal before test
Apr 29 23:25:55.177: INFO: aws-node-l9blv from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.177: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:25:55.177: INFO: kube-proxy-fh79j from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.177: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:25:55.177: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-qh6r9 from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:25:55.177: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:25:55.177: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:25:55.177: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-72-199.us-west-2.compute.internal before test
Apr 29 23:25:55.186: INFO: coredns-5c97f79574-wjwsw from kube-system started at 2020-04-29 22:50:59 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.186: INFO: 	Container coredns ready: true, restart count 0
Apr 29 23:25:55.186: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-sg2vq from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:25:55.186: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:25:55.186: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:25:55.186: INFO: kube-proxy-kklzp from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.186: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:25:55.186: INFO: aws-node-zldl6 from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.186: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:25:55.186: INFO: coredns-5c97f79574-9t82l from kube-system started at 2020-04-29 22:50:59 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.186: INFO: 	Container coredns ready: true, restart count 0
Apr 29 23:25:55.186: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-82-130.us-west-2.compute.internal before test
Apr 29 23:25:55.193: INFO: kube-proxy-8pccx from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.193: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:25:55.193: INFO: aws-node-rbjxk from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.193: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:25:55.193: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-ptxhd from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:25:55.193: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:25:55.193: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:25:55.193: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-83-225.us-west-2.compute.internal before test
Apr 29 23:25:55.200: INFO: kube-proxy-sz6rb from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.200: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:25:55.200: INFO: aws-node-j8bsf from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:25:55.200: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:25:55.200: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-rmfrj from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:25:55.200: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:25:55.200: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.160a6dfdc028e0e5], Reason = [FailedScheduling], Message = [0/10 nodes are available: 10 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:25:56.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6773" for this suite.
Apr 29 23:26:02.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:26:02.347: INFO: namespace sched-pred-6773 deletion completed in 6.122818947s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:7.390 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:26:02.347: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2316
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-0b904098-9c60-4816-b5ab-c4c6481a8ac6
STEP: Creating a pod to test consume configMaps
Apr 29 23:26:02.555: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5adbd766-1df6-4456-90a9-732014828d77" in namespace "projected-2316" to be "success or failure"
Apr 29 23:26:02.557: INFO: Pod "pod-projected-configmaps-5adbd766-1df6-4456-90a9-732014828d77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.123347ms
Apr 29 23:26:04.561: INFO: Pod "pod-projected-configmaps-5adbd766-1df6-4456-90a9-732014828d77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005304374s
STEP: Saw pod success
Apr 29 23:26:04.561: INFO: Pod "pod-projected-configmaps-5adbd766-1df6-4456-90a9-732014828d77" satisfied condition "success or failure"
Apr 29 23:26:04.563: INFO: Trying to get logs from node ip-192-168-222-85.us-west-2.compute.internal pod pod-projected-configmaps-5adbd766-1df6-4456-90a9-732014828d77 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 23:26:04.579: INFO: Waiting for pod pod-projected-configmaps-5adbd766-1df6-4456-90a9-732014828d77 to disappear
Apr 29 23:26:04.581: INFO: Pod pod-projected-configmaps-5adbd766-1df6-4456-90a9-732014828d77 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:26:04.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2316" for this suite.
Apr 29 23:26:10.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:26:10.690: INFO: namespace projected-2316 deletion completed in 6.105386156s

• [SLOW TEST:8.343 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:26:10.690: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6374
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Apr 29 23:26:10.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 api-versions'
Apr 29 23:26:10.889: INFO: stderr: ""
Apr 29 23:26:10.889: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.k8s.amazonaws.com/v1alpha1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:26:10.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6374" for this suite.
Apr 29 23:26:16.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:26:16.987: INFO: namespace kubectl-6374 deletion completed in 6.094612531s

• [SLOW TEST:6.297 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:26:16.988: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-8363
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-8363
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8363
STEP: Deleting pre-stop pod
Apr 29 23:26:28.162: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:26:28.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8363" for this suite.
Apr 29 23:27:12.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:27:12.265: INFO: namespace prestop-8363 deletion completed in 44.093646365s

• [SLOW TEST:55.277 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:27:12.265: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-391
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-391
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 29 23:27:12.407: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 29 23:27:36.545: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.74.58 8081 | grep -v '^\s*$'] Namespace:pod-network-test-391 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:27:36.545: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:27:37.660: INFO: Found all expected endpoints: [netserver-0]
Apr 29 23:27:37.662: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.95.47 8081 | grep -v '^\s*$'] Namespace:pod-network-test-391 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:27:37.662: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:27:38.748: INFO: Found all expected endpoints: [netserver-1]
Apr 29 23:27:38.751: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.74.65 8081 | grep -v '^\s*$'] Namespace:pod-network-test-391 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:27:38.751: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:27:39.852: INFO: Found all expected endpoints: [netserver-2]
Apr 29 23:27:39.855: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.202.13 8081 | grep -v '^\s*$'] Namespace:pod-network-test-391 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:27:39.855: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:27:40.964: INFO: Found all expected endpoints: [netserver-3]
Apr 29 23:27:40.968: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.131.165 8081 | grep -v '^\s*$'] Namespace:pod-network-test-391 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:27:40.968: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:27:42.076: INFO: Found all expected endpoints: [netserver-4]
Apr 29 23:27:42.079: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.198.76 8081 | grep -v '^\s*$'] Namespace:pod-network-test-391 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:27:42.079: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:27:43.185: INFO: Found all expected endpoints: [netserver-5]
Apr 29 23:27:43.187: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.144.175 8081 | grep -v '^\s*$'] Namespace:pod-network-test-391 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:27:43.188: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:27:44.274: INFO: Found all expected endpoints: [netserver-6]
Apr 29 23:27:44.276: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.201.22 8081 | grep -v '^\s*$'] Namespace:pod-network-test-391 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:27:44.276: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:27:45.385: INFO: Found all expected endpoints: [netserver-7]
Apr 29 23:27:45.387: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.193.43 8081 | grep -v '^\s*$'] Namespace:pod-network-test-391 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:27:45.388: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:27:46.491: INFO: Found all expected endpoints: [netserver-8]
Apr 29 23:27:46.494: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.132.17 8081 | grep -v '^\s*$'] Namespace:pod-network-test-391 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 29 23:27:46.494: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:27:47.604: INFO: Found all expected endpoints: [netserver-9]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:27:47.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-391" for this suite.
Apr 29 23:27:59.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:27:59.702: INFO: namespace pod-network-test-391 deletion completed in 12.093708521s

• [SLOW TEST:47.438 seconds]
[sig-network] Networking
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:27:59.703: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7657
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 23:28:00.523: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 23:28:03.542: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:28:03.547: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:28:04.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7657" for this suite.
Apr 29 23:28:10.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:28:10.782: INFO: namespace webhook-7657 deletion completed in 6.1080824s
STEP: Destroying namespace "webhook-7657-markers" for this suite.
Apr 29 23:28:16.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:28:16.878: INFO: namespace webhook-7657-markers deletion completed in 6.096394763s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.194 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:28:16.897: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1148
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-05709e6d-2fdb-4c06-90a7-648a2c9c672e
STEP: Creating a pod to test consume configMaps
Apr 29 23:28:17.054: INFO: Waiting up to 5m0s for pod "pod-configmaps-eec42fb3-47d2-46ca-bd1a-af319a9a3654" in namespace "configmap-1148" to be "success or failure"
Apr 29 23:28:17.055: INFO: Pod "pod-configmaps-eec42fb3-47d2-46ca-bd1a-af319a9a3654": Phase="Pending", Reason="", readiness=false. Elapsed: 1.855731ms
Apr 29 23:28:19.058: INFO: Pod "pod-configmaps-eec42fb3-47d2-46ca-bd1a-af319a9a3654": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004677885s
STEP: Saw pod success
Apr 29 23:28:19.058: INFO: Pod "pod-configmaps-eec42fb3-47d2-46ca-bd1a-af319a9a3654" satisfied condition "success or failure"
Apr 29 23:28:19.060: INFO: Trying to get logs from node ip-192-168-149-254.us-west-2.compute.internal pod pod-configmaps-eec42fb3-47d2-46ca-bd1a-af319a9a3654 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 23:28:19.081: INFO: Waiting for pod pod-configmaps-eec42fb3-47d2-46ca-bd1a-af319a9a3654 to disappear
Apr 29 23:28:19.082: INFO: Pod pod-configmaps-eec42fb3-47d2-46ca-bd1a-af319a9a3654 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:28:19.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1148" for this suite.
Apr 29 23:28:25.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:28:25.179: INFO: namespace configmap-1148 deletion completed in 6.09349263s

• [SLOW TEST:8.283 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:28:25.180: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6142
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Apr 29 23:28:25.320: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 29 23:28:25.331: INFO: Waiting for terminating namespaces to be deleted...
Apr 29 23:28:25.333: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-144-27.us-west-2.compute.internal before test
Apr 29 23:28:25.342: INFO: aws-node-rccgv from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.342: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:28:25.342: INFO: kube-proxy-4xrfl from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.342: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:28:25.342: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-2ffxh from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:28:25.342: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:28:25.342: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:28:25.342: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-148-117.us-west-2.compute.internal before test
Apr 29 23:28:25.351: INFO: aws-node-wzq6b from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.351: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:28:25.351: INFO: kube-proxy-b4plm from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.351: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:28:25.351: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-69sgr from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:28:25.351: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:28:25.351: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:28:25.351: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-149-254.us-west-2.compute.internal before test
Apr 29 23:28:25.355: INFO: kube-proxy-jvnfx from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.355: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:28:25.355: INFO: sonobuoy from sonobuoy started at 2020-04-29 23:02:27 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.355: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 29 23:28:25.355: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-xtlz4 from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:28:25.355: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:28:25.355: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:28:25.355: INFO: aws-node-kcbxd from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.355: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:28:25.355: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-197-211.us-west-2.compute.internal before test
Apr 29 23:28:25.362: INFO: kube-proxy-xhpsv from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.362: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:28:25.362: INFO: aws-node-ffvhf from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.362: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:28:25.362: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-wlv6l from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:28:25.362: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:28:25.362: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:28:25.362: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-211-77.us-west-2.compute.internal before test
Apr 29 23:28:25.368: INFO: aws-node-xj5sd from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.368: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:28:25.368: INFO: kube-proxy-fck6k from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.368: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:28:25.368: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-xpbv9 from sonobuoy started at 2020-04-29 23:02:31 +0000 UTC (2 container statuses recorded)
Apr 29 23:28:25.368: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:28:25.368: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:28:25.368: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-218-192.us-west-2.compute.internal before test
Apr 29 23:28:25.375: INFO: kube-proxy-b6znl from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.375: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:28:25.375: INFO: sonobuoy-e2e-job-c0680a6adde246b1 from sonobuoy started at 2020-04-29 23:02:31 +0000 UTC (2 container statuses recorded)
Apr 29 23:28:25.375: INFO: 	Container e2e ready: true, restart count 0
Apr 29 23:28:25.375: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:28:25.375: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-8cg7t from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:28:25.375: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:28:25.375: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:28:25.375: INFO: aws-node-s9vwd from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.375: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:28:25.375: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-222-85.us-west-2.compute.internal before test
Apr 29 23:28:25.382: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-qh6r9 from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:28:25.382: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:28:25.382: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:28:25.382: INFO: aws-node-l9blv from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.382: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:28:25.382: INFO: kube-proxy-fh79j from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.382: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:28:25.382: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-72-199.us-west-2.compute.internal before test
Apr 29 23:28:25.389: INFO: kube-proxy-kklzp from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.389: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:28:25.389: INFO: aws-node-zldl6 from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.389: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:28:25.389: INFO: coredns-5c97f79574-9t82l from kube-system started at 2020-04-29 22:50:59 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.389: INFO: 	Container coredns ready: true, restart count 0
Apr 29 23:28:25.389: INFO: coredns-5c97f79574-wjwsw from kube-system started at 2020-04-29 22:50:59 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.389: INFO: 	Container coredns ready: true, restart count 0
Apr 29 23:28:25.389: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-sg2vq from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:28:25.389: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:28:25.389: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:28:25.389: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-82-130.us-west-2.compute.internal before test
Apr 29 23:28:25.395: INFO: kube-proxy-8pccx from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.395: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:28:25.395: INFO: aws-node-rbjxk from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.395: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:28:25.395: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-ptxhd from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:28:25.395: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:28:25.395: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 29 23:28:25.395: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-83-225.us-west-2.compute.internal before test
Apr 29 23:28:25.402: INFO: kube-proxy-sz6rb from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.402: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 29 23:28:25.402: INFO: aws-node-j8bsf from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 29 23:28:25.402: INFO: 	Container aws-node ready: true, restart count 0
Apr 29 23:28:25.402: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-rmfrj from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 29 23:28:25.402: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 29 23:28:25.402: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b32bf5bf-d8e8-465e-8d8e-a9a17de527a7 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-b32bf5bf-d8e8-465e-8d8e-a9a17de527a7 off the node ip-192-168-144-27.us-west-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b32bf5bf-d8e8-465e-8d8e-a9a17de527a7
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:33:29.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6142" for this suite.
Apr 29 23:33:43.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:33:43.562: INFO: namespace sched-pred-6142 deletion completed in 14.091785117s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:318.382 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:33:43.562: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-9a8a7b20-444c-4baa-8a69-021fa8f85c5d
STEP: Creating a pod to test consume secrets
Apr 29 23:33:43.739: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a3fbee9e-5734-4b77-aa2d-3007817a80df" in namespace "projected-438" to be "success or failure"
Apr 29 23:33:43.741: INFO: Pod "pod-projected-secrets-a3fbee9e-5734-4b77-aa2d-3007817a80df": Phase="Pending", Reason="", readiness=false. Elapsed: 1.86216ms
Apr 29 23:33:45.744: INFO: Pod "pod-projected-secrets-a3fbee9e-5734-4b77-aa2d-3007817a80df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00483203s
STEP: Saw pod success
Apr 29 23:33:45.744: INFO: Pod "pod-projected-secrets-a3fbee9e-5734-4b77-aa2d-3007817a80df" satisfied condition "success or failure"
Apr 29 23:33:45.746: INFO: Trying to get logs from node ip-192-168-144-27.us-west-2.compute.internal pod pod-projected-secrets-a3fbee9e-5734-4b77-aa2d-3007817a80df container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 23:33:45.766: INFO: Waiting for pod pod-projected-secrets-a3fbee9e-5734-4b77-aa2d-3007817a80df to disappear
Apr 29 23:33:45.768: INFO: Pod pod-projected-secrets-a3fbee9e-5734-4b77-aa2d-3007817a80df no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:33:45.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-438" for this suite.
Apr 29 23:33:51.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:33:51.863: INFO: namespace projected-438 deletion completed in 6.091925695s

• [SLOW TEST:8.302 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:33:51.863: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1999
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Apr 29 23:33:52.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-1999 -- logs-generator --log-lines-total 100 --run-duration 20s'
Apr 29 23:33:52.074: INFO: stderr: ""
Apr 29 23:33:52.074: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Apr 29 23:33:52.074: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Apr 29 23:33:52.074: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1999" to be "running and ready, or succeeded"
Apr 29 23:33:52.077: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.495723ms
Apr 29 23:33:54.081: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006533188s
Apr 29 23:33:54.081: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Apr 29 23:33:54.081: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Apr 29 23:33:54.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 logs logs-generator logs-generator --namespace=kubectl-1999'
Apr 29 23:33:54.145: INFO: stderr: ""
Apr 29 23:33:54.145: INFO: stdout: "I0429 23:33:53.167926       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/5xd 349\nI0429 23:33:53.368026       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/gkhs 242\nI0429 23:33:53.568037       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/c78 503\nI0429 23:33:53.768039       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/ljt 314\nI0429 23:33:53.968035       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/24d 516\n"
STEP: limiting log lines
Apr 29 23:33:54.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 logs logs-generator logs-generator --namespace=kubectl-1999 --tail=1'
Apr 29 23:33:54.210: INFO: stderr: ""
Apr 29 23:33:54.210: INFO: stdout: "I0429 23:33:54.168055       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/647z 325\n"
STEP: limiting log bytes
Apr 29 23:33:54.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 logs logs-generator logs-generator --namespace=kubectl-1999 --limit-bytes=1'
Apr 29 23:33:54.270: INFO: stderr: ""
Apr 29 23:33:54.270: INFO: stdout: "I"
STEP: exposing timestamps
Apr 29 23:33:54.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 logs logs-generator logs-generator --namespace=kubectl-1999 --tail=1 --timestamps'
Apr 29 23:33:54.330: INFO: stderr: ""
Apr 29 23:33:54.330: INFO: stdout: "2020-04-29T23:33:54.168136683Z I0429 23:33:54.168055       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/647z 325\n"
STEP: restricting to a time range
Apr 29 23:33:56.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 logs logs-generator logs-generator --namespace=kubectl-1999 --since=1s'
Apr 29 23:33:56.891: INFO: stderr: ""
Apr 29 23:33:56.891: INFO: stdout: "I0429 23:33:55.968047       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/qxnl 494\nI0429 23:33:56.168044       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/4qfc 369\nI0429 23:33:56.368063       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/dqk 409\nI0429 23:33:56.568048       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/p69 335\nI0429 23:33:56.768046       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/jq9v 410\n"
Apr 29 23:33:56.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 logs logs-generator logs-generator --namespace=kubectl-1999 --since=24h'
Apr 29 23:33:56.951: INFO: stderr: ""
Apr 29 23:33:56.951: INFO: stdout: "I0429 23:33:53.167926       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/5xd 349\nI0429 23:33:53.368026       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/gkhs 242\nI0429 23:33:53.568037       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/c78 503\nI0429 23:33:53.768039       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/ljt 314\nI0429 23:33:53.968035       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/24d 516\nI0429 23:33:54.168055       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/647z 325\nI0429 23:33:54.368059       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/65xc 549\nI0429 23:33:54.568069       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/l2f 532\nI0429 23:33:54.768044       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/t9c 244\nI0429 23:33:54.968039       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/nqn 531\nI0429 23:33:55.168065       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/6qn 451\nI0429 23:33:55.368007       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/hrn 377\nI0429 23:33:55.568053       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/st6 411\nI0429 23:33:55.768064       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/8rx8 299\nI0429 23:33:55.968047       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/qxnl 494\nI0429 23:33:56.168044       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/4qfc 369\nI0429 23:33:56.368063       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/dqk 409\nI0429 23:33:56.568048       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/p69 335\nI0429 23:33:56.768046       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/jq9v 410\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Apr 29 23:33:56.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete pod logs-generator --namespace=kubectl-1999'
Apr 29 23:34:01.675: INFO: stderr: ""
Apr 29 23:34:01.675: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:34:01.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1999" for this suite.
Apr 29 23:34:07.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:34:07.808: INFO: namespace kubectl-1999 deletion completed in 6.128527239s

• [SLOW TEST:15.944 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:34:07.808: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7830
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7830
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7830
STEP: creating replication controller externalsvc in namespace services-7830
I0429 23:34:07.985533      21 runners.go:184] Created replication controller with name: externalsvc, namespace: services-7830, replica count: 2
I0429 23:34:11.035817      21 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Apr 29 23:34:11.061: INFO: Creating new exec pod
Apr 29 23:34:13.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=services-7830 execpodzcdb9 -- /bin/sh -x -c nslookup clusterip-service'
Apr 29 23:34:13.244: INFO: stderr: "+ nslookup clusterip-service\n"
Apr 29 23:34:13.244: INFO: stdout: "Server:\t\t10.100.0.10\nAddress:\t10.100.0.10#53\n\nclusterip-service.services-7830.svc.cluster.local\tcanonical name = externalsvc.services-7830.svc.cluster.local.\nName:\texternalsvc.services-7830.svc.cluster.local\nAddress: 10.100.142.112\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7830, will wait for the garbage collector to delete the pods
Apr 29 23:34:13.304: INFO: Deleting ReplicationController externalsvc took: 6.401042ms
Apr 29 23:34:13.604: INFO: Terminating ReplicationController externalsvc pods took: 300.139051ms
Apr 29 23:34:18.029: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:34:18.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7830" for this suite.
Apr 29 23:34:24.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:34:24.143: INFO: namespace services-7830 deletion completed in 6.092381372s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:16.335 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:34:24.143: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9302
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 29 23:34:24.293: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c148d54-511c-4924-9cd2-13d91ce7a7f5" in namespace "projected-9302" to be "success or failure"
Apr 29 23:34:24.295: INFO: Pod "downwardapi-volume-8c148d54-511c-4924-9cd2-13d91ce7a7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008742ms
Apr 29 23:34:26.298: INFO: Pod "downwardapi-volume-8c148d54-511c-4924-9cd2-13d91ce7a7f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00476158s
STEP: Saw pod success
Apr 29 23:34:26.298: INFO: Pod "downwardapi-volume-8c148d54-511c-4924-9cd2-13d91ce7a7f5" satisfied condition "success or failure"
Apr 29 23:34:26.300: INFO: Trying to get logs from node ip-192-168-82-130.us-west-2.compute.internal pod downwardapi-volume-8c148d54-511c-4924-9cd2-13d91ce7a7f5 container client-container: <nil>
STEP: delete the pod
Apr 29 23:34:26.320: INFO: Waiting for pod downwardapi-volume-8c148d54-511c-4924-9cd2-13d91ce7a7f5 to disappear
Apr 29 23:34:26.322: INFO: Pod downwardapi-volume-8c148d54-511c-4924-9cd2-13d91ce7a7f5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:34:26.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9302" for this suite.
Apr 29 23:34:32.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:34:32.419: INFO: namespace projected-9302 deletion completed in 6.093042429s

• [SLOW TEST:8.276 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:34:32.419: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3773
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-113e24f8-7fa3-45b8-9440-43195512d39e
STEP: Creating secret with name s-test-opt-upd-06d8a661-862f-448a-8a16-eff88e42217c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-113e24f8-7fa3-45b8-9440-43195512d39e
STEP: Updating secret s-test-opt-upd-06d8a661-862f-448a-8a16-eff88e42217c
STEP: Creating secret with name s-test-opt-create-ea619a8c-8d18-41c3-9258-282604c35f3c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:36:01.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3773" for this suite.
Apr 29 23:36:13.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:36:13.273: INFO: namespace secrets-3773 deletion completed in 12.208186146s

• [SLOW TEST:100.855 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:36:13.274: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr 29 23:36:13.424: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4188 /api/v1/namespaces/watch-4188/configmaps/e2e-watch-test-configmap-a 09c4d3d7-33b9-4df5-af35-0daa2bb3090c 13321 0 2020-04-29 23:36:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 29 23:36:13.424: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4188 /api/v1/namespaces/watch-4188/configmaps/e2e-watch-test-configmap-a 09c4d3d7-33b9-4df5-af35-0daa2bb3090c 13321 0 2020-04-29 23:36:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr 29 23:36:23.432: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4188 /api/v1/namespaces/watch-4188/configmaps/e2e-watch-test-configmap-a 09c4d3d7-33b9-4df5-af35-0daa2bb3090c 13351 0 2020-04-29 23:36:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr 29 23:36:23.432: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4188 /api/v1/namespaces/watch-4188/configmaps/e2e-watch-test-configmap-a 09c4d3d7-33b9-4df5-af35-0daa2bb3090c 13351 0 2020-04-29 23:36:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr 29 23:36:33.441: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4188 /api/v1/namespaces/watch-4188/configmaps/e2e-watch-test-configmap-a 09c4d3d7-33b9-4df5-af35-0daa2bb3090c 13376 0 2020-04-29 23:36:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 29 23:36:33.442: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4188 /api/v1/namespaces/watch-4188/configmaps/e2e-watch-test-configmap-a 09c4d3d7-33b9-4df5-af35-0daa2bb3090c 13376 0 2020-04-29 23:36:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr 29 23:36:43.451: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4188 /api/v1/namespaces/watch-4188/configmaps/e2e-watch-test-configmap-a 09c4d3d7-33b9-4df5-af35-0daa2bb3090c 13401 0 2020-04-29 23:36:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 29 23:36:43.451: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4188 /api/v1/namespaces/watch-4188/configmaps/e2e-watch-test-configmap-a 09c4d3d7-33b9-4df5-af35-0daa2bb3090c 13401 0 2020-04-29 23:36:13 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr 29 23:36:53.459: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4188 /api/v1/namespaces/watch-4188/configmaps/e2e-watch-test-configmap-b 0328ae25-9206-4b7f-8b94-4af28103df13 13427 0 2020-04-29 23:36:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 29 23:36:53.459: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4188 /api/v1/namespaces/watch-4188/configmaps/e2e-watch-test-configmap-b 0328ae25-9206-4b7f-8b94-4af28103df13 13427 0 2020-04-29 23:36:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr 29 23:37:03.469: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4188 /api/v1/namespaces/watch-4188/configmaps/e2e-watch-test-configmap-b 0328ae25-9206-4b7f-8b94-4af28103df13 13456 0 2020-04-29 23:36:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 29 23:37:03.469: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4188 /api/v1/namespaces/watch-4188/configmaps/e2e-watch-test-configmap-b 0328ae25-9206-4b7f-8b94-4af28103df13 13456 0 2020-04-29 23:36:53 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:37:13.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4188" for this suite.
Apr 29 23:37:19.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:37:19.565: INFO: namespace watch-4188 deletion completed in 6.091243448s

• [SLOW TEST:66.291 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:37:19.565: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4892
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Apr 29 23:37:19.727: INFO: Waiting up to 5m0s for pod "downward-api-85430bd1-b5fb-4db2-8929-0759a3453fe5" in namespace "downward-api-4892" to be "success or failure"
Apr 29 23:37:19.729: INFO: Pod "downward-api-85430bd1-b5fb-4db2-8929-0759a3453fe5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.873361ms
Apr 29 23:37:21.732: INFO: Pod "downward-api-85430bd1-b5fb-4db2-8929-0759a3453fe5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004678526s
STEP: Saw pod success
Apr 29 23:37:21.732: INFO: Pod "downward-api-85430bd1-b5fb-4db2-8929-0759a3453fe5" satisfied condition "success or failure"
Apr 29 23:37:21.734: INFO: Trying to get logs from node ip-192-168-82-130.us-west-2.compute.internal pod downward-api-85430bd1-b5fb-4db2-8929-0759a3453fe5 container dapi-container: <nil>
STEP: delete the pod
Apr 29 23:37:21.752: INFO: Waiting for pod downward-api-85430bd1-b5fb-4db2-8929-0759a3453fe5 to disappear
Apr 29 23:37:21.754: INFO: Pod downward-api-85430bd1-b5fb-4db2-8929-0759a3453fe5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:37:21.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4892" for this suite.
Apr 29 23:37:27.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:37:27.858: INFO: namespace downward-api-4892 deletion completed in 6.100179197s

• [SLOW TEST:8.293 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:37:27.858: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6521
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:37:28.010: INFO: Creating deployment "test-recreate-deployment"
Apr 29 23:37:28.014: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr 29 23:37:28.018: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Apr 29 23:37:30.024: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr 29 23:37:30.026: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800248, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800248, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800248, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800248, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 23:37:32.029: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800248, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800248, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800248, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800248, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 23:37:34.029: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800248, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800248, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800248, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800248, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 23:37:36.029: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr 29 23:37:36.035: INFO: Updating deployment test-recreate-deployment
Apr 29 23:37:36.035: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Apr 29 23:37:36.107: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6521 /apis/apps/v1/namespaces/deployment-6521/deployments/test-recreate-deployment 09ac17d0-82b5-4703-9e24-2bd9ca73d768 13616 2 2020-04-29 23:37:28 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00492bd48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-04-29 23:37:36 +0000 UTC,LastTransitionTime:2020-04-29 23:37:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-04-29 23:37:36 +0000 UTC,LastTransitionTime:2020-04-29 23:37:28 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Apr 29 23:37:36.110: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-6521 /apis/apps/v1/namespaces/deployment-6521/replicasets/test-recreate-deployment-5f94c574ff 071e1251-e821-4002-aaef-b6aa0727266a 13615 1 2020-04-29 23:37:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 09ac17d0-82b5-4703-9e24-2bd9ca73d768 0xc00274e2e7 0xc00274e2e8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00274e358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 23:37:36.110: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr 29 23:37:36.110: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-6521 /apis/apps/v1/namespaces/deployment-6521/replicasets/test-recreate-deployment-68fc85c7bb 3899cbd5-6983-4d6e-b1c3-d111e376244f 13603 2 2020-04-29 23:37:28 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 09ac17d0-82b5-4703-9e24-2bd9ca73d768 0xc00274e3c7 0xc00274e3c8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00274e438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 23:37:36.112: INFO: Pod "test-recreate-deployment-5f94c574ff-4ktzs" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-4ktzs test-recreate-deployment-5f94c574ff- deployment-6521 /api/v1/namespaces/deployment-6521/pods/test-recreate-deployment-5f94c574ff-4ktzs 6eb5641c-a39c-4d6b-aa95-e5c6ae1d1e91 13614 0 2020-04-29 23:37:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 071e1251-e821-4002-aaef-b6aa0727266a 0xc00274e8e7 0xc00274e8e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zrqfl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zrqfl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zrqfl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-144-27.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:37:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:37:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:37:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:37:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.144.27,PodIP:,StartTime:2020-04-29 23:37:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:37:36.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6521" for this suite.
Apr 29 23:37:42.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:37:42.214: INFO: namespace deployment-6521 deletion completed in 6.098563498s

• [SLOW TEST:14.356 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:37:42.215: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7837
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:37:47.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7837" for this suite.
Apr 29 23:37:53.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:37:53.356: INFO: namespace watch-7837 deletion completed in 6.186254853s

• [SLOW TEST:11.142 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:37:53.356: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-j95p
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 23:37:53.542: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-j95p" in namespace "subpath-3310" to be "success or failure"
Apr 29 23:37:53.544: INFO: Pod "pod-subpath-test-configmap-j95p": Phase="Pending", Reason="", readiness=false. Elapsed: 1.901908ms
Apr 29 23:37:55.547: INFO: Pod "pod-subpath-test-configmap-j95p": Phase="Running", Reason="", readiness=true. Elapsed: 2.004670169s
Apr 29 23:37:57.549: INFO: Pod "pod-subpath-test-configmap-j95p": Phase="Running", Reason="", readiness=true. Elapsed: 4.007645777s
Apr 29 23:37:59.552: INFO: Pod "pod-subpath-test-configmap-j95p": Phase="Running", Reason="", readiness=true. Elapsed: 6.010625262s
Apr 29 23:38:01.555: INFO: Pod "pod-subpath-test-configmap-j95p": Phase="Running", Reason="", readiness=true. Elapsed: 8.013606976s
Apr 29 23:38:03.559: INFO: Pod "pod-subpath-test-configmap-j95p": Phase="Running", Reason="", readiness=true. Elapsed: 10.0167637s
Apr 29 23:38:05.562: INFO: Pod "pod-subpath-test-configmap-j95p": Phase="Running", Reason="", readiness=true. Elapsed: 12.019717438s
Apr 29 23:38:07.564: INFO: Pod "pod-subpath-test-configmap-j95p": Phase="Running", Reason="", readiness=true. Elapsed: 14.022631686s
Apr 29 23:38:09.567: INFO: Pod "pod-subpath-test-configmap-j95p": Phase="Running", Reason="", readiness=true. Elapsed: 16.025508512s
Apr 29 23:38:11.570: INFO: Pod "pod-subpath-test-configmap-j95p": Phase="Running", Reason="", readiness=true. Elapsed: 18.028433871s
Apr 29 23:38:13.573: INFO: Pod "pod-subpath-test-configmap-j95p": Phase="Running", Reason="", readiness=true. Elapsed: 20.031235595s
Apr 29 23:38:15.576: INFO: Pod "pod-subpath-test-configmap-j95p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.034268017s
STEP: Saw pod success
Apr 29 23:38:15.576: INFO: Pod "pod-subpath-test-configmap-j95p" satisfied condition "success or failure"
Apr 29 23:38:15.578: INFO: Trying to get logs from node ip-192-168-72-199.us-west-2.compute.internal pod pod-subpath-test-configmap-j95p container test-container-subpath-configmap-j95p: <nil>
STEP: delete the pod
Apr 29 23:38:15.601: INFO: Waiting for pod pod-subpath-test-configmap-j95p to disappear
Apr 29 23:38:15.603: INFO: Pod pod-subpath-test-configmap-j95p no longer exists
STEP: Deleting pod pod-subpath-test-configmap-j95p
Apr 29 23:38:15.603: INFO: Deleting pod "pod-subpath-test-configmap-j95p" in namespace "subpath-3310"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:38:15.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3310" for this suite.
Apr 29 23:38:21.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:38:21.706: INFO: namespace subpath-3310 deletion completed in 6.097926495s

• [SLOW TEST:28.350 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:38:21.706: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9647
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr 29 23:38:21.854: INFO: Pod name pod-release: Found 0 pods out of 1
Apr 29 23:38:26.857: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:38:27.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9647" for this suite.
Apr 29 23:38:33.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:38:33.964: INFO: namespace replication-controller-9647 deletion completed in 6.091569056s

• [SLOW TEST:12.258 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:38:33.964: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8753
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 29 23:38:34.151: INFO: Number of nodes with available pods: 0
Apr 29 23:38:34.151: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:38:35.159: INFO: Number of nodes with available pods: 0
Apr 29 23:38:35.159: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:38:36.158: INFO: Number of nodes with available pods: 8
Apr 29 23:38:36.158: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:38:37.158: INFO: Number of nodes with available pods: 10
Apr 29 23:38:37.158: INFO: Number of running nodes: 10, number of available pods: 10
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr 29 23:38:37.176: INFO: Number of nodes with available pods: 10
Apr 29 23:38:37.176: INFO: Number of running nodes: 10, number of available pods: 10
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8753, will wait for the garbage collector to delete the pods
Apr 29 23:38:38.248: INFO: Deleting DaemonSet.extensions daemon-set took: 9.669092ms
Apr 29 23:38:38.348: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.138814ms
Apr 29 23:38:51.350: INFO: Number of nodes with available pods: 0
Apr 29 23:38:51.350: INFO: Number of running nodes: 0, number of available pods: 0
Apr 29 23:38:51.354: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8753/daemonsets","resourceVersion":"14217"},"items":null}

Apr 29 23:38:51.356: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8753/pods","resourceVersion":"14217"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:38:51.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8753" for this suite.
Apr 29 23:38:57.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:38:57.476: INFO: namespace daemonsets-8753 deletion completed in 6.089777395s

• [SLOW TEST:23.512 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:38:57.476: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4409
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 23:38:57.920: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 23:39:00.939: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:39:01.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4409" for this suite.
Apr 29 23:39:07.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:39:07.118: INFO: namespace webhook-4409 deletion completed in 6.103547496s
STEP: Destroying namespace "webhook-4409-markers" for this suite.
Apr 29 23:39:13.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:39:13.215: INFO: namespace webhook-4409-markers deletion completed in 6.096753242s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.757 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:39:13.233: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7160
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-cf543f8a-afd1-4b3c-a44e-40f02b1f91a4
STEP: Creating secret with name secret-projected-all-test-volume-04985c84-e063-45ee-b6f8-de4308373443
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr 29 23:39:13.405: INFO: Waiting up to 5m0s for pod "projected-volume-767d7585-a642-4e7b-a4c9-69b5eb0aa92b" in namespace "projected-7160" to be "success or failure"
Apr 29 23:39:13.407: INFO: Pod "projected-volume-767d7585-a642-4e7b-a4c9-69b5eb0aa92b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.953545ms
Apr 29 23:39:15.410: INFO: Pod "projected-volume-767d7585-a642-4e7b-a4c9-69b5eb0aa92b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004574159s
Apr 29 23:39:17.412: INFO: Pod "projected-volume-767d7585-a642-4e7b-a4c9-69b5eb0aa92b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006981087s
STEP: Saw pod success
Apr 29 23:39:17.412: INFO: Pod "projected-volume-767d7585-a642-4e7b-a4c9-69b5eb0aa92b" satisfied condition "success or failure"
Apr 29 23:39:17.414: INFO: Trying to get logs from node ip-192-168-149-254.us-west-2.compute.internal pod projected-volume-767d7585-a642-4e7b-a4c9-69b5eb0aa92b container projected-all-volume-test: <nil>
STEP: delete the pod
Apr 29 23:39:17.434: INFO: Waiting for pod projected-volume-767d7585-a642-4e7b-a4c9-69b5eb0aa92b to disappear
Apr 29 23:39:17.436: INFO: Pod projected-volume-767d7585-a642-4e7b-a4c9-69b5eb0aa92b no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:39:17.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7160" for this suite.
Apr 29 23:39:23.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:39:23.532: INFO: namespace projected-7160 deletion completed in 6.0920584s

• [SLOW TEST:10.298 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:39:23.532: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-466
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-466.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-466.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 29 23:39:33.709: INFO: DNS probes using dns-466/dns-test-d19b88f5-5541-4bff-8863-339ced65144e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:39:33.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-466" for this suite.
Apr 29 23:39:39.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:39:39.814: INFO: namespace dns-466 deletion completed in 6.091748331s

• [SLOW TEST:16.282 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:39:39.814: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5019
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-985656f2-3247-498c-81fc-8ef03b338652
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-985656f2-3247-498c-81fc-8ef03b338652
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:39:44.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5019" for this suite.
Apr 29 23:40:08.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:40:08.120: INFO: namespace projected-5019 deletion completed in 24.108303779s

• [SLOW TEST:28.305 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:40:08.120: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4620
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:40:19.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4620" for this suite.
Apr 29 23:40:25.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:40:25.394: INFO: namespace resourcequota-4620 deletion completed in 6.095822272s

• [SLOW TEST:17.274 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:40:25.394: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8116
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 23:40:26.241: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 29 23:40:28.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800426, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800426, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800426, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800426, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 23:40:31.261: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:40:31.265: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1515-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:40:32.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8116" for this suite.
Apr 29 23:40:38.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:40:38.466: INFO: namespace webhook-8116 deletion completed in 6.09059407s
STEP: Destroying namespace "webhook-8116-markers" for this suite.
Apr 29 23:40:44.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:40:44.556: INFO: namespace webhook-8116-markers deletion completed in 6.089324155s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:19.179 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:40:44.574: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-243
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-f3af6312-9050-49ac-b2ee-0eccbaeac8d3
STEP: Creating a pod to test consume configMaps
Apr 29 23:40:44.729: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bbc768e8-1368-49da-b365-4f912bc03934" in namespace "projected-243" to be "success or failure"
Apr 29 23:40:44.731: INFO: Pod "pod-projected-configmaps-bbc768e8-1368-49da-b365-4f912bc03934": Phase="Pending", Reason="", readiness=false. Elapsed: 1.871586ms
Apr 29 23:40:46.733: INFO: Pod "pod-projected-configmaps-bbc768e8-1368-49da-b365-4f912bc03934": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004674047s
STEP: Saw pod success
Apr 29 23:40:46.733: INFO: Pod "pod-projected-configmaps-bbc768e8-1368-49da-b365-4f912bc03934" satisfied condition "success or failure"
Apr 29 23:40:46.736: INFO: Trying to get logs from node ip-192-168-72-199.us-west-2.compute.internal pod pod-projected-configmaps-bbc768e8-1368-49da-b365-4f912bc03934 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 23:40:46.754: INFO: Waiting for pod pod-projected-configmaps-bbc768e8-1368-49da-b365-4f912bc03934 to disappear
Apr 29 23:40:46.755: INFO: Pod pod-projected-configmaps-bbc768e8-1368-49da-b365-4f912bc03934 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:40:46.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-243" for this suite.
Apr 29 23:40:52.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:40:52.850: INFO: namespace projected-243 deletion completed in 6.09107539s

• [SLOW TEST:8.277 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:40:52.850: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2480
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 29 23:40:52.999: INFO: Waiting up to 5m0s for pod "pod-48c3025e-baf5-4470-9231-6414fe230817" in namespace "emptydir-2480" to be "success or failure"
Apr 29 23:40:53.001: INFO: Pod "pod-48c3025e-baf5-4470-9231-6414fe230817": Phase="Pending", Reason="", readiness=false. Elapsed: 1.830963ms
Apr 29 23:40:55.004: INFO: Pod "pod-48c3025e-baf5-4470-9231-6414fe230817": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004623718s
STEP: Saw pod success
Apr 29 23:40:55.004: INFO: Pod "pod-48c3025e-baf5-4470-9231-6414fe230817" satisfied condition "success or failure"
Apr 29 23:40:55.006: INFO: Trying to get logs from node ip-192-168-72-199.us-west-2.compute.internal pod pod-48c3025e-baf5-4470-9231-6414fe230817 container test-container: <nil>
STEP: delete the pod
Apr 29 23:40:55.021: INFO: Waiting for pod pod-48c3025e-baf5-4470-9231-6414fe230817 to disappear
Apr 29 23:40:55.023: INFO: Pod pod-48c3025e-baf5-4470-9231-6414fe230817 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:40:55.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2480" for this suite.
Apr 29 23:41:01.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:41:01.125: INFO: namespace emptydir-2480 deletion completed in 6.097914452s

• [SLOW TEST:8.274 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:41:01.125: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2111
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Apr 29 23:41:01.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 create -f - --namespace=kubectl-2111'
Apr 29 23:41:01.500: INFO: stderr: ""
Apr 29 23:41:01.500: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 23:41:01.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2111'
Apr 29 23:41:01.553: INFO: stderr: ""
Apr 29 23:41:01.553: INFO: stdout: "update-demo-nautilus-5h46n update-demo-nautilus-5lgxb "
Apr 29 23:41:01.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-5h46n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2111'
Apr 29 23:41:01.604: INFO: stderr: ""
Apr 29 23:41:01.604: INFO: stdout: ""
Apr 29 23:41:01.604: INFO: update-demo-nautilus-5h46n is created but not running
Apr 29 23:41:06.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2111'
Apr 29 23:41:06.668: INFO: stderr: ""
Apr 29 23:41:06.668: INFO: stdout: "update-demo-nautilus-5h46n update-demo-nautilus-5lgxb "
Apr 29 23:41:06.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-5h46n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2111'
Apr 29 23:41:06.721: INFO: stderr: ""
Apr 29 23:41:06.721: INFO: stdout: "true"
Apr 29 23:41:06.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-5h46n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2111'
Apr 29 23:41:06.773: INFO: stderr: ""
Apr 29 23:41:06.773: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 23:41:06.773: INFO: validating pod update-demo-nautilus-5h46n
Apr 29 23:41:06.779: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 23:41:06.779: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 23:41:06.779: INFO: update-demo-nautilus-5h46n is verified up and running
Apr 29 23:41:06.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-5lgxb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2111'
Apr 29 23:41:06.833: INFO: stderr: ""
Apr 29 23:41:06.833: INFO: stdout: "true"
Apr 29 23:41:06.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-5lgxb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2111'
Apr 29 23:41:06.885: INFO: stderr: ""
Apr 29 23:41:06.885: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 23:41:06.885: INFO: validating pod update-demo-nautilus-5lgxb
Apr 29 23:41:06.889: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 23:41:06.889: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 23:41:06.889: INFO: update-demo-nautilus-5lgxb is verified up and running
STEP: rolling-update to new replication controller
Apr 29 23:41:06.890: INFO: scanned /root for discovery docs: <nil>
Apr 29 23:41:06.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2111'
Apr 29 23:41:29.185: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr 29 23:41:29.185: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 23:41:29.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2111'
Apr 29 23:41:29.240: INFO: stderr: ""
Apr 29 23:41:29.240: INFO: stdout: "update-demo-kitten-mvxrv update-demo-kitten-n2wxg "
Apr 29 23:41:29.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-kitten-mvxrv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2111'
Apr 29 23:41:29.290: INFO: stderr: ""
Apr 29 23:41:29.290: INFO: stdout: "true"
Apr 29 23:41:29.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-kitten-mvxrv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2111'
Apr 29 23:41:29.352: INFO: stderr: ""
Apr 29 23:41:29.352: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr 29 23:41:29.352: INFO: validating pod update-demo-kitten-mvxrv
Apr 29 23:41:29.357: INFO: got data: {
  "image": "kitten.jpg"
}

Apr 29 23:41:29.357: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr 29 23:41:29.357: INFO: update-demo-kitten-mvxrv is verified up and running
Apr 29 23:41:29.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-kitten-n2wxg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2111'
Apr 29 23:41:29.409: INFO: stderr: ""
Apr 29 23:41:29.409: INFO: stdout: "true"
Apr 29 23:41:29.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-kitten-n2wxg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2111'
Apr 29 23:41:29.458: INFO: stderr: ""
Apr 29 23:41:29.458: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr 29 23:41:29.458: INFO: validating pod update-demo-kitten-n2wxg
Apr 29 23:41:29.462: INFO: got data: {
  "image": "kitten.jpg"
}

Apr 29 23:41:29.462: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr 29 23:41:29.462: INFO: update-demo-kitten-n2wxg is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:41:29.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2111" for this suite.
Apr 29 23:41:57.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:41:57.557: INFO: namespace kubectl-2111 deletion completed in 28.091120037s

• [SLOW TEST:56.432 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:41:57.557: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4730
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Apr 29 23:41:57.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 create -f - --namespace=kubectl-4730'
Apr 29 23:41:57.847: INFO: stderr: ""
Apr 29 23:41:57.848: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 29 23:41:57.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4730'
Apr 29 23:41:57.904: INFO: stderr: ""
Apr 29 23:41:57.904: INFO: stdout: "update-demo-nautilus-bt8vg update-demo-nautilus-x4vb9 "
Apr 29 23:41:57.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-bt8vg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4730'
Apr 29 23:41:57.954: INFO: stderr: ""
Apr 29 23:41:57.954: INFO: stdout: ""
Apr 29 23:41:57.955: INFO: update-demo-nautilus-bt8vg is created but not running
Apr 29 23:42:02.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4730'
Apr 29 23:42:03.012: INFO: stderr: ""
Apr 29 23:42:03.012: INFO: stdout: "update-demo-nautilus-bt8vg update-demo-nautilus-x4vb9 "
Apr 29 23:42:03.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-bt8vg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4730'
Apr 29 23:42:03.066: INFO: stderr: ""
Apr 29 23:42:03.066: INFO: stdout: "true"
Apr 29 23:42:03.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-bt8vg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4730'
Apr 29 23:42:03.118: INFO: stderr: ""
Apr 29 23:42:03.118: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 23:42:03.118: INFO: validating pod update-demo-nautilus-bt8vg
Apr 29 23:42:03.122: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 23:42:03.122: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 23:42:03.122: INFO: update-demo-nautilus-bt8vg is verified up and running
Apr 29 23:42:03.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-x4vb9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4730'
Apr 29 23:42:03.173: INFO: stderr: ""
Apr 29 23:42:03.173: INFO: stdout: "true"
Apr 29 23:42:03.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-x4vb9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4730'
Apr 29 23:42:03.224: INFO: stderr: ""
Apr 29 23:42:03.224: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 29 23:42:03.224: INFO: validating pod update-demo-nautilus-x4vb9
Apr 29 23:42:03.228: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 29 23:42:03.228: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 29 23:42:03.228: INFO: update-demo-nautilus-x4vb9 is verified up and running
STEP: using delete to clean up resources
Apr 29 23:42:03.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete --grace-period=0 --force -f - --namespace=kubectl-4730'
Apr 29 23:42:03.283: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 23:42:03.283: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 29 23:42:03.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4730'
Apr 29 23:42:03.342: INFO: stderr: "No resources found in kubectl-4730 namespace.\n"
Apr 29 23:42:03.342: INFO: stdout: ""
Apr 29 23:42:03.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -l name=update-demo --namespace=kubectl-4730 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 29 23:42:03.393: INFO: stderr: ""
Apr 29 23:42:03.393: INFO: stdout: "update-demo-nautilus-bt8vg\nupdate-demo-nautilus-x4vb9\n"
Apr 29 23:42:03.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4730'
Apr 29 23:42:03.952: INFO: stderr: "No resources found in kubectl-4730 namespace.\n"
Apr 29 23:42:03.952: INFO: stdout: ""
Apr 29 23:42:03.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -l name=update-demo --namespace=kubectl-4730 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 29 23:42:04.013: INFO: stderr: ""
Apr 29 23:42:04.013: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:42:04.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4730" for this suite.
Apr 29 23:42:10.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:42:10.140: INFO: namespace kubectl-4730 deletion completed in 6.123246031s

• [SLOW TEST:12.583 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:42:10.140: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9764
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-a3b947c6-ff1d-425d-a6ee-175e81be41b7
STEP: Creating a pod to test consume secrets
Apr 29 23:42:10.316: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b4190c16-7e24-436f-943e-144a87a0883e" in namespace "projected-9764" to be "success or failure"
Apr 29 23:42:10.330: INFO: Pod "pod-projected-secrets-b4190c16-7e24-436f-943e-144a87a0883e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.737915ms
Apr 29 23:42:12.332: INFO: Pod "pod-projected-secrets-b4190c16-7e24-436f-943e-144a87a0883e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016519445s
STEP: Saw pod success
Apr 29 23:42:12.332: INFO: Pod "pod-projected-secrets-b4190c16-7e24-436f-943e-144a87a0883e" satisfied condition "success or failure"
Apr 29 23:42:12.335: INFO: Trying to get logs from node ip-192-168-148-117.us-west-2.compute.internal pod pod-projected-secrets-b4190c16-7e24-436f-943e-144a87a0883e container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 29 23:42:12.359: INFO: Waiting for pod pod-projected-secrets-b4190c16-7e24-436f-943e-144a87a0883e to disappear
Apr 29 23:42:12.361: INFO: Pod pod-projected-secrets-b4190c16-7e24-436f-943e-144a87a0883e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:42:12.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9764" for this suite.
Apr 29 23:42:18.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:42:18.465: INFO: namespace projected-9764 deletion completed in 6.100030812s

• [SLOW TEST:8.325 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:42:18.465: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5238
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:42:20.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5238" for this suite.
Apr 29 23:42:28.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:42:28.735: INFO: namespace containers-5238 deletion completed in 8.098108144s

• [SLOW TEST:10.269 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:42:28.735: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 29 23:42:28.886: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0de97c00-fc7b-40e0-a698-2021b86f72bf" in namespace "projected-3158" to be "success or failure"
Apr 29 23:42:28.888: INFO: Pod "downwardapi-volume-0de97c00-fc7b-40e0-a698-2021b86f72bf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.899816ms
Apr 29 23:42:30.891: INFO: Pod "downwardapi-volume-0de97c00-fc7b-40e0-a698-2021b86f72bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004702554s
STEP: Saw pod success
Apr 29 23:42:30.891: INFO: Pod "downwardapi-volume-0de97c00-fc7b-40e0-a698-2021b86f72bf" satisfied condition "success or failure"
Apr 29 23:42:30.893: INFO: Trying to get logs from node ip-192-168-148-117.us-west-2.compute.internal pod downwardapi-volume-0de97c00-fc7b-40e0-a698-2021b86f72bf container client-container: <nil>
STEP: delete the pod
Apr 29 23:42:30.909: INFO: Waiting for pod downwardapi-volume-0de97c00-fc7b-40e0-a698-2021b86f72bf to disappear
Apr 29 23:42:30.911: INFO: Pod downwardapi-volume-0de97c00-fc7b-40e0-a698-2021b86f72bf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:42:30.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3158" for this suite.
Apr 29 23:42:36.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:42:37.008: INFO: namespace projected-3158 deletion completed in 6.093459647s

• [SLOW TEST:8.274 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:42:37.008: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6505
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-fe766f7f-4d86-477f-9a65-2b81d18f45cb
STEP: Creating a pod to test consume configMaps
Apr 29 23:42:37.166: INFO: Waiting up to 5m0s for pod "pod-configmaps-a97a6498-ff7b-4a39-aaa8-dc6f1aa50e30" in namespace "configmap-6505" to be "success or failure"
Apr 29 23:42:37.169: INFO: Pod "pod-configmaps-a97a6498-ff7b-4a39-aaa8-dc6f1aa50e30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.441697ms
Apr 29 23:42:39.172: INFO: Pod "pod-configmaps-a97a6498-ff7b-4a39-aaa8-dc6f1aa50e30": Phase="Running", Reason="", readiness=true. Elapsed: 2.005223903s
Apr 29 23:42:41.174: INFO: Pod "pod-configmaps-a97a6498-ff7b-4a39-aaa8-dc6f1aa50e30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00809267s
STEP: Saw pod success
Apr 29 23:42:41.174: INFO: Pod "pod-configmaps-a97a6498-ff7b-4a39-aaa8-dc6f1aa50e30" satisfied condition "success or failure"
Apr 29 23:42:41.177: INFO: Trying to get logs from node ip-192-168-148-117.us-west-2.compute.internal pod pod-configmaps-a97a6498-ff7b-4a39-aaa8-dc6f1aa50e30 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 23:42:41.193: INFO: Waiting for pod pod-configmaps-a97a6498-ff7b-4a39-aaa8-dc6f1aa50e30 to disappear
Apr 29 23:42:41.195: INFO: Pod pod-configmaps-a97a6498-ff7b-4a39-aaa8-dc6f1aa50e30 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:42:41.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6505" for this suite.
Apr 29 23:42:47.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:42:47.291: INFO: namespace configmap-6505 deletion completed in 6.092641198s

• [SLOW TEST:10.282 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:42:47.291: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-242
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:42:51.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-242" for this suite.
Apr 29 23:42:57.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:42:57.611: INFO: namespace emptydir-wrapper-242 deletion completed in 6.093615525s

• [SLOW TEST:10.320 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:42:57.611: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4431
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:42:57.753: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 29 23:43:00.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-4431 create -f -'
Apr 29 23:43:00.616: INFO: stderr: ""
Apr 29 23:43:00.616: INFO: stdout: "e2e-test-crd-publish-openapi-5278-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 29 23:43:00.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-4431 delete e2e-test-crd-publish-openapi-5278-crds test-cr'
Apr 29 23:43:00.675: INFO: stderr: ""
Apr 29 23:43:00.675: INFO: stdout: "e2e-test-crd-publish-openapi-5278-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Apr 29 23:43:00.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-4431 apply -f -'
Apr 29 23:43:00.796: INFO: stderr: ""
Apr 29 23:43:00.796: INFO: stdout: "e2e-test-crd-publish-openapi-5278-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 29 23:43:00.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-4431 delete e2e-test-crd-publish-openapi-5278-crds test-cr'
Apr 29 23:43:00.891: INFO: stderr: ""
Apr 29 23:43:00.891: INFO: stdout: "e2e-test-crd-publish-openapi-5278-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr 29 23:43:00.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 explain e2e-test-crd-publish-openapi-5278-crds'
Apr 29 23:43:01.036: INFO: stderr: ""
Apr 29 23:43:01.036: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5278-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:43:03.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4431" for this suite.
Apr 29 23:43:09.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:43:09.743: INFO: namespace crd-publish-openapi-4431 deletion completed in 6.098344638s

• [SLOW TEST:12.132 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:43:09.743: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7989
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-d8nwm in namespace proxy-7989
I0429 23:43:09.932097      21 runners.go:184] Created replication controller with name: proxy-service-d8nwm, namespace: proxy-7989, replica count: 1
I0429 23:43:10.982375      21 runners.go:184] proxy-service-d8nwm Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0429 23:43:11.982515      21 runners.go:184] proxy-service-d8nwm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0429 23:43:12.982666      21 runners.go:184] proxy-service-d8nwm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0429 23:43:13.982812      21 runners.go:184] proxy-service-d8nwm Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0429 23:43:14.982945      21 runners.go:184] proxy-service-d8nwm Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 23:43:14.987: INFO: setup took 5.100485311s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr 29 23:43:14.994: INFO: (0) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 7.183051ms)
Apr 29 23:43:14.994: INFO: (0) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 7.149704ms)
Apr 29 23:43:14.994: INFO: (0) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 7.366761ms)
Apr 29 23:43:14.994: INFO: (0) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 7.174569ms)
Apr 29 23:43:14.994: INFO: (0) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 7.232657ms)
Apr 29 23:43:14.994: INFO: (0) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 7.338236ms)
Apr 29 23:43:14.994: INFO: (0) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 7.331752ms)
Apr 29 23:43:14.994: INFO: (0) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 7.693361ms)
Apr 29 23:43:14.994: INFO: (0) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 7.661921ms)
Apr 29 23:43:14.995: INFO: (0) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 7.882988ms)
Apr 29 23:43:14.996: INFO: (0) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 8.982377ms)
Apr 29 23:43:14.997: INFO: (0) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 9.863433ms)
Apr 29 23:43:14.997: INFO: (0) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 9.889415ms)
Apr 29 23:43:14.999: INFO: (0) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 11.865567ms)
Apr 29 23:43:14.999: INFO: (0) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 12.199617ms)
Apr 29 23:43:14.999: INFO: (0) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 12.177848ms)
Apr 29 23:43:15.002: INFO: (1) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 3.190778ms)
Apr 29 23:43:15.002: INFO: (1) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 3.274354ms)
Apr 29 23:43:15.003: INFO: (1) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.414913ms)
Apr 29 23:43:15.004: INFO: (1) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 4.0323ms)
Apr 29 23:43:15.004: INFO: (1) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 3.92427ms)
Apr 29 23:43:15.004: INFO: (1) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 4.128002ms)
Apr 29 23:43:15.004: INFO: (1) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 3.910285ms)
Apr 29 23:43:15.004: INFO: (1) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 3.832374ms)
Apr 29 23:43:15.004: INFO: (1) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 3.790822ms)
Apr 29 23:43:15.004: INFO: (1) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 4.789493ms)
Apr 29 23:43:15.007: INFO: (1) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 8.186444ms)
Apr 29 23:43:15.008: INFO: (1) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 7.982337ms)
Apr 29 23:43:15.008: INFO: (1) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 8.038219ms)
Apr 29 23:43:15.008: INFO: (1) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 8.342069ms)
Apr 29 23:43:15.008: INFO: (1) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.944445ms)
Apr 29 23:43:15.008: INFO: (1) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 7.992227ms)
Apr 29 23:43:15.011: INFO: (2) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 3.199444ms)
Apr 29 23:43:15.012: INFO: (2) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 3.747225ms)
Apr 29 23:43:15.012: INFO: (2) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 3.936336ms)
Apr 29 23:43:15.012: INFO: (2) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 4.375758ms)
Apr 29 23:43:15.012: INFO: (2) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 4.037794ms)
Apr 29 23:43:15.012: INFO: (2) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 4.52594ms)
Apr 29 23:43:15.012: INFO: (2) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 4.489223ms)
Apr 29 23:43:15.012: INFO: (2) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.976375ms)
Apr 29 23:43:15.012: INFO: (2) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 4.377383ms)
Apr 29 23:43:15.013: INFO: (2) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 4.041882ms)
Apr 29 23:43:15.014: INFO: (2) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 5.929364ms)
Apr 29 23:43:15.014: INFO: (2) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 6.135457ms)
Apr 29 23:43:15.015: INFO: (2) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 6.755056ms)
Apr 29 23:43:15.015: INFO: (2) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 7.21063ms)
Apr 29 23:43:15.015: INFO: (2) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 7.180632ms)
Apr 29 23:43:15.015: INFO: (2) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 7.096288ms)
Apr 29 23:43:15.020: INFO: (3) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 4.259403ms)
Apr 29 23:43:15.020: INFO: (3) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 4.217843ms)
Apr 29 23:43:15.020: INFO: (3) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 4.30818ms)
Apr 29 23:43:15.020: INFO: (3) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 4.396641ms)
Apr 29 23:43:15.020: INFO: (3) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 4.515549ms)
Apr 29 23:43:15.020: INFO: (3) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 4.87102ms)
Apr 29 23:43:15.020: INFO: (3) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 4.679514ms)
Apr 29 23:43:15.020: INFO: (3) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 4.527389ms)
Apr 29 23:43:15.021: INFO: (3) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 5.102828ms)
Apr 29 23:43:15.021: INFO: (3) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 4.895792ms)
Apr 29 23:43:15.022: INFO: (3) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 5.70159ms)
Apr 29 23:43:15.023: INFO: (3) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 7.313138ms)
Apr 29 23:43:15.023: INFO: (3) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 7.62874ms)
Apr 29 23:43:15.023: INFO: (3) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 7.725311ms)
Apr 29 23:43:15.024: INFO: (3) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.415954ms)
Apr 29 23:43:15.024: INFO: (3) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 7.503355ms)
Apr 29 23:43:15.027: INFO: (4) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 3.264867ms)
Apr 29 23:43:15.027: INFO: (4) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.415646ms)
Apr 29 23:43:15.027: INFO: (4) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.465466ms)
Apr 29 23:43:15.028: INFO: (4) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.684092ms)
Apr 29 23:43:15.028: INFO: (4) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 3.603099ms)
Apr 29 23:43:15.028: INFO: (4) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 3.518886ms)
Apr 29 23:43:15.028: INFO: (4) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.725197ms)
Apr 29 23:43:15.028: INFO: (4) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 3.838466ms)
Apr 29 23:43:15.028: INFO: (4) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 3.910474ms)
Apr 29 23:43:15.029: INFO: (4) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 4.095694ms)
Apr 29 23:43:15.029: INFO: (4) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 5.594387ms)
Apr 29 23:43:15.030: INFO: (4) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 5.878721ms)
Apr 29 23:43:15.031: INFO: (4) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 6.410278ms)
Apr 29 23:43:15.031: INFO: (4) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 6.940912ms)
Apr 29 23:43:15.031: INFO: (4) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 7.016669ms)
Apr 29 23:43:15.031: INFO: (4) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.007624ms)
Apr 29 23:43:15.034: INFO: (5) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 2.922632ms)
Apr 29 23:43:15.035: INFO: (5) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 3.507689ms)
Apr 29 23:43:15.035: INFO: (5) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 3.742095ms)
Apr 29 23:43:15.035: INFO: (5) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 3.670357ms)
Apr 29 23:43:15.036: INFO: (5) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 3.972551ms)
Apr 29 23:43:15.036: INFO: (5) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.966492ms)
Apr 29 23:43:15.036: INFO: (5) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 4.052874ms)
Apr 29 23:43:15.036: INFO: (5) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 4.377884ms)
Apr 29 23:43:15.036: INFO: (5) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 4.618484ms)
Apr 29 23:43:15.036: INFO: (5) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 4.812676ms)
Apr 29 23:43:15.037: INFO: (5) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 5.929606ms)
Apr 29 23:43:15.037: INFO: (5) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 5.938317ms)
Apr 29 23:43:15.039: INFO: (5) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 6.939303ms)
Apr 29 23:43:15.039: INFO: (5) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.2041ms)
Apr 29 23:43:15.039: INFO: (5) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 7.438454ms)
Apr 29 23:43:15.039: INFO: (5) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 7.83504ms)
Apr 29 23:43:15.042: INFO: (6) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 3.024896ms)
Apr 29 23:43:15.043: INFO: (6) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 3.682887ms)
Apr 29 23:43:15.043: INFO: (6) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.66754ms)
Apr 29 23:43:15.043: INFO: (6) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 3.880284ms)
Apr 29 23:43:15.043: INFO: (6) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.563616ms)
Apr 29 23:43:15.044: INFO: (6) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 3.946086ms)
Apr 29 23:43:15.044: INFO: (6) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 4.099524ms)
Apr 29 23:43:15.044: INFO: (6) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 4.395955ms)
Apr 29 23:43:15.044: INFO: (6) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 4.688675ms)
Apr 29 23:43:15.044: INFO: (6) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 4.583691ms)
Apr 29 23:43:15.045: INFO: (6) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 5.647969ms)
Apr 29 23:43:15.047: INFO: (6) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 6.801055ms)
Apr 29 23:43:15.047: INFO: (6) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.113895ms)
Apr 29 23:43:15.047: INFO: (6) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 6.835975ms)
Apr 29 23:43:15.047: INFO: (6) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 7.129819ms)
Apr 29 23:43:15.047: INFO: (6) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 7.284796ms)
Apr 29 23:43:15.050: INFO: (7) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 3.176766ms)
Apr 29 23:43:15.051: INFO: (7) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 3.636531ms)
Apr 29 23:43:15.051: INFO: (7) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.724212ms)
Apr 29 23:43:15.051: INFO: (7) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 4.080499ms)
Apr 29 23:43:15.051: INFO: (7) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 4.073097ms)
Apr 29 23:43:15.051: INFO: (7) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 4.132086ms)
Apr 29 23:43:15.051: INFO: (7) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 4.216446ms)
Apr 29 23:43:15.052: INFO: (7) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 4.211138ms)
Apr 29 23:43:15.052: INFO: (7) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 4.246281ms)
Apr 29 23:43:15.052: INFO: (7) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 4.459971ms)
Apr 29 23:43:15.053: INFO: (7) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 5.664497ms)
Apr 29 23:43:15.054: INFO: (7) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 6.887866ms)
Apr 29 23:43:15.054: INFO: (7) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 6.878057ms)
Apr 29 23:43:15.054: INFO: (7) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 6.968573ms)
Apr 29 23:43:15.055: INFO: (7) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.247643ms)
Apr 29 23:43:15.055: INFO: (7) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 7.270808ms)
Apr 29 23:43:15.058: INFO: (8) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 3.320311ms)
Apr 29 23:43:15.058: INFO: (8) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 3.471509ms)
Apr 29 23:43:15.059: INFO: (8) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 3.561913ms)
Apr 29 23:43:15.059: INFO: (8) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 3.728876ms)
Apr 29 23:43:15.059: INFO: (8) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.643104ms)
Apr 29 23:43:15.059: INFO: (8) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.817057ms)
Apr 29 23:43:15.059: INFO: (8) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.827049ms)
Apr 29 23:43:15.059: INFO: (8) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 3.87069ms)
Apr 29 23:43:15.059: INFO: (8) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 4.132371ms)
Apr 29 23:43:15.059: INFO: (8) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 4.346731ms)
Apr 29 23:43:15.060: INFO: (8) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 5.567095ms)
Apr 29 23:43:15.062: INFO: (8) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 6.924311ms)
Apr 29 23:43:15.062: INFO: (8) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 7.444267ms)
Apr 29 23:43:15.062: INFO: (8) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 7.607953ms)
Apr 29 23:43:15.063: INFO: (8) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.630151ms)
Apr 29 23:43:15.063: INFO: (8) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 7.585597ms)
Apr 29 23:43:15.066: INFO: (9) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 3.090029ms)
Apr 29 23:43:15.066: INFO: (9) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 3.619324ms)
Apr 29 23:43:15.066: INFO: (9) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 3.565916ms)
Apr 29 23:43:15.066: INFO: (9) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.664022ms)
Apr 29 23:43:15.067: INFO: (9) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 3.657952ms)
Apr 29 23:43:15.067: INFO: (9) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 3.931373ms)
Apr 29 23:43:15.067: INFO: (9) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.792329ms)
Apr 29 23:43:15.067: INFO: (9) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 4.028105ms)
Apr 29 23:43:15.067: INFO: (9) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 4.223783ms)
Apr 29 23:43:15.067: INFO: (9) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.991595ms)
Apr 29 23:43:15.069: INFO: (9) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 5.910761ms)
Apr 29 23:43:15.071: INFO: (9) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.804074ms)
Apr 29 23:43:15.071: INFO: (9) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 7.616231ms)
Apr 29 23:43:15.071: INFO: (9) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 7.904712ms)
Apr 29 23:43:15.071: INFO: (9) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 8.304588ms)
Apr 29 23:43:15.071: INFO: (9) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 8.168235ms)
Apr 29 23:43:15.074: INFO: (10) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 2.849913ms)
Apr 29 23:43:15.075: INFO: (10) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 3.101441ms)
Apr 29 23:43:15.075: INFO: (10) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.509529ms)
Apr 29 23:43:15.075: INFO: (10) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.789797ms)
Apr 29 23:43:15.075: INFO: (10) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 3.698177ms)
Apr 29 23:43:15.075: INFO: (10) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.646758ms)
Apr 29 23:43:15.075: INFO: (10) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.890378ms)
Apr 29 23:43:15.075: INFO: (10) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 3.902462ms)
Apr 29 23:43:15.076: INFO: (10) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 3.946542ms)
Apr 29 23:43:15.076: INFO: (10) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 3.918973ms)
Apr 29 23:43:15.077: INFO: (10) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 5.541852ms)
Apr 29 23:43:15.078: INFO: (10) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.069103ms)
Apr 29 23:43:15.079: INFO: (10) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 7.120278ms)
Apr 29 23:43:15.079: INFO: (10) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 7.173789ms)
Apr 29 23:43:15.079: INFO: (10) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 7.602756ms)
Apr 29 23:43:15.079: INFO: (10) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 7.577527ms)
Apr 29 23:43:15.082: INFO: (11) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 3.199033ms)
Apr 29 23:43:15.082: INFO: (11) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 3.077007ms)
Apr 29 23:43:15.083: INFO: (11) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 3.730964ms)
Apr 29 23:43:15.083: INFO: (11) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.645356ms)
Apr 29 23:43:15.083: INFO: (11) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 3.723446ms)
Apr 29 23:43:15.083: INFO: (11) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.887985ms)
Apr 29 23:43:15.083: INFO: (11) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.780089ms)
Apr 29 23:43:15.083: INFO: (11) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 3.835446ms)
Apr 29 23:43:15.083: INFO: (11) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.875699ms)
Apr 29 23:43:15.083: INFO: (11) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 4.187585ms)
Apr 29 23:43:15.085: INFO: (11) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 5.620557ms)
Apr 29 23:43:15.085: INFO: (11) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 5.996214ms)
Apr 29 23:43:15.086: INFO: (11) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 6.972445ms)
Apr 29 23:43:15.087: INFO: (11) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.505887ms)
Apr 29 23:43:15.087: INFO: (11) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 7.204472ms)
Apr 29 23:43:15.087: INFO: (11) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 7.540626ms)
Apr 29 23:43:15.090: INFO: (12) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.06002ms)
Apr 29 23:43:15.090: INFO: (12) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 3.585004ms)
Apr 29 23:43:15.091: INFO: (12) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 3.725669ms)
Apr 29 23:43:15.091: INFO: (12) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.866557ms)
Apr 29 23:43:15.091: INFO: (12) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.940212ms)
Apr 29 23:43:15.091: INFO: (12) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 3.99192ms)
Apr 29 23:43:15.091: INFO: (12) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 4.037739ms)
Apr 29 23:43:15.091: INFO: (12) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 4.087751ms)
Apr 29 23:43:15.091: INFO: (12) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 4.491113ms)
Apr 29 23:43:15.091: INFO: (12) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 4.46769ms)
Apr 29 23:43:15.092: INFO: (12) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 5.629037ms)
Apr 29 23:43:15.094: INFO: (12) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 6.75014ms)
Apr 29 23:43:15.094: INFO: (12) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 7.197728ms)
Apr 29 23:43:15.094: INFO: (12) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.486605ms)
Apr 29 23:43:15.095: INFO: (12) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 8.109043ms)
Apr 29 23:43:15.095: INFO: (12) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 8.044258ms)
Apr 29 23:43:15.098: INFO: (13) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.028296ms)
Apr 29 23:43:15.098: INFO: (13) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 3.294727ms)
Apr 29 23:43:15.099: INFO: (13) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.491208ms)
Apr 29 23:43:15.099: INFO: (13) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 3.333823ms)
Apr 29 23:43:15.099: INFO: (13) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 3.759169ms)
Apr 29 23:43:15.099: INFO: (13) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.60179ms)
Apr 29 23:43:15.099: INFO: (13) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 3.896429ms)
Apr 29 23:43:15.100: INFO: (13) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 4.25616ms)
Apr 29 23:43:15.100: INFO: (13) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 4.03336ms)
Apr 29 23:43:15.100: INFO: (13) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 3.955797ms)
Apr 29 23:43:15.101: INFO: (13) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 5.967069ms)
Apr 29 23:43:15.103: INFO: (13) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 7.214836ms)
Apr 29 23:43:15.103: INFO: (13) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 7.172745ms)
Apr 29 23:43:15.103: INFO: (13) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 7.681837ms)
Apr 29 23:43:15.103: INFO: (13) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 7.341884ms)
Apr 29 23:43:15.103: INFO: (13) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.454572ms)
Apr 29 23:43:15.106: INFO: (14) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 2.938567ms)
Apr 29 23:43:15.107: INFO: (14) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 3.509461ms)
Apr 29 23:43:15.107: INFO: (14) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.708873ms)
Apr 29 23:43:15.107: INFO: (14) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.772546ms)
Apr 29 23:43:15.107: INFO: (14) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 3.982666ms)
Apr 29 23:43:15.107: INFO: (14) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 4.206767ms)
Apr 29 23:43:15.107: INFO: (14) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 4.180172ms)
Apr 29 23:43:15.107: INFO: (14) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 4.144027ms)
Apr 29 23:43:15.107: INFO: (14) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 4.205252ms)
Apr 29 23:43:15.108: INFO: (14) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 4.409624ms)
Apr 29 23:43:15.109: INFO: (14) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 5.588965ms)
Apr 29 23:43:15.110: INFO: (14) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 7.298635ms)
Apr 29 23:43:15.111: INFO: (14) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 7.293085ms)
Apr 29 23:43:15.111: INFO: (14) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 7.5183ms)
Apr 29 23:43:15.111: INFO: (14) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 7.56565ms)
Apr 29 23:43:15.111: INFO: (14) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.722511ms)
Apr 29 23:43:15.114: INFO: (15) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 3.130442ms)
Apr 29 23:43:15.114: INFO: (15) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.155337ms)
Apr 29 23:43:15.115: INFO: (15) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.481598ms)
Apr 29 23:43:15.115: INFO: (15) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.635731ms)
Apr 29 23:43:15.115: INFO: (15) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 3.702884ms)
Apr 29 23:43:15.115: INFO: (15) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 3.68572ms)
Apr 29 23:43:15.115: INFO: (15) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 3.750496ms)
Apr 29 23:43:15.116: INFO: (15) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 4.183848ms)
Apr 29 23:43:15.116: INFO: (15) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 4.385897ms)
Apr 29 23:43:15.116: INFO: (15) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 4.295004ms)
Apr 29 23:43:15.116: INFO: (15) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 5.556952ms)
Apr 29 23:43:15.118: INFO: (15) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 6.521098ms)
Apr 29 23:43:15.118: INFO: (15) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 6.839685ms)
Apr 29 23:43:15.118: INFO: (15) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 6.983354ms)
Apr 29 23:43:15.118: INFO: (15) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 7.26749ms)
Apr 29 23:43:15.119: INFO: (15) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 7.448306ms)
Apr 29 23:43:15.122: INFO: (16) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.241711ms)
Apr 29 23:43:15.122: INFO: (16) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 3.157533ms)
Apr 29 23:43:15.122: INFO: (16) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 3.324568ms)
Apr 29 23:43:15.123: INFO: (16) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 3.543273ms)
Apr 29 23:43:15.123: INFO: (16) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 3.686719ms)
Apr 29 23:43:15.123: INFO: (16) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.807887ms)
Apr 29 23:43:15.123: INFO: (16) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 4.059343ms)
Apr 29 23:43:15.123: INFO: (16) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 4.07995ms)
Apr 29 23:43:15.123: INFO: (16) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 4.165294ms)
Apr 29 23:43:15.124: INFO: (16) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 4.308864ms)
Apr 29 23:43:15.125: INFO: (16) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 5.849259ms)
Apr 29 23:43:15.126: INFO: (16) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 6.81673ms)
Apr 29 23:43:15.126: INFO: (16) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 6.868827ms)
Apr 29 23:43:15.126: INFO: (16) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 7.177134ms)
Apr 29 23:43:15.126: INFO: (16) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.153185ms)
Apr 29 23:43:15.127: INFO: (16) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 7.51254ms)
Apr 29 23:43:15.130: INFO: (17) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 2.996025ms)
Apr 29 23:43:15.130: INFO: (17) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 3.50417ms)
Apr 29 23:43:15.131: INFO: (17) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.590595ms)
Apr 29 23:43:15.131: INFO: (17) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 3.574327ms)
Apr 29 23:43:15.131: INFO: (17) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.61137ms)
Apr 29 23:43:15.131: INFO: (17) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 3.934329ms)
Apr 29 23:43:15.131: INFO: (17) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.942926ms)
Apr 29 23:43:15.131: INFO: (17) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 4.093209ms)
Apr 29 23:43:15.131: INFO: (17) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 4.353769ms)
Apr 29 23:43:15.131: INFO: (17) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 4.014441ms)
Apr 29 23:43:15.133: INFO: (17) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 5.738307ms)
Apr 29 23:43:15.133: INFO: (17) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 5.969803ms)
Apr 29 23:43:15.134: INFO: (17) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 6.669649ms)
Apr 29 23:43:15.134: INFO: (17) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.051178ms)
Apr 29 23:43:15.134: INFO: (17) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 7.099037ms)
Apr 29 23:43:15.134: INFO: (17) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 7.106044ms)
Apr 29 23:43:15.137: INFO: (18) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 3.025983ms)
Apr 29 23:43:15.138: INFO: (18) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 3.150796ms)
Apr 29 23:43:15.138: INFO: (18) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 3.186268ms)
Apr 29 23:43:15.138: INFO: (18) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.64618ms)
Apr 29 23:43:15.138: INFO: (18) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.699141ms)
Apr 29 23:43:15.138: INFO: (18) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.708779ms)
Apr 29 23:43:15.138: INFO: (18) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 3.456882ms)
Apr 29 23:43:15.138: INFO: (18) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 3.584589ms)
Apr 29 23:43:15.139: INFO: (18) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 3.96676ms)
Apr 29 23:43:15.139: INFO: (18) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 3.834436ms)
Apr 29 23:43:15.140: INFO: (18) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 5.686602ms)
Apr 29 23:43:15.142: INFO: (18) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 6.909752ms)
Apr 29 23:43:15.142: INFO: (18) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 7.432729ms)
Apr 29 23:43:15.142: INFO: (18) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 7.717753ms)
Apr 29 23:43:15.142: INFO: (18) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.679498ms)
Apr 29 23:43:15.143: INFO: (18) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 7.819117ms)
Apr 29 23:43:15.145: INFO: (19) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 2.836851ms)
Apr 29 23:43:15.146: INFO: (19) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">test<... (200; 3.34599ms)
Apr 29 23:43:15.146: INFO: (19) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:460/proxy/: tls baz (200; 3.501951ms)
Apr 29 23:43:15.146: INFO: (19) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75/proxy/rewriteme">test</a> (200; 3.550636ms)
Apr 29 23:43:15.147: INFO: (19) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:443/proxy/tlsrewritem... (200; 3.592578ms)
Apr 29 23:43:15.147: INFO: (19) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:162/proxy/: bar (200; 3.832318ms)
Apr 29 23:43:15.147: INFO: (19) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/: <a href="/api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:1080/proxy/rewriteme">... (200; 3.870835ms)
Apr 29 23:43:15.147: INFO: (19) /api/v1/namespaces/proxy-7989/pods/https:proxy-service-d8nwm-wnq75:462/proxy/: tls qux (200; 3.962748ms)
Apr 29 23:43:15.147: INFO: (19) /api/v1/namespaces/proxy-7989/pods/proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 4.041045ms)
Apr 29 23:43:15.147: INFO: (19) /api/v1/namespaces/proxy-7989/pods/http:proxy-service-d8nwm-wnq75:160/proxy/: foo (200; 4.001906ms)
Apr 29 23:43:15.148: INFO: (19) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname2/proxy/: bar (200; 5.366024ms)
Apr 29 23:43:15.150: INFO: (19) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname2/proxy/: bar (200; 6.676901ms)
Apr 29 23:43:15.150: INFO: (19) /api/v1/namespaces/proxy-7989/services/proxy-service-d8nwm:portname1/proxy/: foo (200; 6.888836ms)
Apr 29 23:43:15.150: INFO: (19) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname2/proxy/: tls qux (200; 7.148795ms)
Apr 29 23:43:15.151: INFO: (19) /api/v1/namespaces/proxy-7989/services/https:proxy-service-d8nwm:tlsportname1/proxy/: tls baz (200; 7.744352ms)
Apr 29 23:43:15.151: INFO: (19) /api/v1/namespaces/proxy-7989/services/http:proxy-service-d8nwm:portname1/proxy/: foo (200; 8.369363ms)
STEP: deleting ReplicationController proxy-service-d8nwm in namespace proxy-7989, will wait for the garbage collector to delete the pods
Apr 29 23:43:15.209: INFO: Deleting ReplicationController proxy-service-d8nwm took: 6.032096ms
Apr 29 23:43:15.510: INFO: Terminating ReplicationController proxy-service-d8nwm pods took: 300.15508ms
[AfterEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:43:21.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7989" for this suite.
Apr 29 23:43:27.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:43:27.505: INFO: namespace proxy-7989 deletion completed in 6.090560894s

• [SLOW TEST:17.762 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:43:27.505: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-970
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 29 23:43:31.685: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 23:43:31.687: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 23:43:33.687: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 23:43:33.690: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 23:43:35.687: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 23:43:35.690: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 23:43:37.687: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 23:43:37.690: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 23:43:39.687: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 23:43:39.690: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 29 23:43:41.687: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 29 23:43:41.689: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:43:41.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-970" for this suite.
Apr 29 23:43:53.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:43:53.784: INFO: namespace container-lifecycle-hook-970 deletion completed in 12.090679612s

• [SLOW TEST:26.279 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:43:53.784: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5435
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-5435/configmap-test-0ec33e5c-5644-46b9-9b02-ee6753f19e2d
STEP: Creating a pod to test consume configMaps
Apr 29 23:43:53.939: INFO: Waiting up to 5m0s for pod "pod-configmaps-f8a0bbc0-046a-4be4-94cc-f7999d3e9be1" in namespace "configmap-5435" to be "success or failure"
Apr 29 23:43:53.941: INFO: Pod "pod-configmaps-f8a0bbc0-046a-4be4-94cc-f7999d3e9be1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.952395ms
Apr 29 23:43:55.944: INFO: Pod "pod-configmaps-f8a0bbc0-046a-4be4-94cc-f7999d3e9be1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004733402s
STEP: Saw pod success
Apr 29 23:43:55.944: INFO: Pod "pod-configmaps-f8a0bbc0-046a-4be4-94cc-f7999d3e9be1" satisfied condition "success or failure"
Apr 29 23:43:55.946: INFO: Trying to get logs from node ip-192-168-149-254.us-west-2.compute.internal pod pod-configmaps-f8a0bbc0-046a-4be4-94cc-f7999d3e9be1 container env-test: <nil>
STEP: delete the pod
Apr 29 23:43:55.963: INFO: Waiting for pod pod-configmaps-f8a0bbc0-046a-4be4-94cc-f7999d3e9be1 to disappear
Apr 29 23:43:55.964: INFO: Pod pod-configmaps-f8a0bbc0-046a-4be4-94cc-f7999d3e9be1 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:43:55.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5435" for this suite.
Apr 29 23:44:02.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:44:02.229: INFO: namespace configmap-5435 deletion completed in 6.260870424s

• [SLOW TEST:8.445 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:44:02.229: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-5069
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:44:10.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5069" for this suite.
Apr 29 23:44:16.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:44:16.537: INFO: namespace job-5069 deletion completed in 6.145054149s

• [SLOW TEST:14.308 seconds]
[sig-apps] Job
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:44:16.537: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2039
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:45:16.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2039" for this suite.
Apr 29 23:45:28.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:45:28.799: INFO: namespace container-probe-2039 deletion completed in 12.094994642s

• [SLOW TEST:72.262 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:45:28.799: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8811
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 29 23:45:32.972: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 23:45:32.974: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 23:45:34.975: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 23:45:34.977: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 29 23:45:36.975: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 29 23:45:36.978: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:45:36.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8811" for this suite.
Apr 29 23:45:49.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:45:49.084: INFO: namespace container-lifecycle-hook-8811 deletion completed in 12.091221204s

• [SLOW TEST:20.285 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:45:49.084: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8560
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8560
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-8560
STEP: creating replication controller externalsvc in namespace services-8560
I0429 23:45:49.255532      21 runners.go:184] Created replication controller with name: externalsvc, namespace: services-8560, replica count: 2
I0429 23:45:52.305864      21 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Apr 29 23:45:52.332: INFO: Creating new exec pod
Apr 29 23:45:54.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=services-8560 execpodnxtqj -- /bin/sh -x -c nslookup nodeport-service'
Apr 29 23:45:54.507: INFO: stderr: "+ nslookup nodeport-service\n"
Apr 29 23:45:54.507: INFO: stdout: "Server:\t\t10.100.0.10\nAddress:\t10.100.0.10#53\n\nnodeport-service.services-8560.svc.cluster.local\tcanonical name = externalsvc.services-8560.svc.cluster.local.\nName:\texternalsvc.services-8560.svc.cluster.local\nAddress: 10.100.40.54\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8560, will wait for the garbage collector to delete the pods
Apr 29 23:45:54.566: INFO: Deleting ReplicationController externalsvc took: 5.987158ms
Apr 29 23:45:54.866: INFO: Terminating ReplicationController externalsvc pods took: 300.140612ms
Apr 29 23:46:01.524: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:46:01.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8560" for this suite.
Apr 29 23:46:07.591: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:46:07.681: INFO: namespace services-8560 deletion completed in 6.123015911s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:18.597 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:46:07.681: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9567
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:46:07.839: INFO: (0) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 10.071283ms)
Apr 29 23:46:07.843: INFO: (1) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.850854ms)
Apr 29 23:46:07.846: INFO: (2) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.601292ms)
Apr 29 23:46:07.850: INFO: (3) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.543243ms)
Apr 29 23:46:07.854: INFO: (4) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.583394ms)
Apr 29 23:46:07.857: INFO: (5) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.587718ms)
Apr 29 23:46:07.861: INFO: (6) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.515045ms)
Apr 29 23:46:07.864: INFO: (7) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.683978ms)
Apr 29 23:46:07.868: INFO: (8) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.590121ms)
Apr 29 23:46:07.872: INFO: (9) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.644648ms)
Apr 29 23:46:07.875: INFO: (10) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.597382ms)
Apr 29 23:46:07.879: INFO: (11) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.722215ms)
Apr 29 23:46:07.883: INFO: (12) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.521585ms)
Apr 29 23:46:07.886: INFO: (13) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.490918ms)
Apr 29 23:46:07.890: INFO: (14) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.552072ms)
Apr 29 23:46:07.893: INFO: (15) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.65794ms)
Apr 29 23:46:07.897: INFO: (16) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.524396ms)
Apr 29 23:46:07.900: INFO: (17) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.522488ms)
Apr 29 23:46:07.904: INFO: (18) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.570619ms)
Apr 29 23:46:07.908: INFO: (19) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.514052ms)
[AfterEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:46:07.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9567" for this suite.
Apr 29 23:46:13.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:46:14.030: INFO: namespace proxy-9567 deletion completed in 6.119080819s

• [SLOW TEST:6.349 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:46:14.031: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9706
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr 29 23:46:14.197: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9706 /api/v1/namespaces/watch-9706/configmaps/e2e-watch-test-label-changed 254a8d1b-cef5-429d-a3cc-b26486083585 16546 0 2020-04-29 23:46:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 29 23:46:14.197: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9706 /api/v1/namespaces/watch-9706/configmaps/e2e-watch-test-label-changed 254a8d1b-cef5-429d-a3cc-b26486083585 16547 0 2020-04-29 23:46:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr 29 23:46:14.197: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9706 /api/v1/namespaces/watch-9706/configmaps/e2e-watch-test-label-changed 254a8d1b-cef5-429d-a3cc-b26486083585 16548 0 2020-04-29 23:46:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr 29 23:46:24.231: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9706 /api/v1/namespaces/watch-9706/configmaps/e2e-watch-test-label-changed 254a8d1b-cef5-429d-a3cc-b26486083585 16580 0 2020-04-29 23:46:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 29 23:46:24.231: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9706 /api/v1/namespaces/watch-9706/configmaps/e2e-watch-test-label-changed 254a8d1b-cef5-429d-a3cc-b26486083585 16581 0 2020-04-29 23:46:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Apr 29 23:46:24.231: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9706 /api/v1/namespaces/watch-9706/configmaps/e2e-watch-test-label-changed 254a8d1b-cef5-429d-a3cc-b26486083585 16582 0 2020-04-29 23:46:14 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:46:24.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9706" for this suite.
Apr 29 23:46:30.245: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:46:30.329: INFO: namespace watch-9706 deletion completed in 6.094122117s

• [SLOW TEST:16.298 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:46:30.329: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6376
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 29 23:46:30.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6376'
Apr 29 23:46:30.544: INFO: stderr: ""
Apr 29 23:46:30.544: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Apr 29 23:46:35.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pod e2e-test-httpd-pod --namespace=kubectl-6376 -o json'
Apr 29 23:46:35.647: INFO: stderr: ""
Apr 29 23:46:35.647: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2020-04-29T23:46:30Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6376\",\n        \"resourceVersion\": \"16621\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6376/pods/e2e-test-httpd-pod\",\n        \"uid\": \"709015e0-9c2f-40da-b0e9-9d5fce0bc55a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-d7kmb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-192-168-83-225.us-west-2.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-d7kmb\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-d7kmb\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-04-29T23:46:30Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-04-29T23:46:31Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-04-29T23:46:31Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-04-29T23:46:30Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://27acabaf9d86f14e65f1f553e79c87a6cd545c1a34f24a96ad03f65c37936dcc\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-04-29T23:46:31Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.83.225\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.74.58\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.74.58\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-04-29T23:46:30Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr 29 23:46:35.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 replace -f - --namespace=kubectl-6376'
Apr 29 23:46:35.795: INFO: stderr: ""
Apr 29 23:46:35.795: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Apr 29 23:46:35.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete pods e2e-test-httpd-pod --namespace=kubectl-6376'
Apr 29 23:46:37.714: INFO: stderr: ""
Apr 29 23:46:37.714: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:46:37.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6376" for this suite.
Apr 29 23:46:43.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:46:43.812: INFO: namespace kubectl-6376 deletion completed in 6.093578383s

• [SLOW TEST:13.483 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:46:43.812: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7850
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 29 23:46:43.962: INFO: Waiting up to 5m0s for pod "pod-7a45f7e6-8f12-42ad-bf68-19c9c69f5e4d" in namespace "emptydir-7850" to be "success or failure"
Apr 29 23:46:43.964: INFO: Pod "pod-7a45f7e6-8f12-42ad-bf68-19c9c69f5e4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047673ms
Apr 29 23:46:45.967: INFO: Pod "pod-7a45f7e6-8f12-42ad-bf68-19c9c69f5e4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.0050845s
Apr 29 23:46:47.970: INFO: Pod "pod-7a45f7e6-8f12-42ad-bf68-19c9c69f5e4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007837049s
STEP: Saw pod success
Apr 29 23:46:47.970: INFO: Pod "pod-7a45f7e6-8f12-42ad-bf68-19c9c69f5e4d" satisfied condition "success or failure"
Apr 29 23:46:47.972: INFO: Trying to get logs from node ip-192-168-148-117.us-west-2.compute.internal pod pod-7a45f7e6-8f12-42ad-bf68-19c9c69f5e4d container test-container: <nil>
STEP: delete the pod
Apr 29 23:46:47.986: INFO: Waiting for pod pod-7a45f7e6-8f12-42ad-bf68-19c9c69f5e4d to disappear
Apr 29 23:46:47.988: INFO: Pod pod-7a45f7e6-8f12-42ad-bf68-19c9c69f5e4d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:46:47.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7850" for this suite.
Apr 29 23:46:54.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:46:54.086: INFO: namespace emptydir-7850 deletion completed in 6.094264343s

• [SLOW TEST:10.274 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:46:54.086: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5656
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 23:46:54.675: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 29 23:46:56.682: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800814, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800814, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800814, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800814, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 23:46:59.696: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Apr 29 23:47:01.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 attach --namespace=webhook-5656 to-be-attached-pod -i -c=container1'
Apr 29 23:47:01.792: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:47:01.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5656" for this suite.
Apr 29 23:47:13.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:47:13.934: INFO: namespace webhook-5656 deletion completed in 12.105687228s
STEP: Destroying namespace "webhook-5656-markers" for this suite.
Apr 29 23:47:19.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:47:20.029: INFO: namespace webhook-5656-markers deletion completed in 6.094880385s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:25.960 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:47:20.047: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 29 23:47:20.198: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a3a51ca-09d6-4a7a-924f-6d522b4af55b" in namespace "downward-api-7310" to be "success or failure"
Apr 29 23:47:20.201: INFO: Pod "downwardapi-volume-0a3a51ca-09d6-4a7a-924f-6d522b4af55b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.701117ms
Apr 29 23:47:22.204: INFO: Pod "downwardapi-volume-0a3a51ca-09d6-4a7a-924f-6d522b4af55b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005568638s
STEP: Saw pod success
Apr 29 23:47:22.204: INFO: Pod "downwardapi-volume-0a3a51ca-09d6-4a7a-924f-6d522b4af55b" satisfied condition "success or failure"
Apr 29 23:47:22.206: INFO: Trying to get logs from node ip-192-168-211-77.us-west-2.compute.internal pod downwardapi-volume-0a3a51ca-09d6-4a7a-924f-6d522b4af55b container client-container: <nil>
STEP: delete the pod
Apr 29 23:47:22.224: INFO: Waiting for pod downwardapi-volume-0a3a51ca-09d6-4a7a-924f-6d522b4af55b to disappear
Apr 29 23:47:22.226: INFO: Pod downwardapi-volume-0a3a51ca-09d6-4a7a-924f-6d522b4af55b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:47:22.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7310" for this suite.
Apr 29 23:47:28.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:47:28.322: INFO: namespace downward-api-7310 deletion completed in 6.092685553s

• [SLOW TEST:8.275 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:47:28.322: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9603
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 29 23:47:28.472: INFO: Waiting up to 5m0s for pod "pod-818c6296-3680-467a-aa7b-ec28e8132346" in namespace "emptydir-9603" to be "success or failure"
Apr 29 23:47:28.474: INFO: Pod "pod-818c6296-3680-467a-aa7b-ec28e8132346": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011735ms
Apr 29 23:47:30.477: INFO: Pod "pod-818c6296-3680-467a-aa7b-ec28e8132346": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00471045s
STEP: Saw pod success
Apr 29 23:47:30.477: INFO: Pod "pod-818c6296-3680-467a-aa7b-ec28e8132346" satisfied condition "success or failure"
Apr 29 23:47:30.479: INFO: Trying to get logs from node ip-192-168-149-254.us-west-2.compute.internal pod pod-818c6296-3680-467a-aa7b-ec28e8132346 container test-container: <nil>
STEP: delete the pod
Apr 29 23:47:30.499: INFO: Waiting for pod pod-818c6296-3680-467a-aa7b-ec28e8132346 to disappear
Apr 29 23:47:30.500: INFO: Pod pod-818c6296-3680-467a-aa7b-ec28e8132346 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:47:30.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9603" for this suite.
Apr 29 23:47:36.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:47:36.600: INFO: namespace emptydir-9603 deletion completed in 6.09616522s

• [SLOW TEST:8.278 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:47:36.600: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7559
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Apr 29 23:47:36.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 create -f - --namespace=kubectl-7559'
Apr 29 23:47:36.860: INFO: stderr: ""
Apr 29 23:47:36.860: INFO: stdout: "pod/pause created\n"
Apr 29 23:47:36.860: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr 29 23:47:36.860: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7559" to be "running and ready"
Apr 29 23:47:36.863: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.307644ms
Apr 29 23:47:38.866: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006279889s
Apr 29 23:47:38.866: INFO: Pod "pause" satisfied condition "running and ready"
Apr 29 23:47:38.866: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Apr 29 23:47:38.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 label pods pause testing-label=testing-label-value --namespace=kubectl-7559'
Apr 29 23:47:38.926: INFO: stderr: ""
Apr 29 23:47:38.926: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr 29 23:47:38.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pod pause -L testing-label --namespace=kubectl-7559'
Apr 29 23:47:38.978: INFO: stderr: ""
Apr 29 23:47:38.978: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr 29 23:47:38.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 label pods pause testing-label- --namespace=kubectl-7559'
Apr 29 23:47:39.036: INFO: stderr: ""
Apr 29 23:47:39.036: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr 29 23:47:39.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pod pause -L testing-label --namespace=kubectl-7559'
Apr 29 23:47:39.088: INFO: stderr: ""
Apr 29 23:47:39.088: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Apr 29 23:47:39.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete --grace-period=0 --force -f - --namespace=kubectl-7559'
Apr 29 23:47:39.158: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 29 23:47:39.158: INFO: stdout: "pod \"pause\" force deleted\n"
Apr 29 23:47:39.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get rc,svc -l name=pause --no-headers --namespace=kubectl-7559'
Apr 29 23:47:39.217: INFO: stderr: "No resources found in kubectl-7559 namespace.\n"
Apr 29 23:47:39.217: INFO: stdout: ""
Apr 29 23:47:39.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -l name=pause --namespace=kubectl-7559 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 29 23:47:39.267: INFO: stderr: ""
Apr 29 23:47:39.267: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:47:39.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7559" for this suite.
Apr 29 23:47:45.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:47:45.366: INFO: namespace kubectl-7559 deletion completed in 6.094278297s

• [SLOW TEST:8.766 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:47:45.366: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9899
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Apr 29 23:47:48.058: INFO: Successfully updated pod "annotationupdate85f53536-211d-4b6f-bfbc-f288e1d92148"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:47:52.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9899" for this suite.
Apr 29 23:48:04.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:48:04.194: INFO: namespace downward-api-9899 deletion completed in 12.11092139s

• [SLOW TEST:18.828 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:48:04.194: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9330
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:48:04.346: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr 29 23:48:09.349: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 29 23:48:09.349: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr 29 23:48:11.352: INFO: Creating deployment "test-rollover-deployment"
Apr 29 23:48:11.358: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr 29 23:48:13.362: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr 29 23:48:13.367: INFO: Ensure that both replica sets have 1 created replica
Apr 29 23:48:13.371: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr 29 23:48:13.376: INFO: Updating deployment test-rollover-deployment
Apr 29 23:48:13.376: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr 29 23:48:15.380: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr 29 23:48:15.384: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr 29 23:48:15.388: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 23:48:15.388: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800893, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 23:48:17.394: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 23:48:17.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800893, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 23:48:19.394: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 23:48:19.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800897, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 23:48:21.394: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 23:48:21.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800897, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 23:48:23.393: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 23:48:23.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800897, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 23:48:25.394: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 23:48:25.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800897, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 23:48:27.393: INFO: all replica sets need to contain the pod-template-hash label
Apr 29 23:48:27.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800897, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723800891, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 23:48:29.393: INFO: 
Apr 29 23:48:29.393: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Apr 29 23:48:29.404: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9330 /apis/apps/v1/namespaces/deployment-9330/deployments/test-rollover-deployment 0ec7f198-2e40-42d9-af67-a20556fb0304 17243 2 2020-04-29 23:48:11 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00471f8d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-04-29 23:48:11 +0000 UTC,LastTransitionTime:2020-04-29 23:48:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-04-29 23:48:27 +0000 UTC,LastTransitionTime:2020-04-29 23:48:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 29 23:48:29.407: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-9330 /apis/apps/v1/namespaces/deployment-9330/replicasets/test-rollover-deployment-7d7dc6548c 7bc31ca5-542f-48bb-b8df-63a0ae3d80c9 17232 2 2020-04-29 23:48:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 0ec7f198-2e40-42d9-af67-a20556fb0304 0xc00471fda7 0xc00471fda8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00471fe08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 29 23:48:29.407: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr 29 23:48:29.407: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9330 /apis/apps/v1/namespaces/deployment-9330/replicasets/test-rollover-controller b0c92c00-2edf-44f1-9546-c53b736470dd 17241 2 2020-04-29 23:48:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 0ec7f198-2e40-42d9-af67-a20556fb0304 0xc00471fcd7 0xc00471fcd8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00471fd38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 23:48:29.407: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-9330 /apis/apps/v1/namespaces/deployment-9330/replicasets/test-rollover-deployment-f6c94f66c 8ec4f7d7-b5af-48ca-b558-1591334ccdef 17172 2 2020-04-29 23:48:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 0ec7f198-2e40-42d9-af67-a20556fb0304 0xc00471fe70 0xc00471fe71}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00471fee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 23:48:29.409: INFO: Pod "test-rollover-deployment-7d7dc6548c-4tpjr" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-4tpjr test-rollover-deployment-7d7dc6548c- deployment-9330 /api/v1/namespaces/deployment-9330/pods/test-rollover-deployment-7d7dc6548c-4tpjr 25e7d7b7-3ab7-45fa-8f0e-f5683d8dd2d9 17205 0 2020-04-29 23:48:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 7bc31ca5-542f-48bb-b8df-63a0ae3d80c9 0xc0047e2467 0xc0047e2468}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wz9bm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wz9bm,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wz9bm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-82-130.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:48:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:48:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:48:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:48:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.82.130,PodIP:192.168.69.140,StartTime:2020-04-29 23:48:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-29 23:48:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://b7928fc403d8b26bb1b95398314c496e7c1947be23d2163ffe692a935827fe17,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.69.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:48:29.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9330" for this suite.
Apr 29 23:48:35.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:48:35.510: INFO: namespace deployment-9330 deletion completed in 6.097114969s

• [SLOW TEST:31.316 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:48:35.510: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-7762
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:48:35.660: INFO: Waiting up to 5m0s for pod "busybox-user-65534-5dfeee6e-516a-4ee7-a09f-c1e2e4c1ada3" in namespace "security-context-test-7762" to be "success or failure"
Apr 29 23:48:35.662: INFO: Pod "busybox-user-65534-5dfeee6e-516a-4ee7-a09f-c1e2e4c1ada3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.809956ms
Apr 29 23:48:37.665: INFO: Pod "busybox-user-65534-5dfeee6e-516a-4ee7-a09f-c1e2e4c1ada3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004319212s
Apr 29 23:48:39.669: INFO: Pod "busybox-user-65534-5dfeee6e-516a-4ee7-a09f-c1e2e4c1ada3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008924461s
Apr 29 23:48:39.669: INFO: Pod "busybox-user-65534-5dfeee6e-516a-4ee7-a09f-c1e2e4c1ada3" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:48:39.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7762" for this suite.
Apr 29 23:48:45.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:48:45.765: INFO: namespace security-context-test-7762 deletion completed in 6.0918784s

• [SLOW TEST:10.255 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:48:45.766: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7611
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-3f13ff7b-0266-4eac-8019-892ed6b19b21
STEP: Creating a pod to test consume configMaps
Apr 29 23:48:45.920: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-066e2007-12c6-435a-9bd1-1e328bb37ad1" in namespace "projected-7611" to be "success or failure"
Apr 29 23:48:45.922: INFO: Pod "pod-projected-configmaps-066e2007-12c6-435a-9bd1-1e328bb37ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.800221ms
Apr 29 23:48:47.925: INFO: Pod "pod-projected-configmaps-066e2007-12c6-435a-9bd1-1e328bb37ad1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004647495s
STEP: Saw pod success
Apr 29 23:48:47.925: INFO: Pod "pod-projected-configmaps-066e2007-12c6-435a-9bd1-1e328bb37ad1" satisfied condition "success or failure"
Apr 29 23:48:47.927: INFO: Trying to get logs from node ip-192-168-144-27.us-west-2.compute.internal pod pod-projected-configmaps-066e2007-12c6-435a-9bd1-1e328bb37ad1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 23:48:47.946: INFO: Waiting for pod pod-projected-configmaps-066e2007-12c6-435a-9bd1-1e328bb37ad1 to disappear
Apr 29 23:48:47.949: INFO: Pod pod-projected-configmaps-066e2007-12c6-435a-9bd1-1e328bb37ad1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:48:47.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7611" for this suite.
Apr 29 23:48:53.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:48:54.046: INFO: namespace projected-7611 deletion completed in 6.092920347s

• [SLOW TEST:8.280 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:48:54.046: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5723
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 29 23:48:54.228: INFO: Number of nodes with available pods: 0
Apr 29 23:48:54.228: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:48:55.235: INFO: Number of nodes with available pods: 0
Apr 29 23:48:55.235: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:48:56.235: INFO: Number of nodes with available pods: 7
Apr 29 23:48:56.235: INFO: Node ip-192-168-197-211.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:48:57.236: INFO: Number of nodes with available pods: 10
Apr 29 23:48:57.236: INFO: Number of running nodes: 10, number of available pods: 10
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr 29 23:48:57.254: INFO: Number of nodes with available pods: 9
Apr 29 23:48:57.254: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:48:58.261: INFO: Number of nodes with available pods: 9
Apr 29 23:48:58.261: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:48:59.261: INFO: Number of nodes with available pods: 9
Apr 29 23:48:59.261: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:49:00.261: INFO: Number of nodes with available pods: 9
Apr 29 23:49:00.261: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:49:01.261: INFO: Number of nodes with available pods: 9
Apr 29 23:49:01.261: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:49:02.262: INFO: Number of nodes with available pods: 9
Apr 29 23:49:02.262: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:49:03.291: INFO: Number of nodes with available pods: 9
Apr 29 23:49:03.291: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:49:04.263: INFO: Number of nodes with available pods: 9
Apr 29 23:49:04.263: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:49:05.261: INFO: Number of nodes with available pods: 9
Apr 29 23:49:05.261: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:49:06.261: INFO: Number of nodes with available pods: 9
Apr 29 23:49:06.261: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:49:07.261: INFO: Number of nodes with available pods: 9
Apr 29 23:49:07.261: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:49:08.261: INFO: Number of nodes with available pods: 9
Apr 29 23:49:08.261: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:49:09.261: INFO: Number of nodes with available pods: 9
Apr 29 23:49:09.261: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:49:10.261: INFO: Number of nodes with available pods: 9
Apr 29 23:49:10.261: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:49:11.261: INFO: Number of nodes with available pods: 9
Apr 29 23:49:11.261: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:49:12.261: INFO: Number of nodes with available pods: 9
Apr 29 23:49:12.261: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:49:13.261: INFO: Number of nodes with available pods: 9
Apr 29 23:49:13.261: INFO: Node ip-192-168-83-225.us-west-2.compute.internal is running more than one daemon pod
Apr 29 23:49:14.261: INFO: Number of nodes with available pods: 10
Apr 29 23:49:14.261: INFO: Number of running nodes: 10, number of available pods: 10
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5723, will wait for the garbage collector to delete the pods
Apr 29 23:49:14.328: INFO: Deleting DaemonSet.extensions daemon-set took: 9.843423ms
Apr 29 23:49:14.628: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.296377ms
Apr 29 23:49:22.231: INFO: Number of nodes with available pods: 0
Apr 29 23:49:22.231: INFO: Number of running nodes: 0, number of available pods: 0
Apr 29 23:49:22.234: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5723/daemonsets","resourceVersion":"17655"},"items":null}

Apr 29 23:49:22.236: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5723/pods","resourceVersion":"17655"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:49:22.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5723" for this suite.
Apr 29 23:49:28.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:49:28.362: INFO: namespace daemonsets-5723 deletion completed in 6.096906201s

• [SLOW TEST:34.317 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:49:28.363: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5593
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Apr 29 23:49:59.030: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0429 23:49:59.030140      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:49:59.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5593" for this suite.
Apr 29 23:50:05.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:50:05.209: INFO: namespace gc-5593 deletion completed in 6.175833029s

• [SLOW TEST:36.847 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:50:05.210: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5825
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:50:05.384: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"7bc0bd1e-1fb4-4dc3-8242-86be4eec03cc", Controller:(*bool)(0xc003852586), BlockOwnerDeletion:(*bool)(0xc003852587)}}
Apr 29 23:50:05.392: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"e9de7a99-382f-4c4f-a904-ea50f30ee2be", Controller:(*bool)(0xc003852756), BlockOwnerDeletion:(*bool)(0xc003852757)}}
Apr 29 23:50:05.399: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"634c0aba-a819-44bd-9324-c4bfe9e62a3e", Controller:(*bool)(0xc00306f1fe), BlockOwnerDeletion:(*bool)(0xc00306f1ff)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:50:10.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5825" for this suite.
Apr 29 23:50:16.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:50:16.504: INFO: namespace gc-5825 deletion completed in 6.093548727s

• [SLOW TEST:11.293 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:50:16.504: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7452
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-sv4t
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 23:50:16.674: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-sv4t" in namespace "subpath-7452" to be "success or failure"
Apr 29 23:50:16.676: INFO: Pod "pod-subpath-test-secret-sv4t": Phase="Pending", Reason="", readiness=false. Elapsed: 1.920358ms
Apr 29 23:50:18.679: INFO: Pod "pod-subpath-test-secret-sv4t": Phase="Running", Reason="", readiness=true. Elapsed: 2.005123417s
Apr 29 23:50:20.682: INFO: Pod "pod-subpath-test-secret-sv4t": Phase="Running", Reason="", readiness=true. Elapsed: 4.008319376s
Apr 29 23:50:22.685: INFO: Pod "pod-subpath-test-secret-sv4t": Phase="Running", Reason="", readiness=true. Elapsed: 6.011470847s
Apr 29 23:50:24.701: INFO: Pod "pod-subpath-test-secret-sv4t": Phase="Running", Reason="", readiness=true. Elapsed: 8.027009463s
Apr 29 23:50:26.704: INFO: Pod "pod-subpath-test-secret-sv4t": Phase="Running", Reason="", readiness=true. Elapsed: 10.029824476s
Apr 29 23:50:28.706: INFO: Pod "pod-subpath-test-secret-sv4t": Phase="Running", Reason="", readiness=true. Elapsed: 12.032685044s
Apr 29 23:50:30.709: INFO: Pod "pod-subpath-test-secret-sv4t": Phase="Running", Reason="", readiness=true. Elapsed: 14.035549856s
Apr 29 23:50:32.712: INFO: Pod "pod-subpath-test-secret-sv4t": Phase="Running", Reason="", readiness=true. Elapsed: 16.038481154s
Apr 29 23:50:34.715: INFO: Pod "pod-subpath-test-secret-sv4t": Phase="Running", Reason="", readiness=true. Elapsed: 18.041278577s
Apr 29 23:50:36.722: INFO: Pod "pod-subpath-test-secret-sv4t": Phase="Running", Reason="", readiness=true. Elapsed: 20.048093875s
Apr 29 23:50:38.725: INFO: Pod "pod-subpath-test-secret-sv4t": Phase="Running", Reason="", readiness=true. Elapsed: 22.050963711s
Apr 29 23:50:40.728: INFO: Pod "pod-subpath-test-secret-sv4t": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.053813542s
STEP: Saw pod success
Apr 29 23:50:40.728: INFO: Pod "pod-subpath-test-secret-sv4t" satisfied condition "success or failure"
Apr 29 23:50:40.730: INFO: Trying to get logs from node ip-192-168-148-117.us-west-2.compute.internal pod pod-subpath-test-secret-sv4t container test-container-subpath-secret-sv4t: <nil>
STEP: delete the pod
Apr 29 23:50:40.751: INFO: Waiting for pod pod-subpath-test-secret-sv4t to disappear
Apr 29 23:50:40.753: INFO: Pod pod-subpath-test-secret-sv4t no longer exists
STEP: Deleting pod pod-subpath-test-secret-sv4t
Apr 29 23:50:40.753: INFO: Deleting pod "pod-subpath-test-secret-sv4t" in namespace "subpath-7452"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:50:40.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7452" for this suite.
Apr 29 23:50:46.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:50:46.853: INFO: namespace subpath-7452 deletion completed in 6.094592582s

• [SLOW TEST:30.350 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:50:46.854: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7139
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:50:58.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7139" for this suite.
Apr 29 23:51:04.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:51:04.150: INFO: namespace resourcequota-7139 deletion completed in 6.099121691s

• [SLOW TEST:17.296 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:51:04.150: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3607
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 29 23:51:04.313: INFO: Waiting up to 5m0s for pod "pod-a2842aef-503a-45e6-9581-524ce2bc1801" in namespace "emptydir-3607" to be "success or failure"
Apr 29 23:51:04.315: INFO: Pod "pod-a2842aef-503a-45e6-9581-524ce2bc1801": Phase="Pending", Reason="", readiness=false. Elapsed: 1.946494ms
Apr 29 23:51:06.317: INFO: Pod "pod-a2842aef-503a-45e6-9581-524ce2bc1801": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004871081s
STEP: Saw pod success
Apr 29 23:51:06.317: INFO: Pod "pod-a2842aef-503a-45e6-9581-524ce2bc1801" satisfied condition "success or failure"
Apr 29 23:51:06.320: INFO: Trying to get logs from node ip-192-168-148-117.us-west-2.compute.internal pod pod-a2842aef-503a-45e6-9581-524ce2bc1801 container test-container: <nil>
STEP: delete the pod
Apr 29 23:51:06.337: INFO: Waiting for pod pod-a2842aef-503a-45e6-9581-524ce2bc1801 to disappear
Apr 29 23:51:06.339: INFO: Pod pod-a2842aef-503a-45e6-9581-524ce2bc1801 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:51:06.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3607" for this suite.
Apr 29 23:51:12.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:51:12.438: INFO: namespace emptydir-3607 deletion completed in 6.094810938s

• [SLOW TEST:8.287 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:51:12.438: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5359
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:51:12.584: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Apr 29 23:51:13.607: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:51:14.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5359" for this suite.
Apr 29 23:51:20.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:51:20.713: INFO: namespace replication-controller-5359 deletion completed in 6.096794056s

• [SLOW TEST:8.275 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:51:20.713: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6474
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 29 23:51:20.866: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6bdee058-e4f1-4ee2-a7b2-9efb56b49e75" in namespace "projected-6474" to be "success or failure"
Apr 29 23:51:20.868: INFO: Pod "downwardapi-volume-6bdee058-e4f1-4ee2-a7b2-9efb56b49e75": Phase="Pending", Reason="", readiness=false. Elapsed: 1.904984ms
Apr 29 23:51:22.871: INFO: Pod "downwardapi-volume-6bdee058-e4f1-4ee2-a7b2-9efb56b49e75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004684763s
STEP: Saw pod success
Apr 29 23:51:22.871: INFO: Pod "downwardapi-volume-6bdee058-e4f1-4ee2-a7b2-9efb56b49e75" satisfied condition "success or failure"
Apr 29 23:51:22.873: INFO: Trying to get logs from node ip-192-168-211-77.us-west-2.compute.internal pod downwardapi-volume-6bdee058-e4f1-4ee2-a7b2-9efb56b49e75 container client-container: <nil>
STEP: delete the pod
Apr 29 23:51:22.892: INFO: Waiting for pod downwardapi-volume-6bdee058-e4f1-4ee2-a7b2-9efb56b49e75 to disappear
Apr 29 23:51:22.894: INFO: Pod downwardapi-volume-6bdee058-e4f1-4ee2-a7b2-9efb56b49e75 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:51:22.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6474" for this suite.
Apr 29 23:51:28.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:51:29.007: INFO: namespace projected-6474 deletion completed in 6.110039135s

• [SLOW TEST:8.294 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:51:29.007: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1018
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-f94722a6-4ab5-4ea3-aba3-f36f80638c28 in namespace container-probe-1018
Apr 29 23:51:31.164: INFO: Started pod busybox-f94722a6-4ab5-4ea3-aba3-f36f80638c28 in namespace container-probe-1018
STEP: checking the pod's current state and verifying that restartCount is present
Apr 29 23:51:31.166: INFO: Initial restart count of pod busybox-f94722a6-4ab5-4ea3-aba3-f36f80638c28 is 0
Apr 29 23:52:19.243: INFO: Restart count of pod container-probe-1018/busybox-f94722a6-4ab5-4ea3-aba3-f36f80638c28 is now 1 (48.077033677s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:52:19.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1018" for this suite.
Apr 29 23:52:25.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:52:25.350: INFO: namespace container-probe-1018 deletion completed in 6.093329402s

• [SLOW TEST:56.342 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:52:25.350: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3839
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3839
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-3839
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3839
Apr 29 23:52:25.504: INFO: Found 0 stateful pods, waiting for 1
Apr 29 23:52:35.507: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr 29 23:52:35.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-3839 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 23:52:35.679: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 23:52:35.679: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 23:52:35.679: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 23:52:35.683: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 29 23:52:45.685: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 23:52:45.685: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 23:52:45.695: INFO: POD   NODE                                           PHASE    GRACE  CONDITIONS
Apr 29 23:52:45.695: INFO: ss-0  ip-192-168-149-254.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:36 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:25 +0000 UTC  }]
Apr 29 23:52:45.695: INFO: 
Apr 29 23:52:45.695: INFO: StatefulSet ss has not reached scale 3, at 1
Apr 29 23:52:46.698: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997838484s
Apr 29 23:52:47.701: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994791083s
Apr 29 23:52:48.704: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991824621s
Apr 29 23:52:49.707: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988881603s
Apr 29 23:52:50.710: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.985914709s
Apr 29 23:52:51.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.982885256s
Apr 29 23:52:52.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.979907081s
Apr 29 23:52:53.719: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.977033147s
Apr 29 23:52:54.723: INFO: Verifying statefulset ss doesn't scale past 3 for another 973.640982ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3839
Apr 29 23:52:55.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-3839 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 23:52:55.894: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 29 23:52:55.894: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 23:52:55.894: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 23:52:55.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-3839 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 23:52:56.058: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 29 23:52:56.058: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 23:52:56.058: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 23:52:56.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-3839 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 29 23:52:56.218: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 29 23:52:56.218: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 29 23:52:56.218: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 29 23:52:56.221: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Apr 29 23:53:06.224: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 23:53:06.225: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 29 23:53:06.225: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr 29 23:53:06.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-3839 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 23:53:06.472: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 23:53:06.472: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 23:53:06.472: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 23:53:06.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-3839 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 23:53:06.641: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 23:53:06.641: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 23:53:06.641: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 23:53:06.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-3839 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 29 23:53:06.795: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 29 23:53:06.795: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 29 23:53:06.795: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 29 23:53:06.795: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 23:53:06.798: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr 29 23:53:16.803: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 23:53:16.803: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 23:53:16.803: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 29 23:53:16.811: INFO: POD   NODE                                           PHASE    GRACE  CONDITIONS
Apr 29 23:53:16.811: INFO: ss-0  ip-192-168-149-254.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:25 +0000 UTC  }]
Apr 29 23:53:16.811: INFO: ss-1  ip-192-168-222-85.us-west-2.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:45 +0000 UTC  }]
Apr 29 23:53:16.811: INFO: ss-2  ip-192-168-82-130.us-west-2.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:45 +0000 UTC  }]
Apr 29 23:53:16.811: INFO: 
Apr 29 23:53:16.811: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 29 23:53:17.814: INFO: POD   NODE                                           PHASE    GRACE  CONDITIONS
Apr 29 23:53:17.814: INFO: ss-0  ip-192-168-149-254.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:25 +0000 UTC  }]
Apr 29 23:53:17.814: INFO: ss-1  ip-192-168-222-85.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:45 +0000 UTC  }]
Apr 29 23:53:17.814: INFO: ss-2  ip-192-168-82-130.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:45 +0000 UTC  }]
Apr 29 23:53:17.814: INFO: 
Apr 29 23:53:17.814: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 29 23:53:18.817: INFO: POD   NODE                                           PHASE    GRACE  CONDITIONS
Apr 29 23:53:18.817: INFO: ss-0  ip-192-168-149-254.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:25 +0000 UTC  }]
Apr 29 23:53:18.817: INFO: ss-1  ip-192-168-222-85.us-west-2.compute.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:45 +0000 UTC  }]
Apr 29 23:53:18.817: INFO: ss-2  ip-192-168-82-130.us-west-2.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:45 +0000 UTC  }]
Apr 29 23:53:18.817: INFO: 
Apr 29 23:53:18.817: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 29 23:53:19.820: INFO: POD   NODE                                           PHASE    GRACE  CONDITIONS
Apr 29 23:53:19.820: INFO: ss-0  ip-192-168-149-254.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:25 +0000 UTC  }]
Apr 29 23:53:19.820: INFO: ss-1  ip-192-168-222-85.us-west-2.compute.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:45 +0000 UTC  }]
Apr 29 23:53:19.820: INFO: 
Apr 29 23:53:19.820: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 29 23:53:20.823: INFO: POD   NODE                                           PHASE    GRACE  CONDITIONS
Apr 29 23:53:20.823: INFO: ss-0  ip-192-168-149-254.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:06 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:25 +0000 UTC  }]
Apr 29 23:53:20.823: INFO: ss-1  ip-192-168-222-85.us-west-2.compute.internal   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:53:07 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-29 23:52:45 +0000 UTC  }]
Apr 29 23:53:20.823: INFO: 
Apr 29 23:53:20.823: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 29 23:53:21.826: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.986089705s
Apr 29 23:53:22.829: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.983313347s
Apr 29 23:53:23.832: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.980442077s
Apr 29 23:53:24.834: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.977453378s
Apr 29 23:53:25.837: INFO: Verifying statefulset ss doesn't scale past 0 for another 974.689571ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3839
Apr 29 23:53:26.840: INFO: Scaling statefulset ss to 0
Apr 29 23:53:26.846: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Apr 29 23:53:26.848: INFO: Deleting all statefulset in ns statefulset-3839
Apr 29 23:53:26.850: INFO: Scaling statefulset ss to 0
Apr 29 23:53:26.856: INFO: Waiting for statefulset status.replicas updated to 0
Apr 29 23:53:26.858: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:53:26.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3839" for this suite.
Apr 29 23:53:32.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:53:32.965: INFO: namespace statefulset-3839 deletion completed in 6.092703025s

• [SLOW TEST:67.616 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:53:32.965: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6792
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:53:35.139: INFO: Waiting up to 5m0s for pod "client-envvars-8e54ebd6-92b8-4bab-a323-4b17581a3191" in namespace "pods-6792" to be "success or failure"
Apr 29 23:53:35.141: INFO: Pod "client-envvars-8e54ebd6-92b8-4bab-a323-4b17581a3191": Phase="Pending", Reason="", readiness=false. Elapsed: 2.508461ms
Apr 29 23:53:37.144: INFO: Pod "client-envvars-8e54ebd6-92b8-4bab-a323-4b17581a3191": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005036166s
STEP: Saw pod success
Apr 29 23:53:37.144: INFO: Pod "client-envvars-8e54ebd6-92b8-4bab-a323-4b17581a3191" satisfied condition "success or failure"
Apr 29 23:53:37.146: INFO: Trying to get logs from node ip-192-168-211-77.us-west-2.compute.internal pod client-envvars-8e54ebd6-92b8-4bab-a323-4b17581a3191 container env3cont: <nil>
STEP: delete the pod
Apr 29 23:53:37.162: INFO: Waiting for pod client-envvars-8e54ebd6-92b8-4bab-a323-4b17581a3191 to disappear
Apr 29 23:53:37.164: INFO: Pod client-envvars-8e54ebd6-92b8-4bab-a323-4b17581a3191 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:53:37.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6792" for this suite.
Apr 29 23:53:49.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:53:49.260: INFO: namespace pods-6792 deletion completed in 12.092555439s

• [SLOW TEST:16.295 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:53:49.260: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-595
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr 29 23:53:49.432: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-595 /api/v1/namespaces/watch-595/configmaps/e2e-watch-test-resource-version 00017a27-3489-49c6-bcdb-363ede7d8773 18962 0 2020-04-29 23:53:49 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 29 23:53:49.432: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-595 /api/v1/namespaces/watch-595/configmaps/e2e-watch-test-resource-version 00017a27-3489-49c6-bcdb-363ede7d8773 18963 0 2020-04-29 23:53:49 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:53:49.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-595" for this suite.
Apr 29 23:53:55.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:53:55.528: INFO: namespace watch-595 deletion completed in 6.092956893s

• [SLOW TEST:6.268 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:53:55.529: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-1131
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:53:55.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1131" for this suite.
Apr 29 23:54:01.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:54:01.900: INFO: namespace custom-resource-definition-1131 deletion completed in 6.223026621s

• [SLOW TEST:6.371 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:54:01.900: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3806
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-1079d2da-55c8-4afd-9aca-d83e92d3f734
STEP: Creating a pod to test consume secrets
Apr 29 23:54:02.141: INFO: Waiting up to 5m0s for pod "pod-secrets-4b70b484-f80e-4167-96d5-0a29fec29a9a" in namespace "secrets-3806" to be "success or failure"
Apr 29 23:54:02.149: INFO: Pod "pod-secrets-4b70b484-f80e-4167-96d5-0a29fec29a9a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.879689ms
Apr 29 23:54:04.151: INFO: Pod "pod-secrets-4b70b484-f80e-4167-96d5-0a29fec29a9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010439349s
STEP: Saw pod success
Apr 29 23:54:04.151: INFO: Pod "pod-secrets-4b70b484-f80e-4167-96d5-0a29fec29a9a" satisfied condition "success or failure"
Apr 29 23:54:04.153: INFO: Trying to get logs from node ip-192-168-82-130.us-west-2.compute.internal pod pod-secrets-4b70b484-f80e-4167-96d5-0a29fec29a9a container secret-volume-test: <nil>
STEP: delete the pod
Apr 29 23:54:04.172: INFO: Waiting for pod pod-secrets-4b70b484-f80e-4167-96d5-0a29fec29a9a to disappear
Apr 29 23:54:04.174: INFO: Pod pod-secrets-4b70b484-f80e-4167-96d5-0a29fec29a9a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:54:04.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3806" for this suite.
Apr 29 23:54:10.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:54:10.277: INFO: namespace secrets-3806 deletion completed in 6.099437302s

• [SLOW TEST:8.376 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:54:10.277: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-6735
STEP: creating replication controller nodeport-test in namespace services-6735
I0429 23:54:10.442210      21 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-6735, replica count: 2
Apr 29 23:54:13.492: INFO: Creating new exec pod
I0429 23:54:13.492513      21 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 29 23:54:16.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=services-6735 execpodpxrtv -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Apr 29 23:54:16.685: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Apr 29 23:54:16.685: INFO: stdout: ""
Apr 29 23:54:16.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=services-6735 execpodpxrtv -- /bin/sh -x -c nc -zv -t -w 2 10.100.224.53 80'
Apr 29 23:54:16.823: INFO: stderr: "+ nc -zv -t -w 2 10.100.224.53 80\nConnection to 10.100.224.53 80 port [tcp/http] succeeded!\n"
Apr 29 23:54:16.823: INFO: stdout: ""
Apr 29 23:54:16.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=services-6735 execpodpxrtv -- /bin/sh -x -c nc -zv -t -w 2 192.168.144.27 30731'
Apr 29 23:54:16.968: INFO: stderr: "+ nc -zv -t -w 2 192.168.144.27 30731\nConnection to 192.168.144.27 30731 port [tcp/30731] succeeded!\n"
Apr 29 23:54:16.968: INFO: stdout: ""
Apr 29 23:54:16.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=services-6735 execpodpxrtv -- /bin/sh -x -c nc -zv -t -w 2 192.168.148.117 30731'
Apr 29 23:54:17.105: INFO: stderr: "+ nc -zv -t -w 2 192.168.148.117 30731\nConnection to 192.168.148.117 30731 port [tcp/30731] succeeded!\n"
Apr 29 23:54:17.105: INFO: stdout: ""
Apr 29 23:54:17.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=services-6735 execpodpxrtv -- /bin/sh -x -c nc -zv -t -w 2 34.213.218.100 30731'
Apr 29 23:54:17.254: INFO: stderr: "+ nc -zv -t -w 2 34.213.218.100 30731\nConnection to 34.213.218.100 30731 port [tcp/30731] succeeded!\n"
Apr 29 23:54:17.254: INFO: stdout: ""
Apr 29 23:54:17.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=services-6735 execpodpxrtv -- /bin/sh -x -c nc -zv -t -w 2 34.220.223.200 30731'
Apr 29 23:54:17.401: INFO: stderr: "+ nc -zv -t -w 2 34.220.223.200 30731\nConnection to 34.220.223.200 30731 port [tcp/30731] succeeded!\n"
Apr 29 23:54:17.401: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:54:17.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6735" for this suite.
Apr 29 23:54:23.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:54:23.500: INFO: namespace services-6735 deletion completed in 6.094671993s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:13.223 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:54:23.500: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9641
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:54:23.643: INFO: Creating ReplicaSet my-hostname-basic-9557b27f-6329-4762-86fc-b725bfb9bd00
Apr 29 23:54:23.648: INFO: Pod name my-hostname-basic-9557b27f-6329-4762-86fc-b725bfb9bd00: Found 0 pods out of 1
Apr 29 23:54:28.651: INFO: Pod name my-hostname-basic-9557b27f-6329-4762-86fc-b725bfb9bd00: Found 1 pods out of 1
Apr 29 23:54:28.651: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9557b27f-6329-4762-86fc-b725bfb9bd00" is running
Apr 29 23:54:28.653: INFO: Pod "my-hostname-basic-9557b27f-6329-4762-86fc-b725bfb9bd00-9bpf6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-29 23:54:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-29 23:54:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-29 23:54:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-29 23:54:23 +0000 UTC Reason: Message:}])
Apr 29 23:54:28.653: INFO: Trying to dial the pod
Apr 29 23:54:33.663: INFO: Controller my-hostname-basic-9557b27f-6329-4762-86fc-b725bfb9bd00: Got expected result from replica 1 [my-hostname-basic-9557b27f-6329-4762-86fc-b725bfb9bd00-9bpf6]: "my-hostname-basic-9557b27f-6329-4762-86fc-b725bfb9bd00-9bpf6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:54:33.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9641" for this suite.
Apr 29 23:54:39.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:54:39.760: INFO: namespace replicaset-9641 deletion completed in 6.093099254s

• [SLOW TEST:16.259 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:54:39.760: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7319
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Apr 29 23:54:39.911: INFO: Waiting up to 5m0s for pod "downward-api-de832d91-571c-441c-bbaa-325ad2179a51" in namespace "downward-api-7319" to be "success or failure"
Apr 29 23:54:39.914: INFO: Pod "downward-api-de832d91-571c-441c-bbaa-325ad2179a51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.35466ms
Apr 29 23:54:41.916: INFO: Pod "downward-api-de832d91-571c-441c-bbaa-325ad2179a51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004922629s
STEP: Saw pod success
Apr 29 23:54:41.916: INFO: Pod "downward-api-de832d91-571c-441c-bbaa-325ad2179a51" satisfied condition "success or failure"
Apr 29 23:54:41.918: INFO: Trying to get logs from node ip-192-168-222-85.us-west-2.compute.internal pod downward-api-de832d91-571c-441c-bbaa-325ad2179a51 container dapi-container: <nil>
STEP: delete the pod
Apr 29 23:54:41.938: INFO: Waiting for pod downward-api-de832d91-571c-441c-bbaa-325ad2179a51 to disappear
Apr 29 23:54:41.940: INFO: Pod downward-api-de832d91-571c-441c-bbaa-325ad2179a51 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:54:41.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7319" for this suite.
Apr 29 23:54:47.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:54:48.037: INFO: namespace downward-api-7319 deletion completed in 6.093073342s

• [SLOW TEST:8.277 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:54:48.038: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-88
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Apr 29 23:54:48.203: INFO: Waiting up to 5m0s for pod "var-expansion-a7a5e8bb-fb5b-4220-bf9b-cec7f78f3bdf" in namespace "var-expansion-88" to be "success or failure"
Apr 29 23:54:48.205: INFO: Pod "var-expansion-a7a5e8bb-fb5b-4220-bf9b-cec7f78f3bdf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043475ms
Apr 29 23:54:50.208: INFO: Pod "var-expansion-a7a5e8bb-fb5b-4220-bf9b-cec7f78f3bdf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004822968s
STEP: Saw pod success
Apr 29 23:54:50.208: INFO: Pod "var-expansion-a7a5e8bb-fb5b-4220-bf9b-cec7f78f3bdf" satisfied condition "success or failure"
Apr 29 23:54:50.210: INFO: Trying to get logs from node ip-192-168-211-77.us-west-2.compute.internal pod var-expansion-a7a5e8bb-fb5b-4220-bf9b-cec7f78f3bdf container dapi-container: <nil>
STEP: delete the pod
Apr 29 23:54:50.224: INFO: Waiting for pod var-expansion-a7a5e8bb-fb5b-4220-bf9b-cec7f78f3bdf to disappear
Apr 29 23:54:50.226: INFO: Pod var-expansion-a7a5e8bb-fb5b-4220-bf9b-cec7f78f3bdf no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:54:50.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-88" for this suite.
Apr 29 23:54:56.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:54:56.323: INFO: namespace var-expansion-88 deletion completed in 6.093980427s

• [SLOW TEST:8.286 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:54:56.323: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-5889
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Apr 29 23:54:56.466: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Apr 29 23:54:56.745: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Apr 29 23:54:58.777: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801296, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801296, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801296, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801296, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 23:55:00.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801296, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801296, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801296, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801296, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 23:55:02.780: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801296, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801296, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801296, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801296, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 29 23:55:05.202: INFO: Waited 416.710124ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:55:05.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5889" for this suite.
Apr 29 23:55:11.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:55:11.903: INFO: namespace aggregator-5889 deletion completed in 6.208366153s

• [SLOW TEST:15.579 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:55:11.904: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-4dql
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 23:55:12.074: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-4dql" in namespace "subpath-8826" to be "success or failure"
Apr 29 23:55:12.075: INFO: Pod "pod-subpath-test-projected-4dql": Phase="Pending", Reason="", readiness=false. Elapsed: 1.938955ms
Apr 29 23:55:14.078: INFO: Pod "pod-subpath-test-projected-4dql": Phase="Running", Reason="", readiness=true. Elapsed: 2.004543642s
Apr 29 23:55:16.081: INFO: Pod "pod-subpath-test-projected-4dql": Phase="Running", Reason="", readiness=true. Elapsed: 4.007302284s
Apr 29 23:55:18.084: INFO: Pod "pod-subpath-test-projected-4dql": Phase="Running", Reason="", readiness=true. Elapsed: 6.010130943s
Apr 29 23:55:20.087: INFO: Pod "pod-subpath-test-projected-4dql": Phase="Running", Reason="", readiness=true. Elapsed: 8.013042603s
Apr 29 23:55:22.089: INFO: Pod "pod-subpath-test-projected-4dql": Phase="Running", Reason="", readiness=true. Elapsed: 10.015825315s
Apr 29 23:55:24.093: INFO: Pod "pod-subpath-test-projected-4dql": Phase="Running", Reason="", readiness=true. Elapsed: 12.018991747s
Apr 29 23:55:26.095: INFO: Pod "pod-subpath-test-projected-4dql": Phase="Running", Reason="", readiness=true. Elapsed: 14.021754549s
Apr 29 23:55:28.098: INFO: Pod "pod-subpath-test-projected-4dql": Phase="Running", Reason="", readiness=true. Elapsed: 16.024568466s
Apr 29 23:55:30.101: INFO: Pod "pod-subpath-test-projected-4dql": Phase="Running", Reason="", readiness=true. Elapsed: 18.027283823s
Apr 29 23:55:32.104: INFO: Pod "pod-subpath-test-projected-4dql": Phase="Running", Reason="", readiness=true. Elapsed: 20.030113846s
Apr 29 23:55:34.106: INFO: Pod "pod-subpath-test-projected-4dql": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.032792384s
STEP: Saw pod success
Apr 29 23:55:34.106: INFO: Pod "pod-subpath-test-projected-4dql" satisfied condition "success or failure"
Apr 29 23:55:34.108: INFO: Trying to get logs from node ip-192-168-72-199.us-west-2.compute.internal pod pod-subpath-test-projected-4dql container test-container-subpath-projected-4dql: <nil>
STEP: delete the pod
Apr 29 23:55:34.128: INFO: Waiting for pod pod-subpath-test-projected-4dql to disappear
Apr 29 23:55:34.130: INFO: Pod pod-subpath-test-projected-4dql no longer exists
STEP: Deleting pod pod-subpath-test-projected-4dql
Apr 29 23:55:34.131: INFO: Deleting pod "pod-subpath-test-projected-4dql" in namespace "subpath-8826"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:55:34.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8826" for this suite.
Apr 29 23:55:40.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:55:40.230: INFO: namespace subpath-8826 deletion completed in 6.093275546s

• [SLOW TEST:28.326 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:55:40.230: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-614
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-rljq
STEP: Creating a pod to test atomic-volume-subpath
Apr 29 23:55:40.398: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rljq" in namespace "subpath-614" to be "success or failure"
Apr 29 23:55:40.400: INFO: Pod "pod-subpath-test-downwardapi-rljq": Phase="Pending", Reason="", readiness=false. Elapsed: 1.935459ms
Apr 29 23:55:42.402: INFO: Pod "pod-subpath-test-downwardapi-rljq": Phase="Running", Reason="", readiness=true. Elapsed: 2.004515195s
Apr 29 23:55:44.405: INFO: Pod "pod-subpath-test-downwardapi-rljq": Phase="Running", Reason="", readiness=true. Elapsed: 4.00701626s
Apr 29 23:55:46.410: INFO: Pod "pod-subpath-test-downwardapi-rljq": Phase="Running", Reason="", readiness=true. Elapsed: 6.011885857s
Apr 29 23:55:48.412: INFO: Pod "pod-subpath-test-downwardapi-rljq": Phase="Running", Reason="", readiness=true. Elapsed: 8.014579557s
Apr 29 23:55:50.415: INFO: Pod "pod-subpath-test-downwardapi-rljq": Phase="Running", Reason="", readiness=true. Elapsed: 10.017312504s
Apr 29 23:55:52.418: INFO: Pod "pod-subpath-test-downwardapi-rljq": Phase="Running", Reason="", readiness=true. Elapsed: 12.020348222s
Apr 29 23:55:54.421: INFO: Pod "pod-subpath-test-downwardapi-rljq": Phase="Running", Reason="", readiness=true. Elapsed: 14.023156719s
Apr 29 23:55:56.424: INFO: Pod "pod-subpath-test-downwardapi-rljq": Phase="Running", Reason="", readiness=true. Elapsed: 16.025790115s
Apr 29 23:55:58.426: INFO: Pod "pod-subpath-test-downwardapi-rljq": Phase="Running", Reason="", readiness=true. Elapsed: 18.028532525s
Apr 29 23:56:00.429: INFO: Pod "pod-subpath-test-downwardapi-rljq": Phase="Running", Reason="", readiness=true. Elapsed: 20.031476238s
Apr 29 23:56:02.432: INFO: Pod "pod-subpath-test-downwardapi-rljq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.034301153s
STEP: Saw pod success
Apr 29 23:56:02.432: INFO: Pod "pod-subpath-test-downwardapi-rljq" satisfied condition "success or failure"
Apr 29 23:56:02.434: INFO: Trying to get logs from node ip-192-168-211-77.us-west-2.compute.internal pod pod-subpath-test-downwardapi-rljq container test-container-subpath-downwardapi-rljq: <nil>
STEP: delete the pod
Apr 29 23:56:02.450: INFO: Waiting for pod pod-subpath-test-downwardapi-rljq to disappear
Apr 29 23:56:02.453: INFO: Pod pod-subpath-test-downwardapi-rljq no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rljq
Apr 29 23:56:02.453: INFO: Deleting pod "pod-subpath-test-downwardapi-rljq" in namespace "subpath-614"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:56:02.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-614" for this suite.
Apr 29 23:56:08.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:56:08.554: INFO: namespace subpath-614 deletion completed in 6.095395555s

• [SLOW TEST:28.324 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:56:08.554: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-207
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Apr 29 23:56:08.701: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:56:22.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-207" for this suite.
Apr 29 23:56:28.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:56:28.760: INFO: namespace crd-publish-openapi-207 deletion completed in 6.092532529s

• [SLOW TEST:20.206 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:56:28.762: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1037
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Apr 29 23:56:28.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 create -f - --namespace=kubectl-1037'
Apr 29 23:56:29.051: INFO: stderr: ""
Apr 29 23:56:29.051: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 29 23:56:30.055: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 23:56:30.055: INFO: Found 0 / 1
Apr 29 23:56:31.054: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 23:56:31.054: INFO: Found 0 / 1
Apr 29 23:56:32.055: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 23:56:32.055: INFO: Found 0 / 1
Apr 29 23:56:33.054: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 23:56:33.054: INFO: Found 0 / 1
Apr 29 23:56:34.055: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 23:56:34.055: INFO: Found 0 / 1
Apr 29 23:56:35.054: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 23:56:35.054: INFO: Found 0 / 1
Apr 29 23:56:36.055: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 23:56:36.055: INFO: Found 0 / 1
Apr 29 23:56:37.055: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 23:56:37.055: INFO: Found 1 / 1
Apr 29 23:56:37.055: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr 29 23:56:37.057: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 23:56:37.057: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 29 23:56:37.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 patch pod redis-master-2cp6x --namespace=kubectl-1037 -p {"metadata":{"annotations":{"x":"y"}}}'
Apr 29 23:56:37.121: INFO: stderr: ""
Apr 29 23:56:37.121: INFO: stdout: "pod/redis-master-2cp6x patched\n"
STEP: checking annotations
Apr 29 23:56:37.123: INFO: Selector matched 1 pods for map[app:redis]
Apr 29 23:56:37.123: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:56:37.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1037" for this suite.
Apr 29 23:56:49.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:56:49.220: INFO: namespace kubectl-1037 deletion completed in 12.092935932s

• [SLOW TEST:20.459 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:56:49.220: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6475
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Apr 29 23:56:49.364: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Apr 29 23:56:57.990: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 29 23:57:00.607: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:57:08.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6475" for this suite.
Apr 29 23:57:14.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:57:14.782: INFO: namespace crd-publish-openapi-6475 deletion completed in 6.130203685s

• [SLOW TEST:25.562 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:57:14.783: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5178
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 29 23:57:15.676: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 29 23:57:18.695: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:57:28.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5178" for this suite.
Apr 29 23:57:34.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:57:34.888: INFO: namespace webhook-5178 deletion completed in 6.091380438s
STEP: Destroying namespace "webhook-5178-markers" for this suite.
Apr 29 23:57:40.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:57:40.980: INFO: namespace webhook-5178-markers deletion completed in 6.091988055s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.216 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:57:40.999: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8799
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Apr 29 23:57:41.144: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:57:51.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8799" for this suite.
Apr 29 23:57:57.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:57:57.493: INFO: namespace pods-8799 deletion completed in 6.090630056s

• [SLOW TEST:16.494 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:57:57.493: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4567
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:57:57.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4567" for this suite.
Apr 29 23:58:03.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:58:03.758: INFO: namespace resourcequota-4567 deletion completed in 6.094512458s

• [SLOW TEST:6.265 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:58:03.758: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9002
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 29 23:58:03.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-9002'
Apr 29 23:58:03.962: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 29 23:58:03.962: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Apr 29 23:58:05.970: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-9h8dw]
Apr 29 23:58:05.970: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-9h8dw" in namespace "kubectl-9002" to be "running and ready"
Apr 29 23:58:05.972: INFO: Pod "e2e-test-httpd-rc-9h8dw": Phase="Running", Reason="", readiness=true. Elapsed: 2.263718ms
Apr 29 23:58:05.972: INFO: Pod "e2e-test-httpd-rc-9h8dw" satisfied condition "running and ready"
Apr 29 23:58:05.972: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-9h8dw]
Apr 29 23:58:05.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 logs rc/e2e-test-httpd-rc --namespace=kubectl-9002'
Apr 29 23:58:06.045: INFO: stderr: ""
Apr 29 23:58:06.045: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.74.13. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 192.168.74.13. Set the 'ServerName' directive globally to suppress this message\n[Wed Apr 29 23:58:05.113535 2020] [mpm_event:notice] [pid 1:tid 139811121359720] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Wed Apr 29 23:58:05.113573 2020] [core:notice] [pid 1:tid 139811121359720] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Apr 29 23:58:06.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete rc e2e-test-httpd-rc --namespace=kubectl-9002'
Apr 29 23:58:06.112: INFO: stderr: ""
Apr 29 23:58:06.112: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:58:06.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9002" for this suite.
Apr 29 23:58:12.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:58:12.209: INFO: namespace kubectl-9002 deletion completed in 6.093099455s

• [SLOW TEST:8.451 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:58:12.210: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8428
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-33c15a40-add2-42d9-b52d-2c1d7b11acaf
STEP: Creating a pod to test consume configMaps
Apr 29 23:58:12.365: INFO: Waiting up to 5m0s for pod "pod-configmaps-1032b54e-c181-471b-b35d-e7ccd3dbe3e3" in namespace "configmap-8428" to be "success or failure"
Apr 29 23:58:12.368: INFO: Pod "pod-configmaps-1032b54e-c181-471b-b35d-e7ccd3dbe3e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.119081ms
Apr 29 23:58:14.370: INFO: Pod "pod-configmaps-1032b54e-c181-471b-b35d-e7ccd3dbe3e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004935518s
STEP: Saw pod success
Apr 29 23:58:14.370: INFO: Pod "pod-configmaps-1032b54e-c181-471b-b35d-e7ccd3dbe3e3" satisfied condition "success or failure"
Apr 29 23:58:14.373: INFO: Trying to get logs from node ip-192-168-197-211.us-west-2.compute.internal pod pod-configmaps-1032b54e-c181-471b-b35d-e7ccd3dbe3e3 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 29 23:58:14.391: INFO: Waiting for pod pod-configmaps-1032b54e-c181-471b-b35d-e7ccd3dbe3e3 to disappear
Apr 29 23:58:14.393: INFO: Pod pod-configmaps-1032b54e-c181-471b-b35d-e7ccd3dbe3e3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:58:14.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8428" for this suite.
Apr 29 23:58:20.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:58:20.489: INFO: namespace configmap-8428 deletion completed in 6.092241016s

• [SLOW TEST:8.279 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:58:20.489: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2464
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:58:20.631: INFO: Creating deployment "webserver-deployment"
Apr 29 23:58:20.635: INFO: Waiting for observed generation 1
Apr 29 23:58:22.640: INFO: Waiting for all required pods to come up
Apr 29 23:58:22.642: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr 29 23:58:22.643: INFO: Waiting for deployment "webserver-deployment" to complete
Apr 29 23:58:22.647: INFO: Updating deployment "webserver-deployment" with a non-existent image
Apr 29 23:58:22.653: INFO: Updating deployment webserver-deployment
Apr 29 23:58:22.653: INFO: Waiting for observed generation 2
Apr 29 23:58:24.658: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr 29 23:58:24.660: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr 29 23:58:24.662: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 29 23:58:24.667: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr 29 23:58:24.667: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr 29 23:58:24.669: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 29 23:58:24.673: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Apr 29 23:58:24.673: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Apr 29 23:58:24.677: INFO: Updating deployment webserver-deployment
Apr 29 23:58:24.677: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Apr 29 23:58:24.681: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr 29 23:58:24.683: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Apr 29 23:58:26.689: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2464 /apis/apps/v1/namespaces/deployment-2464/deployments/webserver-deployment 60a268dc-9dbc-41d8-a00d-f51c7434dc27 20692 3 2020-04-29 23:58:20 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0058cb4a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:12,UnavailableReplicas:21,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-04-29 23:58:24 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-04-29 23:58:26 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,},},ReadyReplicas:12,CollisionCount:nil,},}

Apr 29 23:58:26.693: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-2464 /apis/apps/v1/namespaces/deployment-2464/replicasets/webserver-deployment-c7997dcc8 7314d295-7779-4d58-ad96-5142492395bd 20583 3 2020-04-29 23:58:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 60a268dc-9dbc-41d8-a00d-f51c7434dc27 0xc0058cba97 0xc0058cba98}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0058cbb18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 29 23:58:26.693: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Apr 29 23:58:26.694: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-2464 /apis/apps/v1/namespaces/deployment-2464/replicasets/webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 20691 3 2020-04-29 23:58:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 60a268dc-9dbc-41d8-a00d-f51c7434dc27 0xc0058cb997 0xc0058cb998}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0058cb9f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:12,AvailableReplicas:12,Conditions:[]ReplicaSetCondition{},},}
Apr 29 23:58:26.697: INFO: Pod "webserver-deployment-595b5b9587-45t8w" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-45t8w webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-45t8w 3e988d10-18da-4bea-bd14-0730840c2d9f 20661 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc00531e8c7 0xc00531e8c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-199.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.72.199,PodIP:192.168.64.176,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-29 23:58:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://56f5bcb0dd464c51f96c4c47fd1a5c4ee6727db6cf8d265b635435c4a6562cda,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.64.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.698: INFO: Pod "webserver-deployment-595b5b9587-694zg" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-694zg webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-694zg 0ffa00a4-8078-4832-bd86-67cb8fe14709 20543 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc00531ea97 0xc00531ea98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-149-254.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.149.254,PodIP:,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.698: INFO: Pod "webserver-deployment-595b5b9587-69xz2" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-69xz2 webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-69xz2 f42a8afa-3d09-4465-969b-1adc19641a05 20547 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc00531ec37 0xc00531ec38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-83-225.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.83.225,PodIP:,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.698: INFO: Pod "webserver-deployment-595b5b9587-8jk7n" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8jk7n webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-8jk7n c4084d38-b98e-455a-a5a1-1b93438b8a64 20678 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc00531ee37 0xc00531ee38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-148-117.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.148.117,PodIP:192.168.150.193,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-29 23:58:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2c302981454a2795d51aebfccadd3479e2649e263ad0b1f2444295764d36b493,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.193,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.698: INFO: Pod "webserver-deployment-595b5b9587-8vthb" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8vthb webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-8vthb 2b46b09b-9a83-4a67-9d21-931e224e249e 20570 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc00531f047 0xc00531f048}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-144-27.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.144.27,PodIP:,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.698: INFO: Pod "webserver-deployment-595b5b9587-9hldc" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9hldc webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-9hldc de4def78-f4b7-4ede-9abb-eee44c842e8e 20407 0 2020-04-29 23:58:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc00531f197 0xc00531f198}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-148-117.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.148.117,PodIP:192.168.136.176,StartTime:2020-04-29 23:58:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-29 23:58:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://4dfaeb820beb3f1995d0a3b7092c172ec73cc757e4d35dceaa87c7a7ee88e03c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.136.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.698: INFO: Pod "webserver-deployment-595b5b9587-9rrcf" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-9rrcf webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-9rrcf 6273a6ea-5a1c-47f9-82d3-06d14d53c0c8 20573 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc00531f3d7 0xc00531f3d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-211-77.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.211.77,PodIP:,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.699: INFO: Pod "webserver-deployment-595b5b9587-h62d6" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-h62d6 webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-h62d6 46b30d6d-acd0-4db0-b7d9-78b8bc9053d6 20568 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc00531f577 0xc00531f578}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-197-211.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.197.211,PodIP:,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.699: INFO: Pod "webserver-deployment-595b5b9587-hfngc" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hfngc webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-hfngc 02ba5d8d-8c51-48b0-a32f-3e06d318d819 20538 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc00531f707 0xc00531f708}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-197-211.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.197.211,PodIP:,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.699: INFO: Pod "webserver-deployment-595b5b9587-mpwp4" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mpwp4 webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-mpwp4 959d0be5-a045-4eda-b4b4-1b636c95c0cb 20653 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc00531f877 0xc00531f878}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-82-130.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.82.130,PodIP:192.168.76.140,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-29 23:58:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6c3538ad4e6ea910fff7597b2f950196610d8dad64fa4e7b2a66a3003d2f4adf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.76.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.699: INFO: Pod "webserver-deployment-595b5b9587-mzxc8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mzxc8 webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-mzxc8 bebc86a2-11ea-401e-b804-4617e8444737 20417 0 2020-04-29 23:58:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc00531fa57 0xc00531fa58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-218-192.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.218.192,PodIP:192.168.208.105,StartTime:2020-04-29 23:58:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-29 23:58:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b37f1f94370de5105aa89949fec2bc65b46b179977b5552ddc483032d6ae0843,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.208.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.699: INFO: Pod "webserver-deployment-595b5b9587-nwqwx" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-nwqwx webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-nwqwx f6d0fcc8-89b3-48e5-97d9-83a327917779 20404 0 2020-04-29 23:58:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc00531fc47 0xc00531fc48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-222-85.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.222.85,PodIP:192.168.197.61,StartTime:2020-04-29 23:58:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-29 23:58:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://6124a3f1d67959f127b6dfbe67e4aa9f7f17f4feb752717f996b2ae1552a4f1f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.197.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.699: INFO: Pod "webserver-deployment-595b5b9587-p5kqb" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p5kqb webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-p5kqb dfc87345-c52c-4b68-975f-f633899b2056 20411 0 2020-04-29 23:58:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc00531fde7 0xc00531fde8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-149-254.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.149.254,PodIP:192.168.142.124,StartTime:2020-04-29 23:58:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-29 23:58:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b62ce4b2b194e4e193c58f9c90e1da290d198f2907dddb3c1655616a0833871f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.142.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.700: INFO: Pod "webserver-deployment-595b5b9587-ql2zl" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-ql2zl webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-ql2zl acdc237d-bfce-4e67-879e-e747ba08a73a 20525 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc00531ffd7 0xc00531ffd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-144-27.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.144.27,PodIP:,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.700: INFO: Pod "webserver-deployment-595b5b9587-qn5bq" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qn5bq webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-qn5bq 6436c320-221d-4180-8cfc-4995c4d4c226 20689 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc005060157 0xc005060158}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-222-85.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.222.85,PodIP:192.168.203.161,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-29 23:58:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://47205c759dda0f93d6677f895f67179ec2c4b9927b84f7e29d6b124d02f670f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.203.161,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.700: INFO: Pod "webserver-deployment-595b5b9587-sfx9l" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-sfx9l webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-sfx9l 57f4f218-43c7-4be1-806d-856cb5842c9b 20405 0 2020-04-29 23:58:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc005060377 0xc005060378}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-83-225.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.83.225,PodIP:192.168.75.11,StartTime:2020-04-29 23:58:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-29 23:58:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a3a78b7512fb5f1323040d04e48873f42c0a586b9119ab18f48e00094023435d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.75.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.700: INFO: Pod "webserver-deployment-595b5b9587-tbmfr" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-tbmfr webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-tbmfr 4dec851e-475a-4364-8671-d9d781823743 20566 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc005060517 0xc005060518}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-218-192.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.218.192,PodIP:,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.700: INFO: Pod "webserver-deployment-595b5b9587-v8xsj" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-v8xsj webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-v8xsj d891f09c-d245-483f-bc2e-a8474e9d07a1 20395 0 2020-04-29 23:58:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc005060747 0xc005060748}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-82-130.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.82.130,PodIP:192.168.84.159,StartTime:2020-04-29 23:58:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-29 23:58:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://1758bcb5974b9395c864b6bb97a1fe50b1fa30f9ef1337017a70aabec400737e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.84.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.701: INFO: Pod "webserver-deployment-595b5b9587-wvvlp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wvvlp webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-wvvlp 96055525-632c-4e49-9ee2-27e82e39352f 20398 0 2020-04-29 23:58:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc0050608f7 0xc0050608f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-199.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.72.199,PodIP:192.168.72.224,StartTime:2020-04-29 23:58:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-29 23:58:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://5a52dced28ba9bd89358570056db7dbb95d7159c6395c3cdf6dc4b3b7758b170,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.72.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.701: INFO: Pod "webserver-deployment-595b5b9587-zmgnv" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zmgnv webserver-deployment-595b5b9587- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-595b5b9587-zmgnv 3e339416-8795-41aa-ae8c-a78475b1fe0e 20392 0 2020-04-29 23:58:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 067f9dce-2a27-4b61-833e-631a17f29ef1 0xc005060b67 0xc005060b68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-211-77.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.211.77,PodIP:192.168.211.193,StartTime:2020-04-29 23:58:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-29 23:58:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://806965d9eaacdbe5c6549905a7129387f4d64a2fcf68e6259eb735d1f54e4c7d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.211.193,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.701: INFO: Pod "webserver-deployment-c7997dcc8-2sbrf" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-2sbrf webserver-deployment-c7997dcc8- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-c7997dcc8-2sbrf f5a572ff-d51c-4c68-95c8-6818819a280a 20545 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7314d295-7779-4d58-ad96-5142492395bd 0xc005060cf7 0xc005060cf8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-218-192.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.218.192,PodIP:,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.701: INFO: Pod "webserver-deployment-c7997dcc8-4dxd2" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4dxd2 webserver-deployment-c7997dcc8- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-c7997dcc8-4dxd2 3966b06a-e0f2-4556-8465-b2eff4375836 20693 0 2020-04-29 23:58:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7314d295-7779-4d58-ad96-5142492395bd 0xc005060f17 0xc005060f18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-222-85.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.222.85,PodIP:192.168.222.112,StartTime:2020-04-29 23:58:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.222.112,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.701: INFO: Pod "webserver-deployment-c7997dcc8-bgm6t" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-bgm6t webserver-deployment-c7997dcc8- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-c7997dcc8-bgm6t a7e3ed6f-25d9-4bd3-bcee-6ee83cf3a951 20557 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7314d295-7779-4d58-ad96-5142492395bd 0xc005061117 0xc005061118}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-149-254.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.149.254,PodIP:,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.702: INFO: Pod "webserver-deployment-c7997dcc8-flbt4" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-flbt4 webserver-deployment-c7997dcc8- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-c7997dcc8-flbt4 32116789-758c-4d8e-bf8a-38fb4d2d81d0 20560 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7314d295-7779-4d58-ad96-5142492395bd 0xc005061337 0xc005061338}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-83-225.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.83.225,PodIP:,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.702: INFO: Pod "webserver-deployment-c7997dcc8-hgxkl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-hgxkl webserver-deployment-c7997dcc8- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-c7997dcc8-hgxkl ff55bb8e-14a1-4bb7-ae62-7e004f6ac5b5 20637 0 2020-04-29 23:58:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7314d295-7779-4d58-ad96-5142492395bd 0xc0050614b7 0xc0050614b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-197-211.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.197.211,PodIP:192.168.193.43,StartTime:2020-04-29 23:58:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.193.43,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.702: INFO: Pod "webserver-deployment-c7997dcc8-k9c85" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-k9c85 webserver-deployment-c7997dcc8- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-c7997dcc8-k9c85 2543fe9e-ef50-4853-b232-51e89bdd8ef5 20646 0 2020-04-29 23:58:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7314d295-7779-4d58-ad96-5142492395bd 0xc0050616d7 0xc0050616d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-83-225.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.83.225,PodIP:192.168.79.98,StartTime:2020-04-29 23:58:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.79.98,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.702: INFO: Pod "webserver-deployment-c7997dcc8-l76kr" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-l76kr webserver-deployment-c7997dcc8- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-c7997dcc8-l76kr 3775591e-d891-4cfe-b3dd-da6ffc9feeb2 20548 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7314d295-7779-4d58-ad96-5142492395bd 0xc005061887 0xc005061888}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-144-27.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.144.27,PodIP:,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.703: INFO: Pod "webserver-deployment-c7997dcc8-lzwhc" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lzwhc webserver-deployment-c7997dcc8- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-c7997dcc8-lzwhc b7d439f3-b55f-45bf-a054-edad95b9dfc2 20686 0 2020-04-29 23:58:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7314d295-7779-4d58-ad96-5142492395bd 0xc005061aa7 0xc005061aa8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-148-117.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.148.117,PodIP:192.168.139.217,StartTime:2020-04-29 23:58:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.139.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.703: INFO: Pod "webserver-deployment-c7997dcc8-srfcp" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-srfcp webserver-deployment-c7997dcc8- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-c7997dcc8-srfcp a71d9dce-7c6a-4016-836a-e5bfbbb5187e 20541 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7314d295-7779-4d58-ad96-5142492395bd 0xc005061cb7 0xc005061cb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-72-199.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.72.199,PodIP:,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.703: INFO: Pod "webserver-deployment-c7997dcc8-tkshj" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-tkshj webserver-deployment-c7997dcc8- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-c7997dcc8-tkshj bd0fe89e-0687-411c-9a09-41efe520d278 20592 0 2020-04-29 23:58:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7314d295-7779-4d58-ad96-5142492395bd 0xc005061e57 0xc005061e58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-82-130.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.82.130,PodIP:192.168.90.142,StartTime:2020-04-29 23:58:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.90.142,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.705: INFO: Pod "webserver-deployment-c7997dcc8-w5kwz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-w5kwz webserver-deployment-c7997dcc8- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-c7997dcc8-w5kwz 5010be51-1403-40c1-a16f-b70e73eb8c78 20569 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7314d295-7779-4d58-ad96-5142492395bd 0xc005042067 0xc005042068}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-149-254.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.149.254,PodIP:,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.706: INFO: Pod "webserver-deployment-c7997dcc8-xl5nf" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xl5nf webserver-deployment-c7997dcc8- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-c7997dcc8-xl5nf 04ff6d2d-5345-4378-9d44-165c7d230f0c 20579 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7314d295-7779-4d58-ad96-5142492395bd 0xc005042217 0xc005042218}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-211-77.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.211.77,PodIP:,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 29 23:58:26.707: INFO: Pod "webserver-deployment-c7997dcc8-z4zbz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-z4zbz webserver-deployment-c7997dcc8- deployment-2464 /api/v1/namespaces/deployment-2464/pods/webserver-deployment-c7997dcc8-z4zbz fa837a7e-f9b8-4ceb-83c3-75671311c8ef 20556 0 2020-04-29 23:58:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7314d295-7779-4d58-ad96-5142492395bd 0xc005042477 0xc005042478}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5z5m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5z5m,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5z5m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-211-77.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-29 23:58:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.211.77,PodIP:,StartTime:2020-04-29 23:58:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:58:26.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2464" for this suite.
Apr 29 23:58:34.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:58:34.807: INFO: namespace deployment-2464 deletion completed in 8.09566602s

• [SLOW TEST:14.317 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:58:34.807: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4698
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 29 23:58:34.967: INFO: Waiting up to 5m0s for pod "pod-0314368c-dacf-45dd-9b44-a0f320f63ba5" in namespace "emptydir-4698" to be "success or failure"
Apr 29 23:58:34.969: INFO: Pod "pod-0314368c-dacf-45dd-9b44-a0f320f63ba5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069264ms
Apr 29 23:58:36.972: INFO: Pod "pod-0314368c-dacf-45dd-9b44-a0f320f63ba5": Phase="Running", Reason="", readiness=true. Elapsed: 2.005150224s
Apr 29 23:58:38.975: INFO: Pod "pod-0314368c-dacf-45dd-9b44-a0f320f63ba5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008180758s
STEP: Saw pod success
Apr 29 23:58:38.975: INFO: Pod "pod-0314368c-dacf-45dd-9b44-a0f320f63ba5" satisfied condition "success or failure"
Apr 29 23:58:38.977: INFO: Trying to get logs from node ip-192-168-148-117.us-west-2.compute.internal pod pod-0314368c-dacf-45dd-9b44-a0f320f63ba5 container test-container: <nil>
STEP: delete the pod
Apr 29 23:58:38.997: INFO: Waiting for pod pod-0314368c-dacf-45dd-9b44-a0f320f63ba5 to disappear
Apr 29 23:58:38.999: INFO: Pod pod-0314368c-dacf-45dd-9b44-a0f320f63ba5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:58:38.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4698" for this suite.
Apr 29 23:58:45.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:58:45.095: INFO: namespace emptydir-4698 deletion completed in 6.092488986s

• [SLOW TEST:10.289 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:58:45.096: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-911
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Apr 29 23:58:45.440: INFO: Waiting up to 5m0s for pod "client-containers-1b203053-18ea-45ef-8b3a-914225c4bfa6" in namespace "containers-911" to be "success or failure"
Apr 29 23:58:45.442: INFO: Pod "client-containers-1b203053-18ea-45ef-8b3a-914225c4bfa6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.963001ms
Apr 29 23:58:47.445: INFO: Pod "client-containers-1b203053-18ea-45ef-8b3a-914225c4bfa6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00457743s
STEP: Saw pod success
Apr 29 23:58:47.445: INFO: Pod "client-containers-1b203053-18ea-45ef-8b3a-914225c4bfa6" satisfied condition "success or failure"
Apr 29 23:58:47.447: INFO: Trying to get logs from node ip-192-168-82-130.us-west-2.compute.internal pod client-containers-1b203053-18ea-45ef-8b3a-914225c4bfa6 container test-container: <nil>
STEP: delete the pod
Apr 29 23:58:47.464: INFO: Waiting for pod client-containers-1b203053-18ea-45ef-8b3a-914225c4bfa6 to disappear
Apr 29 23:58:47.466: INFO: Pod client-containers-1b203053-18ea-45ef-8b3a-914225c4bfa6 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:58:47.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-911" for this suite.
Apr 29 23:58:53.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:58:53.563: INFO: namespace containers-911 deletion completed in 6.093345676s

• [SLOW TEST:8.468 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:58:53.563: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3421
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 29 23:58:53.706: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:58:55.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3421" for this suite.
Apr 29 23:59:43.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 29 23:59:43.895: INFO: namespace pods-3421 deletion completed in 48.094153128s

• [SLOW TEST:50.331 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 29 23:59:43.895: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-8682
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr 29 23:59:47.062: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 29 23:59:48.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8682" for this suite.
Apr 30 00:00:16.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:00:16.317: INFO: namespace replicaset-8682 deletion completed in 28.240712251s

• [SLOW TEST:32.422 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:00:16.317: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 30 00:00:20.511: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 30 00:00:20.513: INFO: Pod pod-with-prestop-http-hook still exists
Apr 30 00:00:22.513: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 30 00:00:22.516: INFO: Pod pod-with-prestop-http-hook still exists
Apr 30 00:00:24.513: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 30 00:00:24.516: INFO: Pod pod-with-prestop-http-hook still exists
Apr 30 00:00:26.513: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 30 00:00:26.516: INFO: Pod pod-with-prestop-http-hook still exists
Apr 30 00:00:28.513: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 30 00:00:28.516: INFO: Pod pod-with-prestop-http-hook still exists
Apr 30 00:00:30.513: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 30 00:00:30.516: INFO: Pod pod-with-prestop-http-hook still exists
Apr 30 00:00:32.513: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 30 00:00:32.516: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:00:32.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7755" for this suite.
Apr 30 00:00:44.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:00:44.622: INFO: namespace container-lifecycle-hook-7755 deletion completed in 12.091954568s

• [SLOW TEST:28.305 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:00:44.622: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8202
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 30 00:00:48.807: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 30 00:00:48.809: INFO: Pod pod-with-poststart-http-hook still exists
Apr 30 00:00:50.809: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 30 00:00:50.812: INFO: Pod pod-with-poststart-http-hook still exists
Apr 30 00:00:52.809: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 30 00:00:52.812: INFO: Pod pod-with-poststart-http-hook still exists
Apr 30 00:00:54.809: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 30 00:00:54.812: INFO: Pod pod-with-poststart-http-hook still exists
Apr 30 00:00:56.809: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 30 00:00:56.812: INFO: Pod pod-with-poststart-http-hook still exists
Apr 30 00:00:58.809: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 30 00:00:58.812: INFO: Pod pod-with-poststart-http-hook still exists
Apr 30 00:01:00.809: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 30 00:01:00.812: INFO: Pod pod-with-poststart-http-hook still exists
Apr 30 00:01:02.809: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 30 00:01:02.812: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:01:02.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8202" for this suite.
Apr 30 00:01:14.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:01:14.980: INFO: namespace container-lifecycle-hook-8202 deletion completed in 12.164071277s

• [SLOW TEST:30.358 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:01:14.980: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5007
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 30 00:01:15.606: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 30 00:01:17.613: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801675, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801675, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801675, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801675, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 30 00:01:20.629: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:01:20.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5007" for this suite.
Apr 30 00:01:32.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:01:32.773: INFO: namespace webhook-5007 deletion completed in 12.095582412s
STEP: Destroying namespace "webhook-5007-markers" for this suite.
Apr 30 00:01:38.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:01:38.868: INFO: namespace webhook-5007-markers deletion completed in 6.094232002s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.905 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:01:38.886: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6120
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Apr 30 00:01:39.028: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:01:41.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6120" for this suite.
Apr 30 00:01:47.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:01:47.971: INFO: namespace init-container-6120 deletion completed in 6.093285357s

• [SLOW TEST:9.086 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:01:47.971: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9771
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-9771
I0430 00:01:48.146285      21 runners.go:184] Created replication controller with name: externalname-service, namespace: services-9771, replica count: 2
Apr 30 00:01:51.196: INFO: Creating new exec pod
I0430 00:01:51.196577      21 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 30 00:01:56.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=services-9771 execpodhnrlw -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Apr 30 00:01:56.367: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 30 00:01:56.367: INFO: stdout: ""
Apr 30 00:01:56.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=services-9771 execpodhnrlw -- /bin/sh -x -c nc -zv -t -w 2 10.100.59.25 80'
Apr 30 00:01:56.508: INFO: stderr: "+ nc -zv -t -w 2 10.100.59.25 80\nConnection to 10.100.59.25 80 port [tcp/http] succeeded!\n"
Apr 30 00:01:56.508: INFO: stdout: ""
Apr 30 00:01:56.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=services-9771 execpodhnrlw -- /bin/sh -x -c nc -zv -t -w 2 192.168.144.27 30131'
Apr 30 00:01:56.656: INFO: stderr: "+ nc -zv -t -w 2 192.168.144.27 30131\nConnection to 192.168.144.27 30131 port [tcp/30131] succeeded!\n"
Apr 30 00:01:56.656: INFO: stdout: ""
Apr 30 00:01:56.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=services-9771 execpodhnrlw -- /bin/sh -x -c nc -zv -t -w 2 192.168.148.117 30131'
Apr 30 00:01:56.800: INFO: stderr: "+ nc -zv -t -w 2 192.168.148.117 30131\nConnection to 192.168.148.117 30131 port [tcp/30131] succeeded!\n"
Apr 30 00:01:56.800: INFO: stdout: ""
Apr 30 00:01:56.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=services-9771 execpodhnrlw -- /bin/sh -x -c nc -zv -t -w 2 34.213.218.100 30131'
Apr 30 00:01:56.956: INFO: stderr: "+ nc -zv -t -w 2 34.213.218.100 30131\nConnection to 34.213.218.100 30131 port [tcp/30131] succeeded!\n"
Apr 30 00:01:56.956: INFO: stdout: ""
Apr 30 00:01:56.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=services-9771 execpodhnrlw -- /bin/sh -x -c nc -zv -t -w 2 34.220.223.200 30131'
Apr 30 00:01:57.101: INFO: stderr: "+ nc -zv -t -w 2 34.220.223.200 30131\nConnection to 34.220.223.200 30131 port [tcp/30131] succeeded!\n"
Apr 30 00:01:57.101: INFO: stdout: ""
Apr 30 00:01:57.101: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:01:57.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9771" for this suite.
Apr 30 00:02:03.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:02:03.237: INFO: namespace services-9771 deletion completed in 6.099195085s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:15.266 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:02:03.237: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4226
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-4e048f0c-a589-4739-817f-34a92dcfea4f
STEP: Creating a pod to test consume secrets
Apr 30 00:02:03.405: INFO: Waiting up to 5m0s for pod "pod-secrets-2f571317-3ca3-4ebb-b082-830055f03f72" in namespace "secrets-4226" to be "success or failure"
Apr 30 00:02:03.407: INFO: Pod "pod-secrets-2f571317-3ca3-4ebb-b082-830055f03f72": Phase="Pending", Reason="", readiness=false. Elapsed: 1.928988ms
Apr 30 00:02:05.410: INFO: Pod "pod-secrets-2f571317-3ca3-4ebb-b082-830055f03f72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004871448s
STEP: Saw pod success
Apr 30 00:02:05.410: INFO: Pod "pod-secrets-2f571317-3ca3-4ebb-b082-830055f03f72" satisfied condition "success or failure"
Apr 30 00:02:05.413: INFO: Trying to get logs from node ip-192-168-83-225.us-west-2.compute.internal pod pod-secrets-2f571317-3ca3-4ebb-b082-830055f03f72 container secret-volume-test: <nil>
STEP: delete the pod
Apr 30 00:02:05.433: INFO: Waiting for pod pod-secrets-2f571317-3ca3-4ebb-b082-830055f03f72 to disappear
Apr 30 00:02:05.435: INFO: Pod pod-secrets-2f571317-3ca3-4ebb-b082-830055f03f72 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:02:05.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4226" for this suite.
Apr 30 00:02:11.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:02:11.533: INFO: namespace secrets-4226 deletion completed in 6.09358703s

• [SLOW TEST:8.296 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:02:11.534: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-878
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-09004cc2-6c6c-48ba-b666-6de7dc4c4c67
STEP: Creating a pod to test consume secrets
Apr 30 00:02:11.700: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9cef09ce-5b0a-4b55-a60a-3f0fd0621708" in namespace "projected-878" to be "success or failure"
Apr 30 00:02:11.702: INFO: Pod "pod-projected-secrets-9cef09ce-5b0a-4b55-a60a-3f0fd0621708": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011444ms
Apr 30 00:02:13.704: INFO: Pod "pod-projected-secrets-9cef09ce-5b0a-4b55-a60a-3f0fd0621708": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004715639s
STEP: Saw pod success
Apr 30 00:02:13.704: INFO: Pod "pod-projected-secrets-9cef09ce-5b0a-4b55-a60a-3f0fd0621708" satisfied condition "success or failure"
Apr 30 00:02:13.706: INFO: Trying to get logs from node ip-192-168-222-85.us-west-2.compute.internal pod pod-projected-secrets-9cef09ce-5b0a-4b55-a60a-3f0fd0621708 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 30 00:02:13.730: INFO: Waiting for pod pod-projected-secrets-9cef09ce-5b0a-4b55-a60a-3f0fd0621708 to disappear
Apr 30 00:02:13.732: INFO: Pod pod-projected-secrets-9cef09ce-5b0a-4b55-a60a-3f0fd0621708 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:02:13.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-878" for this suite.
Apr 30 00:02:19.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:02:19.833: INFO: namespace projected-878 deletion completed in 6.097840888s

• [SLOW TEST:8.300 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:02:19.834: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9511
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-60bf483d-c3c5-4c5c-81a8-4063994bb094
STEP: Creating configMap with name cm-test-opt-upd-9b7150fe-b670-495e-8985-d3e2467716b0
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-60bf483d-c3c5-4c5c-81a8-4063994bb094
STEP: Updating configmap cm-test-opt-upd-9b7150fe-b670-495e-8985-d3e2467716b0
STEP: Creating configMap with name cm-test-opt-create-3fc10889-e412-4d76-be1a-26b7106a525f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:03:40.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9511" for this suite.
Apr 30 00:03:52.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:03:52.497: INFO: namespace projected-9511 deletion completed in 12.095722119s

• [SLOW TEST:92.663 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:03:52.497: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9986
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-5b3d6def-0f22-4ff9-97ef-fe2f8a34febe
STEP: Creating a pod to test consume secrets
Apr 30 00:03:52.666: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-09cfb3c6-d4ee-43db-8d32-a9f8bbc7daa0" in namespace "projected-9986" to be "success or failure"
Apr 30 00:03:52.668: INFO: Pod "pod-projected-secrets-09cfb3c6-d4ee-43db-8d32-a9f8bbc7daa0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.454243ms
Apr 30 00:03:54.671: INFO: Pod "pod-projected-secrets-09cfb3c6-d4ee-43db-8d32-a9f8bbc7daa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005173139s
STEP: Saw pod success
Apr 30 00:03:54.671: INFO: Pod "pod-projected-secrets-09cfb3c6-d4ee-43db-8d32-a9f8bbc7daa0" satisfied condition "success or failure"
Apr 30 00:03:54.673: INFO: Trying to get logs from node ip-192-168-149-254.us-west-2.compute.internal pod pod-projected-secrets-09cfb3c6-d4ee-43db-8d32-a9f8bbc7daa0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 30 00:03:54.687: INFO: Waiting for pod pod-projected-secrets-09cfb3c6-d4ee-43db-8d32-a9f8bbc7daa0 to disappear
Apr 30 00:03:54.690: INFO: Pod pod-projected-secrets-09cfb3c6-d4ee-43db-8d32-a9f8bbc7daa0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:03:54.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9986" for this suite.
Apr 30 00:04:00.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:04:00.787: INFO: namespace projected-9986 deletion completed in 6.093098336s

• [SLOW TEST:8.290 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:04:00.787: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-931
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-608efccf-20aa-4a3b-97b7-67bd9813c0c6
Apr 30 00:04:00.937: INFO: Pod name my-hostname-basic-608efccf-20aa-4a3b-97b7-67bd9813c0c6: Found 0 pods out of 1
Apr 30 00:04:05.940: INFO: Pod name my-hostname-basic-608efccf-20aa-4a3b-97b7-67bd9813c0c6: Found 1 pods out of 1
Apr 30 00:04:05.940: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-608efccf-20aa-4a3b-97b7-67bd9813c0c6" are running
Apr 30 00:04:05.942: INFO: Pod "my-hostname-basic-608efccf-20aa-4a3b-97b7-67bd9813c0c6-ltrp2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-30 00:04:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-30 00:04:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-30 00:04:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-30 00:04:00 +0000 UTC Reason: Message:}])
Apr 30 00:04:05.942: INFO: Trying to dial the pod
Apr 30 00:04:10.950: INFO: Controller my-hostname-basic-608efccf-20aa-4a3b-97b7-67bd9813c0c6: Got expected result from replica 1 [my-hostname-basic-608efccf-20aa-4a3b-97b7-67bd9813c0c6-ltrp2]: "my-hostname-basic-608efccf-20aa-4a3b-97b7-67bd9813c0c6-ltrp2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:04:10.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-931" for this suite.
Apr 30 00:04:16.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:04:17.092: INFO: namespace replication-controller-931 deletion completed in 6.137724498s

• [SLOW TEST:16.305 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:04:17.092: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9212
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-71084686-565e-4ce1-be4d-9f15554a11fe
STEP: Creating a pod to test consume secrets
Apr 30 00:04:17.283: INFO: Waiting up to 5m0s for pod "pod-secrets-a67b9763-b971-4f03-91b6-ee7e370d006f" in namespace "secrets-9212" to be "success or failure"
Apr 30 00:04:17.285: INFO: Pod "pod-secrets-a67b9763-b971-4f03-91b6-ee7e370d006f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.89497ms
Apr 30 00:04:19.288: INFO: Pod "pod-secrets-a67b9763-b971-4f03-91b6-ee7e370d006f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004530985s
STEP: Saw pod success
Apr 30 00:04:19.288: INFO: Pod "pod-secrets-a67b9763-b971-4f03-91b6-ee7e370d006f" satisfied condition "success or failure"
Apr 30 00:04:19.290: INFO: Trying to get logs from node ip-192-168-211-77.us-west-2.compute.internal pod pod-secrets-a67b9763-b971-4f03-91b6-ee7e370d006f container secret-env-test: <nil>
STEP: delete the pod
Apr 30 00:04:19.307: INFO: Waiting for pod pod-secrets-a67b9763-b971-4f03-91b6-ee7e370d006f to disappear
Apr 30 00:04:19.309: INFO: Pod pod-secrets-a67b9763-b971-4f03-91b6-ee7e370d006f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:04:19.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9212" for this suite.
Apr 30 00:04:26.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:04:26.918: INFO: namespace secrets-9212 deletion completed in 6.093794893s

• [SLOW TEST:9.826 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:04:26.918: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1828
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:04:27.094: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr 30 00:04:27.101: INFO: Number of nodes with available pods: 0
Apr 30 00:04:27.101: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Apr 30 00:04:27.115: INFO: Number of nodes with available pods: 0
Apr 30 00:04:27.115: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:04:28.124: INFO: Number of nodes with available pods: 0
Apr 30 00:04:28.124: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:04:29.118: INFO: Number of nodes with available pods: 1
Apr 30 00:04:29.118: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr 30 00:04:29.132: INFO: Number of nodes with available pods: 1
Apr 30 00:04:29.132: INFO: Number of running nodes: 0, number of available pods: 1
Apr 30 00:04:30.136: INFO: Number of nodes with available pods: 0
Apr 30 00:04:30.136: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr 30 00:04:30.147: INFO: Number of nodes with available pods: 0
Apr 30 00:04:30.147: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:04:31.150: INFO: Number of nodes with available pods: 0
Apr 30 00:04:31.150: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:04:32.150: INFO: Number of nodes with available pods: 0
Apr 30 00:04:32.150: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:04:33.151: INFO: Number of nodes with available pods: 0
Apr 30 00:04:33.151: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:04:34.150: INFO: Number of nodes with available pods: 0
Apr 30 00:04:34.150: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:04:35.150: INFO: Number of nodes with available pods: 0
Apr 30 00:04:35.150: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:04:36.150: INFO: Number of nodes with available pods: 0
Apr 30 00:04:36.150: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:04:37.150: INFO: Number of nodes with available pods: 0
Apr 30 00:04:37.150: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:04:38.150: INFO: Number of nodes with available pods: 0
Apr 30 00:04:38.150: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:04:39.150: INFO: Number of nodes with available pods: 0
Apr 30 00:04:39.150: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:04:40.150: INFO: Number of nodes with available pods: 0
Apr 30 00:04:40.150: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:04:41.150: INFO: Number of nodes with available pods: 0
Apr 30 00:04:41.150: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:04:42.150: INFO: Number of nodes with available pods: 0
Apr 30 00:04:42.150: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:04:43.150: INFO: Number of nodes with available pods: 1
Apr 30 00:04:43.150: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1828, will wait for the garbage collector to delete the pods
Apr 30 00:04:43.221: INFO: Deleting DaemonSet.extensions daemon-set took: 10.284532ms
Apr 30 00:04:43.522: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.152074ms
Apr 30 00:04:51.324: INFO: Number of nodes with available pods: 0
Apr 30 00:04:51.324: INFO: Number of running nodes: 0, number of available pods: 0
Apr 30 00:04:51.328: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1828/daemonsets","resourceVersion":"22710"},"items":null}

Apr 30 00:04:51.333: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1828/pods","resourceVersion":"22710"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:04:51.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1828" for this suite.
Apr 30 00:04:57.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:04:57.461: INFO: namespace daemonsets-1828 deletion completed in 6.092584548s

• [SLOW TEST:30.542 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:04:57.462: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9968
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Apr 30 00:04:59.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec pod-sharedvolume-08036413-4571-4c1a-b9ef-1ceddd3f2739 -c busybox-main-container --namespace=emptydir-9968 -- cat /usr/share/volumeshare/shareddata.txt'
Apr 30 00:04:59.846: INFO: stderr: ""
Apr 30 00:04:59.846: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:04:59.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9968" for this suite.
Apr 30 00:05:05.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:05:05.954: INFO: namespace emptydir-9968 deletion completed in 6.103762895s

• [SLOW TEST:8.492 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:05:05.954: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5381
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-1732cc26-c711-4a60-a225-4177854f49f5
STEP: Creating a pod to test consume configMaps
Apr 30 00:05:06.109: INFO: Waiting up to 5m0s for pod "pod-configmaps-ebec6e4e-3142-4dcd-ba8d-259d377dbc16" in namespace "configmap-5381" to be "success or failure"
Apr 30 00:05:06.111: INFO: Pod "pod-configmaps-ebec6e4e-3142-4dcd-ba8d-259d377dbc16": Phase="Pending", Reason="", readiness=false. Elapsed: 1.946921ms
Apr 30 00:05:08.114: INFO: Pod "pod-configmaps-ebec6e4e-3142-4dcd-ba8d-259d377dbc16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004829409s
STEP: Saw pod success
Apr 30 00:05:08.114: INFO: Pod "pod-configmaps-ebec6e4e-3142-4dcd-ba8d-259d377dbc16" satisfied condition "success or failure"
Apr 30 00:05:08.116: INFO: Trying to get logs from node ip-192-168-148-117.us-west-2.compute.internal pod pod-configmaps-ebec6e4e-3142-4dcd-ba8d-259d377dbc16 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 30 00:05:08.136: INFO: Waiting for pod pod-configmaps-ebec6e4e-3142-4dcd-ba8d-259d377dbc16 to disappear
Apr 30 00:05:08.138: INFO: Pod pod-configmaps-ebec6e4e-3142-4dcd-ba8d-259d377dbc16 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:05:08.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5381" for this suite.
Apr 30 00:05:14.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:05:14.235: INFO: namespace configmap-5381 deletion completed in 6.093507599s

• [SLOW TEST:8.281 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:05:14.236: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1296
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-3600648b-31f1-42dc-8394-722bf37808a0 in namespace container-probe-1296
Apr 30 00:05:16.394: INFO: Started pod liveness-3600648b-31f1-42dc-8394-722bf37808a0 in namespace container-probe-1296
STEP: checking the pod's current state and verifying that restartCount is present
Apr 30 00:05:16.396: INFO: Initial restart count of pod liveness-3600648b-31f1-42dc-8394-722bf37808a0 is 0
Apr 30 00:05:38.429: INFO: Restart count of pod container-probe-1296/liveness-3600648b-31f1-42dc-8394-722bf37808a0 is now 1 (22.033320381s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:05:38.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1296" for this suite.
Apr 30 00:05:44.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:05:44.536: INFO: namespace container-probe-1296 deletion completed in 6.094193476s

• [SLOW TEST:30.301 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:05:44.537: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7838
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7838.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7838.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7838.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7838.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7838.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7838.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 30 00:05:52.719: INFO: DNS probes using dns-7838/dns-test-071fcec8-7424-4667-9f05-f4b7164deb44 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:05:52.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7838" for this suite.
Apr 30 00:05:58.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:05:58.843: INFO: namespace dns-7838 deletion completed in 6.092248456s

• [SLOW TEST:14.306 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:05:58.843: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3218
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-d04da599-49f0-4b6e-991a-a85705fb2e02
STEP: Creating a pod to test consume secrets
Apr 30 00:05:59.011: INFO: Waiting up to 5m0s for pod "pod-secrets-5fb48189-fe6e-4722-988b-81c210b5dec9" in namespace "secrets-3218" to be "success or failure"
Apr 30 00:05:59.013: INFO: Pod "pod-secrets-5fb48189-fe6e-4722-988b-81c210b5dec9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.869886ms
Apr 30 00:06:01.015: INFO: Pod "pod-secrets-5fb48189-fe6e-4722-988b-81c210b5dec9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00465402s
STEP: Saw pod success
Apr 30 00:06:01.015: INFO: Pod "pod-secrets-5fb48189-fe6e-4722-988b-81c210b5dec9" satisfied condition "success or failure"
Apr 30 00:06:01.018: INFO: Trying to get logs from node ip-192-168-211-77.us-west-2.compute.internal pod pod-secrets-5fb48189-fe6e-4722-988b-81c210b5dec9 container secret-volume-test: <nil>
STEP: delete the pod
Apr 30 00:06:01.036: INFO: Waiting for pod pod-secrets-5fb48189-fe6e-4722-988b-81c210b5dec9 to disappear
Apr 30 00:06:01.038: INFO: Pod pod-secrets-5fb48189-fe6e-4722-988b-81c210b5dec9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:06:01.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3218" for this suite.
Apr 30 00:06:07.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:06:07.137: INFO: namespace secrets-3218 deletion completed in 6.095802898s

• [SLOW TEST:8.294 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:06:07.137: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5071
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:06:07.290: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-823dcf9b-3423-4b56-95da-cb69bff5a4ce" in namespace "security-context-test-5071" to be "success or failure"
Apr 30 00:06:07.292: INFO: Pod "busybox-readonly-false-823dcf9b-3423-4b56-95da-cb69bff5a4ce": Phase="Pending", Reason="", readiness=false. Elapsed: 1.908838ms
Apr 30 00:06:09.296: INFO: Pod "busybox-readonly-false-823dcf9b-3423-4b56-95da-cb69bff5a4ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006040913s
Apr 30 00:06:11.299: INFO: Pod "busybox-readonly-false-823dcf9b-3423-4b56-95da-cb69bff5a4ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008908207s
Apr 30 00:06:11.299: INFO: Pod "busybox-readonly-false-823dcf9b-3423-4b56-95da-cb69bff5a4ce" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:06:11.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5071" for this suite.
Apr 30 00:06:17.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:06:17.395: INFO: namespace security-context-test-5071 deletion completed in 6.09208248s

• [SLOW TEST:10.258 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:06:17.395: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7280
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Apr 30 00:06:17.537: INFO: namespace kubectl-7280
Apr 30 00:06:17.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 create -f - --namespace=kubectl-7280'
Apr 30 00:06:17.685: INFO: stderr: ""
Apr 30 00:06:17.685: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 30 00:06:18.688: INFO: Selector matched 1 pods for map[app:redis]
Apr 30 00:06:18.688: INFO: Found 0 / 1
Apr 30 00:06:19.688: INFO: Selector matched 1 pods for map[app:redis]
Apr 30 00:06:19.688: INFO: Found 0 / 1
Apr 30 00:06:20.688: INFO: Selector matched 1 pods for map[app:redis]
Apr 30 00:06:20.688: INFO: Found 0 / 1
Apr 30 00:06:21.688: INFO: Selector matched 1 pods for map[app:redis]
Apr 30 00:06:21.688: INFO: Found 0 / 1
Apr 30 00:06:22.688: INFO: Selector matched 1 pods for map[app:redis]
Apr 30 00:06:22.688: INFO: Found 1 / 1
Apr 30 00:06:22.688: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 30 00:06:22.691: INFO: Selector matched 1 pods for map[app:redis]
Apr 30 00:06:22.691: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 30 00:06:22.691: INFO: wait on redis-master startup in kubectl-7280 
Apr 30 00:06:22.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 logs redis-master-9sf6d redis-master --namespace=kubectl-7280'
Apr 30 00:06:22.763: INFO: stderr: ""
Apr 30 00:06:22.763: INFO: stdout: "1:C 30 Apr 2020 00:06:21.653 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 30 Apr 2020 00:06:21.653 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 30 Apr 2020 00:06:21.653 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 30 Apr 2020 00:06:21.654 * Running mode=standalone, port=6379.\n1:M 30 Apr 2020 00:06:21.654 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 30 Apr 2020 00:06:21.654 # Server initialized\n1:M 30 Apr 2020 00:06:21.654 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 30 Apr 2020 00:06:21.654 * Ready to accept connections\n"
STEP: exposing RC
Apr 30 00:06:22.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7280'
Apr 30 00:06:22.841: INFO: stderr: ""
Apr 30 00:06:22.841: INFO: stdout: "service/rm2 exposed\n"
Apr 30 00:06:22.848: INFO: Service rm2 in namespace kubectl-7280 found.
STEP: exposing service
Apr 30 00:06:24.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7280'
Apr 30 00:06:24.927: INFO: stderr: ""
Apr 30 00:06:24.927: INFO: stdout: "service/rm3 exposed\n"
Apr 30 00:06:24.931: INFO: Service rm3 in namespace kubectl-7280 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:06:26.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7280" for this suite.
Apr 30 00:06:38.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:06:39.053: INFO: namespace kubectl-7280 deletion completed in 12.110074199s

• [SLOW TEST:21.657 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:06:39.053: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-5957
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr 30 00:06:39.584: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Apr 30 00:06:41.594: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801999, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801999, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801999, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723801999, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 30 00:06:44.610: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:06:44.614: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:06:45.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5957" for this suite.
Apr 30 00:06:51.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:06:51.878: INFO: namespace crd-webhook-5957 deletion completed in 6.095473631s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:12.846 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:06:51.899: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0430 00:07:32.069747      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 30 00:07:32.069: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:07:32.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2412" for this suite.
Apr 30 00:07:38.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:07:38.167: INFO: namespace gc-2412 deletion completed in 6.094061122s

• [SLOW TEST:46.268 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:07:38.168: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5928
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 30 00:07:38.708: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 30 00:07:40.718: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723802058, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723802058, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723802058, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723802058, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 30 00:07:43.734: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:07:43.738: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1046-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:07:44.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5928" for this suite.
Apr 30 00:07:50.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:07:50.998: INFO: namespace webhook-5928 deletion completed in 6.095284781s
STEP: Destroying namespace "webhook-5928-markers" for this suite.
Apr 30 00:07:57.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:07:57.094: INFO: namespace webhook-5928-markers deletion completed in 6.095869264s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.946 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:07:57.113: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0430 00:07:58.298446      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 30 00:07:58.298: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:07:58.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8140" for this suite.
Apr 30 00:08:04.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:08:04.399: INFO: namespace gc-8140 deletion completed in 6.097487514s

• [SLOW TEST:7.286 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:08:04.399: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6555
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 30 00:08:04.570: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5744e206-e8e3-4e9e-95c5-653c14483456" in namespace "projected-6555" to be "success or failure"
Apr 30 00:08:04.572: INFO: Pod "downwardapi-volume-5744e206-e8e3-4e9e-95c5-653c14483456": Phase="Pending", Reason="", readiness=false. Elapsed: 1.94395ms
Apr 30 00:08:06.575: INFO: Pod "downwardapi-volume-5744e206-e8e3-4e9e-95c5-653c14483456": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004980428s
STEP: Saw pod success
Apr 30 00:08:06.575: INFO: Pod "downwardapi-volume-5744e206-e8e3-4e9e-95c5-653c14483456" satisfied condition "success or failure"
Apr 30 00:08:06.577: INFO: Trying to get logs from node ip-192-168-149-254.us-west-2.compute.internal pod downwardapi-volume-5744e206-e8e3-4e9e-95c5-653c14483456 container client-container: <nil>
STEP: delete the pod
Apr 30 00:08:06.596: INFO: Waiting for pod downwardapi-volume-5744e206-e8e3-4e9e-95c5-653c14483456 to disappear
Apr 30 00:08:06.598: INFO: Pod downwardapi-volume-5744e206-e8e3-4e9e-95c5-653c14483456 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:08:06.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6555" for this suite.
Apr 30 00:08:12.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:08:12.696: INFO: namespace projected-6555 deletion completed in 6.094890418s

• [SLOW TEST:8.297 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:08:12.697: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2558
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Apr 30 00:08:22.893: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0430 00:08:22.893412      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 30 00:08:22.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2558" for this suite.
Apr 30 00:08:28.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:08:29.004: INFO: namespace gc-2558 deletion completed in 6.107236462s

• [SLOW TEST:16.307 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:08:29.004: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9139
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:08:33.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9139" for this suite.
Apr 30 00:08:39.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:08:39.258: INFO: namespace kubelet-test-9139 deletion completed in 6.091878226s

• [SLOW TEST:10.254 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:08:39.259: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7991
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 30 00:08:39.407: INFO: Waiting up to 5m0s for pod "pod-d50c8b27-82b1-4c56-9a05-d9d8308defff" in namespace "emptydir-7991" to be "success or failure"
Apr 30 00:08:39.410: INFO: Pod "pod-d50c8b27-82b1-4c56-9a05-d9d8308defff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.300378ms
Apr 30 00:08:41.412: INFO: Pod "pod-d50c8b27-82b1-4c56-9a05-d9d8308defff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004906158s
STEP: Saw pod success
Apr 30 00:08:41.412: INFO: Pod "pod-d50c8b27-82b1-4c56-9a05-d9d8308defff" satisfied condition "success or failure"
Apr 30 00:08:41.415: INFO: Trying to get logs from node ip-192-168-72-199.us-west-2.compute.internal pod pod-d50c8b27-82b1-4c56-9a05-d9d8308defff container test-container: <nil>
STEP: delete the pod
Apr 30 00:08:41.433: INFO: Waiting for pod pod-d50c8b27-82b1-4c56-9a05-d9d8308defff to disappear
Apr 30 00:08:41.434: INFO: Pod pod-d50c8b27-82b1-4c56-9a05-d9d8308defff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:08:41.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7991" for this suite.
Apr 30 00:08:47.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:08:47.528: INFO: namespace emptydir-7991 deletion completed in 6.08968023s

• [SLOW TEST:8.269 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:08:47.528: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9714
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 30 00:08:48.156: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 30 00:08:50.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723802128, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723802128, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723802128, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723802128, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 30 00:08:53.182: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:09:05.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9714" for this suite.
Apr 30 00:09:11.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:09:11.420: INFO: namespace webhook-9714 deletion completed in 6.11936997s
STEP: Destroying namespace "webhook-9714-markers" for this suite.
Apr 30 00:09:17.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:09:17.512: INFO: namespace webhook-9714-markers deletion completed in 6.092926475s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:30.004 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:09:17.532: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7350
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-7350/secret-test-0f3adb5d-0ae9-4b70-9307-71e7a7795592
STEP: Creating a pod to test consume secrets
Apr 30 00:09:17.696: INFO: Waiting up to 5m0s for pod "pod-configmaps-ef3c8cf3-1be0-44e6-b5de-9830c54f5a34" in namespace "secrets-7350" to be "success or failure"
Apr 30 00:09:17.698: INFO: Pod "pod-configmaps-ef3c8cf3-1be0-44e6-b5de-9830c54f5a34": Phase="Pending", Reason="", readiness=false. Elapsed: 1.756054ms
Apr 30 00:09:19.700: INFO: Pod "pod-configmaps-ef3c8cf3-1be0-44e6-b5de-9830c54f5a34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004355151s
STEP: Saw pod success
Apr 30 00:09:19.700: INFO: Pod "pod-configmaps-ef3c8cf3-1be0-44e6-b5de-9830c54f5a34" satisfied condition "success or failure"
Apr 30 00:09:19.702: INFO: Trying to get logs from node ip-192-168-149-254.us-west-2.compute.internal pod pod-configmaps-ef3c8cf3-1be0-44e6-b5de-9830c54f5a34 container env-test: <nil>
STEP: delete the pod
Apr 30 00:09:19.722: INFO: Waiting for pod pod-configmaps-ef3c8cf3-1be0-44e6-b5de-9830c54f5a34 to disappear
Apr 30 00:09:19.724: INFO: Pod pod-configmaps-ef3c8cf3-1be0-44e6-b5de-9830c54f5a34 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:09:19.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7350" for this suite.
Apr 30 00:09:25.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:09:25.819: INFO: namespace secrets-7350 deletion completed in 6.091899774s

• [SLOW TEST:8.288 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:09:25.819: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3959
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 30 00:09:25.971: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a18f9856-3e5c-4746-934a-98edb5e56efa" in namespace "downward-api-3959" to be "success or failure"
Apr 30 00:09:25.973: INFO: Pod "downwardapi-volume-a18f9856-3e5c-4746-934a-98edb5e56efa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.895934ms
Apr 30 00:09:27.976: INFO: Pod "downwardapi-volume-a18f9856-3e5c-4746-934a-98edb5e56efa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004397612s
STEP: Saw pod success
Apr 30 00:09:27.976: INFO: Pod "downwardapi-volume-a18f9856-3e5c-4746-934a-98edb5e56efa" satisfied condition "success or failure"
Apr 30 00:09:27.978: INFO: Trying to get logs from node ip-192-168-72-199.us-west-2.compute.internal pod downwardapi-volume-a18f9856-3e5c-4746-934a-98edb5e56efa container client-container: <nil>
STEP: delete the pod
Apr 30 00:09:27.995: INFO: Waiting for pod downwardapi-volume-a18f9856-3e5c-4746-934a-98edb5e56efa to disappear
Apr 30 00:09:27.996: INFO: Pod downwardapi-volume-a18f9856-3e5c-4746-934a-98edb5e56efa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:09:27.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3959" for this suite.
Apr 30 00:09:34.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:09:34.099: INFO: namespace downward-api-3959 deletion completed in 6.098583796s

• [SLOW TEST:8.279 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:09:34.099: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1257
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-282
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4337
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:10:03.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1257" for this suite.
Apr 30 00:10:09.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:10:09.660: INFO: namespace namespaces-1257 deletion completed in 6.102345711s
STEP: Destroying namespace "nsdeletetest-282" for this suite.
Apr 30 00:10:09.663: INFO: Namespace nsdeletetest-282 was already deleted
STEP: Destroying namespace "nsdeletetest-4337" for this suite.
Apr 30 00:10:15.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:10:15.755: INFO: namespace nsdeletetest-4337 deletion completed in 6.092177614s

• [SLOW TEST:41.656 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:10:15.755: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-622
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Apr 30 00:10:15.898: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 30 00:10:18.514: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:10:26.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-622" for this suite.
Apr 30 00:10:32.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:10:32.656: INFO: namespace crd-publish-openapi-622 deletion completed in 6.104618344s

• [SLOW TEST:16.900 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:10:32.656: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5968
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 30 00:10:32.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4385ea77-4de2-4cfb-8877-77c9837092de" in namespace "projected-5968" to be "success or failure"
Apr 30 00:10:32.809: INFO: Pod "downwardapi-volume-4385ea77-4de2-4cfb-8877-77c9837092de": Phase="Pending", Reason="", readiness=false. Elapsed: 1.905036ms
Apr 30 00:10:34.812: INFO: Pod "downwardapi-volume-4385ea77-4de2-4cfb-8877-77c9837092de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004745205s
Apr 30 00:10:36.815: INFO: Pod "downwardapi-volume-4385ea77-4de2-4cfb-8877-77c9837092de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007387294s
STEP: Saw pod success
Apr 30 00:10:36.815: INFO: Pod "downwardapi-volume-4385ea77-4de2-4cfb-8877-77c9837092de" satisfied condition "success or failure"
Apr 30 00:10:36.817: INFO: Trying to get logs from node ip-192-168-144-27.us-west-2.compute.internal pod downwardapi-volume-4385ea77-4de2-4cfb-8877-77c9837092de container client-container: <nil>
STEP: delete the pod
Apr 30 00:10:36.837: INFO: Waiting for pod downwardapi-volume-4385ea77-4de2-4cfb-8877-77c9837092de to disappear
Apr 30 00:10:36.838: INFO: Pod downwardapi-volume-4385ea77-4de2-4cfb-8877-77c9837092de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:10:36.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5968" for this suite.
Apr 30 00:10:42.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:10:42.934: INFO: namespace projected-5968 deletion completed in 6.091711166s

• [SLOW TEST:10.278 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:10:42.934: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2931
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Apr 30 00:10:43.078: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-371654096 proxy --unix-socket=/tmp/kubectl-proxy-unix099387047/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:10:43.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2931" for this suite.
Apr 30 00:10:49.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:10:49.215: INFO: namespace kubectl-2931 deletion completed in 6.091588814s

• [SLOW TEST:6.281 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:10:49.215: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7292
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Apr 30 00:10:49.366: INFO: Waiting up to 5m0s for pod "pod-5e7bcd75-c3c1-494f-aee2-da116f41109f" in namespace "emptydir-7292" to be "success or failure"
Apr 30 00:10:49.368: INFO: Pod "pod-5e7bcd75-c3c1-494f-aee2-da116f41109f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.994891ms
Apr 30 00:10:51.371: INFO: Pod "pod-5e7bcd75-c3c1-494f-aee2-da116f41109f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004795757s
STEP: Saw pod success
Apr 30 00:10:51.371: INFO: Pod "pod-5e7bcd75-c3c1-494f-aee2-da116f41109f" satisfied condition "success or failure"
Apr 30 00:10:51.373: INFO: Trying to get logs from node ip-192-168-82-130.us-west-2.compute.internal pod pod-5e7bcd75-c3c1-494f-aee2-da116f41109f container test-container: <nil>
STEP: delete the pod
Apr 30 00:10:51.391: INFO: Waiting for pod pod-5e7bcd75-c3c1-494f-aee2-da116f41109f to disappear
Apr 30 00:10:51.393: INFO: Pod pod-5e7bcd75-c3c1-494f-aee2-da116f41109f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:10:51.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7292" for this suite.
Apr 30 00:10:57.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:10:57.488: INFO: namespace emptydir-7292 deletion completed in 6.091271943s

• [SLOW TEST:8.273 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:10:57.488: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7315
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-f1d687b6-8587-47db-b231-aef5269e1ef7
STEP: Creating a pod to test consume configMaps
Apr 30 00:10:57.645: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ea8653df-e903-416b-868d-db8b9acb149d" in namespace "projected-7315" to be "success or failure"
Apr 30 00:10:57.647: INFO: Pod "pod-projected-configmaps-ea8653df-e903-416b-868d-db8b9acb149d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.384745ms
Apr 30 00:10:59.650: INFO: Pod "pod-projected-configmaps-ea8653df-e903-416b-868d-db8b9acb149d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005306261s
STEP: Saw pod success
Apr 30 00:10:59.650: INFO: Pod "pod-projected-configmaps-ea8653df-e903-416b-868d-db8b9acb149d" satisfied condition "success or failure"
Apr 30 00:10:59.653: INFO: Trying to get logs from node ip-192-168-144-27.us-west-2.compute.internal pod pod-projected-configmaps-ea8653df-e903-416b-868d-db8b9acb149d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 30 00:10:59.668: INFO: Waiting for pod pod-projected-configmaps-ea8653df-e903-416b-868d-db8b9acb149d to disappear
Apr 30 00:10:59.670: INFO: Pod pod-projected-configmaps-ea8653df-e903-416b-868d-db8b9acb149d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:10:59.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7315" for this suite.
Apr 30 00:11:05.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:11:05.775: INFO: namespace projected-7315 deletion completed in 6.101147636s

• [SLOW TEST:8.287 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:11:05.776: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2137
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 30 00:11:06.492: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 30 00:11:09.512: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:11:09.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2137" for this suite.
Apr 30 00:11:15.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:11:15.770: INFO: namespace webhook-2137 deletion completed in 6.092858856s
STEP: Destroying namespace "webhook-2137-markers" for this suite.
Apr 30 00:11:21.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:11:21.863: INFO: namespace webhook-2137-markers deletion completed in 6.092184504s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.107 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:11:21.882: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-d5019849-e511-4d88-982b-4aa9c1bc4d82 in namespace container-probe-8356
Apr 30 00:11:24.040: INFO: Started pod busybox-d5019849-e511-4d88-982b-4aa9c1bc4d82 in namespace container-probe-8356
STEP: checking the pod's current state and verifying that restartCount is present
Apr 30 00:11:24.042: INFO: Initial restart count of pod busybox-d5019849-e511-4d88-982b-4aa9c1bc4d82 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:15:24.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8356" for this suite.
Apr 30 00:15:30.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:15:30.499: INFO: namespace container-probe-8356 deletion completed in 6.093439921s

• [SLOW TEST:248.617 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:15:30.499: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-3872
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:15:30.658: INFO: (0) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 10.121144ms)
Apr 30 00:15:30.662: INFO: (1) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.901401ms)
Apr 30 00:15:30.666: INFO: (2) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.880045ms)
Apr 30 00:15:30.670: INFO: (3) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.650084ms)
Apr 30 00:15:30.674: INFO: (4) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.75215ms)
Apr 30 00:15:30.677: INFO: (5) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.790637ms)
Apr 30 00:15:30.681: INFO: (6) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.678021ms)
Apr 30 00:15:30.685: INFO: (7) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.728951ms)
Apr 30 00:15:30.688: INFO: (8) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.551164ms)
Apr 30 00:15:30.692: INFO: (9) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.65654ms)
Apr 30 00:15:30.696: INFO: (10) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.690188ms)
Apr 30 00:15:30.700: INFO: (11) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.645421ms)
Apr 30 00:15:30.703: INFO: (12) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.864312ms)
Apr 30 00:15:30.707: INFO: (13) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.601788ms)
Apr 30 00:15:30.711: INFO: (14) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.742281ms)
Apr 30 00:15:30.715: INFO: (15) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.588828ms)
Apr 30 00:15:30.718: INFO: (16) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.651973ms)
Apr 30 00:15:30.722: INFO: (17) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.591347ms)
Apr 30 00:15:30.725: INFO: (18) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.656146ms)
Apr 30 00:15:30.729: INFO: (19) /api/v1/nodes/ip-192-168-144-27.us-west-2.compute.internal:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="awsagent... (200; 3.686808ms)
[AfterEach] version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:15:30.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3872" for this suite.
Apr 30 00:15:36.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:15:36.822: INFO: namespace proxy-3872 deletion completed in 6.08983616s

• [SLOW TEST:6.323 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:15:36.823: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3827
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 30 00:15:36.978: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df7099de-6281-45fe-826b-1d0f1c8b6ef1" in namespace "projected-3827" to be "success or failure"
Apr 30 00:15:36.979: INFO: Pod "downwardapi-volume-df7099de-6281-45fe-826b-1d0f1c8b6ef1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.877836ms
Apr 30 00:15:38.982: INFO: Pod "downwardapi-volume-df7099de-6281-45fe-826b-1d0f1c8b6ef1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004543424s
STEP: Saw pod success
Apr 30 00:15:38.982: INFO: Pod "downwardapi-volume-df7099de-6281-45fe-826b-1d0f1c8b6ef1" satisfied condition "success or failure"
Apr 30 00:15:38.984: INFO: Trying to get logs from node ip-192-168-83-225.us-west-2.compute.internal pod downwardapi-volume-df7099de-6281-45fe-826b-1d0f1c8b6ef1 container client-container: <nil>
STEP: delete the pod
Apr 30 00:15:39.002: INFO: Waiting for pod downwardapi-volume-df7099de-6281-45fe-826b-1d0f1c8b6ef1 to disappear
Apr 30 00:15:39.004: INFO: Pod downwardapi-volume-df7099de-6281-45fe-826b-1d0f1c8b6ef1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:15:39.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3827" for this suite.
Apr 30 00:15:45.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:15:45.097: INFO: namespace projected-3827 deletion completed in 6.089841583s

• [SLOW TEST:8.274 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:15:45.097: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2357
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:16:02.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2357" for this suite.
Apr 30 00:16:08.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:16:08.424: INFO: namespace resourcequota-2357 deletion completed in 6.12797099s

• [SLOW TEST:23.327 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:16:08.425: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:16:08.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 create -f - --namespace=kubectl-3395'
Apr 30 00:16:08.831: INFO: stderr: ""
Apr 30 00:16:08.831: INFO: stdout: "replicationcontroller/redis-master created\n"
Apr 30 00:16:08.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 create -f - --namespace=kubectl-3395'
Apr 30 00:16:08.983: INFO: stderr: ""
Apr 30 00:16:08.983: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 30 00:16:09.987: INFO: Selector matched 1 pods for map[app:redis]
Apr 30 00:16:09.987: INFO: Found 0 / 1
Apr 30 00:16:10.987: INFO: Selector matched 1 pods for map[app:redis]
Apr 30 00:16:10.987: INFO: Found 1 / 1
Apr 30 00:16:10.987: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 30 00:16:10.989: INFO: Selector matched 1 pods for map[app:redis]
Apr 30 00:16:10.989: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 30 00:16:10.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 describe pod redis-master-hr2rr --namespace=kubectl-3395'
Apr 30 00:16:11.057: INFO: stderr: ""
Apr 30 00:16:11.057: INFO: stdout: "Name:         redis-master-hr2rr\nNamespace:    kubectl-3395\nPriority:     0\nNode:         ip-192-168-72-199.us-west-2.compute.internal/192.168.72.199\nStart Time:   Thu, 30 Apr 2020 00:16:08 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           192.168.73.22\nIPs:\n  IP:           192.168.73.22\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://26a9b124c5cca7c7f6407e21ef314779b7de06bbf32d00db32179315d5af1e80\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 30 Apr 2020 00:16:09 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-sdq4m (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-sdq4m:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-sdq4m\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                   Message\n  ----    ------     ----  ----                                                   -------\n  Normal  Scheduled  3s    default-scheduler                                      Successfully assigned kubectl-3395/redis-master-hr2rr to ip-192-168-72-199.us-west-2.compute.internal\n  Normal  Pulled     2s    kubelet, ip-192-168-72-199.us-west-2.compute.internal  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    2s    kubelet, ip-192-168-72-199.us-west-2.compute.internal  Created container redis-master\n  Normal  Started    2s    kubelet, ip-192-168-72-199.us-west-2.compute.internal  Started container redis-master\n"
Apr 30 00:16:11.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 describe rc redis-master --namespace=kubectl-3395'
Apr 30 00:16:11.129: INFO: stderr: ""
Apr 30 00:16:11.130: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-3395\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-hr2rr\n"
Apr 30 00:16:11.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 describe service redis-master --namespace=kubectl-3395'
Apr 30 00:16:11.191: INFO: stderr: ""
Apr 30 00:16:11.191: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-3395\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.100.197.101\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         192.168.73.22:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr 30 00:16:11.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 describe node ip-192-168-144-27.us-west-2.compute.internal'
Apr 30 00:16:11.297: INFO: stderr: ""
Apr 30 00:16:11.297: INFO: stdout: "Name:               ip-192-168-144-27.us-west-2.compute.internal\nRoles:              <none>\nLabels:             AMIType=AL2_x86_64\n                    NGType=custom\n                    Name=eks-2020042915-islandecxe41-ng-al2-cpu\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=c5.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-west-2\n                    failure-domain.beta.kubernetes.io/zone=us-west-2b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-192-168-144-27.us-west-2.compute.internal\n                    kubernetes.io/os=linux\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 29 Apr 2020 22:50:44 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 30 Apr 2020 00:15:23 +0000   Wed, 29 Apr 2020 22:50:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 30 Apr 2020 00:15:23 +0000   Wed, 29 Apr 2020 22:50:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 30 Apr 2020 00:15:23 +0000   Wed, 29 Apr 2020 22:50:41 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 30 Apr 2020 00:15:23 +0000   Wed, 29 Apr 2020 22:51:24 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   192.168.144.27\n  ExternalIP:   34.213.218.100\n  Hostname:     ip-192-168-144-27.us-west-2.compute.internal\n  InternalDNS:  ip-192-168-144-27.us-west-2.compute.internal\n  ExternalDNS:  ec2-34-213-218-100.us-west-2.compute.amazonaws.com\nCapacity:\n attachable-volumes-aws-ebs:  25\n cpu:                         4\n ephemeral-storage:           41930732Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      7807336Ki\n pods:                        58\nAllocatable:\n attachable-volumes-aws-ebs:  25\n cpu:                         3920m\n ephemeral-storage:           37569620724\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      6790504Ki\n pods:                        58\nSystem Info:\n Machine ID:                 ec2be4f68b3939360b5a36265e78f5fb\n System UUID:                EC2BE4F6-8B39-3936-0B5A-36265E78F5FB\n Boot ID:                    15249ba3-a1db-4e0e-af41-b9bd08d106b9\n Kernel Version:             4.14.173-137.229.amzn2.x86_64\n OS Image:                   Amazon Linux 2\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://19.3.6\n Kubelet Version:            v1.16.8-eks-e16311\n Kube-Proxy Version:         v1.16.8-eks-e16311\nProviderID:                  aws:///us-west-2b/i-035accda411e75a06\nNon-terminated Pods:         (3 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                aws-node-rccgv                                             10m (0%)      0 (0%)      0 (0%)           0 (0%)         85m\n  kube-system                kube-proxy-4xrfl                                           100m (2%)     0 (0%)      0 (0%)           0 (0%)         85m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-2ffxh    0 (0%)        0 (0%)      0 (0%)           0 (0%)         73m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests   Limits\n  --------                    --------   ------\n  cpu                         110m (2%)  0 (0%)\n  memory                      0 (0%)     0 (0%)\n  ephemeral-storage           0 (0%)     0 (0%)\n  attachable-volumes-aws-ebs  0          0\nEvents:                       <none>\n"
Apr 30 00:16:11.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 describe namespace kubectl-3395'
Apr 30 00:16:11.383: INFO: stderr: ""
Apr 30 00:16:11.383: INFO: stdout: "Name:         kubectl-3395\nLabels:       e2e-framework=kubectl\n              e2e-run=4c6ae5b2-dbe3-43bd-9697-b2549e28748e\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:16:11.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3395" for this suite.
Apr 30 00:16:23.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:16:23.478: INFO: namespace kubectl-3395 deletion completed in 12.089949833s

• [SLOW TEST:15.053 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:16:23.478: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7289
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:16:23.622: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:16:25.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7289" for this suite.
Apr 30 00:17:13.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:17:13.824: INFO: namespace pods-7289 deletion completed in 48.169571649s

• [SLOW TEST:50.346 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:17:13.824: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9238
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Apr 30 00:17:13.975: INFO: Waiting up to 5m0s for pod "var-expansion-32315cdc-12af-4064-8f7f-a2cc351da83f" in namespace "var-expansion-9238" to be "success or failure"
Apr 30 00:17:13.977: INFO: Pod "var-expansion-32315cdc-12af-4064-8f7f-a2cc351da83f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.876164ms
Apr 30 00:17:15.979: INFO: Pod "var-expansion-32315cdc-12af-4064-8f7f-a2cc351da83f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004819618s
STEP: Saw pod success
Apr 30 00:17:15.980: INFO: Pod "var-expansion-32315cdc-12af-4064-8f7f-a2cc351da83f" satisfied condition "success or failure"
Apr 30 00:17:15.982: INFO: Trying to get logs from node ip-192-168-149-254.us-west-2.compute.internal pod var-expansion-32315cdc-12af-4064-8f7f-a2cc351da83f container dapi-container: <nil>
STEP: delete the pod
Apr 30 00:17:16.001: INFO: Waiting for pod var-expansion-32315cdc-12af-4064-8f7f-a2cc351da83f to disappear
Apr 30 00:17:16.003: INFO: Pod var-expansion-32315cdc-12af-4064-8f7f-a2cc351da83f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:17:16.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9238" for this suite.
Apr 30 00:17:22.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:17:22.096: INFO: namespace var-expansion-9238 deletion completed in 6.089398091s

• [SLOW TEST:8.272 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:17:22.096: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3164
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 30 00:17:22.248: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a99af23-2a3a-479f-8abf-a043aeba0fcb" in namespace "projected-3164" to be "success or failure"
Apr 30 00:17:22.250: INFO: Pod "downwardapi-volume-9a99af23-2a3a-479f-8abf-a043aeba0fcb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.897191ms
Apr 30 00:17:24.253: INFO: Pod "downwardapi-volume-9a99af23-2a3a-479f-8abf-a043aeba0fcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004726658s
STEP: Saw pod success
Apr 30 00:17:24.253: INFO: Pod "downwardapi-volume-9a99af23-2a3a-479f-8abf-a043aeba0fcb" satisfied condition "success or failure"
Apr 30 00:17:24.255: INFO: Trying to get logs from node ip-192-168-72-199.us-west-2.compute.internal pod downwardapi-volume-9a99af23-2a3a-479f-8abf-a043aeba0fcb container client-container: <nil>
STEP: delete the pod
Apr 30 00:17:24.273: INFO: Waiting for pod downwardapi-volume-9a99af23-2a3a-479f-8abf-a043aeba0fcb to disappear
Apr 30 00:17:24.275: INFO: Pod downwardapi-volume-9a99af23-2a3a-479f-8abf-a043aeba0fcb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:17:24.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3164" for this suite.
Apr 30 00:17:30.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:17:30.372: INFO: namespace projected-3164 deletion completed in 6.093788563s

• [SLOW TEST:8.277 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:17:30.373: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:17:46.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2586" for this suite.
Apr 30 00:17:52.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:17:52.692: INFO: namespace resourcequota-2586 deletion completed in 6.100699903s

• [SLOW TEST:22.319 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:17:52.692: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4861
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-8a355bdc-6080-4235-859d-dd602ec14a02
STEP: Creating configMap with name cm-test-opt-upd-8059d4da-1e8f-49e9-846c-2838f0d48a98
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-8a355bdc-6080-4235-859d-dd602ec14a02
STEP: Updating configmap cm-test-opt-upd-8059d4da-1e8f-49e9-846c-2838f0d48a98
STEP: Creating configMap with name cm-test-opt-create-6086e568-ca0b-44c8-8997-bbf324e53c97
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:19:09.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4861" for this suite.
Apr 30 00:19:21.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:19:21.308: INFO: namespace configmap-4861 deletion completed in 12.099198156s

• [SLOW TEST:88.616 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:19:21.308: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4776
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Apr 30 00:19:21.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=kubectl-4776 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Apr 30 00:19:23.479: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Apr 30 00:19:23.479: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:19:25.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4776" for this suite.
Apr 30 00:19:33.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:19:33.585: INFO: namespace kubectl-4776 deletion completed in 8.093001402s

• [SLOW TEST:12.277 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:19:33.585: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1358
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Apr 30 00:19:33.738: INFO: Waiting up to 5m0s for pod "client-containers-8ff758fb-89a1-45e6-a674-3ffc9bd1a732" in namespace "containers-1358" to be "success or failure"
Apr 30 00:19:33.740: INFO: Pod "client-containers-8ff758fb-89a1-45e6-a674-3ffc9bd1a732": Phase="Pending", Reason="", readiness=false. Elapsed: 1.850125ms
Apr 30 00:19:35.743: INFO: Pod "client-containers-8ff758fb-89a1-45e6-a674-3ffc9bd1a732": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004624165s
STEP: Saw pod success
Apr 30 00:19:35.743: INFO: Pod "client-containers-8ff758fb-89a1-45e6-a674-3ffc9bd1a732" satisfied condition "success or failure"
Apr 30 00:19:35.745: INFO: Trying to get logs from node ip-192-168-222-85.us-west-2.compute.internal pod client-containers-8ff758fb-89a1-45e6-a674-3ffc9bd1a732 container test-container: <nil>
STEP: delete the pod
Apr 30 00:19:35.762: INFO: Waiting for pod client-containers-8ff758fb-89a1-45e6-a674-3ffc9bd1a732 to disappear
Apr 30 00:19:35.764: INFO: Pod client-containers-8ff758fb-89a1-45e6-a674-3ffc9bd1a732 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:19:35.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1358" for this suite.
Apr 30 00:19:41.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:19:41.858: INFO: namespace containers-1358 deletion completed in 6.090191484s

• [SLOW TEST:8.273 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:19:41.858: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6605
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-ac6bf5cc-5b6a-4025-9b3e-ddd23f50214a
STEP: Creating a pod to test consume secrets
Apr 30 00:19:42.022: INFO: Waiting up to 5m0s for pod "pod-secrets-6e148316-1243-4476-aeec-1eb19919d487" in namespace "secrets-6605" to be "success or failure"
Apr 30 00:19:42.024: INFO: Pod "pod-secrets-6e148316-1243-4476-aeec-1eb19919d487": Phase="Pending", Reason="", readiness=false. Elapsed: 1.802838ms
Apr 30 00:19:44.027: INFO: Pod "pod-secrets-6e148316-1243-4476-aeec-1eb19919d487": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004612s
STEP: Saw pod success
Apr 30 00:19:44.027: INFO: Pod "pod-secrets-6e148316-1243-4476-aeec-1eb19919d487" satisfied condition "success or failure"
Apr 30 00:19:44.029: INFO: Trying to get logs from node ip-192-168-149-254.us-west-2.compute.internal pod pod-secrets-6e148316-1243-4476-aeec-1eb19919d487 container secret-volume-test: <nil>
STEP: delete the pod
Apr 30 00:19:44.045: INFO: Waiting for pod pod-secrets-6e148316-1243-4476-aeec-1eb19919d487 to disappear
Apr 30 00:19:44.046: INFO: Pod pod-secrets-6e148316-1243-4476-aeec-1eb19919d487 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:19:44.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6605" for this suite.
Apr 30 00:19:50.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:19:50.141: INFO: namespace secrets-6605 deletion completed in 6.090779243s

• [SLOW TEST:8.283 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:19:50.141: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5062
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 30 00:19:50.295: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94c27152-e3e9-4b2e-ad88-0174dd17341e" in namespace "downward-api-5062" to be "success or failure"
Apr 30 00:19:50.297: INFO: Pod "downwardapi-volume-94c27152-e3e9-4b2e-ad88-0174dd17341e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.105014ms
Apr 30 00:19:52.300: INFO: Pod "downwardapi-volume-94c27152-e3e9-4b2e-ad88-0174dd17341e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00511643s
STEP: Saw pod success
Apr 30 00:19:52.300: INFO: Pod "downwardapi-volume-94c27152-e3e9-4b2e-ad88-0174dd17341e" satisfied condition "success or failure"
Apr 30 00:19:52.302: INFO: Trying to get logs from node ip-192-168-222-85.us-west-2.compute.internal pod downwardapi-volume-94c27152-e3e9-4b2e-ad88-0174dd17341e container client-container: <nil>
STEP: delete the pod
Apr 30 00:19:52.317: INFO: Waiting for pod downwardapi-volume-94c27152-e3e9-4b2e-ad88-0174dd17341e to disappear
Apr 30 00:19:52.320: INFO: Pod downwardapi-volume-94c27152-e3e9-4b2e-ad88-0174dd17341e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:19:52.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5062" for this suite.
Apr 30 00:19:58.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:19:58.422: INFO: namespace downward-api-5062 deletion completed in 6.097733096s

• [SLOW TEST:8.280 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:19:58.422: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6629
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 30 00:19:58.572: INFO: Waiting up to 5m0s for pod "pod-d736b26c-0652-4a79-ab60-2802ae754a12" in namespace "emptydir-6629" to be "success or failure"
Apr 30 00:19:58.574: INFO: Pod "pod-d736b26c-0652-4a79-ab60-2802ae754a12": Phase="Pending", Reason="", readiness=false. Elapsed: 1.924399ms
Apr 30 00:20:00.577: INFO: Pod "pod-d736b26c-0652-4a79-ab60-2802ae754a12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004745811s
STEP: Saw pod success
Apr 30 00:20:00.577: INFO: Pod "pod-d736b26c-0652-4a79-ab60-2802ae754a12" satisfied condition "success or failure"
Apr 30 00:20:00.579: INFO: Trying to get logs from node ip-192-168-149-254.us-west-2.compute.internal pod pod-d736b26c-0652-4a79-ab60-2802ae754a12 container test-container: <nil>
STEP: delete the pod
Apr 30 00:20:00.595: INFO: Waiting for pod pod-d736b26c-0652-4a79-ab60-2802ae754a12 to disappear
Apr 30 00:20:00.597: INFO: Pod pod-d736b26c-0652-4a79-ab60-2802ae754a12 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:20:00.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6629" for this suite.
Apr 30 00:20:06.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:20:06.874: INFO: namespace emptydir-6629 deletion completed in 6.272231764s

• [SLOW TEST:8.452 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:20:06.874: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8261
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-68783187-b6d9-4fad-bda7-f6231326dfe2
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-68783187-b6d9-4fad-bda7-f6231326dfe2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:20:11.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8261" for this suite.
Apr 30 00:20:23.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:20:23.173: INFO: namespace configmap-8261 deletion completed in 12.09302413s

• [SLOW TEST:16.299 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:20:23.173: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1566
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 30 00:20:23.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1566'
Apr 30 00:20:23.378: INFO: stderr: ""
Apr 30 00:20:23.378: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Apr 30 00:20:23.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete pods e2e-test-httpd-pod --namespace=kubectl-1566'
Apr 30 00:20:31.398: INFO: stderr: ""
Apr 30 00:20:31.398: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:20:31.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1566" for this suite.
Apr 30 00:20:37.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:20:37.491: INFO: namespace kubectl-1566 deletion completed in 6.088431865s

• [SLOW TEST:14.318 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:20:37.491: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4622
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:20:53.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4622" for this suite.
Apr 30 00:20:59.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:20:59.796: INFO: namespace resourcequota-4622 deletion completed in 6.090263671s

• [SLOW TEST:22.305 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:20:59.797: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 30 00:21:02.493: INFO: Successfully updated pod "pod-update-064b86ed-f006-426e-8200-54e8a48ffcde"
STEP: verifying the updated pod is in kubernetes
Apr 30 00:21:02.497: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:21:02.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4166" for this suite.
Apr 30 00:21:14.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:21:14.590: INFO: namespace pods-4166 deletion completed in 12.089487588s

• [SLOW TEST:14.793 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:21:14.590: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8801
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Apr 30 00:21:14.740: INFO: Waiting up to 5m0s for pod "downward-api-c6d8f8ee-16a8-4bac-9284-c098502b5f75" in namespace "downward-api-8801" to be "success or failure"
Apr 30 00:21:14.742: INFO: Pod "downward-api-c6d8f8ee-16a8-4bac-9284-c098502b5f75": Phase="Pending", Reason="", readiness=false. Elapsed: 1.806358ms
Apr 30 00:21:16.745: INFO: Pod "downward-api-c6d8f8ee-16a8-4bac-9284-c098502b5f75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004737268s
STEP: Saw pod success
Apr 30 00:21:16.745: INFO: Pod "downward-api-c6d8f8ee-16a8-4bac-9284-c098502b5f75" satisfied condition "success or failure"
Apr 30 00:21:16.747: INFO: Trying to get logs from node ip-192-168-72-199.us-west-2.compute.internal pod downward-api-c6d8f8ee-16a8-4bac-9284-c098502b5f75 container dapi-container: <nil>
STEP: delete the pod
Apr 30 00:21:16.765: INFO: Waiting for pod downward-api-c6d8f8ee-16a8-4bac-9284-c098502b5f75 to disappear
Apr 30 00:21:16.767: INFO: Pod downward-api-c6d8f8ee-16a8-4bac-9284-c098502b5f75 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:21:16.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8801" for this suite.
Apr 30 00:21:22.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:21:22.864: INFO: namespace downward-api-8801 deletion completed in 6.093188171s

• [SLOW TEST:8.274 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:21:22.864: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7039
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 30 00:21:23.203: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 30 00:21:25.213: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723802883, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723802883, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723802883, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723802883, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 30 00:21:28.230: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:21:28.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7039" for this suite.
Apr 30 00:21:34.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:21:34.358: INFO: namespace webhook-7039 deletion completed in 6.091313804s
STEP: Destroying namespace "webhook-7039-markers" for this suite.
Apr 30 00:21:40.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:21:40.448: INFO: namespace webhook-7039-markers deletion completed in 6.090800242s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.604 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:21:40.468: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7678
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Apr 30 00:21:40.613: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Apr 30 00:21:40.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 create -f - --namespace=kubectl-7678'
Apr 30 00:21:40.734: INFO: stderr: ""
Apr 30 00:21:40.734: INFO: stdout: "service/redis-slave created\n"
Apr 30 00:21:40.734: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Apr 30 00:21:40.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 create -f - --namespace=kubectl-7678'
Apr 30 00:21:40.899: INFO: stderr: ""
Apr 30 00:21:40.899: INFO: stdout: "service/redis-master created\n"
Apr 30 00:21:40.899: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr 30 00:21:40.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 create -f - --namespace=kubectl-7678'
Apr 30 00:21:41.037: INFO: stderr: ""
Apr 30 00:21:41.037: INFO: stdout: "service/frontend created\n"
Apr 30 00:21:41.038: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Apr 30 00:21:41.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 create -f - --namespace=kubectl-7678'
Apr 30 00:21:41.198: INFO: stderr: ""
Apr 30 00:21:41.198: INFO: stdout: "deployment.apps/frontend created\n"
Apr 30 00:21:41.198: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 30 00:21:41.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 create -f - --namespace=kubectl-7678'
Apr 30 00:21:41.351: INFO: stderr: ""
Apr 30 00:21:41.351: INFO: stdout: "deployment.apps/redis-master created\n"
Apr 30 00:21:41.351: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Apr 30 00:21:41.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 create -f - --namespace=kubectl-7678'
Apr 30 00:21:41.468: INFO: stderr: ""
Apr 30 00:21:41.468: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Apr 30 00:21:41.468: INFO: Waiting for all frontend pods to be Running.
Apr 30 00:21:51.518: INFO: Waiting for frontend to serve content.
Apr 30 00:21:51.532: INFO: Trying to add a new entry to the guestbook.
Apr 30 00:21:51.543: INFO: Verifying that added entry can be retrieved.
Apr 30 00:21:51.554: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Apr 30 00:21:56.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete --grace-period=0 --force -f - --namespace=kubectl-7678'
Apr 30 00:21:56.644: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 30 00:21:56.644: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Apr 30 00:21:56.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete --grace-period=0 --force -f - --namespace=kubectl-7678'
Apr 30 00:21:56.729: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 30 00:21:56.729: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr 30 00:21:56.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete --grace-period=0 --force -f - --namespace=kubectl-7678'
Apr 30 00:21:56.808: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 30 00:21:56.808: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 30 00:21:56.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete --grace-period=0 --force -f - --namespace=kubectl-7678'
Apr 30 00:21:56.877: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 30 00:21:56.877: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 30 00:21:56.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete --grace-period=0 --force -f - --namespace=kubectl-7678'
Apr 30 00:21:56.948: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 30 00:21:56.948: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr 30 00:21:56.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete --grace-period=0 --force -f - --namespace=kubectl-7678'
Apr 30 00:21:57.011: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 30 00:21:57.011: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:21:57.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7678" for this suite.
Apr 30 00:22:03.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:22:03.110: INFO: namespace kubectl-7678 deletion completed in 6.094173447s

• [SLOW TEST:22.641 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:22:03.110: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5785
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5785
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5785
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5785
Apr 30 00:22:03.273: INFO: Found 0 stateful pods, waiting for 1
Apr 30 00:22:13.276: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr 30 00:22:13.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-5785 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 30 00:22:13.440: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 30 00:22:13.440: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 30 00:22:13.440: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 30 00:22:13.443: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 30 00:22:23.446: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 30 00:22:23.446: INFO: Waiting for statefulset status.replicas updated to 0
Apr 30 00:22:23.456: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999976s
Apr 30 00:22:24.459: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99781775s
Apr 30 00:22:25.463: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99472389s
Apr 30 00:22:26.466: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991671529s
Apr 30 00:22:27.469: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.988570102s
Apr 30 00:22:28.474: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.985182141s
Apr 30 00:22:29.477: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.980503973s
Apr 30 00:22:30.480: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.977389021s
Apr 30 00:22:31.483: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.974188527s
Apr 30 00:22:32.486: INFO: Verifying statefulset ss doesn't scale past 1 for another 971.174558ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5785
Apr 30 00:22:33.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-5785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 30 00:22:33.663: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 30 00:22:33.663: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 30 00:22:33.663: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 30 00:22:33.667: INFO: Found 1 stateful pods, waiting for 3
Apr 30 00:22:43.670: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 30 00:22:43.670: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 30 00:22:43.670: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr 30 00:22:43.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-5785 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 30 00:22:43.832: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 30 00:22:43.832: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 30 00:22:43.832: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 30 00:22:43.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-5785 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 30 00:22:43.993: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 30 00:22:43.993: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 30 00:22:43.993: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 30 00:22:43.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-5785 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 30 00:22:44.168: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 30 00:22:44.168: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 30 00:22:44.168: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 30 00:22:44.168: INFO: Waiting for statefulset status.replicas updated to 0
Apr 30 00:22:44.171: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr 30 00:22:54.179: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 30 00:22:54.179: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 30 00:22:54.179: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 30 00:22:54.186: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999786s
Apr 30 00:22:55.190: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997781102s
Apr 30 00:22:56.193: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994009807s
Apr 30 00:22:57.197: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991198747s
Apr 30 00:22:58.201: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986865136s
Apr 30 00:22:59.204: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.98339704s
Apr 30 00:23:00.207: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980529929s
Apr 30 00:23:01.210: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.977491596s
Apr 30 00:23:02.214: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.97450357s
Apr 30 00:23:03.217: INFO: Verifying statefulset ss doesn't scale past 3 for another 970.530115ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5785
Apr 30 00:23:04.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-5785 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 30 00:23:04.380: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 30 00:23:04.380: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 30 00:23:04.380: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 30 00:23:04.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-5785 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 30 00:23:04.547: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 30 00:23:04.547: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 30 00:23:04.547: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 30 00:23:04.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-5785 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 30 00:23:04.722: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 30 00:23:04.722: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 30 00:23:04.722: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 30 00:23:04.722: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Apr 30 00:23:24.738: INFO: Deleting all statefulset in ns statefulset-5785
Apr 30 00:23:24.740: INFO: Scaling statefulset ss to 0
Apr 30 00:23:24.748: INFO: Waiting for statefulset status.replicas updated to 0
Apr 30 00:23:24.750: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:23:24.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5785" for this suite.
Apr 30 00:23:30.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:23:30.859: INFO: namespace statefulset-5785 deletion completed in 6.09117576s

• [SLOW TEST:87.749 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:23:30.859: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6359
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 30 00:23:31.013: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c1bec437-fa54-4c48-81be-16485c6fbe98" in namespace "downward-api-6359" to be "success or failure"
Apr 30 00:23:31.015: INFO: Pod "downwardapi-volume-c1bec437-fa54-4c48-81be-16485c6fbe98": Phase="Pending", Reason="", readiness=false. Elapsed: 1.997637ms
Apr 30 00:23:33.018: INFO: Pod "downwardapi-volume-c1bec437-fa54-4c48-81be-16485c6fbe98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00494314s
STEP: Saw pod success
Apr 30 00:23:33.018: INFO: Pod "downwardapi-volume-c1bec437-fa54-4c48-81be-16485c6fbe98" satisfied condition "success or failure"
Apr 30 00:23:33.020: INFO: Trying to get logs from node ip-192-168-197-211.us-west-2.compute.internal pod downwardapi-volume-c1bec437-fa54-4c48-81be-16485c6fbe98 container client-container: <nil>
STEP: delete the pod
Apr 30 00:23:33.038: INFO: Waiting for pod downwardapi-volume-c1bec437-fa54-4c48-81be-16485c6fbe98 to disappear
Apr 30 00:23:33.040: INFO: Pod downwardapi-volume-c1bec437-fa54-4c48-81be-16485c6fbe98 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:23:33.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6359" for this suite.
Apr 30 00:23:39.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:23:39.135: INFO: namespace downward-api-6359 deletion completed in 6.091412701s

• [SLOW TEST:8.276 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:23:39.136: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4338
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-09564a4c-3d1e-4ad7-b405-dc6e91c46d59
STEP: Creating a pod to test consume configMaps
Apr 30 00:23:39.291: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-51ef2369-c646-4e86-bed0-d86d871d7d35" in namespace "projected-4338" to be "success or failure"
Apr 30 00:23:39.294: INFO: Pod "pod-projected-configmaps-51ef2369-c646-4e86-bed0-d86d871d7d35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.54713ms
Apr 30 00:23:41.297: INFO: Pod "pod-projected-configmaps-51ef2369-c646-4e86-bed0-d86d871d7d35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005350631s
STEP: Saw pod success
Apr 30 00:23:41.297: INFO: Pod "pod-projected-configmaps-51ef2369-c646-4e86-bed0-d86d871d7d35" satisfied condition "success or failure"
Apr 30 00:23:41.299: INFO: Trying to get logs from node ip-192-168-222-85.us-west-2.compute.internal pod pod-projected-configmaps-51ef2369-c646-4e86-bed0-d86d871d7d35 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 30 00:23:41.316: INFO: Waiting for pod pod-projected-configmaps-51ef2369-c646-4e86-bed0-d86d871d7d35 to disappear
Apr 30 00:23:41.318: INFO: Pod pod-projected-configmaps-51ef2369-c646-4e86-bed0-d86d871d7d35 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:23:41.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4338" for this suite.
Apr 30 00:23:47.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:23:47.410: INFO: namespace projected-4338 deletion completed in 6.087589484s

• [SLOW TEST:8.274 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:23:47.410: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3593
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:24:03.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3593" for this suite.
Apr 30 00:24:09.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:24:09.768: INFO: namespace resourcequota-3593 deletion completed in 6.166981834s

• [SLOW TEST:22.358 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:24:09.768: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8369
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 30 00:24:11.944: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:24:11.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8369" for this suite.
Apr 30 00:24:17.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:24:18.048: INFO: namespace container-runtime-8369 deletion completed in 6.088634294s

• [SLOW TEST:8.280 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:24:18.048: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1926
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Apr 30 00:24:18.222: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 30 00:24:20.838: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:24:28.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1926" for this suite.
Apr 30 00:24:34.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:24:34.962: INFO: namespace crd-publish-openapi-1926 deletion completed in 6.089463049s

• [SLOW TEST:16.914 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:24:34.962: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4727
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:24:35.108: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:24:41.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4727" for this suite.
Apr 30 00:24:47.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:24:47.406: INFO: namespace custom-resource-definition-4727 deletion completed in 6.097412385s

• [SLOW TEST:12.444 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:24:47.406: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4838
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1294
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1423
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:24:53.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4838" for this suite.
Apr 30 00:24:59.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:24:59.955: INFO: namespace namespaces-4838 deletion completed in 6.089730682s
STEP: Destroying namespace "nsdeletetest-1294" for this suite.
Apr 30 00:24:59.957: INFO: Namespace nsdeletetest-1294 was already deleted
STEP: Destroying namespace "nsdeletetest-1423" for this suite.
Apr 30 00:25:05.967: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:25:06.100: INFO: namespace nsdeletetest-1423 deletion completed in 6.143054474s

• [SLOW TEST:18.694 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:25:06.100: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6028
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 30 00:25:06.334: INFO: Waiting up to 5m0s for pod "pod-8a8eb879-54bd-4694-91f7-1319d048d53f" in namespace "emptydir-6028" to be "success or failure"
Apr 30 00:25:06.336: INFO: Pod "pod-8a8eb879-54bd-4694-91f7-1319d048d53f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.983587ms
Apr 30 00:25:08.339: INFO: Pod "pod-8a8eb879-54bd-4694-91f7-1319d048d53f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00499184s
STEP: Saw pod success
Apr 30 00:25:08.339: INFO: Pod "pod-8a8eb879-54bd-4694-91f7-1319d048d53f" satisfied condition "success or failure"
Apr 30 00:25:08.341: INFO: Trying to get logs from node ip-192-168-197-211.us-west-2.compute.internal pod pod-8a8eb879-54bd-4694-91f7-1319d048d53f container test-container: <nil>
STEP: delete the pod
Apr 30 00:25:08.359: INFO: Waiting for pod pod-8a8eb879-54bd-4694-91f7-1319d048d53f to disappear
Apr 30 00:25:08.361: INFO: Pod pod-8a8eb879-54bd-4694-91f7-1319d048d53f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:25:08.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6028" for this suite.
Apr 30 00:25:14.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:25:14.461: INFO: namespace emptydir-6028 deletion completed in 6.095686824s

• [SLOW TEST:8.360 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:25:14.461: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1379
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-12dc6139-3ce4-46b6-bd13-763c9c3f2424
STEP: Creating a pod to test consume secrets
Apr 30 00:25:14.758: INFO: Waiting up to 5m0s for pod "pod-secrets-c2259f2e-cc64-48a6-8162-7fe956345dab" in namespace "secrets-1379" to be "success or failure"
Apr 30 00:25:14.760: INFO: Pod "pod-secrets-c2259f2e-cc64-48a6-8162-7fe956345dab": Phase="Pending", Reason="", readiness=false. Elapsed: 1.83501ms
Apr 30 00:25:16.763: INFO: Pod "pod-secrets-c2259f2e-cc64-48a6-8162-7fe956345dab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004613964s
STEP: Saw pod success
Apr 30 00:25:16.763: INFO: Pod "pod-secrets-c2259f2e-cc64-48a6-8162-7fe956345dab" satisfied condition "success or failure"
Apr 30 00:25:16.765: INFO: Trying to get logs from node ip-192-168-211-77.us-west-2.compute.internal pod pod-secrets-c2259f2e-cc64-48a6-8162-7fe956345dab container secret-volume-test: <nil>
STEP: delete the pod
Apr 30 00:25:16.783: INFO: Waiting for pod pod-secrets-c2259f2e-cc64-48a6-8162-7fe956345dab to disappear
Apr 30 00:25:16.785: INFO: Pod pod-secrets-c2259f2e-cc64-48a6-8162-7fe956345dab no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:25:16.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1379" for this suite.
Apr 30 00:25:22.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:25:22.876: INFO: namespace secrets-1379 deletion completed in 6.088032703s

• [SLOW TEST:8.415 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:25:22.877: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-618
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:25:27.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-618" for this suite.
Apr 30 00:26:11.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:26:11.144: INFO: namespace kubelet-test-618 deletion completed in 44.091977345s

• [SLOW TEST:48.266 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:26:11.144: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7453
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Apr 30 00:26:11.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 create -f - --namespace=kubectl-7453'
Apr 30 00:26:11.513: INFO: stderr: ""
Apr 30 00:26:11.513: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 30 00:26:11.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7453'
Apr 30 00:26:11.569: INFO: stderr: ""
Apr 30 00:26:11.569: INFO: stdout: "update-demo-nautilus-6fsqk update-demo-nautilus-q678f "
Apr 30 00:26:11.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-6fsqk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Apr 30 00:26:11.620: INFO: stderr: ""
Apr 30 00:26:11.620: INFO: stdout: ""
Apr 30 00:26:11.620: INFO: update-demo-nautilus-6fsqk is created but not running
Apr 30 00:26:16.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7453'
Apr 30 00:26:16.675: INFO: stderr: ""
Apr 30 00:26:16.675: INFO: stdout: "update-demo-nautilus-6fsqk update-demo-nautilus-q678f "
Apr 30 00:26:16.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-6fsqk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Apr 30 00:26:16.726: INFO: stderr: ""
Apr 30 00:26:16.726: INFO: stdout: "true"
Apr 30 00:26:16.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-6fsqk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Apr 30 00:26:16.777: INFO: stderr: ""
Apr 30 00:26:16.777: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 30 00:26:16.777: INFO: validating pod update-demo-nautilus-6fsqk
Apr 30 00:26:16.780: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 30 00:26:16.781: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 30 00:26:16.781: INFO: update-demo-nautilus-6fsqk is verified up and running
Apr 30 00:26:16.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-q678f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Apr 30 00:26:16.833: INFO: stderr: ""
Apr 30 00:26:16.833: INFO: stdout: "true"
Apr 30 00:26:16.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-q678f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Apr 30 00:26:16.887: INFO: stderr: ""
Apr 30 00:26:16.887: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 30 00:26:16.887: INFO: validating pod update-demo-nautilus-q678f
Apr 30 00:26:16.890: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 30 00:26:16.890: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 30 00:26:16.890: INFO: update-demo-nautilus-q678f is verified up and running
STEP: scaling down the replication controller
Apr 30 00:26:16.891: INFO: scanned /root for discovery docs: <nil>
Apr 30 00:26:16.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7453'
Apr 30 00:26:17.958: INFO: stderr: ""
Apr 30 00:26:17.958: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 30 00:26:17.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7453'
Apr 30 00:26:18.012: INFO: stderr: ""
Apr 30 00:26:18.013: INFO: stdout: "update-demo-nautilus-6fsqk update-demo-nautilus-q678f "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 30 00:26:23.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7453'
Apr 30 00:26:23.070: INFO: stderr: ""
Apr 30 00:26:23.070: INFO: stdout: "update-demo-nautilus-6fsqk "
Apr 30 00:26:23.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-6fsqk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Apr 30 00:26:23.121: INFO: stderr: ""
Apr 30 00:26:23.121: INFO: stdout: "true"
Apr 30 00:26:23.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-6fsqk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Apr 30 00:26:23.172: INFO: stderr: ""
Apr 30 00:26:23.172: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 30 00:26:23.172: INFO: validating pod update-demo-nautilus-6fsqk
Apr 30 00:26:23.176: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 30 00:26:23.176: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 30 00:26:23.176: INFO: update-demo-nautilus-6fsqk is verified up and running
STEP: scaling up the replication controller
Apr 30 00:26:23.177: INFO: scanned /root for discovery docs: <nil>
Apr 30 00:26:23.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7453'
Apr 30 00:26:24.249: INFO: stderr: ""
Apr 30 00:26:24.249: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 30 00:26:24.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7453'
Apr 30 00:26:24.305: INFO: stderr: ""
Apr 30 00:26:24.305: INFO: stdout: "update-demo-nautilus-6fsqk update-demo-nautilus-kg5pp "
Apr 30 00:26:24.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-6fsqk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Apr 30 00:26:24.362: INFO: stderr: ""
Apr 30 00:26:24.362: INFO: stdout: "true"
Apr 30 00:26:24.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-6fsqk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Apr 30 00:26:24.415: INFO: stderr: ""
Apr 30 00:26:24.415: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 30 00:26:24.415: INFO: validating pod update-demo-nautilus-6fsqk
Apr 30 00:26:24.419: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 30 00:26:24.419: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 30 00:26:24.419: INFO: update-demo-nautilus-6fsqk is verified up and running
Apr 30 00:26:24.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-kg5pp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Apr 30 00:26:24.474: INFO: stderr: ""
Apr 30 00:26:24.474: INFO: stdout: ""
Apr 30 00:26:24.474: INFO: update-demo-nautilus-kg5pp is created but not running
Apr 30 00:26:29.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7453'
Apr 30 00:26:29.528: INFO: stderr: ""
Apr 30 00:26:29.528: INFO: stdout: "update-demo-nautilus-6fsqk update-demo-nautilus-kg5pp "
Apr 30 00:26:29.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-6fsqk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Apr 30 00:26:29.579: INFO: stderr: ""
Apr 30 00:26:29.579: INFO: stdout: "true"
Apr 30 00:26:29.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-6fsqk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Apr 30 00:26:29.632: INFO: stderr: ""
Apr 30 00:26:29.632: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 30 00:26:29.632: INFO: validating pod update-demo-nautilus-6fsqk
Apr 30 00:26:29.635: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 30 00:26:29.635: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 30 00:26:29.635: INFO: update-demo-nautilus-6fsqk is verified up and running
Apr 30 00:26:29.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-kg5pp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Apr 30 00:26:29.690: INFO: stderr: ""
Apr 30 00:26:29.690: INFO: stdout: "true"
Apr 30 00:26:29.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods update-demo-nautilus-kg5pp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7453'
Apr 30 00:26:29.747: INFO: stderr: ""
Apr 30 00:26:29.747: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 30 00:26:29.747: INFO: validating pod update-demo-nautilus-kg5pp
Apr 30 00:26:29.753: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 30 00:26:29.753: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 30 00:26:29.753: INFO: update-demo-nautilus-kg5pp is verified up and running
STEP: using delete to clean up resources
Apr 30 00:26:29.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete --grace-period=0 --force -f - --namespace=kubectl-7453'
Apr 30 00:26:29.810: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 30 00:26:29.810: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 30 00:26:29.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7453'
Apr 30 00:26:29.869: INFO: stderr: "No resources found in kubectl-7453 namespace.\n"
Apr 30 00:26:29.869: INFO: stdout: ""
Apr 30 00:26:29.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -l name=update-demo --namespace=kubectl-7453 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 30 00:26:29.925: INFO: stderr: ""
Apr 30 00:26:29.925: INFO: stdout: "update-demo-nautilus-6fsqk\nupdate-demo-nautilus-kg5pp\n"
Apr 30 00:26:30.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7453'
Apr 30 00:26:30.483: INFO: stderr: "No resources found in kubectl-7453 namespace.\n"
Apr 30 00:26:30.483: INFO: stdout: ""
Apr 30 00:26:30.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 get pods -l name=update-demo --namespace=kubectl-7453 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 30 00:26:30.537: INFO: stderr: ""
Apr 30 00:26:30.537: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:26:30.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7453" for this suite.
Apr 30 00:26:36.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:26:36.635: INFO: namespace kubectl-7453 deletion completed in 6.09312461s

• [SLOW TEST:25.491 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:26:36.635: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1941
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1941.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1941.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 0.202.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.202.0_udp@PTR;check="$$(dig +tcp +noall +answer +search 0.202.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.202.0_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1941.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1941.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1941.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1941.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1941.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1941.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 0.202.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.202.0_udp@PTR;check="$$(dig +tcp +noall +answer +search 0.202.100.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.100.202.0_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 30 00:26:40.820: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1941.svc.cluster.local from pod dns-1941/dns-test-bfe0ce81-f51e-4ddb-8d6b-b85e3bd66dab: the server could not find the requested resource (get pods dns-test-bfe0ce81-f51e-4ddb-8d6b-b85e3bd66dab)
Apr 30 00:26:40.822: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1941.svc.cluster.local from pod dns-1941/dns-test-bfe0ce81-f51e-4ddb-8d6b-b85e3bd66dab: the server could not find the requested resource (get pods dns-test-bfe0ce81-f51e-4ddb-8d6b-b85e3bd66dab)
Apr 30 00:26:40.841: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1941.svc.cluster.local from pod dns-1941/dns-test-bfe0ce81-f51e-4ddb-8d6b-b85e3bd66dab: the server could not find the requested resource (get pods dns-test-bfe0ce81-f51e-4ddb-8d6b-b85e3bd66dab)
Apr 30 00:26:40.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1941.svc.cluster.local from pod dns-1941/dns-test-bfe0ce81-f51e-4ddb-8d6b-b85e3bd66dab: the server could not find the requested resource (get pods dns-test-bfe0ce81-f51e-4ddb-8d6b-b85e3bd66dab)
Apr 30 00:26:40.857: INFO: Lookups using dns-1941/dns-test-bfe0ce81-f51e-4ddb-8d6b-b85e3bd66dab failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-1941.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1941.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1941.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1941.svc.cluster.local]

Apr 30 00:26:45.903: INFO: DNS probes using dns-1941/dns-test-bfe0ce81-f51e-4ddb-8d6b-b85e3bd66dab succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:26:45.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1941" for this suite.
Apr 30 00:26:51.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:26:52.053: INFO: namespace dns-1941 deletion completed in 6.091391123s

• [SLOW TEST:15.418 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:26:52.053: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5842
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:26:54.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5842" for this suite.
Apr 30 00:27:38.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:27:38.317: INFO: namespace kubelet-test-5842 deletion completed in 44.089015191s

• [SLOW TEST:46.264 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:27:38.318: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1661
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:27:38.465: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr 30 00:27:43.468: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 30 00:27:43.468: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Apr 30 00:27:43.484: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1661 /apis/apps/v1/namespaces/deployment-1661/deployments/test-cleanup-deployment 8292d947-efc7-4ef1-8e59-188de0a40aac 29200 1 2020-04-30 00:27:43 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003679768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Apr 30 00:27:43.486: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:27:43.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1661" for this suite.
Apr 30 00:27:49.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:27:49.586: INFO: namespace deployment-1661 deletion completed in 6.093051316s

• [SLOW TEST:11.268 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:27:49.586: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-60e7a1eb-4beb-4046-acd3-1cad9814d014 in namespace container-probe-1646
Apr 30 00:27:51.743: INFO: Started pod test-webserver-60e7a1eb-4beb-4046-acd3-1cad9814d014 in namespace container-probe-1646
STEP: checking the pod's current state and verifying that restartCount is present
Apr 30 00:27:51.745: INFO: Initial restart count of pod test-webserver-60e7a1eb-4beb-4046-acd3-1cad9814d014 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:31:52.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1646" for this suite.
Apr 30 00:31:58.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:31:58.205: INFO: namespace container-probe-1646 deletion completed in 6.09308936s

• [SLOW TEST:248.619 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:31:58.206: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9533
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 30 00:31:58.362: INFO: Waiting up to 5m0s for pod "pod-154b1076-ab53-46b9-9b56-5c60f3cc4421" in namespace "emptydir-9533" to be "success or failure"
Apr 30 00:31:58.364: INFO: Pod "pod-154b1076-ab53-46b9-9b56-5c60f3cc4421": Phase="Pending", Reason="", readiness=false. Elapsed: 1.791529ms
Apr 30 00:32:00.367: INFO: Pod "pod-154b1076-ab53-46b9-9b56-5c60f3cc4421": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004528647s
Apr 30 00:32:02.370: INFO: Pod "pod-154b1076-ab53-46b9-9b56-5c60f3cc4421": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007502317s
STEP: Saw pod success
Apr 30 00:32:02.370: INFO: Pod "pod-154b1076-ab53-46b9-9b56-5c60f3cc4421" satisfied condition "success or failure"
Apr 30 00:32:02.372: INFO: Trying to get logs from node ip-192-168-72-199.us-west-2.compute.internal pod pod-154b1076-ab53-46b9-9b56-5c60f3cc4421 container test-container: <nil>
STEP: delete the pod
Apr 30 00:32:02.393: INFO: Waiting for pod pod-154b1076-ab53-46b9-9b56-5c60f3cc4421 to disappear
Apr 30 00:32:02.396: INFO: Pod pod-154b1076-ab53-46b9-9b56-5c60f3cc4421 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:32:02.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9533" for this suite.
Apr 30 00:32:08.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:32:08.547: INFO: namespace emptydir-9533 deletion completed in 6.146839265s

• [SLOW TEST:10.341 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:32:08.547: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3450
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 30 00:32:08.706: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3b08c8a-02f4-457a-9fbf-e84bd1a7d4d9" in namespace "downward-api-3450" to be "success or failure"
Apr 30 00:32:08.708: INFO: Pod "downwardapi-volume-d3b08c8a-02f4-457a-9fbf-e84bd1a7d4d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009925ms
Apr 30 00:32:10.711: INFO: Pod "downwardapi-volume-d3b08c8a-02f4-457a-9fbf-e84bd1a7d4d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00480835s
STEP: Saw pod success
Apr 30 00:32:10.711: INFO: Pod "downwardapi-volume-d3b08c8a-02f4-457a-9fbf-e84bd1a7d4d9" satisfied condition "success or failure"
Apr 30 00:32:10.713: INFO: Trying to get logs from node ip-192-168-72-199.us-west-2.compute.internal pod downwardapi-volume-d3b08c8a-02f4-457a-9fbf-e84bd1a7d4d9 container client-container: <nil>
STEP: delete the pod
Apr 30 00:32:10.728: INFO: Waiting for pod downwardapi-volume-d3b08c8a-02f4-457a-9fbf-e84bd1a7d4d9 to disappear
Apr 30 00:32:10.730: INFO: Pod downwardapi-volume-d3b08c8a-02f4-457a-9fbf-e84bd1a7d4d9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:32:10.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3450" for this suite.
Apr 30 00:32:16.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:32:16.826: INFO: namespace downward-api-3450 deletion completed in 6.091832738s

• [SLOW TEST:8.279 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:32:16.826: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-28
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Apr 30 00:32:16.967: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:32:21.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-28" for this suite.
Apr 30 00:32:33.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:32:33.205: INFO: namespace init-container-28 deletion completed in 12.092459924s

• [SLOW TEST:16.379 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:32:33.205: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1272
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-b7410e69-abf3-4d1d-be70-9c2e20fba475
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:32:33.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1272" for this suite.
Apr 30 00:32:39.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:32:39.446: INFO: namespace configmap-1272 deletion completed in 6.093736555s

• [SLOW TEST:6.241 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:32:39.446: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-515
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Apr 30 00:32:42.120: INFO: Successfully updated pod "labelsupdatecbb4d44a-24ee-4031-a1ce-d483f8af7e84"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:32:44.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-515" for this suite.
Apr 30 00:32:56.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:32:56.229: INFO: namespace downward-api-515 deletion completed in 12.091025786s

• [SLOW TEST:16.783 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:32:56.230: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-364
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:33:07.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-364" for this suite.
Apr 30 00:33:13.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:33:13.529: INFO: namespace resourcequota-364 deletion completed in 6.121597884s

• [SLOW TEST:17.299 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:33:13.530: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4975
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 30 00:33:13.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-4975'
Apr 30 00:33:13.742: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 30 00:33:13.742: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Apr 30 00:33:13.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 delete jobs e2e-test-httpd-job --namespace=kubectl-4975'
Apr 30 00:33:13.810: INFO: stderr: ""
Apr 30 00:33:13.810: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:33:13.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4975" for this suite.
Apr 30 00:33:25.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:33:25.906: INFO: namespace kubectl-4975 deletion completed in 12.092012296s

• [SLOW TEST:12.376 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:33:25.906: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-3032
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Apr 30 00:33:26.059: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-3032" to be "success or failure"
Apr 30 00:33:26.061: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 1.949518ms
Apr 30 00:33:28.064: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004875797s
STEP: Saw pod success
Apr 30 00:33:28.064: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Apr 30 00:33:28.066: INFO: Trying to get logs from node ip-192-168-83-225.us-west-2.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Apr 30 00:33:28.086: INFO: Waiting for pod pod-host-path-test to disappear
Apr 30 00:33:28.087: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:33:28.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-3032" for this suite.
Apr 30 00:33:34.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:33:34.184: INFO: namespace hostpath-3032 deletion completed in 6.092767767s

• [SLOW TEST:8.278 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:33:34.185: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8092
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:33:37.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8092" for this suite.
Apr 30 00:33:49.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:33:49.448: INFO: namespace replication-controller-8092 deletion completed in 12.091839098s

• [SLOW TEST:15.263 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:33:49.448: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-2874
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr 30 00:33:49.886: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Apr 30 00:33:51.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723803629, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723803629, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723803629, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723803629, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 30 00:33:54.913: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:33:54.917: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:33:56.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2874" for this suite.
Apr 30 00:34:02.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:34:02.205: INFO: namespace crd-webhook-2874 deletion completed in 6.146953123s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:12.787 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:34:02.236: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2923
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:34:02.402: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-7bdd1c41-741b-4f4c-b8d5-6617bbe0a92a" in namespace "security-context-test-2923" to be "success or failure"
Apr 30 00:34:02.410: INFO: Pod "busybox-privileged-false-7bdd1c41-741b-4f4c-b8d5-6617bbe0a92a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020589ms
Apr 30 00:34:04.413: INFO: Pod "busybox-privileged-false-7bdd1c41-741b-4f4c-b8d5-6617bbe0a92a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010820533s
Apr 30 00:34:04.413: INFO: Pod "busybox-privileged-false-7bdd1c41-741b-4f4c-b8d5-6617bbe0a92a" satisfied condition "success or failure"
Apr 30 00:34:04.422: INFO: Got logs for pod "busybox-privileged-false-7bdd1c41-741b-4f4c-b8d5-6617bbe0a92a": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:34:04.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2923" for this suite.
Apr 30 00:34:10.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:34:10.529: INFO: namespace security-context-test-2923 deletion completed in 6.102829124s

• [SLOW TEST:8.293 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:34:10.530: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1260
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1260
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-1260
Apr 30 00:34:10.684: INFO: Found 0 stateful pods, waiting for 1
Apr 30 00:34:20.687: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Apr 30 00:34:20.700: INFO: Deleting all statefulset in ns statefulset-1260
Apr 30 00:34:20.702: INFO: Scaling statefulset ss to 0
Apr 30 00:34:40.712: INFO: Waiting for statefulset status.replicas updated to 0
Apr 30 00:34:40.714: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:34:40.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1260" for this suite.
Apr 30 00:34:46.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:34:46.819: INFO: namespace statefulset-1260 deletion completed in 6.090945962s

• [SLOW TEST:36.290 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:34:46.820: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4022
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 30 00:34:46.969: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4c08751a-103d-44b2-8d01-23b3302a899b" in namespace "downward-api-4022" to be "success or failure"
Apr 30 00:34:46.971: INFO: Pod "downwardapi-volume-4c08751a-103d-44b2-8d01-23b3302a899b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.944419ms
Apr 30 00:34:48.974: INFO: Pod "downwardapi-volume-4c08751a-103d-44b2-8d01-23b3302a899b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005057746s
STEP: Saw pod success
Apr 30 00:34:48.974: INFO: Pod "downwardapi-volume-4c08751a-103d-44b2-8d01-23b3302a899b" satisfied condition "success or failure"
Apr 30 00:34:48.976: INFO: Trying to get logs from node ip-192-168-211-77.us-west-2.compute.internal pod downwardapi-volume-4c08751a-103d-44b2-8d01-23b3302a899b container client-container: <nil>
STEP: delete the pod
Apr 30 00:34:48.995: INFO: Waiting for pod downwardapi-volume-4c08751a-103d-44b2-8d01-23b3302a899b to disappear
Apr 30 00:34:48.997: INFO: Pod downwardapi-volume-4c08751a-103d-44b2-8d01-23b3302a899b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:34:48.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4022" for this suite.
Apr 30 00:34:55.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:34:55.094: INFO: namespace downward-api-4022 deletion completed in 6.093721236s

• [SLOW TEST:8.275 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:34:55.094: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1566
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:35:02.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1566" for this suite.
Apr 30 00:35:08.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:35:08.369: INFO: namespace resourcequota-1566 deletion completed in 6.107051163s

• [SLOW TEST:13.275 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:35:08.369: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9006
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Apr 30 00:35:10.554: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-371654096 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Apr 30 00:35:25.687: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:35:25.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9006" for this suite.
Apr 30 00:35:31.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:35:31.787: INFO: namespace pods-9006 deletion completed in 6.094594368s

• [SLOW TEST:23.418 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:35:31.788: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5234
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Apr 30 00:35:31.931: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:35:43.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5234" for this suite.
Apr 30 00:35:49.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:35:49.857: INFO: namespace crd-publish-openapi-5234 deletion completed in 6.091176276s

• [SLOW TEST:18.069 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:35:49.857: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-777
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-003edd7d-837c-41aa-aaab-b4d4a5092a2e
STEP: Creating a pod to test consume configMaps
Apr 30 00:35:50.013: INFO: Waiting up to 5m0s for pod "pod-configmaps-f1802e3f-1eb6-468a-915d-3247ffe44f89" in namespace "configmap-777" to be "success or failure"
Apr 30 00:35:50.015: INFO: Pod "pod-configmaps-f1802e3f-1eb6-468a-915d-3247ffe44f89": Phase="Pending", Reason="", readiness=false. Elapsed: 1.919738ms
Apr 30 00:35:52.017: INFO: Pod "pod-configmaps-f1802e3f-1eb6-468a-915d-3247ffe44f89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004641696s
Apr 30 00:35:54.020: INFO: Pod "pod-configmaps-f1802e3f-1eb6-468a-915d-3247ffe44f89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0074965s
STEP: Saw pod success
Apr 30 00:35:54.020: INFO: Pod "pod-configmaps-f1802e3f-1eb6-468a-915d-3247ffe44f89" satisfied condition "success or failure"
Apr 30 00:35:54.022: INFO: Trying to get logs from node ip-192-168-148-117.us-west-2.compute.internal pod pod-configmaps-f1802e3f-1eb6-468a-915d-3247ffe44f89 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 30 00:35:54.043: INFO: Waiting for pod pod-configmaps-f1802e3f-1eb6-468a-915d-3247ffe44f89 to disappear
Apr 30 00:35:54.045: INFO: Pod pod-configmaps-f1802e3f-1eb6-468a-915d-3247ffe44f89 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:35:54.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-777" for this suite.
Apr 30 00:36:00.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:36:00.147: INFO: namespace configmap-777 deletion completed in 6.098709458s

• [SLOW TEST:10.290 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:36:00.147: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3687
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Apr 30 00:36:00.289: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 30 00:36:00.300: INFO: Waiting for terminating namespaces to be deleted...
Apr 30 00:36:00.303: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-144-27.us-west-2.compute.internal before test
Apr 30 00:36:00.314: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-2ffxh from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:36:00.314: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:36:00.314: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:36:00.314: INFO: aws-node-rccgv from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.314: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:36:00.314: INFO: kube-proxy-4xrfl from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.314: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:36:00.314: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-148-117.us-west-2.compute.internal before test
Apr 30 00:36:00.319: INFO: aws-node-wzq6b from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.319: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:36:00.319: INFO: kube-proxy-b4plm from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.319: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:36:00.319: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-69sgr from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:36:00.319: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:36:00.319: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:36:00.319: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-149-254.us-west-2.compute.internal before test
Apr 30 00:36:00.329: INFO: aws-node-kcbxd from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.329: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:36:00.329: INFO: kube-proxy-jvnfx from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.329: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:36:00.329: INFO: sonobuoy from sonobuoy started at 2020-04-29 23:02:27 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.329: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 30 00:36:00.329: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-xtlz4 from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:36:00.329: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:36:00.329: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:36:00.329: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-197-211.us-west-2.compute.internal before test
Apr 30 00:36:00.337: INFO: kube-proxy-xhpsv from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.337: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:36:00.337: INFO: aws-node-ffvhf from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.337: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:36:00.337: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-wlv6l from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:36:00.337: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:36:00.337: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:36:00.337: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-211-77.us-west-2.compute.internal before test
Apr 30 00:36:00.341: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-xpbv9 from sonobuoy started at 2020-04-29 23:02:31 +0000 UTC (2 container statuses recorded)
Apr 30 00:36:00.341: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:36:00.341: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:36:00.341: INFO: aws-node-xj5sd from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.341: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:36:00.341: INFO: kube-proxy-fck6k from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.341: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:36:00.341: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-218-192.us-west-2.compute.internal before test
Apr 30 00:36:00.348: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-8cg7t from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:36:00.348: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:36:00.348: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:36:00.348: INFO: aws-node-s9vwd from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.348: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:36:00.348: INFO: kube-proxy-b6znl from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.348: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:36:00.348: INFO: sonobuoy-e2e-job-c0680a6adde246b1 from sonobuoy started at 2020-04-29 23:02:31 +0000 UTC (2 container statuses recorded)
Apr 30 00:36:00.348: INFO: 	Container e2e ready: true, restart count 0
Apr 30 00:36:00.348: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 30 00:36:00.348: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-222-85.us-west-2.compute.internal before test
Apr 30 00:36:00.355: INFO: kube-proxy-fh79j from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.355: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:36:00.355: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-qh6r9 from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:36:00.355: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:36:00.355: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:36:00.355: INFO: aws-node-l9blv from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.355: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:36:00.355: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-72-199.us-west-2.compute.internal before test
Apr 30 00:36:00.362: INFO: coredns-5c97f79574-9t82l from kube-system started at 2020-04-29 22:50:59 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.362: INFO: 	Container coredns ready: true, restart count 0
Apr 30 00:36:00.362: INFO: coredns-5c97f79574-wjwsw from kube-system started at 2020-04-29 22:50:59 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.362: INFO: 	Container coredns ready: true, restart count 0
Apr 30 00:36:00.362: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-sg2vq from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:36:00.362: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:36:00.362: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:36:00.362: INFO: kube-proxy-kklzp from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.362: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:36:00.362: INFO: aws-node-zldl6 from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.362: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:36:00.362: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-82-130.us-west-2.compute.internal before test
Apr 30 00:36:00.366: INFO: kube-proxy-8pccx from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.366: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:36:00.366: INFO: aws-node-rbjxk from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.366: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:36:00.366: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-ptxhd from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:36:00.366: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:36:00.366: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:36:00.366: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-83-225.us-west-2.compute.internal before test
Apr 30 00:36:00.373: INFO: kube-proxy-sz6rb from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.373: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:36:00.373: INFO: aws-node-j8bsf from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:36:00.373: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:36:00.373: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-rmfrj from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:36:00.373: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:36:00.373: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-7672bb3d-78e0-47f7-a6d1-6c454dc8fca2 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-7672bb3d-78e0-47f7-a6d1-6c454dc8fca2 off the node ip-192-168-83-225.us-west-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-7672bb3d-78e0-47f7-a6d1-6c454dc8fca2
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:36:04.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3687" for this suite.
Apr 30 00:36:12.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:36:12.522: INFO: namespace sched-pred-3687 deletion completed in 8.092964431s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:12.374 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:36:12.522: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8617
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:36:12.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8617" for this suite.
Apr 30 00:36:24.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:36:24.796: INFO: namespace pods-8617 deletion completed in 12.113742109s

• [SLOW TEST:12.275 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:36:24.797: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3523
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-711ff62a-423a-4296-b7fb-d3acee0df345
STEP: Creating a pod to test consume secrets
Apr 30 00:36:24.963: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2b067f61-34cf-4b7c-84a8-4847f78e24aa" in namespace "projected-3523" to be "success or failure"
Apr 30 00:36:24.965: INFO: Pod "pod-projected-secrets-2b067f61-34cf-4b7c-84a8-4847f78e24aa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.816903ms
Apr 30 00:36:26.968: INFO: Pod "pod-projected-secrets-2b067f61-34cf-4b7c-84a8-4847f78e24aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004924176s
STEP: Saw pod success
Apr 30 00:36:26.968: INFO: Pod "pod-projected-secrets-2b067f61-34cf-4b7c-84a8-4847f78e24aa" satisfied condition "success or failure"
Apr 30 00:36:26.970: INFO: Trying to get logs from node ip-192-168-72-199.us-west-2.compute.internal pod pod-projected-secrets-2b067f61-34cf-4b7c-84a8-4847f78e24aa container secret-volume-test: <nil>
STEP: delete the pod
Apr 30 00:36:26.984: INFO: Waiting for pod pod-projected-secrets-2b067f61-34cf-4b7c-84a8-4847f78e24aa to disappear
Apr 30 00:36:26.986: INFO: Pod pod-projected-secrets-2b067f61-34cf-4b7c-84a8-4847f78e24aa no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:36:26.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3523" for this suite.
Apr 30 00:36:32.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:36:33.086: INFO: namespace projected-3523 deletion completed in 6.096644768s

• [SLOW TEST:8.290 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:36:33.086: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9980
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Apr 30 00:36:33.238: INFO: Waiting up to 5m0s for pod "downward-api-e3c22c72-af4a-45fc-ad66-e544c5b44785" in namespace "downward-api-9980" to be "success or failure"
Apr 30 00:36:33.240: INFO: Pod "downward-api-e3c22c72-af4a-45fc-ad66-e544c5b44785": Phase="Pending", Reason="", readiness=false. Elapsed: 1.870277ms
Apr 30 00:36:35.242: INFO: Pod "downward-api-e3c22c72-af4a-45fc-ad66-e544c5b44785": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004566558s
STEP: Saw pod success
Apr 30 00:36:35.242: INFO: Pod "downward-api-e3c22c72-af4a-45fc-ad66-e544c5b44785" satisfied condition "success or failure"
Apr 30 00:36:35.245: INFO: Trying to get logs from node ip-192-168-148-117.us-west-2.compute.internal pod downward-api-e3c22c72-af4a-45fc-ad66-e544c5b44785 container dapi-container: <nil>
STEP: delete the pod
Apr 30 00:36:35.261: INFO: Waiting for pod downward-api-e3c22c72-af4a-45fc-ad66-e544c5b44785 to disappear
Apr 30 00:36:35.263: INFO: Pod downward-api-e3c22c72-af4a-45fc-ad66-e544c5b44785 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:36:35.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9980" for this suite.
Apr 30 00:36:41.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:36:41.361: INFO: namespace downward-api-9980 deletion completed in 6.093625573s

• [SLOW TEST:8.274 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:36:41.361: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9612
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:36:41.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9612" for this suite.
Apr 30 00:36:47.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:36:47.612: INFO: namespace kubelet-test-9612 deletion completed in 6.08861804s

• [SLOW TEST:6.251 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:36:47.612: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5161
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5161
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Apr 30 00:36:47.767: INFO: Found 0 stateful pods, waiting for 3
Apr 30 00:36:57.770: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 30 00:36:57.770: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 30 00:36:57.770: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr 30 00:36:57.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-5161 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 30 00:36:57.997: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 30 00:36:57.997: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 30 00:36:57.997: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Apr 30 00:37:08.022: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr 30 00:37:18.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-5161 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 30 00:37:18.164: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 30 00:37:18.164: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 30 00:37:18.164: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 30 00:37:28.178: INFO: Waiting for StatefulSet statefulset-5161/ss2 to complete update
Apr 30 00:37:28.178: INFO: Waiting for Pod statefulset-5161/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 30 00:37:28.178: INFO: Waiting for Pod statefulset-5161/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 30 00:37:38.183: INFO: Waiting for StatefulSet statefulset-5161/ss2 to complete update
STEP: Rolling back to a previous revision
Apr 30 00:37:48.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-5161 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 30 00:37:48.328: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 30 00:37:48.328: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 30 00:37:48.328: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 30 00:37:58.353: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr 30 00:38:08.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=statefulset-5161 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 30 00:38:08.514: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 30 00:38:08.514: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 30 00:38:08.514: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 30 00:38:18.529: INFO: Waiting for StatefulSet statefulset-5161/ss2 to complete update
Apr 30 00:38:18.529: INFO: Waiting for Pod statefulset-5161/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Apr 30 00:38:18.529: INFO: Waiting for Pod statefulset-5161/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Apr 30 00:38:18.529: INFO: Waiting for Pod statefulset-5161/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Apr 30 00:38:28.534: INFO: Waiting for StatefulSet statefulset-5161/ss2 to complete update
Apr 30 00:38:28.534: INFO: Waiting for Pod statefulset-5161/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Apr 30 00:38:28.534: INFO: Waiting for Pod statefulset-5161/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Apr 30 00:38:38.534: INFO: Waiting for StatefulSet statefulset-5161/ss2 to complete update
Apr 30 00:38:38.534: INFO: Waiting for Pod statefulset-5161/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Apr 30 00:38:48.534: INFO: Deleting all statefulset in ns statefulset-5161
Apr 30 00:38:48.536: INFO: Scaling statefulset ss2 to 0
Apr 30 00:39:18.546: INFO: Waiting for statefulset status.replicas updated to 0
Apr 30 00:39:18.548: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:39:18.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5161" for this suite.
Apr 30 00:39:24.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:39:24.655: INFO: namespace statefulset-5161 deletion completed in 6.092810967s

• [SLOW TEST:157.044 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:39:24.656: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4496
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4496.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4496.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4496.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4496.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 30 00:39:36.831: INFO: DNS probes using dns-test-d25f72ce-5369-4557-9942-d9f753a45945 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4496.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4496.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4496.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4496.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 30 00:39:44.868: INFO: DNS probes using dns-test-917d904b-0298-4a5c-9505-9b1b2b39ca9d succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4496.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4496.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4496.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4496.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 30 00:39:52.918: INFO: DNS probes using dns-test-b7e5b3d3-a62a-4109-8ed3-f8c0370763d5 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:39:52.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4496" for this suite.
Apr 30 00:39:58.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:39:59.045: INFO: namespace dns-4496 deletion completed in 6.097012426s

• [SLOW TEST:34.389 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:39:59.045: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1491
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:40:23.217: INFO: Container started at 2020-04-30 00:40:00 +0000 UTC, pod became ready at 2020-04-30 00:40:22 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:40:23.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1491" for this suite.
Apr 30 00:40:35.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:40:35.311: INFO: namespace container-probe-1491 deletion completed in 12.089945718s

• [SLOW TEST:36.266 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:40:35.312: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 30 00:40:37.474: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:40:37.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4412" for this suite.
Apr 30 00:40:43.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:40:43.576: INFO: namespace container-runtime-4412 deletion completed in 6.088913923s

• [SLOW TEST:8.264 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:40:43.577: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6811
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-6811/configmap-test-5b85fb88-cd0d-4e81-9a26-7a3ecacc6917
STEP: Creating a pod to test consume configMaps
Apr 30 00:40:43.732: INFO: Waiting up to 5m0s for pod "pod-configmaps-ceefb5c3-4f2a-4821-a2f6-ce858fab769b" in namespace "configmap-6811" to be "success or failure"
Apr 30 00:40:43.734: INFO: Pod "pod-configmaps-ceefb5c3-4f2a-4821-a2f6-ce858fab769b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.895021ms
Apr 30 00:40:45.737: INFO: Pod "pod-configmaps-ceefb5c3-4f2a-4821-a2f6-ce858fab769b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004596412s
Apr 30 00:40:47.739: INFO: Pod "pod-configmaps-ceefb5c3-4f2a-4821-a2f6-ce858fab769b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007234812s
STEP: Saw pod success
Apr 30 00:40:47.739: INFO: Pod "pod-configmaps-ceefb5c3-4f2a-4821-a2f6-ce858fab769b" satisfied condition "success or failure"
Apr 30 00:40:47.741: INFO: Trying to get logs from node ip-192-168-72-199.us-west-2.compute.internal pod pod-configmaps-ceefb5c3-4f2a-4821-a2f6-ce858fab769b container env-test: <nil>
STEP: delete the pod
Apr 30 00:40:47.760: INFO: Waiting for pod pod-configmaps-ceefb5c3-4f2a-4821-a2f6-ce858fab769b to disappear
Apr 30 00:40:47.762: INFO: Pod pod-configmaps-ceefb5c3-4f2a-4821-a2f6-ce858fab769b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:40:47.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6811" for this suite.
Apr 30 00:40:53.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:40:53.856: INFO: namespace configmap-6811 deletion completed in 6.09040754s

• [SLOW TEST:10.279 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:40:53.857: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2294
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 30 00:40:54.007: INFO: Waiting up to 5m0s for pod "pod-401bf07e-2973-4c2b-ad9c-10f06ab5c3cb" in namespace "emptydir-2294" to be "success or failure"
Apr 30 00:40:54.008: INFO: Pod "pod-401bf07e-2973-4c2b-ad9c-10f06ab5c3cb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.867824ms
Apr 30 00:40:56.011: INFO: Pod "pod-401bf07e-2973-4c2b-ad9c-10f06ab5c3cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004659442s
STEP: Saw pod success
Apr 30 00:40:56.011: INFO: Pod "pod-401bf07e-2973-4c2b-ad9c-10f06ab5c3cb" satisfied condition "success or failure"
Apr 30 00:40:56.013: INFO: Trying to get logs from node ip-192-168-148-117.us-west-2.compute.internal pod pod-401bf07e-2973-4c2b-ad9c-10f06ab5c3cb container test-container: <nil>
STEP: delete the pod
Apr 30 00:40:56.034: INFO: Waiting for pod pod-401bf07e-2973-4c2b-ad9c-10f06ab5c3cb to disappear
Apr 30 00:40:56.035: INFO: Pod pod-401bf07e-2973-4c2b-ad9c-10f06ab5c3cb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:40:56.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2294" for this suite.
Apr 30 00:41:02.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:41:02.178: INFO: namespace emptydir-2294 deletion completed in 6.138312991s

• [SLOW TEST:8.321 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:41:02.178: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1316
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:41:02.361: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr 30 00:41:02.373: INFO: Number of nodes with available pods: 0
Apr 30 00:41:02.373: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:41:03.382: INFO: Number of nodes with available pods: 0
Apr 30 00:41:03.382: INFO: Node ip-192-168-144-27.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:41:04.381: INFO: Number of nodes with available pods: 9
Apr 30 00:41:04.381: INFO: Node ip-192-168-222-85.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:41:05.381: INFO: Number of nodes with available pods: 10
Apr 30 00:41:05.381: INFO: Number of running nodes: 10, number of available pods: 10
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr 30 00:41:05.410: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:05.410: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:05.410: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:05.410: INFO: Wrong image for pod: daemon-set-jq9q2. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:05.410: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:05.410: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:05.410: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:05.410: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:05.410: INFO: Wrong image for pod: daemon-set-wbz8l. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:05.410: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:06.419: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:06.419: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:06.419: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:06.419: INFO: Wrong image for pod: daemon-set-jq9q2. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:06.419: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:06.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:06.419: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:06.419: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:06.419: INFO: Wrong image for pod: daemon-set-wbz8l. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:06.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:07.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:07.418: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:07.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:07.418: INFO: Wrong image for pod: daemon-set-jq9q2. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:07.418: INFO: Pod daemon-set-jq9q2 is not available
Apr 30 00:41:07.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:07.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:07.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:07.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:07.418: INFO: Wrong image for pod: daemon-set-wbz8l. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:07.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:08.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:08.418: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:08.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:08.418: INFO: Wrong image for pod: daemon-set-jq9q2. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:08.418: INFO: Pod daemon-set-jq9q2 is not available
Apr 30 00:41:08.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:08.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:08.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:08.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:08.418: INFO: Wrong image for pod: daemon-set-wbz8l. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:08.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:09.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:09.418: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:09.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:09.418: INFO: Wrong image for pod: daemon-set-jq9q2. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:09.418: INFO: Pod daemon-set-jq9q2 is not available
Apr 30 00:41:09.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:09.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:09.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:09.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:09.418: INFO: Wrong image for pod: daemon-set-wbz8l. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:09.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:10.455: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:10.455: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:10.455: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:10.455: INFO: Wrong image for pod: daemon-set-jq9q2. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:10.455: INFO: Pod daemon-set-jq9q2 is not available
Apr 30 00:41:10.455: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:10.455: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:10.455: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:10.455: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:10.455: INFO: Wrong image for pod: daemon-set-wbz8l. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:10.455: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:11.418: INFO: Pod daemon-set-4phsp is not available
Apr 30 00:41:11.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:11.418: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:11.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:11.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:11.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:11.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:11.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:11.418: INFO: Wrong image for pod: daemon-set-wbz8l. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:11.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:12.418: INFO: Pod daemon-set-4phsp is not available
Apr 30 00:41:12.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:12.418: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:12.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:12.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:12.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:12.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:12.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:12.418: INFO: Wrong image for pod: daemon-set-wbz8l. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:12.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:13.418: INFO: Pod daemon-set-4phsp is not available
Apr 30 00:41:13.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:13.418: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:13.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:13.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:13.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:13.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:13.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:13.418: INFO: Wrong image for pod: daemon-set-wbz8l. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:13.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:14.418: INFO: Pod daemon-set-4phsp is not available
Apr 30 00:41:14.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:14.418: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:14.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:14.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:14.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:14.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:14.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:14.418: INFO: Wrong image for pod: daemon-set-wbz8l. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:14.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:15.418: INFO: Pod daemon-set-4phsp is not available
Apr 30 00:41:15.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:15.418: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:15.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:15.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:15.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:15.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:15.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:15.418: INFO: Wrong image for pod: daemon-set-wbz8l. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:15.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:16.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:16.418: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:16.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:16.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:16.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:16.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:16.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:16.418: INFO: Wrong image for pod: daemon-set-wbz8l. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:16.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:17.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:17.418: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:17.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:17.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:17.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:17.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:17.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:17.418: INFO: Wrong image for pod: daemon-set-wbz8l. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:17.418: INFO: Pod daemon-set-wbz8l is not available
Apr 30 00:41:17.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:18.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:18.418: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:18.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:18.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:18.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:18.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:18.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:18.418: INFO: Wrong image for pod: daemon-set-wbz8l. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:18.418: INFO: Pod daemon-set-wbz8l is not available
Apr 30 00:41:18.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:19.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:19.418: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:19.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:19.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:19.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:19.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:19.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:19.418: INFO: Wrong image for pod: daemon-set-wbz8l. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:19.418: INFO: Pod daemon-set-wbz8l is not available
Apr 30 00:41:19.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:20.419: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:20.419: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:20.419: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:20.419: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:20.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:20.419: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:20.419: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:20.419: INFO: Wrong image for pod: daemon-set-wbz8l. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:20.419: INFO: Pod daemon-set-wbz8l is not available
Apr 30 00:41:20.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:21.418: INFO: Pod daemon-set-4jvfx is not available
Apr 30 00:41:21.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:21.418: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:21.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:21.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:21.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:21.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:21.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:21.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:22.419: INFO: Pod daemon-set-4jvfx is not available
Apr 30 00:41:22.419: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:22.419: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:22.419: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:22.419: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:22.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:22.419: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:22.419: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:22.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:23.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:23.418: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:23.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:23.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:23.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:23.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:23.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:23.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:24.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:24.418: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:24.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:24.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:24.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:24.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:24.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:24.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:25.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:25.418: INFO: Wrong image for pod: daemon-set-8k5cf. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:25.418: INFO: Pod daemon-set-8k5cf is not available
Apr 30 00:41:25.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:25.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:25.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:25.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:25.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:25.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:26.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:26.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:26.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:26.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:26.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:26.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:26.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:26.418: INFO: Pod daemon-set-z8vhf is not available
Apr 30 00:41:27.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:27.419: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:27.419: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:27.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:27.419: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:27.419: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:27.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:27.419: INFO: Pod daemon-set-z8vhf is not available
Apr 30 00:41:28.419: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:28.419: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:28.419: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:28.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:28.419: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:28.419: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:28.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:28.419: INFO: Pod daemon-set-z8vhf is not available
Apr 30 00:41:29.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:29.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:29.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:29.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:29.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:29.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:29.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:29.418: INFO: Pod daemon-set-z8vhf is not available
Apr 30 00:41:30.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:30.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:30.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:30.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:30.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:30.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:30.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:30.418: INFO: Pod daemon-set-z8vhf is not available
Apr 30 00:41:31.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:31.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:31.419: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:31.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:31.419: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:31.419: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:31.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:32.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:32.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:32.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:32.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:32.418: INFO: Wrong image for pod: daemon-set-nccgx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:32.418: INFO: Pod daemon-set-nccgx is not available
Apr 30 00:41:32.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:32.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:33.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:33.418: INFO: Pod daemon-set-9fgwv is not available
Apr 30 00:41:33.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:33.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:33.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:33.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:33.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:34.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:34.418: INFO: Pod daemon-set-9fgwv is not available
Apr 30 00:41:34.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:34.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:34.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:34.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:34.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:35.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:35.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:35.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:35.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:35.418: INFO: Wrong image for pod: daemon-set-v6q8h. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:35.418: INFO: Pod daemon-set-v6q8h is not available
Apr 30 00:41:35.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:36.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:36.418: INFO: Pod daemon-set-dtw6d is not available
Apr 30 00:41:36.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:36.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:36.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:36.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:37.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:37.418: INFO: Pod daemon-set-dtw6d is not available
Apr 30 00:41:37.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:37.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:37.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:37.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:38.419: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:38.419: INFO: Pod daemon-set-dtw6d is not available
Apr 30 00:41:38.419: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:38.419: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:38.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:38.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:39.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:39.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:39.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:39.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:39.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:40.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:40.418: INFO: Wrong image for pod: daemon-set-fzg7r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:40.418: INFO: Pod daemon-set-fzg7r is not available
Apr 30 00:41:40.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:40.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:40.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:41.419: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:41.419: INFO: Pod daemon-set-6lzvc is not available
Apr 30 00:41:41.419: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:41.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:41.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:42.419: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:42.419: INFO: Pod daemon-set-6lzvc is not available
Apr 30 00:41:42.419: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:42.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:42.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:43.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:43.418: INFO: Pod daemon-set-6lzvc is not available
Apr 30 00:41:43.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:43.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:43.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:44.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:44.418: INFO: Pod daemon-set-6lzvc is not available
Apr 30 00:41:44.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:44.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:44.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:45.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:45.419: INFO: Pod daemon-set-6lzvc is not available
Apr 30 00:41:45.419: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:45.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:45.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:46.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:46.418: INFO: Pod daemon-set-6lzvc is not available
Apr 30 00:41:46.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:46.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:46.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:47.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:47.418: INFO: Pod daemon-set-6lzvc is not available
Apr 30 00:41:47.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:47.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:47.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:48.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:48.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:48.418: INFO: Pod daemon-set-l9gx9 is not available
Apr 30 00:41:48.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:48.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:49.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:49.418: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:49.418: INFO: Pod daemon-set-l9gx9 is not available
Apr 30 00:41:49.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:49.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:50.419: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:50.419: INFO: Wrong image for pod: daemon-set-l9gx9. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:50.419: INFO: Pod daemon-set-l9gx9 is not available
Apr 30 00:41:50.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:50.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:51.419: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:51.419: INFO: Pod daemon-set-hx526 is not available
Apr 30 00:41:51.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:51.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:52.419: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:52.419: INFO: Pod daemon-set-hx526 is not available
Apr 30 00:41:52.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:52.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:53.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:53.419: INFO: Pod daemon-set-hx526 is not available
Apr 30 00:41:53.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:53.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:54.419: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:54.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:54.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:55.418: INFO: Wrong image for pod: daemon-set-5mx2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:55.418: INFO: Pod daemon-set-5mx2s is not available
Apr 30 00:41:55.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:55.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:56.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:56.419: INFO: Pod daemon-set-mvrh7 is not available
Apr 30 00:41:56.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:57.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:57.418: INFO: Pod daemon-set-mvrh7 is not available
Apr 30 00:41:57.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:58.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:58.418: INFO: Pod daemon-set-mvrh7 is not available
Apr 30 00:41:58.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:59.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:41:59.418: INFO: Pod daemon-set-mvrh7 is not available
Apr 30 00:41:59.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:42:00.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:42:00.419: INFO: Pod daemon-set-mvrh7 is not available
Apr 30 00:42:00.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:42:01.419: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:42:01.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:42:02.418: INFO: Wrong image for pod: daemon-set-lhfqb. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:42:02.418: INFO: Pod daemon-set-lhfqb is not available
Apr 30 00:42:02.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:42:03.419: INFO: Pod daemon-set-sg6d4 is not available
Apr 30 00:42:03.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:42:04.418: INFO: Pod daemon-set-sg6d4 is not available
Apr 30 00:42:04.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:42:05.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:42:06.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:42:06.419: INFO: Pod daemon-set-xs4fx is not available
Apr 30 00:42:07.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:42:07.418: INFO: Pod daemon-set-xs4fx is not available
Apr 30 00:42:08.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:42:08.418: INFO: Pod daemon-set-xs4fx is not available
Apr 30 00:42:09.418: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:42:09.418: INFO: Pod daemon-set-xs4fx is not available
Apr 30 00:42:10.419: INFO: Wrong image for pod: daemon-set-xs4fx. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 30 00:42:10.419: INFO: Pod daemon-set-xs4fx is not available
Apr 30 00:42:11.418: INFO: Pod daemon-set-xrt8d is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Apr 30 00:42:11.429: INFO: Number of nodes with available pods: 9
Apr 30 00:42:11.429: INFO: Node ip-192-168-222-85.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:42:12.471: INFO: Number of nodes with available pods: 9
Apr 30 00:42:12.471: INFO: Node ip-192-168-222-85.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:42:13.437: INFO: Number of nodes with available pods: 9
Apr 30 00:42:13.437: INFO: Node ip-192-168-222-85.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:42:14.436: INFO: Number of nodes with available pods: 9
Apr 30 00:42:14.436: INFO: Node ip-192-168-222-85.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:42:15.436: INFO: Number of nodes with available pods: 9
Apr 30 00:42:15.436: INFO: Node ip-192-168-222-85.us-west-2.compute.internal is running more than one daemon pod
Apr 30 00:42:16.437: INFO: Number of nodes with available pods: 10
Apr 30 00:42:16.437: INFO: Number of running nodes: 10, number of available pods: 10
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1316, will wait for the garbage collector to delete the pods
Apr 30 00:42:16.544: INFO: Deleting DaemonSet.extensions daemon-set took: 11.433077ms
Apr 30 00:42:16.845: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.142998ms
Apr 30 00:42:31.347: INFO: Number of nodes with available pods: 0
Apr 30 00:42:31.347: INFO: Number of running nodes: 0, number of available pods: 0
Apr 30 00:42:31.351: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1316/daemonsets","resourceVersion":"33201"},"items":null}

Apr 30 00:42:31.352: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1316/pods","resourceVersion":"33201"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:42:31.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1316" for this suite.
Apr 30 00:42:39.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:42:39.477: INFO: namespace daemonsets-1316 deletion completed in 8.093085842s

• [SLOW TEST:97.299 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:42:39.477: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:43:06.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4484" for this suite.
Apr 30 00:43:12.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:43:12.880: INFO: namespace container-runtime-4484 deletion completed in 6.092054226s

• [SLOW TEST:33.402 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:43:12.880: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4217
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Apr 30 00:43:15.559: INFO: Successfully updated pod "annotationupdate0a754b38-4ecb-456d-b82e-83d2400e733a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:43:17.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4217" for this suite.
Apr 30 00:43:45.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:43:45.669: INFO: namespace projected-4217 deletion completed in 28.0929036s

• [SLOW TEST:32.790 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:43:45.670: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-9650
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr 30 00:43:51.838: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9650 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 30 00:43:51.838: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 30 00:43:51.952: INFO: Exec stderr: ""
Apr 30 00:43:51.953: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9650 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 30 00:43:51.953: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 30 00:43:52.045: INFO: Exec stderr: ""
Apr 30 00:43:52.045: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9650 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 30 00:43:52.045: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 30 00:43:52.130: INFO: Exec stderr: ""
Apr 30 00:43:52.130: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9650 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 30 00:43:52.130: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 30 00:43:52.218: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr 30 00:43:52.218: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9650 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 30 00:43:52.218: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 30 00:43:52.302: INFO: Exec stderr: ""
Apr 30 00:43:52.302: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9650 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 30 00:43:52.302: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 30 00:43:52.385: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr 30 00:43:52.385: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9650 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 30 00:43:52.385: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 30 00:43:52.485: INFO: Exec stderr: ""
Apr 30 00:43:52.485: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9650 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 30 00:43:52.485: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 30 00:43:52.573: INFO: Exec stderr: ""
Apr 30 00:43:52.573: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9650 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 30 00:43:52.573: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 30 00:43:52.661: INFO: Exec stderr: ""
Apr 30 00:43:52.661: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9650 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 30 00:43:52.661: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
Apr 30 00:43:52.753: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:43:52.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9650" for this suite.
Apr 30 00:44:36.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:44:36.851: INFO: namespace e2e-kubelet-etc-hosts-9650 deletion completed in 44.093284084s

• [SLOW TEST:51.181 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:44:36.851: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7569
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 30 00:44:37.319: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 30 00:44:39.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723804277, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723804277, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723804277, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723804277, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 30 00:44:42.345: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:44:42.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7569" for this suite.
Apr 30 00:44:48.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:44:48.499: INFO: namespace webhook-7569 deletion completed in 6.089508746s
STEP: Destroying namespace "webhook-7569-markers" for this suite.
Apr 30 00:44:54.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:44:54.590: INFO: namespace webhook-7569-markers deletion completed in 6.090664426s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.758 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:44:54.609: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5043
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Apr 30 00:44:54.767: INFO: Waiting up to 5m0s for pod "client-containers-a01d59ec-4d17-493f-a608-ccaec0c7e7b0" in namespace "containers-5043" to be "success or failure"
Apr 30 00:44:54.769: INFO: Pod "client-containers-a01d59ec-4d17-493f-a608-ccaec0c7e7b0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.859022ms
Apr 30 00:44:56.772: INFO: Pod "client-containers-a01d59ec-4d17-493f-a608-ccaec0c7e7b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004720079s
STEP: Saw pod success
Apr 30 00:44:56.772: INFO: Pod "client-containers-a01d59ec-4d17-493f-a608-ccaec0c7e7b0" satisfied condition "success or failure"
Apr 30 00:44:56.774: INFO: Trying to get logs from node ip-192-168-149-254.us-west-2.compute.internal pod client-containers-a01d59ec-4d17-493f-a608-ccaec0c7e7b0 container test-container: <nil>
STEP: delete the pod
Apr 30 00:44:56.793: INFO: Waiting for pod client-containers-a01d59ec-4d17-493f-a608-ccaec0c7e7b0 to disappear
Apr 30 00:44:56.795: INFO: Pod client-containers-a01d59ec-4d17-493f-a608-ccaec0c7e7b0 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:44:56.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5043" for this suite.
Apr 30 00:45:02.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:45:02.900: INFO: namespace containers-5043 deletion completed in 6.101425594s

• [SLOW TEST:8.291 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:45:02.901: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 30 00:45:05.071: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:45:05.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2169" for this suite.
Apr 30 00:45:11.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:45:11.179: INFO: namespace container-runtime-2169 deletion completed in 6.093205948s

• [SLOW TEST:8.279 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:45:11.179: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5630
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-4337
STEP: Creating secret with name secret-test-46338522-3513-4737-814d-44d0ddf658e4
STEP: Creating a pod to test consume secrets
Apr 30 00:45:11.513: INFO: Waiting up to 5m0s for pod "pod-secrets-06647015-03c5-4c2d-a0d6-c218d12a4666" in namespace "secrets-5630" to be "success or failure"
Apr 30 00:45:11.515: INFO: Pod "pod-secrets-06647015-03c5-4c2d-a0d6-c218d12a4666": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027679ms
Apr 30 00:45:13.518: INFO: Pod "pod-secrets-06647015-03c5-4c2d-a0d6-c218d12a4666": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004584495s
STEP: Saw pod success
Apr 30 00:45:13.518: INFO: Pod "pod-secrets-06647015-03c5-4c2d-a0d6-c218d12a4666" satisfied condition "success or failure"
Apr 30 00:45:13.520: INFO: Trying to get logs from node ip-192-168-72-199.us-west-2.compute.internal pod pod-secrets-06647015-03c5-4c2d-a0d6-c218d12a4666 container secret-volume-test: <nil>
STEP: delete the pod
Apr 30 00:45:13.538: INFO: Waiting for pod pod-secrets-06647015-03c5-4c2d-a0d6-c218d12a4666 to disappear
Apr 30 00:45:13.540: INFO: Pod pod-secrets-06647015-03c5-4c2d-a0d6-c218d12a4666 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:45:13.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5630" for this suite.
Apr 30 00:45:19.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:45:19.636: INFO: namespace secrets-5630 deletion completed in 6.092963858s
STEP: Destroying namespace "secret-namespace-4337" for this suite.
Apr 30 00:45:25.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:45:25.739: INFO: namespace secret-namespace-4337 deletion completed in 6.102693344s

• [SLOW TEST:14.560 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:45:25.739: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3393
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-3393
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3393 to expose endpoints map[]
Apr 30 00:45:25.901: INFO: Get endpoints failed (3.414779ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Apr 30 00:45:26.905: INFO: successfully validated that service multi-endpoint-test in namespace services-3393 exposes endpoints map[] (1.007480702s elapsed)
STEP: Creating pod pod1 in namespace services-3393
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3393 to expose endpoints map[pod1:[100]]
Apr 30 00:45:28.932: INFO: successfully validated that service multi-endpoint-test in namespace services-3393 exposes endpoints map[pod1:[100]] (2.019342908s elapsed)
STEP: Creating pod pod2 in namespace services-3393
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3393 to expose endpoints map[pod1:[100] pod2:[101]]
Apr 30 00:45:31.969: INFO: successfully validated that service multi-endpoint-test in namespace services-3393 exposes endpoints map[pod1:[100] pod2:[101]] (3.032817066s elapsed)
STEP: Deleting pod pod1 in namespace services-3393
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3393 to expose endpoints map[pod2:[101]]
Apr 30 00:45:32.985: INFO: successfully validated that service multi-endpoint-test in namespace services-3393 exposes endpoints map[pod2:[101]] (1.011811262s elapsed)
STEP: Deleting pod pod2 in namespace services-3393
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3393 to expose endpoints map[]
Apr 30 00:45:34.000: INFO: successfully validated that service multi-endpoint-test in namespace services-3393 exposes endpoints map[] (1.008751069s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:45:34.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3393" for this suite.
Apr 30 00:45:46.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:45:46.125: INFO: namespace services-3393 deletion completed in 12.091339278s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:20.385 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:45:46.125: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-3497
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:45:46.272: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Creating first CR 
Apr 30 00:45:46.820: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-30T00:45:46Z generation:1 name:name1 resourceVersion:34220 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:26e37aa6-490e-4ba6-8ef2-476fd3312fa3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Apr 30 00:45:56.824: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-30T00:45:56Z generation:1 name:name2 resourceVersion:34248 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ce8ade90-6c2e-4f4d-8dd6-e0945390b090] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Apr 30 00:46:06.829: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-30T00:45:46Z generation:2 name:name1 resourceVersion:34273 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:26e37aa6-490e-4ba6-8ef2-476fd3312fa3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Apr 30 00:46:16.833: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-30T00:45:56Z generation:2 name:name2 resourceVersion:34298 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ce8ade90-6c2e-4f4d-8dd6-e0945390b090] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Apr 30 00:46:26.841: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-30T00:45:46Z generation:2 name:name1 resourceVersion:34328 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:26e37aa6-490e-4ba6-8ef2-476fd3312fa3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Apr 30 00:46:36.849: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-30T00:45:56Z generation:2 name:name2 resourceVersion:34355 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ce8ade90-6c2e-4f4d-8dd6-e0945390b090] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:46:47.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-3497" for this suite.
Apr 30 00:46:53.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:46:53.462: INFO: namespace crd-watch-3497 deletion completed in 6.096988152s

• [SLOW TEST:67.338 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:46:53.463: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1030
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 30 00:46:54.090: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 30 00:46:56.100: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723804414, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723804414, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723804414, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723804414, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 30 00:46:59.119: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:46:59.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1030" for this suite.
Apr 30 00:47:05.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:47:05.382: INFO: namespace webhook-1030 deletion completed in 6.099107317s
STEP: Destroying namespace "webhook-1030-markers" for this suite.
Apr 30 00:47:11.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:47:11.477: INFO: namespace webhook-1030-markers deletion completed in 6.094487166s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.034 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:47:11.497: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-5099
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5099
I0430 00:47:11.648339      21 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5099, replica count: 1
I0430 00:47:12.698633      21 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0430 00:47:13.698765      21 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 30 00:47:13.810: INFO: Created: latency-svc-kxnfc
Apr 30 00:47:13.815: INFO: Got endpoints: latency-svc-kxnfc [16.672263ms]
Apr 30 00:47:13.827: INFO: Created: latency-svc-vdk4j
Apr 30 00:47:13.829: INFO: Created: latency-svc-8lpgp
Apr 30 00:47:13.833: INFO: Got endpoints: latency-svc-vdk4j [17.444865ms]
Apr 30 00:47:13.833: INFO: Got endpoints: latency-svc-8lpgp [17.997953ms]
Apr 30 00:47:13.837: INFO: Created: latency-svc-4hxwm
Apr 30 00:47:13.840: INFO: Created: latency-svc-l82dr
Apr 30 00:47:13.842: INFO: Got endpoints: latency-svc-4hxwm [26.78306ms]
Apr 30 00:47:13.844: INFO: Got endpoints: latency-svc-l82dr [28.506536ms]
Apr 30 00:47:13.846: INFO: Created: latency-svc-vt55l
Apr 30 00:47:13.850: INFO: Got endpoints: latency-svc-vt55l [34.305509ms]
Apr 30 00:47:13.851: INFO: Created: latency-svc-kf5pv
Apr 30 00:47:13.854: INFO: Got endpoints: latency-svc-kf5pv [38.930782ms]
Apr 30 00:47:13.857: INFO: Created: latency-svc-lvl5k
Apr 30 00:47:13.860: INFO: Got endpoints: latency-svc-lvl5k [45.082511ms]
Apr 30 00:47:13.861: INFO: Created: latency-svc-rsrnx
Apr 30 00:47:13.865: INFO: Got endpoints: latency-svc-rsrnx [49.437297ms]
Apr 30 00:47:13.868: INFO: Created: latency-svc-jwlhz
Apr 30 00:47:13.872: INFO: Created: latency-svc-8tw8g
Apr 30 00:47:13.874: INFO: Got endpoints: latency-svc-jwlhz [58.937866ms]
Apr 30 00:47:13.877: INFO: Created: latency-svc-bbnnd
Apr 30 00:47:13.877: INFO: Got endpoints: latency-svc-8tw8g [61.708656ms]
Apr 30 00:47:13.882: INFO: Got endpoints: latency-svc-bbnnd [66.081338ms]
Apr 30 00:47:13.883: INFO: Created: latency-svc-t7m2x
Apr 30 00:47:13.887: INFO: Got endpoints: latency-svc-t7m2x [71.617155ms]
Apr 30 00:47:13.888: INFO: Created: latency-svc-wlc2v
Apr 30 00:47:13.893: INFO: Got endpoints: latency-svc-wlc2v [77.146595ms]
Apr 30 00:47:13.894: INFO: Created: latency-svc-c7zc9
Apr 30 00:47:13.898: INFO: Got endpoints: latency-svc-c7zc9 [82.412728ms]
Apr 30 00:47:13.899: INFO: Created: latency-svc-8d5s7
Apr 30 00:47:13.904: INFO: Got endpoints: latency-svc-8d5s7 [88.63436ms]
Apr 30 00:47:13.904: INFO: Created: latency-svc-l5hp8
Apr 30 00:47:13.909: INFO: Got endpoints: latency-svc-l5hp8 [76.471915ms]
Apr 30 00:47:13.911: INFO: Created: latency-svc-nhkk7
Apr 30 00:47:13.914: INFO: Got endpoints: latency-svc-nhkk7 [80.782474ms]
Apr 30 00:47:13.918: INFO: Created: latency-svc-xpdw2
Apr 30 00:47:13.923: INFO: Got endpoints: latency-svc-xpdw2 [81.373447ms]
Apr 30 00:47:13.924: INFO: Created: latency-svc-mj58n
Apr 30 00:47:13.929: INFO: Got endpoints: latency-svc-mj58n [84.744885ms]
Apr 30 00:47:13.930: INFO: Created: latency-svc-g9nld
Apr 30 00:47:13.935: INFO: Created: latency-svc-8g475
Apr 30 00:47:13.938: INFO: Got endpoints: latency-svc-g9nld [88.520884ms]
Apr 30 00:47:13.939: INFO: Got endpoints: latency-svc-8g475 [85.015152ms]
Apr 30 00:47:13.942: INFO: Created: latency-svc-zgmz6
Apr 30 00:47:13.947: INFO: Created: latency-svc-q8jcq
Apr 30 00:47:13.949: INFO: Got endpoints: latency-svc-zgmz6 [88.030962ms]
Apr 30 00:47:13.952: INFO: Got endpoints: latency-svc-q8jcq [86.94137ms]
Apr 30 00:47:13.953: INFO: Created: latency-svc-2j4z4
Apr 30 00:47:13.957: INFO: Got endpoints: latency-svc-2j4z4 [82.140822ms]
Apr 30 00:47:13.958: INFO: Created: latency-svc-2n2r5
Apr 30 00:47:13.962: INFO: Got endpoints: latency-svc-2n2r5 [84.715639ms]
Apr 30 00:47:13.963: INFO: Created: latency-svc-prm9v
Apr 30 00:47:13.967: INFO: Got endpoints: latency-svc-prm9v [85.605041ms]
Apr 30 00:47:13.968: INFO: Created: latency-svc-jbrp5
Apr 30 00:47:13.972: INFO: Got endpoints: latency-svc-jbrp5 [85.366841ms]
Apr 30 00:47:13.974: INFO: Created: latency-svc-58q22
Apr 30 00:47:13.979: INFO: Got endpoints: latency-svc-58q22 [85.840914ms]
Apr 30 00:47:13.979: INFO: Created: latency-svc-jv6t9
Apr 30 00:47:13.984: INFO: Got endpoints: latency-svc-jv6t9 [85.615344ms]
Apr 30 00:47:13.985: INFO: Created: latency-svc-dp4pw
Apr 30 00:47:13.994: INFO: Got endpoints: latency-svc-dp4pw [89.914329ms]
Apr 30 00:47:13.995: INFO: Created: latency-svc-nmsz9
Apr 30 00:47:14.003: INFO: Created: latency-svc-7g8px
Apr 30 00:47:14.003: INFO: Got endpoints: latency-svc-nmsz9 [94.115593ms]
Apr 30 00:47:14.006: INFO: Got endpoints: latency-svc-7g8px [91.248143ms]
Apr 30 00:47:14.007: INFO: Created: latency-svc-t854g
Apr 30 00:47:14.013: INFO: Created: latency-svc-5cghm
Apr 30 00:47:14.014: INFO: Got endpoints: latency-svc-t854g [90.602833ms]
Apr 30 00:47:14.021: INFO: Created: latency-svc-45s5d
Apr 30 00:47:14.026: INFO: Created: latency-svc-c6xmw
Apr 30 00:47:14.029: INFO: Created: latency-svc-4cc47
Apr 30 00:47:14.036: INFO: Created: latency-svc-7jmq7
Apr 30 00:47:14.040: INFO: Created: latency-svc-9sr8z
Apr 30 00:47:14.045: INFO: Created: latency-svc-wv952
Apr 30 00:47:14.050: INFO: Created: latency-svc-t977m
Apr 30 00:47:14.057: INFO: Created: latency-svc-scbh4
Apr 30 00:47:14.061: INFO: Created: latency-svc-wt92h
Apr 30 00:47:14.066: INFO: Got endpoints: latency-svc-5cghm [137.022887ms]
Apr 30 00:47:14.067: INFO: Created: latency-svc-mjxxr
Apr 30 00:47:14.073: INFO: Created: latency-svc-2sc7p
Apr 30 00:47:14.078: INFO: Created: latency-svc-g88vp
Apr 30 00:47:14.083: INFO: Created: latency-svc-4ks2h
Apr 30 00:47:14.089: INFO: Created: latency-svc-sdl44
Apr 30 00:47:14.094: INFO: Created: latency-svc-l2t4l
Apr 30 00:47:14.115: INFO: Got endpoints: latency-svc-45s5d [176.287613ms]
Apr 30 00:47:14.126: INFO: Created: latency-svc-p86sr
Apr 30 00:47:14.165: INFO: Got endpoints: latency-svc-c6xmw [225.47368ms]
Apr 30 00:47:14.175: INFO: Created: latency-svc-kbnwz
Apr 30 00:47:14.215: INFO: Got endpoints: latency-svc-4cc47 [266.692274ms]
Apr 30 00:47:14.237: INFO: Created: latency-svc-65nzh
Apr 30 00:47:14.265: INFO: Got endpoints: latency-svc-7jmq7 [312.787358ms]
Apr 30 00:47:14.275: INFO: Created: latency-svc-wqxmp
Apr 30 00:47:14.315: INFO: Got endpoints: latency-svc-9sr8z [358.230565ms]
Apr 30 00:47:14.326: INFO: Created: latency-svc-bvtfv
Apr 30 00:47:14.365: INFO: Got endpoints: latency-svc-wv952 [402.855701ms]
Apr 30 00:47:14.376: INFO: Created: latency-svc-mwxvw
Apr 30 00:47:14.415: INFO: Got endpoints: latency-svc-t977m [447.435661ms]
Apr 30 00:47:14.425: INFO: Created: latency-svc-bv9zn
Apr 30 00:47:14.465: INFO: Got endpoints: latency-svc-scbh4 [492.226841ms]
Apr 30 00:47:14.475: INFO: Created: latency-svc-xtrcr
Apr 30 00:47:14.515: INFO: Got endpoints: latency-svc-wt92h [536.254032ms]
Apr 30 00:47:14.525: INFO: Created: latency-svc-pqdkr
Apr 30 00:47:14.566: INFO: Got endpoints: latency-svc-mjxxr [581.989622ms]
Apr 30 00:47:14.575: INFO: Created: latency-svc-9xrw7
Apr 30 00:47:14.615: INFO: Got endpoints: latency-svc-2sc7p [620.651738ms]
Apr 30 00:47:14.624: INFO: Created: latency-svc-6qfnr
Apr 30 00:47:14.665: INFO: Got endpoints: latency-svc-g88vp [661.210682ms]
Apr 30 00:47:14.675: INFO: Created: latency-svc-sw8l5
Apr 30 00:47:14.716: INFO: Got endpoints: latency-svc-4ks2h [709.958387ms]
Apr 30 00:47:14.725: INFO: Created: latency-svc-szd2h
Apr 30 00:47:14.765: INFO: Got endpoints: latency-svc-sdl44 [750.519903ms]
Apr 30 00:47:14.774: INFO: Created: latency-svc-k9dd4
Apr 30 00:47:14.815: INFO: Got endpoints: latency-svc-l2t4l [748.680971ms]
Apr 30 00:47:14.825: INFO: Created: latency-svc-j7k8s
Apr 30 00:47:14.866: INFO: Got endpoints: latency-svc-p86sr [751.159486ms]
Apr 30 00:47:14.876: INFO: Created: latency-svc-ndkjq
Apr 30 00:47:14.915: INFO: Got endpoints: latency-svc-kbnwz [750.018302ms]
Apr 30 00:47:14.925: INFO: Created: latency-svc-2464v
Apr 30 00:47:14.965: INFO: Got endpoints: latency-svc-65nzh [749.42626ms]
Apr 30 00:47:14.975: INFO: Created: latency-svc-mf2h2
Apr 30 00:47:15.016: INFO: Got endpoints: latency-svc-wqxmp [750.847463ms]
Apr 30 00:47:15.026: INFO: Created: latency-svc-w6bvz
Apr 30 00:47:15.065: INFO: Got endpoints: latency-svc-bvtfv [749.914679ms]
Apr 30 00:47:15.074: INFO: Created: latency-svc-l7j9c
Apr 30 00:47:15.115: INFO: Got endpoints: latency-svc-mwxvw [749.810489ms]
Apr 30 00:47:15.128: INFO: Created: latency-svc-z55gc
Apr 30 00:47:15.165: INFO: Got endpoints: latency-svc-bv9zn [750.712967ms]
Apr 30 00:47:15.175: INFO: Created: latency-svc-zmn4h
Apr 30 00:47:15.215: INFO: Got endpoints: latency-svc-xtrcr [749.991242ms]
Apr 30 00:47:15.224: INFO: Created: latency-svc-fcqv4
Apr 30 00:47:15.265: INFO: Got endpoints: latency-svc-pqdkr [749.795974ms]
Apr 30 00:47:15.276: INFO: Created: latency-svc-l7ns9
Apr 30 00:47:15.315: INFO: Got endpoints: latency-svc-9xrw7 [749.668017ms]
Apr 30 00:47:15.325: INFO: Created: latency-svc-7bz5p
Apr 30 00:47:15.365: INFO: Got endpoints: latency-svc-6qfnr [749.970557ms]
Apr 30 00:47:15.374: INFO: Created: latency-svc-fw5lv
Apr 30 00:47:15.416: INFO: Got endpoints: latency-svc-sw8l5 [751.096252ms]
Apr 30 00:47:15.425: INFO: Created: latency-svc-mfmzg
Apr 30 00:47:15.465: INFO: Got endpoints: latency-svc-szd2h [749.333564ms]
Apr 30 00:47:15.475: INFO: Created: latency-svc-7vmv4
Apr 30 00:47:15.515: INFO: Got endpoints: latency-svc-k9dd4 [750.594177ms]
Apr 30 00:47:15.527: INFO: Created: latency-svc-v2tsc
Apr 30 00:47:15.566: INFO: Got endpoints: latency-svc-j7k8s [751.50373ms]
Apr 30 00:47:15.576: INFO: Created: latency-svc-7zr9n
Apr 30 00:47:15.615: INFO: Got endpoints: latency-svc-ndkjq [748.920788ms]
Apr 30 00:47:15.624: INFO: Created: latency-svc-nqbzt
Apr 30 00:47:15.665: INFO: Got endpoints: latency-svc-2464v [749.72769ms]
Apr 30 00:47:15.674: INFO: Created: latency-svc-vlqcd
Apr 30 00:47:15.716: INFO: Got endpoints: latency-svc-mf2h2 [750.874707ms]
Apr 30 00:47:15.725: INFO: Created: latency-svc-qclv5
Apr 30 00:47:15.765: INFO: Got endpoints: latency-svc-w6bvz [748.836043ms]
Apr 30 00:47:15.774: INFO: Created: latency-svc-4pb7q
Apr 30 00:47:15.815: INFO: Got endpoints: latency-svc-l7j9c [749.776645ms]
Apr 30 00:47:15.824: INFO: Created: latency-svc-mxhxk
Apr 30 00:47:15.866: INFO: Got endpoints: latency-svc-z55gc [750.930349ms]
Apr 30 00:47:15.875: INFO: Created: latency-svc-7mqlm
Apr 30 00:47:15.915: INFO: Got endpoints: latency-svc-zmn4h [749.356513ms]
Apr 30 00:47:15.924: INFO: Created: latency-svc-fkgp6
Apr 30 00:47:15.965: INFO: Got endpoints: latency-svc-fcqv4 [750.141444ms]
Apr 30 00:47:15.974: INFO: Created: latency-svc-qqdcs
Apr 30 00:47:16.015: INFO: Got endpoints: latency-svc-l7ns9 [750.080766ms]
Apr 30 00:47:16.025: INFO: Created: latency-svc-657lm
Apr 30 00:47:16.065: INFO: Got endpoints: latency-svc-7bz5p [749.371388ms]
Apr 30 00:47:16.074: INFO: Created: latency-svc-lbrq5
Apr 30 00:47:16.115: INFO: Got endpoints: latency-svc-fw5lv [750.080021ms]
Apr 30 00:47:16.124: INFO: Created: latency-svc-q57vm
Apr 30 00:47:16.165: INFO: Got endpoints: latency-svc-mfmzg [748.800644ms]
Apr 30 00:47:16.175: INFO: Created: latency-svc-b2hlg
Apr 30 00:47:16.215: INFO: Got endpoints: latency-svc-7vmv4 [749.773158ms]
Apr 30 00:47:16.225: INFO: Created: latency-svc-dxdh5
Apr 30 00:47:16.265: INFO: Got endpoints: latency-svc-v2tsc [749.420096ms]
Apr 30 00:47:16.274: INFO: Created: latency-svc-dpvn5
Apr 30 00:47:16.315: INFO: Got endpoints: latency-svc-7zr9n [748.380115ms]
Apr 30 00:47:16.325: INFO: Created: latency-svc-x7xwx
Apr 30 00:47:16.365: INFO: Got endpoints: latency-svc-nqbzt [750.304876ms]
Apr 30 00:47:16.376: INFO: Created: latency-svc-g7dgh
Apr 30 00:47:16.415: INFO: Got endpoints: latency-svc-vlqcd [749.989137ms]
Apr 30 00:47:16.424: INFO: Created: latency-svc-nkg96
Apr 30 00:47:16.465: INFO: Got endpoints: latency-svc-qclv5 [749.260865ms]
Apr 30 00:47:16.475: INFO: Created: latency-svc-fv8wn
Apr 30 00:47:16.515: INFO: Got endpoints: latency-svc-4pb7q [750.679903ms]
Apr 30 00:47:16.525: INFO: Created: latency-svc-6bfdm
Apr 30 00:47:16.565: INFO: Got endpoints: latency-svc-mxhxk [750.266361ms]
Apr 30 00:47:16.575: INFO: Created: latency-svc-sfhrd
Apr 30 00:47:16.615: INFO: Got endpoints: latency-svc-7mqlm [749.313671ms]
Apr 30 00:47:16.625: INFO: Created: latency-svc-4dqff
Apr 30 00:47:16.666: INFO: Got endpoints: latency-svc-fkgp6 [750.797499ms]
Apr 30 00:47:16.675: INFO: Created: latency-svc-xqrdc
Apr 30 00:47:16.715: INFO: Got endpoints: latency-svc-qqdcs [749.742175ms]
Apr 30 00:47:16.724: INFO: Created: latency-svc-vsslg
Apr 30 00:47:16.765: INFO: Got endpoints: latency-svc-657lm [749.820092ms]
Apr 30 00:47:16.774: INFO: Created: latency-svc-2t4zc
Apr 30 00:47:16.815: INFO: Got endpoints: latency-svc-lbrq5 [750.459854ms]
Apr 30 00:47:16.825: INFO: Created: latency-svc-q7fvk
Apr 30 00:47:16.865: INFO: Got endpoints: latency-svc-q57vm [749.854654ms]
Apr 30 00:47:16.874: INFO: Created: latency-svc-w6vcn
Apr 30 00:47:16.915: INFO: Got endpoints: latency-svc-b2hlg [750.494233ms]
Apr 30 00:47:16.925: INFO: Created: latency-svc-qxtcz
Apr 30 00:47:16.966: INFO: Got endpoints: latency-svc-dxdh5 [750.846122ms]
Apr 30 00:47:16.976: INFO: Created: latency-svc-ggnml
Apr 30 00:47:17.015: INFO: Got endpoints: latency-svc-dpvn5 [749.904184ms]
Apr 30 00:47:17.024: INFO: Created: latency-svc-zxbcd
Apr 30 00:47:17.065: INFO: Got endpoints: latency-svc-x7xwx [750.132972ms]
Apr 30 00:47:17.074: INFO: Created: latency-svc-qpnhb
Apr 30 00:47:17.115: INFO: Got endpoints: latency-svc-g7dgh [750.124648ms]
Apr 30 00:47:17.131: INFO: Created: latency-svc-sgld6
Apr 30 00:47:17.164: INFO: Got endpoints: latency-svc-nkg96 [749.739425ms]
Apr 30 00:47:17.174: INFO: Created: latency-svc-t72w4
Apr 30 00:47:17.215: INFO: Got endpoints: latency-svc-fv8wn [749.579594ms]
Apr 30 00:47:17.224: INFO: Created: latency-svc-p4zxm
Apr 30 00:47:17.265: INFO: Got endpoints: latency-svc-6bfdm [750.05186ms]
Apr 30 00:47:17.277: INFO: Created: latency-svc-pgz85
Apr 30 00:47:17.315: INFO: Got endpoints: latency-svc-sfhrd [749.870935ms]
Apr 30 00:47:17.325: INFO: Created: latency-svc-lth8b
Apr 30 00:47:17.365: INFO: Got endpoints: latency-svc-4dqff [749.771675ms]
Apr 30 00:47:17.375: INFO: Created: latency-svc-kpddj
Apr 30 00:47:17.415: INFO: Got endpoints: latency-svc-xqrdc [749.466974ms]
Apr 30 00:47:17.425: INFO: Created: latency-svc-kjrwt
Apr 30 00:47:17.465: INFO: Got endpoints: latency-svc-vsslg [750.14519ms]
Apr 30 00:47:17.474: INFO: Created: latency-svc-bfvz7
Apr 30 00:47:17.515: INFO: Got endpoints: latency-svc-2t4zc [750.377777ms]
Apr 30 00:47:17.525: INFO: Created: latency-svc-dsj8r
Apr 30 00:47:17.565: INFO: Got endpoints: latency-svc-q7fvk [749.518115ms]
Apr 30 00:47:17.576: INFO: Created: latency-svc-j4bb2
Apr 30 00:47:17.615: INFO: Got endpoints: latency-svc-w6vcn [750.132403ms]
Apr 30 00:47:17.624: INFO: Created: latency-svc-7cj95
Apr 30 00:47:17.665: INFO: Got endpoints: latency-svc-qxtcz [750.394154ms]
Apr 30 00:47:17.675: INFO: Created: latency-svc-vcv89
Apr 30 00:47:17.715: INFO: Got endpoints: latency-svc-ggnml [749.64857ms]
Apr 30 00:47:17.725: INFO: Created: latency-svc-dtkct
Apr 30 00:47:17.765: INFO: Got endpoints: latency-svc-zxbcd [750.096583ms]
Apr 30 00:47:17.775: INFO: Created: latency-svc-dm96w
Apr 30 00:47:17.815: INFO: Got endpoints: latency-svc-qpnhb [750.192631ms]
Apr 30 00:47:17.824: INFO: Created: latency-svc-l47k7
Apr 30 00:47:17.866: INFO: Got endpoints: latency-svc-sgld6 [750.427928ms]
Apr 30 00:47:17.875: INFO: Created: latency-svc-tcvzf
Apr 30 00:47:17.916: INFO: Got endpoints: latency-svc-t72w4 [751.832759ms]
Apr 30 00:47:17.926: INFO: Created: latency-svc-r2fwk
Apr 30 00:47:17.965: INFO: Got endpoints: latency-svc-p4zxm [750.383849ms]
Apr 30 00:47:17.975: INFO: Created: latency-svc-55m42
Apr 30 00:47:18.015: INFO: Got endpoints: latency-svc-pgz85 [750.072837ms]
Apr 30 00:47:18.025: INFO: Created: latency-svc-mtmgh
Apr 30 00:47:18.065: INFO: Got endpoints: latency-svc-lth8b [749.780755ms]
Apr 30 00:47:18.075: INFO: Created: latency-svc-492fs
Apr 30 00:47:18.116: INFO: Got endpoints: latency-svc-kpddj [751.266916ms]
Apr 30 00:47:18.127: INFO: Created: latency-svc-c9vpw
Apr 30 00:47:18.165: INFO: Got endpoints: latency-svc-kjrwt [749.568294ms]
Apr 30 00:47:18.175: INFO: Created: latency-svc-xwc8l
Apr 30 00:47:18.215: INFO: Got endpoints: latency-svc-bfvz7 [749.759351ms]
Apr 30 00:47:18.224: INFO: Created: latency-svc-r98d2
Apr 30 00:47:18.265: INFO: Got endpoints: latency-svc-dsj8r [749.600616ms]
Apr 30 00:47:18.275: INFO: Created: latency-svc-vb458
Apr 30 00:47:18.315: INFO: Got endpoints: latency-svc-j4bb2 [749.943451ms]
Apr 30 00:47:18.327: INFO: Created: latency-svc-sgpw7
Apr 30 00:47:18.365: INFO: Got endpoints: latency-svc-7cj95 [749.76647ms]
Apr 30 00:47:18.374: INFO: Created: latency-svc-xsjs9
Apr 30 00:47:18.415: INFO: Got endpoints: latency-svc-vcv89 [748.98315ms]
Apr 30 00:47:18.425: INFO: Created: latency-svc-zvn2s
Apr 30 00:47:18.464: INFO: Got endpoints: latency-svc-dtkct [749.132581ms]
Apr 30 00:47:18.474: INFO: Created: latency-svc-st924
Apr 30 00:47:18.515: INFO: Got endpoints: latency-svc-dm96w [749.84132ms]
Apr 30 00:47:18.524: INFO: Created: latency-svc-x9m6n
Apr 30 00:47:18.565: INFO: Got endpoints: latency-svc-l47k7 [749.502386ms]
Apr 30 00:47:18.574: INFO: Created: latency-svc-rwqjb
Apr 30 00:47:18.615: INFO: Got endpoints: latency-svc-tcvzf [748.91741ms]
Apr 30 00:47:18.624: INFO: Created: latency-svc-6n4jm
Apr 30 00:47:18.665: INFO: Got endpoints: latency-svc-r2fwk [748.802424ms]
Apr 30 00:47:18.678: INFO: Created: latency-svc-2k7rq
Apr 30 00:47:18.715: INFO: Got endpoints: latency-svc-55m42 [749.625723ms]
Apr 30 00:47:18.724: INFO: Created: latency-svc-8d5md
Apr 30 00:47:18.765: INFO: Got endpoints: latency-svc-mtmgh [749.602793ms]
Apr 30 00:47:18.774: INFO: Created: latency-svc-vj74d
Apr 30 00:47:18.815: INFO: Got endpoints: latency-svc-492fs [750.371871ms]
Apr 30 00:47:18.824: INFO: Created: latency-svc-k4mvj
Apr 30 00:47:18.865: INFO: Got endpoints: latency-svc-c9vpw [748.522679ms]
Apr 30 00:47:18.874: INFO: Created: latency-svc-wcpv6
Apr 30 00:47:18.915: INFO: Got endpoints: latency-svc-xwc8l [750.351543ms]
Apr 30 00:47:18.925: INFO: Created: latency-svc-svhkg
Apr 30 00:47:18.965: INFO: Got endpoints: latency-svc-r98d2 [750.750443ms]
Apr 30 00:47:18.975: INFO: Created: latency-svc-nfqq7
Apr 30 00:47:19.014: INFO: Got endpoints: latency-svc-vb458 [749.799442ms]
Apr 30 00:47:19.024: INFO: Created: latency-svc-kf9lh
Apr 30 00:47:19.065: INFO: Got endpoints: latency-svc-sgpw7 [749.790501ms]
Apr 30 00:47:19.074: INFO: Created: latency-svc-nrtdq
Apr 30 00:47:19.115: INFO: Got endpoints: latency-svc-xsjs9 [750.743362ms]
Apr 30 00:47:19.125: INFO: Created: latency-svc-qmjs5
Apr 30 00:47:19.165: INFO: Got endpoints: latency-svc-zvn2s [750.202802ms]
Apr 30 00:47:19.177: INFO: Created: latency-svc-5spsq
Apr 30 00:47:19.216: INFO: Got endpoints: latency-svc-st924 [751.20219ms]
Apr 30 00:47:19.225: INFO: Created: latency-svc-nksz8
Apr 30 00:47:19.265: INFO: Got endpoints: latency-svc-x9m6n [750.659901ms]
Apr 30 00:47:19.275: INFO: Created: latency-svc-9zxpq
Apr 30 00:47:19.314: INFO: Got endpoints: latency-svc-rwqjb [749.775701ms]
Apr 30 00:47:19.327: INFO: Created: latency-svc-xkzkw
Apr 30 00:47:19.365: INFO: Got endpoints: latency-svc-6n4jm [749.851185ms]
Apr 30 00:47:19.374: INFO: Created: latency-svc-7strr
Apr 30 00:47:19.415: INFO: Got endpoints: latency-svc-2k7rq [750.025691ms]
Apr 30 00:47:19.425: INFO: Created: latency-svc-n2t2q
Apr 30 00:47:19.465: INFO: Got endpoints: latency-svc-8d5md [749.992803ms]
Apr 30 00:47:19.475: INFO: Created: latency-svc-r544d
Apr 30 00:47:19.515: INFO: Got endpoints: latency-svc-vj74d [749.37101ms]
Apr 30 00:47:19.524: INFO: Created: latency-svc-qt5hr
Apr 30 00:47:19.565: INFO: Got endpoints: latency-svc-k4mvj [750.194833ms]
Apr 30 00:47:19.575: INFO: Created: latency-svc-p9z6q
Apr 30 00:47:19.615: INFO: Got endpoints: latency-svc-wcpv6 [750.911602ms]
Apr 30 00:47:19.625: INFO: Created: latency-svc-hx6vx
Apr 30 00:47:19.665: INFO: Got endpoints: latency-svc-svhkg [749.342752ms]
Apr 30 00:47:19.674: INFO: Created: latency-svc-6l8z7
Apr 30 00:47:19.715: INFO: Got endpoints: latency-svc-nfqq7 [749.346052ms]
Apr 30 00:47:19.725: INFO: Created: latency-svc-92rht
Apr 30 00:47:19.765: INFO: Got endpoints: latency-svc-kf9lh [750.099892ms]
Apr 30 00:47:19.775: INFO: Created: latency-svc-rclq5
Apr 30 00:47:19.815: INFO: Got endpoints: latency-svc-nrtdq [750.072905ms]
Apr 30 00:47:19.824: INFO: Created: latency-svc-fzcg9
Apr 30 00:47:19.864: INFO: Got endpoints: latency-svc-qmjs5 [749.062066ms]
Apr 30 00:47:19.875: INFO: Created: latency-svc-dzv7m
Apr 30 00:47:19.915: INFO: Got endpoints: latency-svc-5spsq [750.25304ms]
Apr 30 00:47:19.927: INFO: Created: latency-svc-mlbz5
Apr 30 00:47:19.965: INFO: Got endpoints: latency-svc-nksz8 [748.823142ms]
Apr 30 00:47:19.975: INFO: Created: latency-svc-lss2h
Apr 30 00:47:20.016: INFO: Got endpoints: latency-svc-9zxpq [750.324776ms]
Apr 30 00:47:20.026: INFO: Created: latency-svc-d562f
Apr 30 00:47:20.065: INFO: Got endpoints: latency-svc-xkzkw [749.111645ms]
Apr 30 00:47:20.075: INFO: Created: latency-svc-kclth
Apr 30 00:47:20.115: INFO: Got endpoints: latency-svc-7strr [750.073938ms]
Apr 30 00:47:20.126: INFO: Created: latency-svc-846ld
Apr 30 00:47:20.165: INFO: Got endpoints: latency-svc-n2t2q [749.700694ms]
Apr 30 00:47:20.175: INFO: Created: latency-svc-zzh7g
Apr 30 00:47:20.215: INFO: Got endpoints: latency-svc-r544d [750.060803ms]
Apr 30 00:47:20.228: INFO: Created: latency-svc-h97zp
Apr 30 00:47:20.265: INFO: Got endpoints: latency-svc-qt5hr [750.146564ms]
Apr 30 00:47:20.275: INFO: Created: latency-svc-7zzx8
Apr 30 00:47:20.315: INFO: Got endpoints: latency-svc-p9z6q [750.097276ms]
Apr 30 00:47:20.325: INFO: Created: latency-svc-fld4w
Apr 30 00:47:20.365: INFO: Got endpoints: latency-svc-hx6vx [749.145918ms]
Apr 30 00:47:20.374: INFO: Created: latency-svc-pz7gl
Apr 30 00:47:20.415: INFO: Got endpoints: latency-svc-6l8z7 [750.543972ms]
Apr 30 00:47:20.425: INFO: Created: latency-svc-6mtx6
Apr 30 00:47:20.465: INFO: Got endpoints: latency-svc-92rht [750.291269ms]
Apr 30 00:47:20.474: INFO: Created: latency-svc-6bgtp
Apr 30 00:47:20.515: INFO: Got endpoints: latency-svc-rclq5 [750.053232ms]
Apr 30 00:47:20.525: INFO: Created: latency-svc-cpq74
Apr 30 00:47:20.565: INFO: Got endpoints: latency-svc-fzcg9 [750.493948ms]
Apr 30 00:47:20.576: INFO: Created: latency-svc-lvrqr
Apr 30 00:47:20.615: INFO: Got endpoints: latency-svc-dzv7m [750.107888ms]
Apr 30 00:47:20.626: INFO: Created: latency-svc-wfxhk
Apr 30 00:47:20.665: INFO: Got endpoints: latency-svc-mlbz5 [750.243705ms]
Apr 30 00:47:20.675: INFO: Created: latency-svc-8vwsx
Apr 30 00:47:20.715: INFO: Got endpoints: latency-svc-lss2h [750.621061ms]
Apr 30 00:47:20.726: INFO: Created: latency-svc-md955
Apr 30 00:47:20.765: INFO: Got endpoints: latency-svc-d562f [748.792685ms]
Apr 30 00:47:20.774: INFO: Created: latency-svc-szc6s
Apr 30 00:47:20.815: INFO: Got endpoints: latency-svc-kclth [750.375027ms]
Apr 30 00:47:20.825: INFO: Created: latency-svc-lkkks
Apr 30 00:47:20.865: INFO: Got endpoints: latency-svc-846ld [750.779986ms]
Apr 30 00:47:20.876: INFO: Created: latency-svc-cjzl7
Apr 30 00:47:20.915: INFO: Got endpoints: latency-svc-zzh7g [749.646118ms]
Apr 30 00:47:20.924: INFO: Created: latency-svc-dv6kz
Apr 30 00:47:20.965: INFO: Got endpoints: latency-svc-h97zp [749.98323ms]
Apr 30 00:47:20.975: INFO: Created: latency-svc-m9lmv
Apr 30 00:47:21.015: INFO: Got endpoints: latency-svc-7zzx8 [750.532492ms]
Apr 30 00:47:21.025: INFO: Created: latency-svc-8rgmr
Apr 30 00:47:21.065: INFO: Got endpoints: latency-svc-fld4w [749.829248ms]
Apr 30 00:47:21.076: INFO: Created: latency-svc-ldmpg
Apr 30 00:47:21.115: INFO: Got endpoints: latency-svc-pz7gl [750.223568ms]
Apr 30 00:47:21.125: INFO: Created: latency-svc-lt4ks
Apr 30 00:47:21.165: INFO: Got endpoints: latency-svc-6mtx6 [749.502193ms]
Apr 30 00:47:21.175: INFO: Created: latency-svc-hndfn
Apr 30 00:47:21.215: INFO: Got endpoints: latency-svc-6bgtp [749.537876ms]
Apr 30 00:47:21.225: INFO: Created: latency-svc-rlmlz
Apr 30 00:47:21.265: INFO: Got endpoints: latency-svc-cpq74 [749.988198ms]
Apr 30 00:47:21.275: INFO: Created: latency-svc-f4859
Apr 30 00:47:21.318: INFO: Got endpoints: latency-svc-lvrqr [752.33764ms]
Apr 30 00:47:21.328: INFO: Created: latency-svc-whbhz
Apr 30 00:47:21.365: INFO: Got endpoints: latency-svc-wfxhk [749.780425ms]
Apr 30 00:47:21.374: INFO: Created: latency-svc-w82kk
Apr 30 00:47:21.415: INFO: Got endpoints: latency-svc-8vwsx [749.352034ms]
Apr 30 00:47:21.425: INFO: Created: latency-svc-mfv89
Apr 30 00:47:21.465: INFO: Got endpoints: latency-svc-md955 [749.268442ms]
Apr 30 00:47:21.475: INFO: Created: latency-svc-bmq6c
Apr 30 00:47:21.515: INFO: Got endpoints: latency-svc-szc6s [750.460416ms]
Apr 30 00:47:21.525: INFO: Created: latency-svc-njvdn
Apr 30 00:47:21.565: INFO: Got endpoints: latency-svc-lkkks [750.014128ms]
Apr 30 00:47:21.577: INFO: Created: latency-svc-w4qcv
Apr 30 00:47:21.615: INFO: Got endpoints: latency-svc-cjzl7 [749.041806ms]
Apr 30 00:47:21.625: INFO: Created: latency-svc-wzxdb
Apr 30 00:47:21.665: INFO: Got endpoints: latency-svc-dv6kz [750.338001ms]
Apr 30 00:47:21.715: INFO: Got endpoints: latency-svc-m9lmv [750.17526ms]
Apr 30 00:47:21.765: INFO: Got endpoints: latency-svc-8rgmr [749.366272ms]
Apr 30 00:47:21.815: INFO: Got endpoints: latency-svc-ldmpg [749.91321ms]
Apr 30 00:47:21.865: INFO: Got endpoints: latency-svc-lt4ks [749.867592ms]
Apr 30 00:47:21.915: INFO: Got endpoints: latency-svc-hndfn [750.09056ms]
Apr 30 00:47:21.965: INFO: Got endpoints: latency-svc-rlmlz [750.363075ms]
Apr 30 00:47:22.015: INFO: Got endpoints: latency-svc-f4859 [750.015507ms]
Apr 30 00:47:22.065: INFO: Got endpoints: latency-svc-whbhz [747.850803ms]
Apr 30 00:47:22.115: INFO: Got endpoints: latency-svc-w82kk [750.200395ms]
Apr 30 00:47:22.165: INFO: Got endpoints: latency-svc-mfv89 [750.633417ms]
Apr 30 00:47:22.215: INFO: Got endpoints: latency-svc-bmq6c [750.201774ms]
Apr 30 00:47:22.264: INFO: Got endpoints: latency-svc-njvdn [749.338939ms]
Apr 30 00:47:22.315: INFO: Got endpoints: latency-svc-w4qcv [749.536011ms]
Apr 30 00:47:22.365: INFO: Got endpoints: latency-svc-wzxdb [750.209366ms]
Apr 30 00:47:22.365: INFO: Latencies: [17.444865ms 17.997953ms 26.78306ms 28.506536ms 34.305509ms 38.930782ms 45.082511ms 49.437297ms 58.937866ms 61.708656ms 66.081338ms 71.617155ms 76.471915ms 77.146595ms 80.782474ms 81.373447ms 82.140822ms 82.412728ms 84.715639ms 84.744885ms 85.015152ms 85.366841ms 85.605041ms 85.615344ms 85.840914ms 86.94137ms 88.030962ms 88.520884ms 88.63436ms 89.914329ms 90.602833ms 91.248143ms 94.115593ms 137.022887ms 176.287613ms 225.47368ms 266.692274ms 312.787358ms 358.230565ms 402.855701ms 447.435661ms 492.226841ms 536.254032ms 581.989622ms 620.651738ms 661.210682ms 709.958387ms 747.850803ms 748.380115ms 748.522679ms 748.680971ms 748.792685ms 748.800644ms 748.802424ms 748.823142ms 748.836043ms 748.91741ms 748.920788ms 748.98315ms 749.041806ms 749.062066ms 749.111645ms 749.132581ms 749.145918ms 749.260865ms 749.268442ms 749.313671ms 749.333564ms 749.338939ms 749.342752ms 749.346052ms 749.352034ms 749.356513ms 749.366272ms 749.37101ms 749.371388ms 749.420096ms 749.42626ms 749.466974ms 749.502193ms 749.502386ms 749.518115ms 749.536011ms 749.537876ms 749.568294ms 749.579594ms 749.600616ms 749.602793ms 749.625723ms 749.646118ms 749.64857ms 749.668017ms 749.700694ms 749.72769ms 749.739425ms 749.742175ms 749.759351ms 749.76647ms 749.771675ms 749.773158ms 749.775701ms 749.776645ms 749.780425ms 749.780755ms 749.790501ms 749.795974ms 749.799442ms 749.810489ms 749.820092ms 749.829248ms 749.84132ms 749.851185ms 749.854654ms 749.867592ms 749.870935ms 749.904184ms 749.91321ms 749.914679ms 749.943451ms 749.970557ms 749.98323ms 749.988198ms 749.989137ms 749.991242ms 749.992803ms 750.014128ms 750.015507ms 750.018302ms 750.025691ms 750.05186ms 750.053232ms 750.060803ms 750.072837ms 750.072905ms 750.073938ms 750.080021ms 750.080766ms 750.09056ms 750.096583ms 750.097276ms 750.099892ms 750.107888ms 750.124648ms 750.132403ms 750.132972ms 750.141444ms 750.14519ms 750.146564ms 750.17526ms 750.192631ms 750.194833ms 750.200395ms 750.201774ms 750.202802ms 750.209366ms 750.223568ms 750.243705ms 750.25304ms 750.266361ms 750.291269ms 750.304876ms 750.324776ms 750.338001ms 750.351543ms 750.363075ms 750.371871ms 750.375027ms 750.377777ms 750.383849ms 750.394154ms 750.427928ms 750.459854ms 750.460416ms 750.493948ms 750.494233ms 750.519903ms 750.532492ms 750.543972ms 750.594177ms 750.621061ms 750.633417ms 750.659901ms 750.679903ms 750.712967ms 750.743362ms 750.750443ms 750.779986ms 750.797499ms 750.846122ms 750.847463ms 750.874707ms 750.911602ms 750.930349ms 751.096252ms 751.159486ms 751.20219ms 751.266916ms 751.50373ms 751.832759ms 752.33764ms]
Apr 30 00:47:22.365: INFO: 50 %ile: 749.775701ms
Apr 30 00:47:22.365: INFO: 90 %ile: 750.633417ms
Apr 30 00:47:22.365: INFO: 99 %ile: 751.832759ms
Apr 30 00:47:22.365: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:47:22.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5099" for this suite.
Apr 30 00:47:46.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:47:46.467: INFO: namespace svc-latency-5099 deletion completed in 24.097868712s

• [SLOW TEST:34.970 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:47:46.467: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4835
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 30 00:47:46.918: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 30 00:47:49.938: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Apr 30 00:47:49.953: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:47:49.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4835" for this suite.
Apr 30 00:47:55.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:47:56.059: INFO: namespace webhook-4835 deletion completed in 6.090756762s
STEP: Destroying namespace "webhook-4835-markers" for this suite.
Apr 30 00:48:02.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:48:02.255: INFO: namespace webhook-4835-markers deletion completed in 6.196195155s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.841 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:48:02.309: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1662
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-5c9b99da-d060-4c5d-a8a9-54eec9d15bdf
STEP: Creating a pod to test consume secrets
Apr 30 00:48:02.500: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b1e534bb-bd97-4aa4-9a61-e44b4e28618f" in namespace "projected-1662" to be "success or failure"
Apr 30 00:48:02.502: INFO: Pod "pod-projected-secrets-b1e534bb-bd97-4aa4-9a61-e44b4e28618f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.912745ms
Apr 30 00:48:04.505: INFO: Pod "pod-projected-secrets-b1e534bb-bd97-4aa4-9a61-e44b4e28618f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004672907s
STEP: Saw pod success
Apr 30 00:48:04.505: INFO: Pod "pod-projected-secrets-b1e534bb-bd97-4aa4-9a61-e44b4e28618f" satisfied condition "success or failure"
Apr 30 00:48:04.507: INFO: Trying to get logs from node ip-192-168-82-130.us-west-2.compute.internal pod pod-projected-secrets-b1e534bb-bd97-4aa4-9a61-e44b4e28618f container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 30 00:48:04.526: INFO: Waiting for pod pod-projected-secrets-b1e534bb-bd97-4aa4-9a61-e44b4e28618f to disappear
Apr 30 00:48:04.528: INFO: Pod pod-projected-secrets-b1e534bb-bd97-4aa4-9a61-e44b4e28618f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:48:04.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1662" for this suite.
Apr 30 00:48:10.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:48:10.630: INFO: namespace projected-1662 deletion completed in 6.098368442s

• [SLOW TEST:8.321 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:48:10.630: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4788
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Apr 30 00:48:13.308: INFO: Successfully updated pod "labelsupdate9000c0ca-7612-4df3-a1bf-41d019d5b092"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:48:15.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4788" for this suite.
Apr 30 00:48:43.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:48:43.415: INFO: namespace projected-4788 deletion completed in 28.091031812s

• [SLOW TEST:32.785 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:48:43.415: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3733
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 30 00:48:43.571: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c4a4b42a-a99b-478f-a3d6-b73e658fa2cb" in namespace "downward-api-3733" to be "success or failure"
Apr 30 00:48:43.573: INFO: Pod "downwardapi-volume-c4a4b42a-a99b-478f-a3d6-b73e658fa2cb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.764093ms
Apr 30 00:48:45.576: INFO: Pod "downwardapi-volume-c4a4b42a-a99b-478f-a3d6-b73e658fa2cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004367359s
STEP: Saw pod success
Apr 30 00:48:45.576: INFO: Pod "downwardapi-volume-c4a4b42a-a99b-478f-a3d6-b73e658fa2cb" satisfied condition "success or failure"
Apr 30 00:48:45.578: INFO: Trying to get logs from node ip-192-168-197-211.us-west-2.compute.internal pod downwardapi-volume-c4a4b42a-a99b-478f-a3d6-b73e658fa2cb container client-container: <nil>
STEP: delete the pod
Apr 30 00:48:45.595: INFO: Waiting for pod downwardapi-volume-c4a4b42a-a99b-478f-a3d6-b73e658fa2cb to disappear
Apr 30 00:48:45.597: INFO: Pod downwardapi-volume-c4a4b42a-a99b-478f-a3d6-b73e658fa2cb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:48:45.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3733" for this suite.
Apr 30 00:48:51.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:48:51.699: INFO: namespace downward-api-3733 deletion completed in 6.097938375s

• [SLOW TEST:8.283 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:48:51.699: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7741
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:48:51.840: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 30 00:48:54.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-7741 create -f -'
Apr 30 00:48:54.685: INFO: stderr: ""
Apr 30 00:48:54.685: INFO: stdout: "e2e-test-crd-publish-openapi-3755-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 30 00:48:54.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-7741 delete e2e-test-crd-publish-openapi-3755-crds test-cr'
Apr 30 00:48:54.750: INFO: stderr: ""
Apr 30 00:48:54.750: INFO: stdout: "e2e-test-crd-publish-openapi-3755-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Apr 30 00:48:54.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-7741 apply -f -'
Apr 30 00:48:54.919: INFO: stderr: ""
Apr 30 00:48:54.919: INFO: stdout: "e2e-test-crd-publish-openapi-3755-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 30 00:48:54.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-7741 delete e2e-test-crd-publish-openapi-3755-crds test-cr'
Apr 30 00:48:54.976: INFO: stderr: ""
Apr 30 00:48:54.976: INFO: stdout: "e2e-test-crd-publish-openapi-3755-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Apr 30 00:48:54.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 explain e2e-test-crd-publish-openapi-3755-crds'
Apr 30 00:48:55.084: INFO: stderr: ""
Apr 30 00:48:55.084: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3755-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:48:57.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7741" for this suite.
Apr 30 00:49:03.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:49:03.821: INFO: namespace crd-publish-openapi-7741 deletion completed in 6.122508061s

• [SLOW TEST:12.122 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:49:03.821: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3488
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Apr 30 00:49:05.993: INFO: &Pod{ObjectMeta:{send-events-dcdd865b-17f3-4b3d-81f7-35385c625770  events-3488 /api/v1/namespaces/events-3488/pods/send-events-dcdd865b-17f3-4b3d-81f7-35385c625770 1895d636-40c3-4b75-84a3-b5f23b27e3e9 36261 0 2020-04-30 00:49:03 +0000 UTC <nil> <nil> map[name:foo time:976992111] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xswwc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xswwc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xswwc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-83-225.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-30 00:49:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-30 00:49:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-30 00:49:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-30 00:49:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.83.225,PodIP:192.168.74.58,StartTime:2020-04-30 00:49:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-30 00:49:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://122f5bc5e75160a53b441e76b82b66ecce80a1c53dcfc63cdb644209ce7e2dad,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.74.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Apr 30 00:49:07.998: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Apr 30 00:49:10.002: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:49:10.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3488" for this suite.
Apr 30 00:49:54.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:49:54.108: INFO: namespace events-3488 deletion completed in 44.094846855s

• [SLOW TEST:50.287 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:49:54.109: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-s7xc
STEP: Creating a pod to test atomic-volume-subpath
Apr 30 00:49:54.279: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-s7xc" in namespace "subpath-8616" to be "success or failure"
Apr 30 00:49:54.281: INFO: Pod "pod-subpath-test-configmap-s7xc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.825057ms
Apr 30 00:49:56.284: INFO: Pod "pod-subpath-test-configmap-s7xc": Phase="Running", Reason="", readiness=true. Elapsed: 2.004917944s
Apr 30 00:49:58.287: INFO: Pod "pod-subpath-test-configmap-s7xc": Phase="Running", Reason="", readiness=true. Elapsed: 4.007700089s
Apr 30 00:50:00.290: INFO: Pod "pod-subpath-test-configmap-s7xc": Phase="Running", Reason="", readiness=true. Elapsed: 6.010467707s
Apr 30 00:50:02.295: INFO: Pod "pod-subpath-test-configmap-s7xc": Phase="Running", Reason="", readiness=true. Elapsed: 8.0155384s
Apr 30 00:50:04.298: INFO: Pod "pod-subpath-test-configmap-s7xc": Phase="Running", Reason="", readiness=true. Elapsed: 10.018513895s
Apr 30 00:50:06.300: INFO: Pod "pod-subpath-test-configmap-s7xc": Phase="Running", Reason="", readiness=true. Elapsed: 12.021306628s
Apr 30 00:50:08.303: INFO: Pod "pod-subpath-test-configmap-s7xc": Phase="Running", Reason="", readiness=true. Elapsed: 14.02409667s
Apr 30 00:50:10.305: INFO: Pod "pod-subpath-test-configmap-s7xc": Phase="Running", Reason="", readiness=true. Elapsed: 16.026415281s
Apr 30 00:50:12.308: INFO: Pod "pod-subpath-test-configmap-s7xc": Phase="Running", Reason="", readiness=true. Elapsed: 18.029221251s
Apr 30 00:50:14.311: INFO: Pod "pod-subpath-test-configmap-s7xc": Phase="Running", Reason="", readiness=true. Elapsed: 20.031946005s
Apr 30 00:50:16.314: INFO: Pod "pod-subpath-test-configmap-s7xc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.034726709s
STEP: Saw pod success
Apr 30 00:50:16.314: INFO: Pod "pod-subpath-test-configmap-s7xc" satisfied condition "success or failure"
Apr 30 00:50:16.316: INFO: Trying to get logs from node ip-192-168-149-254.us-west-2.compute.internal pod pod-subpath-test-configmap-s7xc container test-container-subpath-configmap-s7xc: <nil>
STEP: delete the pod
Apr 30 00:50:16.336: INFO: Waiting for pod pod-subpath-test-configmap-s7xc to disappear
Apr 30 00:50:16.338: INFO: Pod pod-subpath-test-configmap-s7xc no longer exists
STEP: Deleting pod pod-subpath-test-configmap-s7xc
Apr 30 00:50:16.338: INFO: Deleting pod "pod-subpath-test-configmap-s7xc" in namespace "subpath-8616"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:50:16.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8616" for this suite.
Apr 30 00:50:22.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:50:22.434: INFO: namespace subpath-8616 deletion completed in 6.090091796s

• [SLOW TEST:28.326 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:50:22.434: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-728
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-728
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-728
I0430 00:50:22.601798      21 runners.go:184] Created replication controller with name: externalname-service, namespace: services-728, replica count: 2
Apr 30 00:50:25.652: INFO: Creating new exec pod
I0430 00:50:25.652063      21 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 30 00:50:28.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=services-728 execpod4wfdm -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Apr 30 00:50:28.824: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 30 00:50:28.824: INFO: stdout: ""
Apr 30 00:50:28.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 exec --namespace=services-728 execpod4wfdm -- /bin/sh -x -c nc -zv -t -w 2 10.100.237.189 80'
Apr 30 00:50:28.963: INFO: stderr: "+ nc -zv -t -w 2 10.100.237.189 80\nConnection to 10.100.237.189 80 port [tcp/http] succeeded!\n"
Apr 30 00:50:28.963: INFO: stdout: ""
Apr 30 00:50:28.963: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:50:28.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-728" for this suite.
Apr 30 00:50:35.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:50:35.087: INFO: namespace services-728 deletion completed in 6.095728094s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:12.653 seconds]
[sig-network] Services
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:50:35.088: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7951
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 30 00:50:35.236: INFO: Waiting up to 5m0s for pod "pod-7f00db81-dc70-4733-bb1e-c64b73c40824" in namespace "emptydir-7951" to be "success or failure"
Apr 30 00:50:35.238: INFO: Pod "pod-7f00db81-dc70-4733-bb1e-c64b73c40824": Phase="Pending", Reason="", readiness=false. Elapsed: 1.866104ms
Apr 30 00:50:37.241: INFO: Pod "pod-7f00db81-dc70-4733-bb1e-c64b73c40824": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004681158s
Apr 30 00:50:39.244: INFO: Pod "pod-7f00db81-dc70-4733-bb1e-c64b73c40824": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007251598s
STEP: Saw pod success
Apr 30 00:50:39.244: INFO: Pod "pod-7f00db81-dc70-4733-bb1e-c64b73c40824" satisfied condition "success or failure"
Apr 30 00:50:39.246: INFO: Trying to get logs from node ip-192-168-82-130.us-west-2.compute.internal pod pod-7f00db81-dc70-4733-bb1e-c64b73c40824 container test-container: <nil>
STEP: delete the pod
Apr 30 00:50:39.262: INFO: Waiting for pod pod-7f00db81-dc70-4733-bb1e-c64b73c40824 to disappear
Apr 30 00:50:39.264: INFO: Pod pod-7f00db81-dc70-4733-bb1e-c64b73c40824 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:50:39.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7951" for this suite.
Apr 30 00:50:45.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:50:45.361: INFO: namespace emptydir-7951 deletion completed in 6.093509319s

• [SLOW TEST:10.274 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:50:45.362: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7521
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Apr 30 00:50:45.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 cluster-info'
Apr 30 00:50:45.559: INFO: stderr: ""
Apr 30 00:50:45.559: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.100.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.100.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:50:45.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7521" for this suite.
Apr 30 00:50:51.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:50:51.660: INFO: namespace kubectl-7521 deletion completed in 6.0965959s

• [SLOW TEST:6.298 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:50:51.660: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6154
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Apr 30 00:50:51.809: INFO: Waiting up to 5m0s for pod "downward-api-392efdad-0a8a-471c-b196-1da3e56321f5" in namespace "downward-api-6154" to be "success or failure"
Apr 30 00:50:51.813: INFO: Pod "downward-api-392efdad-0a8a-471c-b196-1da3e56321f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.262461ms
Apr 30 00:50:53.815: INFO: Pod "downward-api-392efdad-0a8a-471c-b196-1da3e56321f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006046012s
STEP: Saw pod success
Apr 30 00:50:53.815: INFO: Pod "downward-api-392efdad-0a8a-471c-b196-1da3e56321f5" satisfied condition "success or failure"
Apr 30 00:50:53.818: INFO: Trying to get logs from node ip-192-168-211-77.us-west-2.compute.internal pod downward-api-392efdad-0a8a-471c-b196-1da3e56321f5 container dapi-container: <nil>
STEP: delete the pod
Apr 30 00:50:53.835: INFO: Waiting for pod downward-api-392efdad-0a8a-471c-b196-1da3e56321f5 to disappear
Apr 30 00:50:53.836: INFO: Pod downward-api-392efdad-0a8a-471c-b196-1da3e56321f5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:50:53.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6154" for this suite.
Apr 30 00:50:59.854: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:50:59.937: INFO: namespace downward-api-6154 deletion completed in 6.093030002s

• [SLOW TEST:8.277 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:50:59.937: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1837
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Apr 30 00:51:00.079: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-371654096 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:51:00.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1837" for this suite.
Apr 30 00:51:06.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:51:06.230: INFO: namespace kubectl-1837 deletion completed in 6.099621752s

• [SLOW TEST:6.293 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:51:06.231: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7726
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:51:06.376: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Apr 30 00:51:07.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-7726 create -f -'
Apr 30 00:51:08.231: INFO: stderr: ""
Apr 30 00:51:08.231: INFO: stdout: "e2e-test-crd-publish-openapi-536-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 30 00:51:08.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-7726 delete e2e-test-crd-publish-openapi-536-crds test-foo'
Apr 30 00:51:08.295: INFO: stderr: ""
Apr 30 00:51:08.295: INFO: stdout: "e2e-test-crd-publish-openapi-536-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Apr 30 00:51:08.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-7726 apply -f -'
Apr 30 00:51:08.413: INFO: stderr: ""
Apr 30 00:51:08.413: INFO: stdout: "e2e-test-crd-publish-openapi-536-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 30 00:51:08.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-7726 delete e2e-test-crd-publish-openapi-536-crds test-foo'
Apr 30 00:51:08.491: INFO: stderr: ""
Apr 30 00:51:08.491: INFO: stdout: "e2e-test-crd-publish-openapi-536-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Apr 30 00:51:08.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-7726 create -f -'
Apr 30 00:51:08.634: INFO: rc: 1
Apr 30 00:51:08.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-7726 apply -f -'
Apr 30 00:51:08.775: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Apr 30 00:51:08.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-7726 create -f -'
Apr 30 00:51:08.880: INFO: rc: 1
Apr 30 00:51:08.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 --namespace=crd-publish-openapi-7726 apply -f -'
Apr 30 00:51:08.986: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Apr 30 00:51:08.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 explain e2e-test-crd-publish-openapi-536-crds'
Apr 30 00:51:09.132: INFO: stderr: ""
Apr 30 00:51:09.132: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-536-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Apr 30 00:51:09.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 explain e2e-test-crd-publish-openapi-536-crds.metadata'
Apr 30 00:51:09.239: INFO: stderr: ""
Apr 30 00:51:09.239: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-536-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Apr 30 00:51:09.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 explain e2e-test-crd-publish-openapi-536-crds.spec'
Apr 30 00:51:09.379: INFO: stderr: ""
Apr 30 00:51:09.379: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-536-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Apr 30 00:51:09.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 explain e2e-test-crd-publish-openapi-536-crds.spec.bars'
Apr 30 00:51:09.521: INFO: stderr: ""
Apr 30 00:51:09.521: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-536-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Apr 30 00:51:09.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-371654096 explain e2e-test-crd-publish-openapi-536-crds.spec.bars2'
Apr 30 00:51:09.662: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:51:11.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7726" for this suite.
Apr 30 00:51:17.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:51:17.362: INFO: namespace crd-publish-openapi-7726 deletion completed in 6.101321064s

• [SLOW TEST:11.132 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:51:17.362: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2722
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Apr 30 00:51:23.533: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0430 00:51:23.533057      21 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:51:23.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2722" for this suite.
Apr 30 00:51:29.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:51:29.655: INFO: namespace gc-2722 deletion completed in 6.118650268s

• [SLOW TEST:12.293 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:51:29.655: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Apr 30 00:51:29.800: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:51:33.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9228" for this suite.
Apr 30 00:51:39.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:51:40.040: INFO: namespace init-container-9228 deletion completed in 6.090549443s

• [SLOW TEST:10.384 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:51:40.040: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-8033
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Apr 30 00:51:40.184: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 30 00:52:40.195: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:52:40.198: INFO: Starting informer...
STEP: Starting pods...
Apr 30 00:52:40.413: INFO: Pod1 is running on ip-192-168-83-225.us-west-2.compute.internal. Tainting Node
Apr 30 00:52:42.627: INFO: Pod2 is running on ip-192-168-83-225.us-west-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Apr 30 00:52:51.398: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Apr 30 00:53:11.395: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:53:11.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-8033" for this suite.
Apr 30 00:53:17.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:53:17.505: INFO: namespace taint-multiple-pods-8033 deletion completed in 6.095856188s

• [SLOW TEST:97.464 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:53:17.505: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5698
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 30 00:53:19.666: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:53:19.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5698" for this suite.
Apr 30 00:53:25.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:53:25.770: INFO: namespace container-runtime-5698 deletion completed in 6.089267296s

• [SLOW TEST:8.265 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:53:25.770: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1115
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1115, will wait for the garbage collector to delete the pods
Apr 30 00:53:27.992: INFO: Deleting Job.batch foo took: 11.084044ms
Apr 30 00:53:28.292: INFO: Terminating Job.batch foo pods took: 300.170699ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:54:02.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1115" for this suite.
Apr 30 00:54:08.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:54:08.209: INFO: namespace job-1115 deletion completed in 6.106744407s

• [SLOW TEST:42.439 seconds]
[sig-apps] Job
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:54:08.209: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-728
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Apr 30 00:54:08.354: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 30 00:54:08.366: INFO: Waiting for terminating namespaces to be deleted...
Apr 30 00:54:08.368: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-144-27.us-west-2.compute.internal before test
Apr 30 00:54:08.377: INFO: aws-node-rccgv from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.377: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:54:08.377: INFO: kube-proxy-4xrfl from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.377: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:54:08.377: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-2ffxh from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:54:08.377: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:54:08.377: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:54:08.377: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-148-117.us-west-2.compute.internal before test
Apr 30 00:54:08.386: INFO: aws-node-wzq6b from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.386: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:54:08.386: INFO: kube-proxy-b4plm from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.386: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:54:08.386: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-69sgr from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:54:08.386: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:54:08.386: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:54:08.386: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-149-254.us-west-2.compute.internal before test
Apr 30 00:54:08.395: INFO: aws-node-kcbxd from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.395: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:54:08.395: INFO: kube-proxy-jvnfx from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.395: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:54:08.395: INFO: sonobuoy from sonobuoy started at 2020-04-29 23:02:27 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.395: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 30 00:54:08.395: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-xtlz4 from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:54:08.395: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:54:08.395: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:54:08.395: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-197-211.us-west-2.compute.internal before test
Apr 30 00:54:08.403: INFO: kube-proxy-xhpsv from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.403: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:54:08.403: INFO: aws-node-ffvhf from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.403: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:54:08.403: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-wlv6l from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:54:08.403: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:54:08.403: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:54:08.403: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-211-77.us-west-2.compute.internal before test
Apr 30 00:54:08.409: INFO: aws-node-xj5sd from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.409: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:54:08.409: INFO: kube-proxy-fck6k from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.409: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:54:08.409: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-xpbv9 from sonobuoy started at 2020-04-29 23:02:31 +0000 UTC (2 container statuses recorded)
Apr 30 00:54:08.409: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:54:08.409: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:54:08.409: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-218-192.us-west-2.compute.internal before test
Apr 30 00:54:08.416: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-8cg7t from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:54:08.416: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:54:08.416: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:54:08.416: INFO: aws-node-s9vwd from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.416: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:54:08.416: INFO: kube-proxy-b6znl from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.416: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:54:08.416: INFO: sonobuoy-e2e-job-c0680a6adde246b1 from sonobuoy started at 2020-04-29 23:02:31 +0000 UTC (2 container statuses recorded)
Apr 30 00:54:08.416: INFO: 	Container e2e ready: true, restart count 0
Apr 30 00:54:08.416: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 30 00:54:08.416: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-222-85.us-west-2.compute.internal before test
Apr 30 00:54:08.424: INFO: aws-node-l9blv from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.424: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:54:08.424: INFO: kube-proxy-fh79j from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.424: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:54:08.424: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-qh6r9 from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:54:08.424: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:54:08.424: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:54:08.424: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-72-199.us-west-2.compute.internal before test
Apr 30 00:54:08.431: INFO: kube-proxy-kklzp from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.431: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:54:08.431: INFO: aws-node-zldl6 from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.431: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:54:08.431: INFO: coredns-5c97f79574-9t82l from kube-system started at 2020-04-29 22:50:59 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.431: INFO: 	Container coredns ready: true, restart count 0
Apr 30 00:54:08.431: INFO: coredns-5c97f79574-wjwsw from kube-system started at 2020-04-29 22:50:59 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.431: INFO: 	Container coredns ready: true, restart count 0
Apr 30 00:54:08.431: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-sg2vq from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:54:08.431: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:54:08.431: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:54:08.431: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-82-130.us-west-2.compute.internal before test
Apr 30 00:54:08.438: INFO: kube-proxy-8pccx from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.438: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:54:08.438: INFO: aws-node-rbjxk from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.438: INFO: 	Container aws-node ready: true, restart count 0
Apr 30 00:54:08.438: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-ptxhd from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:54:08.438: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:54:08.438: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:54:08.438: INFO: 
Logging pods the kubelet thinks is on node ip-192-168-83-225.us-west-2.compute.internal before test
Apr 30 00:54:08.445: INFO: sonobuoy-systemd-logs-daemon-set-2b2e0588ced54520-rmfrj from sonobuoy started at 2020-04-29 23:02:32 +0000 UTC (2 container statuses recorded)
Apr 30 00:54:08.445: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 30 00:54:08.445: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 30 00:54:08.445: INFO: kube-proxy-sz6rb from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.445: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 30 00:54:08.445: INFO: aws-node-j8bsf from kube-system started at 2020-04-29 22:50:44 +0000 UTC (1 container statuses recorded)
Apr 30 00:54:08.445: INFO: 	Container aws-node ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-fffdc153-df42-4e91-9cb7-c6c6e7c96bba 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-fffdc153-df42-4e91-9cb7-c6c6e7c96bba off the node ip-192-168-144-27.us-west-2.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-fffdc153-df42-4e91-9cb7-c6c6e7c96bba
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:54:16.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-728" for this suite.
Apr 30 00:54:24.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:54:24.632: INFO: namespace sched-pred-728 deletion completed in 8.091763944s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:16.423 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:54:24.632: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8944
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8944
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8944
STEP: Creating statefulset with conflicting port in namespace statefulset-8944
STEP: Waiting until pod test-pod will start running in namespace statefulset-8944
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8944
Apr 30 00:54:28.800: INFO: Observed stateful pod in namespace: statefulset-8944, name: ss-0, uid: 16faf0ba-1710-4e9b-ba18-66979127c496, status phase: Failed. Waiting for statefulset controller to delete.
Apr 30 00:54:28.806: INFO: Observed stateful pod in namespace: statefulset-8944, name: ss-0, uid: 16faf0ba-1710-4e9b-ba18-66979127c496, status phase: Failed. Waiting for statefulset controller to delete.
Apr 30 00:54:28.812: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8944
STEP: Removing pod with conflicting port in namespace statefulset-8944
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8944 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Apr 30 00:54:32.829: INFO: Deleting all statefulset in ns statefulset-8944
Apr 30 00:54:32.831: INFO: Scaling statefulset ss to 0
Apr 30 00:54:42.841: INFO: Waiting for statefulset status.replicas updated to 0
Apr 30 00:54:42.843: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:54:42.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8944" for this suite.
Apr 30 00:54:48.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:54:48.949: INFO: namespace statefulset-8944 deletion completed in 6.091654783s

• [SLOW TEST:24.317 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:54:48.950: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6953
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 30 00:54:49.102: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0dc33488-43ec-4a4c-b4e9-fb82a63424cd" in namespace "projected-6953" to be "success or failure"
Apr 30 00:54:49.105: INFO: Pod "downwardapi-volume-0dc33488-43ec-4a4c-b4e9-fb82a63424cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.484352ms
Apr 30 00:54:51.108: INFO: Pod "downwardapi-volume-0dc33488-43ec-4a4c-b4e9-fb82a63424cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005270862s
STEP: Saw pod success
Apr 30 00:54:51.108: INFO: Pod "downwardapi-volume-0dc33488-43ec-4a4c-b4e9-fb82a63424cd" satisfied condition "success or failure"
Apr 30 00:54:51.110: INFO: Trying to get logs from node ip-192-168-83-225.us-west-2.compute.internal pod downwardapi-volume-0dc33488-43ec-4a4c-b4e9-fb82a63424cd container client-container: <nil>
STEP: delete the pod
Apr 30 00:54:51.124: INFO: Waiting for pod downwardapi-volume-0dc33488-43ec-4a4c-b4e9-fb82a63424cd to disappear
Apr 30 00:54:51.126: INFO: Pod downwardapi-volume-0dc33488-43ec-4a4c-b4e9-fb82a63424cd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:54:51.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6953" for this suite.
Apr 30 00:54:57.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:54:57.220: INFO: namespace projected-6953 deletion completed in 6.089858051s

• [SLOW TEST:8.271 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:54:57.221: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2704
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-64645a22-245a-45b8-9ab0-3d0498d759ba
STEP: Creating a pod to test consume secrets
Apr 30 00:54:57.390: INFO: Waiting up to 5m0s for pod "pod-secrets-d4a1a589-3b0d-446c-a181-5b896b188756" in namespace "secrets-2704" to be "success or failure"
Apr 30 00:54:57.392: INFO: Pod "pod-secrets-d4a1a589-3b0d-446c-a181-5b896b188756": Phase="Pending", Reason="", readiness=false. Elapsed: 1.935772ms
Apr 30 00:54:59.394: INFO: Pod "pod-secrets-d4a1a589-3b0d-446c-a181-5b896b188756": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004742975s
STEP: Saw pod success
Apr 30 00:54:59.394: INFO: Pod "pod-secrets-d4a1a589-3b0d-446c-a181-5b896b188756" satisfied condition "success or failure"
Apr 30 00:54:59.396: INFO: Trying to get logs from node ip-192-168-82-130.us-west-2.compute.internal pod pod-secrets-d4a1a589-3b0d-446c-a181-5b896b188756 container secret-volume-test: <nil>
STEP: delete the pod
Apr 30 00:54:59.411: INFO: Waiting for pod pod-secrets-d4a1a589-3b0d-446c-a181-5b896b188756 to disappear
Apr 30 00:54:59.413: INFO: Pod pod-secrets-d4a1a589-3b0d-446c-a181-5b896b188756 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:54:59.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2704" for this suite.
Apr 30 00:55:05.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:55:05.516: INFO: namespace secrets-2704 deletion completed in 6.098932344s

• [SLOW TEST:8.295 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:55:05.516: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8068
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 30 00:55:05.663: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr 30 00:55:05.669: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 30 00:55:10.672: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 30 00:55:10.672: INFO: Creating deployment "test-rolling-update-deployment"
Apr 30 00:55:10.676: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr 30 00:55:10.688: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr 30 00:55:12.698: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr 30 00:55:12.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723804910, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723804910, loc:(*time.Location)(0x789e8e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63723804912, loc:(*time.Location)(0x789e8e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63723804910, loc:(*time.Location)(0x789e8e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 30 00:55:14.706: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Apr 30 00:55:14.715: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8068 /apis/apps/v1/namespaces/deployment-8068/deployments/test-rolling-update-deployment c2963cfb-c334-40f3-b377-a6187f602e7a 38136 1 2020-04-30 00:55:10 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0021cefe8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-04-30 00:55:10 +0000 UTC,LastTransitionTime:2020-04-30 00:55:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-04-30 00:55:12 +0000 UTC,LastTransitionTime:2020-04-30 00:55:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 30 00:55:14.718: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-8068 /apis/apps/v1/namespaces/deployment-8068/replicasets/test-rolling-update-deployment-55d946486 91ca41f9-cdb6-47cc-9f8b-b6b0ceeb5857 38125 1 2020-04-30 00:55:10 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment c2963cfb-c334-40f3-b377-a6187f602e7a 0xc0021cf870 0xc0021cf871}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0021cf918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 30 00:55:14.718: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr 30 00:55:14.718: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8068 /apis/apps/v1/namespaces/deployment-8068/replicasets/test-rolling-update-controller 820b1085-47de-427c-a566-1bbb4b85aa14 38135 2 2020-04-30 00:55:05 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment c2963cfb-c334-40f3-b377-a6187f602e7a 0xc0021cf727 0xc0021cf728}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0021cf7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 30 00:55:14.721: INFO: Pod "test-rolling-update-deployment-55d946486-sbfwb" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-sbfwb test-rolling-update-deployment-55d946486- deployment-8068 /api/v1/namespaces/deployment-8068/pods/test-rolling-update-deployment-55d946486-sbfwb 28594ece-1c99-43f1-b4ee-56b721524bdf 38124 0 2020-04-30 00:55:10 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 91ca41f9-cdb6-47cc-9f8b-b6b0ceeb5857 0xc0021cff10 0xc0021cff11}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zfjpc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zfjpc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zfjpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-192-168-149-254.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-30 00:55:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-30 00:55:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-30 00:55:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-30 00:55:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.149.254,PodIP:192.168.153.153,StartTime:2020-04-30 00:55:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-30 00:55:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://a4f9a38ddc0d12a576cd723a35d8bf878396eec0f828db3126ee2465e6f483e6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.153.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:55:14.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8068" for this suite.
Apr 30 00:55:20.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:55:20.820: INFO: namespace deployment-8068 deletion completed in 6.095186047s

• [SLOW TEST:15.304 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 30 00:55:20.820: INFO: >>> kubeConfig: /tmp/kubeconfig-371654096
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-249
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-249.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-249.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-249.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-249.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-249.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-249.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 30 00:55:22.999: INFO: DNS probes using dns-249/dns-test-b120285a-9611-4496-a07d-c21f85a6802a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 30 00:55:23.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-249" for this suite.
Apr 30 00:55:29.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 30 00:55:29.108: INFO: namespace dns-249 deletion completed in 6.0926469s

• [SLOW TEST:8.288 seconds]
[sig-network] DNS
/workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.8-beta.0.65+ef1ba35b1a4560/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSApr 30 00:55:29.108: INFO: Running AfterSuite actions on all nodes
Apr 30 00:55:29.108: INFO: Running AfterSuite actions on node 1
Apr 30 00:55:29.108: INFO: Skipping dumping logs from cluster

Ran 276 of 4731 Specs in 6762.065 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4455 Skipped
PASS

Ginkgo ran 1 suite in 1h52m43.089481415s
Test Suite Passed
