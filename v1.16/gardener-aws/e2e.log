Conformance test: not doing test setup.
I1203 14:40:03.194620    5076 e2e.go:92] Starting e2e run "3efac81b-2654-45a6-ada6-042aa5d94e8d" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1575384001 - Will randomize all specs
Will run 276 of 4732 specs

Dec  3 14:40:03.425: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Deleting namespaces
STEP: Waiting for namespaces to vanish
I1203 14:40:03.790270    5076 suites.go:70] Waiting for deletion of the following namespaces: []
Dec  3 14:40:05.880: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec  3 14:40:06.149: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec  3 14:40:06.531: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec  3 14:40:06.531: INFO: expected 12 pod replicas in namespace 'kube-system', 12 are Running and Ready.
Dec  3 14:40:06.531: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec  3 14:40:06.629: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec  3 14:40:06.629: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec  3 14:40:06.629: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Dec  3 14:40:06.629: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Dec  3 14:40:06.629: INFO: e2e test version: v1.16.3
Dec  3 14:40:06.717: INFO: kube-apiserver version: v1.16.3
Dec  3 14:40:06.717: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:40:06.809: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:40:06.810: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
Dec  3 14:40:07.169: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Dec  3 14:40:07.439: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec  3 14:40:07.897: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-9755'
Dec  3 14:40:08.351: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 14:40:08.351: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Dec  3 14:40:08.530: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-pt9vd]
Dec  3 14:40:08.530: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-pt9vd" in namespace "kubectl-9755" to be "running and ready"
Dec  3 14:40:08.619: INFO: Pod "e2e-test-httpd-rc-pt9vd": Phase="Pending", Reason="", readiness=false. Elapsed: 89.295515ms
Dec  3 14:40:10.709: INFO: Pod "e2e-test-httpd-rc-pt9vd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.179457362s
Dec  3 14:40:12.799: INFO: Pod "e2e-test-httpd-rc-pt9vd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.269160057s
Dec  3 14:40:14.889: INFO: Pod "e2e-test-httpd-rc-pt9vd": Phase="Running", Reason="", readiness=true. Elapsed: 6.359270733s
Dec  3 14:40:14.889: INFO: Pod "e2e-test-httpd-rc-pt9vd" satisfied condition "running and ready"
Dec  3 14:40:14.889: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-pt9vd]
Dec  3 14:40:14.889: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs rc/e2e-test-httpd-rc --namespace=kubectl-9755'
Dec  3 14:40:15.676: INFO: stderr: ""
Dec  3 14:40:15.685: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 100.64.1.8. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 100.64.1.8. Set the 'ServerName' directive globally to suppress this message\n[Tue Dec 03 14:40:13.517081 2019] [mpm_event:notice] [pid 1:tid 140668145372008] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Tue Dec 03 14:40:13.517130 2019] [core:notice] [pid 1:tid 140668145372008] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Dec  3 14:40:15.685: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-httpd-rc --namespace=kubectl-9755'
Dec  3 14:40:16.219: INFO: stderr: ""
Dec  3 14:40:16.219: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:40:16.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9755" for this suite.
Dec  3 14:40:28.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:40:31.709: INFO: namespace kubectl-9755 deletion completed in 15.397777086s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:40:31.709: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8240
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-972290d4-6c7c-4a33-8e2d-1e36accfe01c
STEP: Creating a pod to test consume configMaps
Dec  3 14:40:32.537: INFO: Waiting up to 5m0s for pod "pod-configmaps-375a394a-35fa-42c0-981e-3df77f959c99" in namespace "configmap-8240" to be "success or failure"
Dec  3 14:40:32.626: INFO: Pod "pod-configmaps-375a394a-35fa-42c0-981e-3df77f959c99": Phase="Pending", Reason="", readiness=false. Elapsed: 89.289453ms
Dec  3 14:40:34.716: INFO: Pod "pod-configmaps-375a394a-35fa-42c0-981e-3df77f959c99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179292636s
STEP: Saw pod success
Dec  3 14:40:34.716: INFO: Pod "pod-configmaps-375a394a-35fa-42c0-981e-3df77f959c99" satisfied condition "success or failure"
Dec  3 14:40:34.806: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-configmaps-375a394a-35fa-42c0-981e-3df77f959c99 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 14:40:35.130: INFO: Waiting for pod pod-configmaps-375a394a-35fa-42c0-981e-3df77f959c99 to disappear
Dec  3 14:40:35.220: INFO: Pod pod-configmaps-375a394a-35fa-42c0-981e-3df77f959c99 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:40:35.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8240" for this suite.
Dec  3 14:40:41.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:40:44.718: INFO: namespace configmap-8240 deletion completed in 9.408561225s
â€¢SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:40:44.718: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 14:40:45.448: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cc547f3c-aef0-4733-972f-be22a3d554b7" in namespace "downward-api-696" to be "success or failure"
Dec  3 14:40:45.538: INFO: Pod "downwardapi-volume-cc547f3c-aef0-4733-972f-be22a3d554b7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.561658ms
Dec  3 14:40:47.628: INFO: Pod "downwardapi-volume-cc547f3c-aef0-4733-972f-be22a3d554b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180124483s
STEP: Saw pod success
Dec  3 14:40:47.628: INFO: Pod "downwardapi-volume-cc547f3c-aef0-4733-972f-be22a3d554b7" satisfied condition "success or failure"
Dec  3 14:40:47.718: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-cc547f3c-aef0-4733-972f-be22a3d554b7 container client-container: <nil>
STEP: delete the pod
Dec  3 14:40:47.906: INFO: Waiting for pod downwardapi-volume-cc547f3c-aef0-4733-972f-be22a3d554b7 to disappear
Dec  3 14:40:47.995: INFO: Pod downwardapi-volume-cc547f3c-aef0-4733-972f-be22a3d554b7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:40:47.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-696" for this suite.
Dec  3 14:40:54.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:40:57.480: INFO: namespace downward-api-696 deletion completed in 9.393855555s
â€¢SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:40:57.480: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4191
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 14:40:58.122: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec  3 14:41:03.161: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4191 create -f -'
Dec  3 14:41:25.021: INFO: stderr: ""
Dec  3 14:41:25.022: INFO: stdout: "e2e-test-crd-publish-openapi-9357-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec  3 14:41:25.022: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4191 delete e2e-test-crd-publish-openapi-9357-crds test-cr'
Dec  3 14:41:30.543: INFO: stderr: ""
Dec  3 14:41:30.544: INFO: stdout: "e2e-test-crd-publish-openapi-9357-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec  3 14:41:30.544: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4191 apply -f -'
Dec  3 14:41:31.447: INFO: stderr: ""
Dec  3 14:41:31.447: INFO: stdout: "e2e-test-crd-publish-openapi-9357-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec  3 14:41:31.447: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-4191 delete e2e-test-crd-publish-openapi-9357-crds test-cr'
Dec  3 14:41:32.048: INFO: stderr: ""
Dec  3 14:41:32.048: INFO: stdout: "e2e-test-crd-publish-openapi-9357-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec  3 14:41:32.049: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config explain e2e-test-crd-publish-openapi-9357-crds'
Dec  3 14:41:32.991: INFO: stderr: ""
Dec  3 14:41:32.991: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9357-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:41:38.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4191" for this suite.
Dec  3 14:41:45.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:41:48.355: INFO: namespace crd-publish-openapi-4191 deletion completed in 9.410229354s
â€¢SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:41:48.356: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9221
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:42:49.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9221" for this suite.
Dec  3 14:43:17.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:43:20.713: INFO: namespace container-probe-9221 deletion completed in 31.403816526s
â€¢SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:43:20.714: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7811
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7811
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-7811
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7811
Dec  3 14:43:21.623: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Dec  3 14:43:31.714: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec  3 14:43:31.804: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7811 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  3 14:43:33.173: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  3 14:43:33.173: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  3 14:43:33.173: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  3 14:43:33.264: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  3 14:43:43.354: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 14:43:43.354: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 14:43:43.715: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998959s
Dec  3 14:43:44.806: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.909667645s
Dec  3 14:43:45.897: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.818229111s
Dec  3 14:43:46.987: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.72744258s
Dec  3 14:43:48.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.636801349s
Dec  3 14:43:49.176: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.54574477s
Dec  3 14:43:50.266: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.448888094s
Dec  3 14:43:51.357: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.35801794s
Dec  3 14:43:52.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.267288517s
Dec  3 14:43:53.538: INFO: Verifying statefulset ss doesn't scale past 3 for another 177.035349ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7811
Dec  3 14:43:54.629: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7811 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 14:43:55.951: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  3 14:43:55.951: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  3 14:43:55.951: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  3 14:43:55.951: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7811 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 14:43:57.253: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  3 14:43:57.253: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  3 14:43:57.253: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  3 14:43:57.253: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7811 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 14:43:58.519: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  3 14:43:58.520: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  3 14:43:58.520: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  3 14:43:58.609: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 14:43:58.609: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 14:43:58.609: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec  3 14:43:58.699: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7811 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  3 14:44:00.008: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  3 14:44:00.008: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  3 14:44:00.008: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  3 14:44:00.008: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7811 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  3 14:44:01.320: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  3 14:44:01.320: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  3 14:44:01.320: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  3 14:44:01.320: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-7811 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  3 14:44:02.624: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  3 14:44:02.624: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  3 14:44:02.624: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  3 14:44:02.624: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 14:44:02.714: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec  3 14:44:12.894: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 14:44:12.894: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 14:44:12.894: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 14:44:13.163: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Dec  3 14:44:13.163: INFO: ss-0  ip-10-250-27-214.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:43:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:44:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:44:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:43:21 +0000 UTC  }]
Dec  3 14:44:13.163: INFO: ss-1  ip-10-250-10-98.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:43:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:44:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:44:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:43:43 +0000 UTC  }]
Dec  3 14:44:13.163: INFO: ss-2  ip-10-250-10-98.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:43:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:44:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:44:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:43:43 +0000 UTC  }]
Dec  3 14:44:13.163: INFO: 
Dec  3 14:44:13.163: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  3 14:44:14.254: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Dec  3 14:44:14.254: INFO: ss-0  ip-10-250-27-214.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:43:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:44:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:44:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:43:21 +0000 UTC  }]
Dec  3 14:44:14.254: INFO: ss-1  ip-10-250-10-98.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:43:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:44:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:44:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:43:43 +0000 UTC  }]
Dec  3 14:44:14.254: INFO: ss-2  ip-10-250-10-98.ec2.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:43:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:44:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:44:03 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-03 14:43:43 +0000 UTC  }]
Dec  3 14:44:14.254: INFO: 
Dec  3 14:44:14.254: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  3 14:44:15.344: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.818770709s
Dec  3 14:44:16.434: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.728657541s
Dec  3 14:44:17.524: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.638440631s
Dec  3 14:44:18.615: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.54818401s
Dec  3 14:44:19.704: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.45815965s
Dec  3 14:44:20.794: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.36827801s
Dec  3 14:44:21.884: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.278425682s
Dec  3 14:44:22.974: INFO: Verifying statefulset ss doesn't scale past 0 for another 188.443605ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7811
Dec  3 14:44:24.068: INFO: Scaling statefulset ss to 0
Dec  3 14:44:24.337: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec  3 14:44:24.427: INFO: Deleting all statefulset in ns statefulset-7811
Dec  3 14:44:24.517: INFO: Scaling statefulset ss to 0
Dec  3 14:44:24.786: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 14:44:24.875: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:44:25.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7811" for this suite.
Dec  3 14:44:31.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:44:34.590: INFO: namespace statefulset-7811 deletion completed in 9.355616003s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:44:34.591: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6720
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  3 14:44:35.320: INFO: Waiting up to 5m0s for pod "pod-ca7acbdb-3e85-4ebf-a5f9-3c01bb780d08" in namespace "emptydir-6720" to be "success or failure"
Dec  3 14:44:35.410: INFO: Pod "pod-ca7acbdb-3e85-4ebf-a5f9-3c01bb780d08": Phase="Pending", Reason="", readiness=false. Elapsed: 89.730749ms
Dec  3 14:44:37.500: INFO: Pod "pod-ca7acbdb-3e85-4ebf-a5f9-3c01bb780d08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179619137s
STEP: Saw pod success
Dec  3 14:44:37.500: INFO: Pod "pod-ca7acbdb-3e85-4ebf-a5f9-3c01bb780d08" satisfied condition "success or failure"
Dec  3 14:44:37.589: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-ca7acbdb-3e85-4ebf-a5f9-3c01bb780d08 container test-container: <nil>
STEP: delete the pod
Dec  3 14:44:37.913: INFO: Waiting for pod pod-ca7acbdb-3e85-4ebf-a5f9-3c01bb780d08 to disappear
Dec  3 14:44:38.002: INFO: Pod pod-ca7acbdb-3e85-4ebf-a5f9-3c01bb780d08 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:44:38.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6720" for this suite.
Dec  3 14:44:44.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:44:47.501: INFO: namespace emptydir-6720 deletion completed in 9.406994456s
â€¢S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:44:47.501: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3091
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 14:44:49.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981089, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981089, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981089, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981089, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 14:44:51.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981089, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981089, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981089, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981089, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 14:44:53.716: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981089, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981089, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981089, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981089, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 14:44:56.810: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:44:57.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3091" for this suite.
Dec  3 14:45:03.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:45:06.944: INFO: namespace webhook-3091 deletion completed in 9.405108352s
STEP: Destroying namespace "webhook-3091-markers" for this suite.
Dec  3 14:45:13.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:45:16.344: INFO: namespace webhook-3091-markers deletion completed in 9.400185146s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:45:16.703: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4550
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-sgbs5 in namespace proxy-4550
I1203 14:45:17.525761    5076 runners.go:184] Created replication controller with name: proxy-service-sgbs5, namespace: proxy-4550, replica count: 1
I1203 14:45:18.626439    5076 runners.go:184] proxy-service-sgbs5 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1203 14:45:19.626753    5076 runners.go:184] proxy-service-sgbs5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 14:45:20.627075    5076 runners.go:184] proxy-service-sgbs5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 14:45:21.627418    5076 runners.go:184] proxy-service-sgbs5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 14:45:22.627644    5076 runners.go:184] proxy-service-sgbs5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 14:45:23.627922    5076 runners.go:184] proxy-service-sgbs5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 14:45:24.628179    5076 runners.go:184] proxy-service-sgbs5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 14:45:25.628549    5076 runners.go:184] proxy-service-sgbs5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 14:45:26.628787    5076 runners.go:184] proxy-service-sgbs5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 14:45:27.629046    5076 runners.go:184] proxy-service-sgbs5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 14:45:28.629349    5076 runners.go:184] proxy-service-sgbs5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1203 14:45:29.629600    5076 runners.go:184] proxy-service-sgbs5 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  3 14:45:29.719: INFO: setup took 12.37913221s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec  3 14:45:29.821: INFO: (0) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 101.417086ms)
Dec  3 14:45:29.822: INFO: (0) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 102.288101ms)
Dec  3 14:45:29.822: INFO: (0) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 102.435923ms)
Dec  3 14:45:29.822: INFO: (0) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 102.448944ms)
Dec  3 14:45:29.822: INFO: (0) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 102.343676ms)
Dec  3 14:45:29.822: INFO: (0) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 102.314605ms)
Dec  3 14:45:29.826: INFO: (0) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 106.825176ms)
Dec  3 14:45:29.827: INFO: (0) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 108.043508ms)
Dec  3 14:45:29.829: INFO: (0) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 109.458707ms)
Dec  3 14:45:29.830: INFO: (0) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 110.818275ms)
Dec  3 14:45:29.900: INFO: (0) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 180.201524ms)
Dec  3 14:45:29.900: INFO: (0) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 180.268871ms)
Dec  3 14:45:29.900: INFO: (0) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 180.285072ms)
Dec  3 14:45:29.902: INFO: (0) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 183.000031ms)
Dec  3 14:45:29.904: INFO: (0) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 184.401995ms)
Dec  3 14:45:29.904: INFO: (0) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 184.405714ms)
Dec  3 14:45:29.996: INFO: (1) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 91.860454ms)
Dec  3 14:45:29.996: INFO: (1) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 91.90973ms)
Dec  3 14:45:29.997: INFO: (1) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 92.924529ms)
Dec  3 14:45:29.997: INFO: (1) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 92.965864ms)
Dec  3 14:45:29.997: INFO: (1) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 93.023734ms)
Dec  3 14:45:29.997: INFO: (1) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 93.040739ms)
Dec  3 14:45:29.997: INFO: (1) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 93.047347ms)
Dec  3 14:45:29.997: INFO: (1) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 93.289166ms)
Dec  3 14:45:29.998: INFO: (1) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 94.359978ms)
Dec  3 14:45:29.998: INFO: (1) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 94.442284ms)
Dec  3 14:45:29.998: INFO: (1) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 94.353833ms)
Dec  3 14:45:29.998: INFO: (1) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 94.5899ms)
Dec  3 14:45:29.998: INFO: (1) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 94.425932ms)
Dec  3 14:45:30.000: INFO: (1) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 95.737967ms)
Dec  3 14:45:30.001: INFO: (1) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 97.125297ms)
Dec  3 14:45:30.001: INFO: (1) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 97.288894ms)
Dec  3 14:45:30.094: INFO: (2) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 91.902485ms)
Dec  3 14:45:30.094: INFO: (2) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 92.356155ms)
Dec  3 14:45:30.094: INFO: (2) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 92.096483ms)
Dec  3 14:45:30.094: INFO: (2) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 91.651977ms)
Dec  3 14:45:30.094: INFO: (2) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 92.401679ms)
Dec  3 14:45:30.094: INFO: (2) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 91.943715ms)
Dec  3 14:45:30.094: INFO: (2) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 91.821142ms)
Dec  3 14:45:30.094: INFO: (2) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 92.337678ms)
Dec  3 14:45:30.094: INFO: (2) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 91.882132ms)
Dec  3 14:45:30.094: INFO: (2) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 92.149244ms)
Dec  3 14:45:30.094: INFO: (2) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 92.55861ms)
Dec  3 14:45:30.094: INFO: (2) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 92.61892ms)
Dec  3 14:45:30.096: INFO: (2) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 93.45781ms)
Dec  3 14:45:30.097: INFO: (2) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 94.981633ms)
Dec  3 14:45:30.099: INFO: (2) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 96.535978ms)
Dec  3 14:45:30.099: INFO: (2) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 97.162769ms)
Dec  3 14:45:30.192: INFO: (3) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 92.762044ms)
Dec  3 14:45:30.192: INFO: (3) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 92.698852ms)
Dec  3 14:45:30.192: INFO: (3) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 92.777015ms)
Dec  3 14:45:30.192: INFO: (3) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 92.769514ms)
Dec  3 14:45:30.192: INFO: (3) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 92.930079ms)
Dec  3 14:45:30.192: INFO: (3) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 92.963339ms)
Dec  3 14:45:30.192: INFO: (3) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 92.835907ms)
Dec  3 14:45:30.192: INFO: (3) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 93.067886ms)
Dec  3 14:45:30.192: INFO: (3) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 93.076046ms)
Dec  3 14:45:30.192: INFO: (3) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 92.990844ms)
Dec  3 14:45:30.192: INFO: (3) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 93.042852ms)
Dec  3 14:45:30.192: INFO: (3) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 92.943415ms)
Dec  3 14:45:30.193: INFO: (3) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 94.417582ms)
Dec  3 14:45:30.193: INFO: (3) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 94.523703ms)
Dec  3 14:45:30.195: INFO: (3) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 95.92568ms)
Dec  3 14:45:30.196: INFO: (3) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 97.400144ms)
Dec  3 14:45:30.288: INFO: (4) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 91.371835ms)
Dec  3 14:45:30.288: INFO: (4) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 91.795447ms)
Dec  3 14:45:30.288: INFO: (4) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 91.965809ms)
Dec  3 14:45:30.288: INFO: (4) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 91.978417ms)
Dec  3 14:45:30.288: INFO: (4) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 91.978567ms)
Dec  3 14:45:30.288: INFO: (4) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 92.270964ms)
Dec  3 14:45:30.288: INFO: (4) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 91.976178ms)
Dec  3 14:45:30.289: INFO: (4) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 92.119259ms)
Dec  3 14:45:30.289: INFO: (4) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 92.105265ms)
Dec  3 14:45:30.290: INFO: (4) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 93.400097ms)
Dec  3 14:45:30.290: INFO: (4) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 93.403141ms)
Dec  3 14:45:30.290: INFO: (4) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 93.377967ms)
Dec  3 14:45:30.290: INFO: (4) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 93.444167ms)
Dec  3 14:45:30.291: INFO: (4) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 94.845811ms)
Dec  3 14:45:30.292: INFO: (4) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 95.099382ms)
Dec  3 14:45:30.294: INFO: (4) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 98.002764ms)
Dec  3 14:45:30.388: INFO: (5) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 92.992002ms)
Dec  3 14:45:30.388: INFO: (5) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 93.047447ms)
Dec  3 14:45:30.388: INFO: (5) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 93.072944ms)
Dec  3 14:45:30.388: INFO: (5) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 93.669865ms)
Dec  3 14:45:30.389: INFO: (5) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 94.785833ms)
Dec  3 14:45:30.389: INFO: (5) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 94.800582ms)
Dec  3 14:45:30.390: INFO: (5) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 94.972083ms)
Dec  3 14:45:30.390: INFO: (5) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 94.921351ms)
Dec  3 14:45:30.390: INFO: (5) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 94.844846ms)
Dec  3 14:45:30.390: INFO: (5) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 94.860929ms)
Dec  3 14:45:30.390: INFO: (5) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 94.897207ms)
Dec  3 14:45:30.390: INFO: (5) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 94.912552ms)
Dec  3 14:45:30.390: INFO: (5) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 94.955178ms)
Dec  3 14:45:30.390: INFO: (5) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 94.983506ms)
Dec  3 14:45:30.391: INFO: (5) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 96.60354ms)
Dec  3 14:45:30.391: INFO: (5) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 96.496456ms)
Dec  3 14:45:30.483: INFO: (6) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 91.950735ms)
Dec  3 14:45:30.483: INFO: (6) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 92.09244ms)
Dec  3 14:45:30.483: INFO: (6) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 92.239601ms)
Dec  3 14:45:30.483: INFO: (6) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 92.126927ms)
Dec  3 14:45:30.483: INFO: (6) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 92.050352ms)
Dec  3 14:45:30.483: INFO: (6) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 92.140388ms)
Dec  3 14:45:30.483: INFO: (6) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 92.204433ms)
Dec  3 14:45:30.484: INFO: (6) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 92.968728ms)
Dec  3 14:45:30.485: INFO: (6) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 93.970994ms)
Dec  3 14:45:30.485: INFO: (6) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 94.073696ms)
Dec  3 14:45:30.485: INFO: (6) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 94.086932ms)
Dec  3 14:45:30.485: INFO: (6) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 94.103016ms)
Dec  3 14:45:30.486: INFO: (6) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 94.643869ms)
Dec  3 14:45:30.487: INFO: (6) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 96.003714ms)
Dec  3 14:45:30.487: INFO: (6) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 96.031992ms)
Dec  3 14:45:30.487: INFO: (6) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 96.148332ms)
Dec  3 14:45:30.580: INFO: (7) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 92.383862ms)
Dec  3 14:45:30.580: INFO: (7) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 92.402329ms)
Dec  3 14:45:30.580: INFO: (7) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 92.476372ms)
Dec  3 14:45:30.580: INFO: (7) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 92.474137ms)
Dec  3 14:45:30.580: INFO: (7) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 92.440838ms)
Dec  3 14:45:30.580: INFO: (7) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 92.584071ms)
Dec  3 14:45:30.580: INFO: (7) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 92.448631ms)
Dec  3 14:45:30.580: INFO: (7) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 92.539878ms)
Dec  3 14:45:30.581: INFO: (7) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 92.811207ms)
Dec  3 14:45:30.581: INFO: (7) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 93.078544ms)
Dec  3 14:45:30.581: INFO: (7) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 93.117674ms)
Dec  3 14:45:30.581: INFO: (7) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 93.016392ms)
Dec  3 14:45:30.582: INFO: (7) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 94.550298ms)
Dec  3 14:45:30.584: INFO: (7) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 96.138574ms)
Dec  3 14:45:30.584: INFO: (7) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 96.178684ms)
Dec  3 14:45:30.585: INFO: (7) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 97.674214ms)
Dec  3 14:45:30.679: INFO: (8) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 92.910988ms)
Dec  3 14:45:30.679: INFO: (8) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 92.969205ms)
Dec  3 14:45:30.679: INFO: (8) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 92.914549ms)
Dec  3 14:45:30.679: INFO: (8) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 92.993041ms)
Dec  3 14:45:30.679: INFO: (8) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 92.94971ms)
Dec  3 14:45:30.679: INFO: (8) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 92.999939ms)
Dec  3 14:45:30.679: INFO: (8) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 93.036825ms)
Dec  3 14:45:30.679: INFO: (8) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 93.068099ms)
Dec  3 14:45:30.680: INFO: (8) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 94.323511ms)
Dec  3 14:45:30.680: INFO: (8) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 94.309415ms)
Dec  3 14:45:30.680: INFO: (8) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 94.251821ms)
Dec  3 14:45:30.680: INFO: (8) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 94.273754ms)
Dec  3 14:45:30.681: INFO: (8) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 95.332501ms)
Dec  3 14:45:30.681: INFO: (8) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 95.190817ms)
Dec  3 14:45:30.682: INFO: (8) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 96.781576ms)
Dec  3 14:45:30.684: INFO: (8) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 98.254756ms)
Dec  3 14:45:30.779: INFO: (9) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 94.488602ms)
Dec  3 14:45:30.781: INFO: (9) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 96.799942ms)
Dec  3 14:45:30.781: INFO: (9) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 97.126389ms)
Dec  3 14:45:30.781: INFO: (9) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 96.822221ms)
Dec  3 14:45:30.781: INFO: (9) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 97.050869ms)
Dec  3 14:45:30.781: INFO: (9) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 96.940108ms)
Dec  3 14:45:30.781: INFO: (9) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 97.033129ms)
Dec  3 14:45:30.781: INFO: (9) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 97.267093ms)
Dec  3 14:45:30.782: INFO: (9) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 98.418886ms)
Dec  3 14:45:30.782: INFO: (9) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 98.431859ms)
Dec  3 14:45:30.783: INFO: (9) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 99.429847ms)
Dec  3 14:45:30.783: INFO: (9) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 99.656001ms)
Dec  3 14:45:30.783: INFO: (9) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 99.416506ms)
Dec  3 14:45:30.784: INFO: (9) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 99.454545ms)
Dec  3 14:45:30.784: INFO: (9) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 99.463893ms)
Dec  3 14:45:30.784: INFO: (9) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 99.676903ms)
Dec  3 14:45:30.890: INFO: (10) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 106.413933ms)
Dec  3 14:45:30.890: INFO: (10) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 106.479606ms)
Dec  3 14:45:30.890: INFO: (10) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 106.521964ms)
Dec  3 14:45:30.891: INFO: (10) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 106.710793ms)
Dec  3 14:45:30.891: INFO: (10) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 106.66108ms)
Dec  3 14:45:30.891: INFO: (10) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 107.031539ms)
Dec  3 14:45:30.891: INFO: (10) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 106.900499ms)
Dec  3 14:45:30.891: INFO: (10) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 107.075933ms)
Dec  3 14:45:30.891: INFO: (10) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 106.965381ms)
Dec  3 14:45:30.891: INFO: (10) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 107.223793ms)
Dec  3 14:45:30.892: INFO: (10) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 108.330421ms)
Dec  3 14:45:30.892: INFO: (10) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 108.406188ms)
Dec  3 14:45:30.893: INFO: (10) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 109.588231ms)
Dec  3 14:45:30.893: INFO: (10) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 109.577197ms)
Dec  3 14:45:30.895: INFO: (10) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 111.153073ms)
Dec  3 14:45:30.895: INFO: (10) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 111.243903ms)
Dec  3 14:45:30.987: INFO: (11) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 92.040909ms)
Dec  3 14:45:30.987: INFO: (11) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 91.915469ms)
Dec  3 14:45:30.987: INFO: (11) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 91.944817ms)
Dec  3 14:45:30.987: INFO: (11) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 92.020944ms)
Dec  3 14:45:30.988: INFO: (11) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 92.354279ms)
Dec  3 14:45:30.988: INFO: (11) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 92.398289ms)
Dec  3 14:45:30.988: INFO: (11) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 92.388985ms)
Dec  3 14:45:30.988: INFO: (11) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 92.48465ms)
Dec  3 14:45:30.988: INFO: (11) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 92.401955ms)
Dec  3 14:45:30.988: INFO: (11) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 92.373807ms)
Dec  3 14:45:30.988: INFO: (11) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 92.54369ms)
Dec  3 14:45:30.989: INFO: (11) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 93.70866ms)
Dec  3 14:45:30.989: INFO: (11) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 94.166554ms)
Dec  3 14:45:30.991: INFO: (11) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 95.606292ms)
Dec  3 14:45:30.991: INFO: (11) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 95.621899ms)
Dec  3 14:45:30.991: INFO: (11) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 95.59833ms)
Dec  3 14:45:31.083: INFO: (12) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 92.111631ms)
Dec  3 14:45:31.083: INFO: (12) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 92.067025ms)
Dec  3 14:45:31.084: INFO: (12) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 93.07836ms)
Dec  3 14:45:31.084: INFO: (12) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 93.012654ms)
Dec  3 14:45:31.084: INFO: (12) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 92.962539ms)
Dec  3 14:45:31.084: INFO: (12) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 92.982862ms)
Dec  3 14:45:31.084: INFO: (12) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 92.96979ms)
Dec  3 14:45:31.086: INFO: (12) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 94.283484ms)
Dec  3 14:45:31.086: INFO: (12) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 94.52083ms)
Dec  3 14:45:31.086: INFO: (12) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 94.655593ms)
Dec  3 14:45:31.086: INFO: (12) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 94.749938ms)
Dec  3 14:45:31.087: INFO: (12) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 95.930994ms)
Dec  3 14:45:31.087: INFO: (12) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 95.948318ms)
Dec  3 14:45:31.087: INFO: (12) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 96.020095ms)
Dec  3 14:45:31.089: INFO: (12) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 97.564015ms)
Dec  3 14:45:31.089: INFO: (12) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 97.563348ms)
Dec  3 14:45:31.191: INFO: (13) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 102.149396ms)
Dec  3 14:45:31.191: INFO: (13) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 102.428873ms)
Dec  3 14:45:31.191: INFO: (13) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 102.287941ms)
Dec  3 14:45:31.191: INFO: (13) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 102.274763ms)
Dec  3 14:45:31.191: INFO: (13) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 102.295916ms)
Dec  3 14:45:31.191: INFO: (13) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 102.29273ms)
Dec  3 14:45:31.191: INFO: (13) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 102.32674ms)
Dec  3 14:45:31.191: INFO: (13) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 102.430054ms)
Dec  3 14:45:31.192: INFO: (13) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 102.778466ms)
Dec  3 14:45:31.192: INFO: (13) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 102.696971ms)
Dec  3 14:45:31.192: INFO: (13) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 102.66935ms)
Dec  3 14:45:31.193: INFO: (13) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 103.479771ms)
Dec  3 14:45:31.193: INFO: (13) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 103.661442ms)
Dec  3 14:45:31.194: INFO: (13) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 105.348096ms)
Dec  3 14:45:31.194: INFO: (13) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 105.214013ms)
Dec  3 14:45:31.194: INFO: (13) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 105.262754ms)
Dec  3 14:45:31.286: INFO: (14) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 91.769917ms)
Dec  3 14:45:31.286: INFO: (14) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 91.748634ms)
Dec  3 14:45:31.287: INFO: (14) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 92.73106ms)
Dec  3 14:45:31.287: INFO: (14) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 92.875453ms)
Dec  3 14:45:31.287: INFO: (14) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 92.719456ms)
Dec  3 14:45:31.287: INFO: (14) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 92.65699ms)
Dec  3 14:45:31.287: INFO: (14) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 92.662821ms)
Dec  3 14:45:31.287: INFO: (14) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 92.691521ms)
Dec  3 14:45:31.289: INFO: (14) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 94.139595ms)
Dec  3 14:45:31.289: INFO: (14) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 94.14653ms)
Dec  3 14:45:31.289: INFO: (14) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 94.1825ms)
Dec  3 14:45:31.289: INFO: (14) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 94.214698ms)
Dec  3 14:45:31.289: INFO: (14) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 94.428558ms)
Dec  3 14:45:31.289: INFO: (14) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 94.504895ms)
Dec  3 14:45:31.290: INFO: (14) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 95.623047ms)
Dec  3 14:45:31.292: INFO: (14) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 96.958809ms)
Dec  3 14:45:31.384: INFO: (15) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 91.85692ms)
Dec  3 14:45:31.384: INFO: (15) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 91.844355ms)
Dec  3 14:45:31.384: INFO: (15) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 92.055937ms)
Dec  3 14:45:31.384: INFO: (15) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 91.955071ms)
Dec  3 14:45:31.384: INFO: (15) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 92.376134ms)
Dec  3 14:45:31.384: INFO: (15) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 92.51162ms)
Dec  3 14:45:31.384: INFO: (15) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 92.444622ms)
Dec  3 14:45:31.384: INFO: (15) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 92.436238ms)
Dec  3 14:45:31.384: INFO: (15) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 92.565173ms)
Dec  3 14:45:31.384: INFO: (15) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 92.54524ms)
Dec  3 14:45:31.384: INFO: (15) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 92.481214ms)
Dec  3 14:45:31.384: INFO: (15) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 92.507724ms)
Dec  3 14:45:31.386: INFO: (15) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 94.139472ms)
Dec  3 14:45:31.386: INFO: (15) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 94.273088ms)
Dec  3 14:45:31.387: INFO: (15) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 95.105084ms)
Dec  3 14:45:31.388: INFO: (15) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 96.665605ms)
Dec  3 14:45:31.480: INFO: (16) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 91.722794ms)
Dec  3 14:45:31.480: INFO: (16) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 91.726803ms)
Dec  3 14:45:31.481: INFO: (16) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 91.766895ms)
Dec  3 14:45:31.480: INFO: (16) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 91.802185ms)
Dec  3 14:45:31.481: INFO: (16) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 92.473324ms)
Dec  3 14:45:31.481: INFO: (16) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 92.428931ms)
Dec  3 14:45:31.481: INFO: (16) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 92.417812ms)
Dec  3 14:45:31.481: INFO: (16) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 92.400966ms)
Dec  3 14:45:31.481: INFO: (16) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 92.515751ms)
Dec  3 14:45:31.483: INFO: (16) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 93.929392ms)
Dec  3 14:45:31.483: INFO: (16) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 93.903656ms)
Dec  3 14:45:31.483: INFO: (16) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 94.395688ms)
Dec  3 14:45:31.484: INFO: (16) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 95.587307ms)
Dec  3 14:45:31.484: INFO: (16) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 95.46305ms)
Dec  3 14:45:31.486: INFO: (16) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 96.965507ms)
Dec  3 14:45:31.486: INFO: (16) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 96.960892ms)
Dec  3 14:45:31.579: INFO: (17) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 92.680182ms)
Dec  3 14:45:31.579: INFO: (17) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 93.644135ms)
Dec  3 14:45:31.580: INFO: (17) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 93.580674ms)
Dec  3 14:45:31.580: INFO: (17) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 93.63775ms)
Dec  3 14:45:31.580: INFO: (17) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 93.778473ms)
Dec  3 14:45:31.580: INFO: (17) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 93.646736ms)
Dec  3 14:45:31.580: INFO: (17) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 93.708547ms)
Dec  3 14:45:31.580: INFO: (17) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 93.682634ms)
Dec  3 14:45:31.581: INFO: (17) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 95.150123ms)
Dec  3 14:45:31.581: INFO: (17) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 94.986046ms)
Dec  3 14:45:31.581: INFO: (17) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 94.987652ms)
Dec  3 14:45:31.581: INFO: (17) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 95.107563ms)
Dec  3 14:45:31.582: INFO: (17) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 96.340361ms)
Dec  3 14:45:31.582: INFO: (17) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 96.56437ms)
Dec  3 14:45:31.584: INFO: (17) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 97.948011ms)
Dec  3 14:45:31.585: INFO: (17) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 99.483435ms)
Dec  3 14:45:31.677: INFO: (18) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 91.927671ms)
Dec  3 14:45:31.678: INFO: (18) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 92.103104ms)
Dec  3 14:45:31.677: INFO: (18) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 91.978393ms)
Dec  3 14:45:31.677: INFO: (18) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 91.834685ms)
Dec  3 14:45:31.677: INFO: (18) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 91.773416ms)
Dec  3 14:45:31.678: INFO: (18) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 91.8537ms)
Dec  3 14:45:31.678: INFO: (18) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 92.348962ms)
Dec  3 14:45:31.678: INFO: (18) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 92.309445ms)
Dec  3 14:45:31.678: INFO: (18) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 92.448069ms)
Dec  3 14:45:31.678: INFO: (18) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 92.425987ms)
Dec  3 14:45:31.678: INFO: (18) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 92.520917ms)
Dec  3 14:45:31.678: INFO: (18) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 92.392873ms)
Dec  3 14:45:31.679: INFO: (18) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 93.770274ms)
Dec  3 14:45:31.679: INFO: (18) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 93.71893ms)
Dec  3 14:45:31.681: INFO: (18) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 95.326958ms)
Dec  3 14:45:31.682: INFO: (18) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 96.756313ms)
Dec  3 14:45:31.775: INFO: (19) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">... (200; 92.316745ms)
Dec  3 14:45:31.775: INFO: (19) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:462/proxy/: tls qux (200; 92.566848ms)
Dec  3 14:45:31.775: INFO: (19) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:443/proxy/tlsrewritem... (200; 92.374ms)
Dec  3 14:45:31.775: INFO: (19) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 92.44933ms)
Dec  3 14:45:31.775: INFO: (19) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6/proxy/rewriteme">test</a> (200; 92.394881ms)
Dec  3 14:45:31.775: INFO: (19) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname1/proxy/: tls baz (200; 92.548075ms)
Dec  3 14:45:31.775: INFO: (19) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:1080/proxy/rewriteme">test<... (200; 92.5073ms)
Dec  3 14:45:31.775: INFO: (19) /api/v1/namespaces/proxy-4550/pods/https:proxy-service-sgbs5-4gkt6:460/proxy/: tls baz (200; 92.472493ms)
Dec  3 14:45:31.775: INFO: (19) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:160/proxy/: foo (200; 92.465454ms)
Dec  3 14:45:31.775: INFO: (19) /api/v1/namespaces/proxy-4550/pods/http:proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 92.506555ms)
Dec  3 14:45:31.776: INFO: (19) /api/v1/namespaces/proxy-4550/pods/proxy-service-sgbs5-4gkt6:162/proxy/: bar (200; 93.803635ms)
Dec  3 14:45:31.776: INFO: (19) /api/v1/namespaces/proxy-4550/services/https:proxy-service-sgbs5:tlsportname2/proxy/: tls qux (200; 93.686236ms)
Dec  3 14:45:31.777: INFO: (19) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname2/proxy/: bar (200; 93.952059ms)
Dec  3 14:45:31.778: INFO: (19) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname1/proxy/: foo (200; 95.19484ms)
Dec  3 14:45:31.779: INFO: (19) /api/v1/namespaces/proxy-4550/services/http:proxy-service-sgbs5:portname1/proxy/: foo (200; 96.712324ms)
Dec  3 14:45:31.779: INFO: (19) /api/v1/namespaces/proxy-4550/services/proxy-service-sgbs5:portname2/proxy/: bar (200; 96.852594ms)
STEP: deleting ReplicationController proxy-service-sgbs5 in namespace proxy-4550, will wait for the garbage collector to delete the pods
Dec  3 14:45:32.060: INFO: Deleting ReplicationController proxy-service-sgbs5 took: 91.312365ms
Dec  3 14:45:32.161: INFO: Terminating ReplicationController proxy-service-sgbs5 pods took: 100.378282ms
[AfterEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:45:42.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4550" for this suite.
Dec  3 14:45:48.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:45:52.071: INFO: namespace proxy-4550 deletion completed in 9.419694505s
â€¢SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:45:52.071: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5432
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 14:46:13.070: INFO: Container started at 2019-12-03 14:45:54 +0000 UTC, pod became ready at 2019-12-03 14:46:11 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:46:13.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5432" for this suite.
Dec  3 14:46:25.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:46:28.556: INFO: namespace container-probe-5432 deletion completed in 15.396906977s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:46:28.557: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3862
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  3 14:46:31.655: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:46:31.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3862" for this suite.
Dec  3 14:46:38.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:46:41.338: INFO: namespace container-runtime-3862 deletion completed in 9.408912571s
â€¢S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:46:41.339: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5926
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Dec  3 14:46:42.072: INFO: Waiting up to 5m0s for pod "client-containers-141d6dd4-80ce-459f-8064-160ebaba7c81" in namespace "containers-5926" to be "success or failure"
Dec  3 14:46:42.162: INFO: Pod "client-containers-141d6dd4-80ce-459f-8064-160ebaba7c81": Phase="Pending", Reason="", readiness=false. Elapsed: 89.62279ms
Dec  3 14:46:44.252: INFO: Pod "client-containers-141d6dd4-80ce-459f-8064-160ebaba7c81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179407542s
STEP: Saw pod success
Dec  3 14:46:44.252: INFO: Pod "client-containers-141d6dd4-80ce-459f-8064-160ebaba7c81" satisfied condition "success or failure"
Dec  3 14:46:44.341: INFO: Trying to get logs from node ip-10-250-27-214.ec2.internal pod client-containers-141d6dd4-80ce-459f-8064-160ebaba7c81 container test-container: <nil>
STEP: delete the pod
Dec  3 14:46:44.539: INFO: Waiting for pod client-containers-141d6dd4-80ce-459f-8064-160ebaba7c81 to disappear
Dec  3 14:46:44.629: INFO: Pod client-containers-141d6dd4-80ce-459f-8064-160ebaba7c81 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:46:44.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5926" for this suite.
Dec  3 14:46:50.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:46:54.121: INFO: namespace containers-5926 deletion completed in 9.400291145s
â€¢S
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:46:54.121: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-733
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-733.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-733.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-733.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-733.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 14:47:05.492: INFO: DNS probes using dns-test-8c95ada1-87c0-42b8-9cad-c24b4f3bf374 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-733.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-733.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-733.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-733.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 14:47:08.306: INFO: File wheezy_udp@dns-test-service-3.dns-733.svc.cluster.local from pod  dns-733/dns-test-6a4b6688-42dc-4eea-a074-a5a52dfa0d96 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 14:47:08.400: INFO: File jessie_udp@dns-test-service-3.dns-733.svc.cluster.local from pod  dns-733/dns-test-6a4b6688-42dc-4eea-a074-a5a52dfa0d96 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 14:47:08.400: INFO: Lookups using dns-733/dns-test-6a4b6688-42dc-4eea-a074-a5a52dfa0d96 failed for: [wheezy_udp@dns-test-service-3.dns-733.svc.cluster.local jessie_udp@dns-test-service-3.dns-733.svc.cluster.local]

Dec  3 14:47:13.494: INFO: File wheezy_udp@dns-test-service-3.dns-733.svc.cluster.local from pod  dns-733/dns-test-6a4b6688-42dc-4eea-a074-a5a52dfa0d96 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 14:47:13.588: INFO: File jessie_udp@dns-test-service-3.dns-733.svc.cluster.local from pod  dns-733/dns-test-6a4b6688-42dc-4eea-a074-a5a52dfa0d96 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 14:47:13.588: INFO: Lookups using dns-733/dns-test-6a4b6688-42dc-4eea-a074-a5a52dfa0d96 failed for: [wheezy_udp@dns-test-service-3.dns-733.svc.cluster.local jessie_udp@dns-test-service-3.dns-733.svc.cluster.local]

Dec  3 14:47:18.495: INFO: File wheezy_udp@dns-test-service-3.dns-733.svc.cluster.local from pod  dns-733/dns-test-6a4b6688-42dc-4eea-a074-a5a52dfa0d96 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 14:47:18.590: INFO: File jessie_udp@dns-test-service-3.dns-733.svc.cluster.local from pod  dns-733/dns-test-6a4b6688-42dc-4eea-a074-a5a52dfa0d96 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 14:47:18.591: INFO: Lookups using dns-733/dns-test-6a4b6688-42dc-4eea-a074-a5a52dfa0d96 failed for: [wheezy_udp@dns-test-service-3.dns-733.svc.cluster.local jessie_udp@dns-test-service-3.dns-733.svc.cluster.local]

Dec  3 14:47:23.494: INFO: File wheezy_udp@dns-test-service-3.dns-733.svc.cluster.local from pod  dns-733/dns-test-6a4b6688-42dc-4eea-a074-a5a52dfa0d96 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 14:47:23.588: INFO: File jessie_udp@dns-test-service-3.dns-733.svc.cluster.local from pod  dns-733/dns-test-6a4b6688-42dc-4eea-a074-a5a52dfa0d96 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec  3 14:47:23.588: INFO: Lookups using dns-733/dns-test-6a4b6688-42dc-4eea-a074-a5a52dfa0d96 failed for: [wheezy_udp@dns-test-service-3.dns-733.svc.cluster.local jessie_udp@dns-test-service-3.dns-733.svc.cluster.local]

Dec  3 14:47:28.588: INFO: DNS probes using dns-test-6a4b6688-42dc-4eea-a074-a5a52dfa0d96 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-733.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-733.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-733.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-733.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 14:47:31.596: INFO: DNS probes using dns-test-30a3bdf3-0ed2-4bed-8f90-178254947f81 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:47:31.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-733" for this suite.
Dec  3 14:47:38.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:47:41.300: INFO: namespace dns-733 deletion completed in 9.426102793s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:47:41.301: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2450
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-7f8ab3ba-6acd-436b-81de-8389584afc39
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:47:42.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2450" for this suite.
Dec  3 14:47:48.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:47:51.524: INFO: namespace configmap-2450 deletion completed in 9.406063318s
â€¢SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:47:51.524: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 14:47:53.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981273, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981273, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981273, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981273, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 14:47:56.579: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Dec  3 14:47:56.966: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:47:57.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5084" for this suite.
Dec  3 14:48:03.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:48:06.646: INFO: namespace webhook-5084 deletion completed in 9.399412168s
STEP: Destroying namespace "webhook-5084-markers" for this suite.
Dec  3 14:48:12.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:48:16.050: INFO: namespace webhook-5084-markers deletion completed in 9.403934114s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢SSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:48:16.409: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1590
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Dec  3 14:48:17.047: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1590'
Dec  3 14:48:18.049: INFO: stderr: ""
Dec  3 14:48:18.049: INFO: stdout: "pod/pause created\n"
Dec  3 14:48:18.049: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec  3 14:48:18.049: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1590" to be "running and ready"
Dec  3 14:48:18.139: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 89.545128ms
Dec  3 14:48:20.229: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.179690678s
Dec  3 14:48:20.229: INFO: Pod "pause" satisfied condition "running and ready"
Dec  3 14:48:20.229: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Dec  3 14:48:20.229: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label=testing-label-value --namespace=kubectl-1590'
Dec  3 14:48:20.757: INFO: stderr: ""
Dec  3 14:48:20.757: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec  3 14:48:20.757: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-1590'
Dec  3 14:48:21.190: INFO: stderr: ""
Dec  3 14:48:21.190: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec  3 14:48:21.190: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label- --namespace=kubectl-1590'
Dec  3 14:48:21.737: INFO: stderr: ""
Dec  3 14:48:21.737: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec  3 14:48:21.737: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-1590'
Dec  3 14:48:22.178: INFO: stderr: ""
Dec  3 14:48:22.178: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Dec  3 14:48:22.178: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-1590'
Dec  3 14:48:22.719: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 14:48:22.720: INFO: stdout: "pod \"pause\" force deleted\n"
Dec  3 14:48:22.720: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=pause --no-headers --namespace=kubectl-1590'
Dec  3 14:48:23.239: INFO: stderr: "No resources found in kubectl-1590 namespace.\n"
Dec  3 14:48:23.239: INFO: stdout: ""
Dec  3 14:48:23.239: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=pause --namespace=kubectl-1590 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 14:48:23.667: INFO: stderr: ""
Dec  3 14:48:23.667: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:48:23.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1590" for this suite.
Dec  3 14:48:30.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:48:33.154: INFO: namespace kubectl-1590 deletion completed in 9.39704771s
â€¢SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:48:33.154: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3221
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 14:48:33.886: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6bfb61ec-b4ba-4de4-9423-2a3762e84877" in namespace "downward-api-3221" to be "success or failure"
Dec  3 14:48:33.976: INFO: Pod "downwardapi-volume-6bfb61ec-b4ba-4de4-9423-2a3762e84877": Phase="Pending", Reason="", readiness=false. Elapsed: 89.491403ms
Dec  3 14:48:36.066: INFO: Pod "downwardapi-volume-6bfb61ec-b4ba-4de4-9423-2a3762e84877": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17934075s
STEP: Saw pod success
Dec  3 14:48:36.066: INFO: Pod "downwardapi-volume-6bfb61ec-b4ba-4de4-9423-2a3762e84877" satisfied condition "success or failure"
Dec  3 14:48:36.155: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-6bfb61ec-b4ba-4de4-9423-2a3762e84877 container client-container: <nil>
STEP: delete the pod
Dec  3 14:48:36.480: INFO: Waiting for pod downwardapi-volume-6bfb61ec-b4ba-4de4-9423-2a3762e84877 to disappear
Dec  3 14:48:36.570: INFO: Pod downwardapi-volume-6bfb61ec-b4ba-4de4-9423-2a3762e84877 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:48:36.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3221" for this suite.
Dec  3 14:48:42.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:48:46.065: INFO: namespace downward-api-3221 deletion completed in 9.404995165s
â€¢SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:48:46.065: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-6396
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec  3 14:48:55.425: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6396 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:48:55.425: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:48:56.295: INFO: Exec stderr: ""
Dec  3 14:48:56.295: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6396 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:48:56.295: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:48:57.151: INFO: Exec stderr: ""
Dec  3 14:48:57.151: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6396 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:48:57.151: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:48:58.057: INFO: Exec stderr: ""
Dec  3 14:48:58.057: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6396 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:48:58.057: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:48:58.895: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec  3 14:48:58.895: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6396 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:48:58.895: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:48:59.747: INFO: Exec stderr: ""
Dec  3 14:48:59.747: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6396 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:48:59.747: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:49:00.596: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec  3 14:49:00.596: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6396 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:49:00.596: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:49:01.444: INFO: Exec stderr: ""
Dec  3 14:49:01.445: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6396 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:49:01.445: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:49:02.322: INFO: Exec stderr: ""
Dec  3 14:49:02.322: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6396 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:49:02.322: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:49:03.159: INFO: Exec stderr: ""
Dec  3 14:49:03.159: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6396 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 14:49:03.159: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 14:49:04.010: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:49:04.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6396" for this suite.
Dec  3 14:49:48.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:49:51.502: INFO: namespace e2e-kubelet-etc-hosts-6396 deletion completed in 47.401749761s
â€¢SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:49:51.503: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5111
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:50:03.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5111" for this suite.
Dec  3 14:50:10.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:50:13.287: INFO: namespace resourcequota-5111 deletion completed in 9.402227591s
â€¢SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:50:13.287: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7834
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 14:50:13.925: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:50:16.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7834" for this suite.
Dec  3 14:51:01.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:51:04.304: INFO: namespace pods-7834 deletion completed in 47.412260209s
â€¢SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:51:04.304: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8673
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-8c3fcd16-6701-4465-9f0e-b454e36cfc1b
STEP: Creating a pod to test consume secrets
Dec  3 14:51:05.123: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6d910dc1-fda9-48ad-8e19-cbb66a9bebd9" in namespace "projected-8673" to be "success or failure"
Dec  3 14:51:05.213: INFO: Pod "pod-projected-secrets-6d910dc1-fda9-48ad-8e19-cbb66a9bebd9": Phase="Pending", Reason="", readiness=false. Elapsed: 89.49002ms
Dec  3 14:51:07.303: INFO: Pod "pod-projected-secrets-6d910dc1-fda9-48ad-8e19-cbb66a9bebd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179217031s
STEP: Saw pod success
Dec  3 14:51:07.303: INFO: Pod "pod-projected-secrets-6d910dc1-fda9-48ad-8e19-cbb66a9bebd9" satisfied condition "success or failure"
Dec  3 14:51:07.392: INFO: Trying to get logs from node ip-10-250-27-214.ec2.internal pod pod-projected-secrets-6d910dc1-fda9-48ad-8e19-cbb66a9bebd9 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 14:51:07.589: INFO: Waiting for pod pod-projected-secrets-6d910dc1-fda9-48ad-8e19-cbb66a9bebd9 to disappear
Dec  3 14:51:07.678: INFO: Pod pod-projected-secrets-6d910dc1-fda9-48ad-8e19-cbb66a9bebd9 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:51:07.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8673" for this suite.
Dec  3 14:51:14.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:51:17.173: INFO: namespace projected-8673 deletion completed in 9.405274002s
â€¢
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:51:17.173: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  3 14:51:20.941: INFO: Successfully updated pod "pod-update-427ce02d-b988-4777-b593-0e5c4475aa58"
STEP: verifying the updated pod is in kubernetes
Dec  3 14:51:21.121: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:51:21.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-736" for this suite.
Dec  3 14:51:49.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:51:52.623: INFO: namespace pods-736 deletion completed in 31.412694848s
â€¢SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:51:52.624: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5672
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:52:10.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5672" for this suite.
Dec  3 14:52:16.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:52:20.032: INFO: namespace resourcequota-5672 deletion completed in 9.40135804s
â€¢SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:52:20.032: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7746
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 14:52:20.762: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df1486ed-f587-40e8-aa5d-044d32eebbf5" in namespace "downward-api-7746" to be "success or failure"
Dec  3 14:52:20.852: INFO: Pod "downwardapi-volume-df1486ed-f587-40e8-aa5d-044d32eebbf5": Phase="Pending", Reason="", readiness=false. Elapsed: 89.620628ms
Dec  3 14:52:22.942: INFO: Pod "downwardapi-volume-df1486ed-f587-40e8-aa5d-044d32eebbf5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180182161s
STEP: Saw pod success
Dec  3 14:52:22.942: INFO: Pod "downwardapi-volume-df1486ed-f587-40e8-aa5d-044d32eebbf5" satisfied condition "success or failure"
Dec  3 14:52:23.032: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-df1486ed-f587-40e8-aa5d-044d32eebbf5 container client-container: <nil>
STEP: delete the pod
Dec  3 14:52:23.352: INFO: Waiting for pod downwardapi-volume-df1486ed-f587-40e8-aa5d-044d32eebbf5 to disappear
Dec  3 14:52:23.442: INFO: Pod downwardapi-volume-df1486ed-f587-40e8-aa5d-044d32eebbf5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:52:23.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7746" for this suite.
Dec  3 14:52:29.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:52:32.934: INFO: namespace downward-api-7746 deletion completed in 9.399181878s
â€¢SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:52:32.935: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4459
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Dec  3 14:52:33.574: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4459'
Dec  3 14:52:34.797: INFO: stderr: ""
Dec  3 14:52:34.797: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 14:52:34.797: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4459'
Dec  3 14:52:35.220: INFO: stderr: ""
Dec  3 14:52:35.220: INFO: stdout: "update-demo-nautilus-8njlq update-demo-nautilus-l9fpl "
Dec  3 14:52:35.220: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8njlq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4459'
Dec  3 14:52:35.643: INFO: stderr: ""
Dec  3 14:52:35.643: INFO: stdout: ""
Dec  3 14:52:35.643: INFO: update-demo-nautilus-8njlq is created but not running
Dec  3 14:52:40.643: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4459'
Dec  3 14:52:41.076: INFO: stderr: ""
Dec  3 14:52:41.076: INFO: stdout: "update-demo-nautilus-8njlq update-demo-nautilus-l9fpl "
Dec  3 14:52:41.076: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8njlq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4459'
Dec  3 14:52:41.500: INFO: stderr: ""
Dec  3 14:52:41.500: INFO: stdout: "true"
Dec  3 14:52:41.501: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8njlq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4459'
Dec  3 14:52:41.927: INFO: stderr: ""
Dec  3 14:52:41.927: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 14:52:41.927: INFO: validating pod update-demo-nautilus-8njlq
Dec  3 14:52:42.106: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 14:52:42.106: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 14:52:42.106: INFO: update-demo-nautilus-8njlq is verified up and running
Dec  3 14:52:42.107: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-l9fpl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4459'
Dec  3 14:52:42.532: INFO: stderr: ""
Dec  3 14:52:42.532: INFO: stdout: "true"
Dec  3 14:52:42.532: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-l9fpl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4459'
Dec  3 14:52:42.955: INFO: stderr: ""
Dec  3 14:52:42.955: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 14:52:42.955: INFO: validating pod update-demo-nautilus-l9fpl
Dec  3 14:52:43.135: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 14:52:43.135: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 14:52:43.135: INFO: update-demo-nautilus-l9fpl is verified up and running
STEP: using delete to clean up resources
Dec  3 14:52:43.135: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4459'
Dec  3 14:52:43.648: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 14:52:43.648: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  3 14:52:43.648: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4459'
Dec  3 14:52:44.162: INFO: stderr: "No resources found in kubectl-4459 namespace.\n"
Dec  3 14:52:44.162: INFO: stdout: ""
Dec  3 14:52:44.162: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-4459 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 14:52:44.592: INFO: stderr: ""
Dec  3 14:52:44.592: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:52:44.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4459" for this suite.
Dec  3 14:52:56.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:53:00.080: INFO: namespace kubectl-4459 deletion completed in 15.397796766s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:53:00.081: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9884
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec  3 14:53:00.720: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-9884'
Dec  3 14:53:01.160: INFO: stderr: ""
Dec  3 14:53:01.160: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec  3 14:53:06.261: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod e2e-test-httpd-pod --namespace=kubectl-9884 -o json'
Dec  3 14:53:06.699: INFO: stderr: ""
Dec  3 14:53:06.699: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.64.1.25/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-12-03T14:53:01Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9884\",\n        \"resourceVersion\": \"4780\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9884/pods/e2e-test-httpd-pod\",\n        \"uid\": \"1518f8e2-5a2f-46a6-bca2-b084afde7dfb\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-lbfpw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-250-10-98.ec2.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-lbfpw\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-lbfpw\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-03T14:53:01Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-03T14:53:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-03T14:53:02Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-03T14:53:01Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://f2d153d7fa12a2c44f2c4c8ea8d879e3382ead2d3a2f7dcbbaa526a205030e93\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-03T14:53:01Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.10.98\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.64.1.25\",\n        \"podIPs\": [\n            {\n                \"ip\": \"100.64.1.25\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-03T14:53:01Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec  3 14:53:06.699: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config replace -f - --namespace=kubectl-9884'
Dec  3 14:53:07.670: INFO: stderr: ""
Dec  3 14:53:07.670: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Dec  3 14:53:07.760: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-httpd-pod --namespace=kubectl-9884'
Dec  3 14:53:09.789: INFO: stderr: ""
Dec  3 14:53:09.789: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:53:09.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9884" for this suite.
Dec  3 14:53:16.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:53:19.277: INFO: namespace kubectl-9884 deletion completed in 9.397229635s
â€¢SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:53:19.277: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-515
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:53:22.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-515" for this suite.
Dec  3 14:54:06.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:54:09.860: INFO: namespace kubelet-test-515 deletion completed in 47.398928404s
â€¢SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:54:09.860: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7925
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Dec  3 14:54:11.895: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W1203 14:54:11.895277    5076 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  3 14:54:11.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7925" for this suite.
Dec  3 14:54:18.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:54:21.385: INFO: namespace gc-7925 deletion completed in 9.399911268s
â€¢SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:54:21.385: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1128
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:54:24.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1128" for this suite.
Dec  3 14:55:08.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:55:11.963: INFO: namespace kubelet-test-1128 deletion completed in 47.389015652s
â€¢SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:55:11.964: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-4431
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Dec  3 14:55:12.609: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  3 14:56:13.234: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 14:56:13.324: INFO: Starting informer...
STEP: Starting pod...
Dec  3 14:56:13.505: INFO: Pod is running on ip-10-250-10-98.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Dec  3 14:56:13.777: INFO: Pod wasn't evicted. Proceeding
Dec  3 14:56:13.777: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Dec  3 14:57:29.049: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:57:29.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-4431" for this suite.
Dec  3 14:57:57.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:58:00.538: INFO: namespace taint-single-pod-4431 deletion completed in 31.397961424s
â€¢SSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:58:00.538: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-805
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 14:58:01.267: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-14cc1315-8174-406d-9cba-da98581c6c97" in namespace "security-context-test-805" to be "success or failure"
Dec  3 14:58:01.357: INFO: Pod "alpine-nnp-false-14cc1315-8174-406d-9cba-da98581c6c97": Phase="Pending", Reason="", readiness=false. Elapsed: 89.508579ms
Dec  3 14:58:03.448: INFO: Pod "alpine-nnp-false-14cc1315-8174-406d-9cba-da98581c6c97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180769432s
Dec  3 14:58:03.450: INFO: Pod "alpine-nnp-false-14cc1315-8174-406d-9cba-da98581c6c97" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:58:03.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-805" for this suite.
Dec  3 14:58:10.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:58:13.169: INFO: namespace security-context-test-805 deletion completed in 9.397201886s
â€¢SSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:58:13.169: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5426
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 14:58:13.898: INFO: Waiting up to 5m0s for pod "busybox-user-65534-919a03e0-ffe7-40be-a23b-3aa14dc27bab" in namespace "security-context-test-5426" to be "success or failure"
Dec  3 14:58:13.988: INFO: Pod "busybox-user-65534-919a03e0-ffe7-40be-a23b-3aa14dc27bab": Phase="Pending", Reason="", readiness=false. Elapsed: 89.560395ms
Dec  3 14:58:16.078: INFO: Pod "busybox-user-65534-919a03e0-ffe7-40be-a23b-3aa14dc27bab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179788024s
Dec  3 14:58:16.078: INFO: Pod "busybox-user-65534-919a03e0-ffe7-40be-a23b-3aa14dc27bab" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:58:16.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5426" for this suite.
Dec  3 14:58:22.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:58:25.567: INFO: namespace security-context-test-5426 deletion completed in 9.398953933s
â€¢SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:58:25.568: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1307
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 14:58:27.569: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981907, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981907, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981907, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981907, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 14:58:30.752: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:58:45.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1307" for this suite.
Dec  3 14:58:51.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:58:54.564: INFO: namespace webhook-1307 deletion completed in 9.401914383s
STEP: Destroying namespace "webhook-1307-markers" for this suite.
Dec  3 14:59:00.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:59:03.967: INFO: namespace webhook-1307-markers deletion completed in 9.403589985s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:59:04.327: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9965
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 14:59:06.033: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981945, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981945, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981945, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710981945, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 14:59:09.217: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:59:09.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9965" for this suite.
Dec  3 14:59:22.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:59:25.346: INFO: namespace webhook-9965 deletion completed in 15.400047003s
STEP: Destroying namespace "webhook-9965-markers" for this suite.
Dec  3 14:59:31.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:59:34.754: INFO: namespace webhook-9965-markers deletion completed in 9.408455103s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:59:35.114: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1104
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-023efa5b-226a-43b6-8a71-eafec04205a6
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-023efa5b-226a-43b6-8a71-eafec04205a6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 14:59:40.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1104" for this suite.
Dec  3 14:59:55.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 14:59:58.301: INFO: namespace configmap-1104 deletion completed in 17.401264097s
â€¢SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 14:59:58.301: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9756
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-9756
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9756 to expose endpoints map[]
Dec  3 14:59:59.123: INFO: successfully validated that service endpoint-test2 in namespace services-9756 exposes endpoints map[] (89.298203ms elapsed)
STEP: Creating pod pod1 in namespace services-9756
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9756 to expose endpoints map[pod1:[80]]
Dec  3 15:00:00.574: INFO: successfully validated that service endpoint-test2 in namespace services-9756 exposes endpoints map[pod1:[80]] (1.358453685s elapsed)
STEP: Creating pod pod2 in namespace services-9756
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9756 to expose endpoints map[pod1:[80] pod2:[80]]
Dec  3 15:00:03.571: INFO: successfully validated that service endpoint-test2 in namespace services-9756 exposes endpoints map[pod1:[80] pod2:[80]] (2.905740008s elapsed)
STEP: Deleting pod pod1 in namespace services-9756
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9756 to expose endpoints map[pod2:[80]]
Dec  3 15:00:03.865: INFO: successfully validated that service endpoint-test2 in namespace services-9756 exposes endpoints map[pod2:[80]] (179.530622ms elapsed)
STEP: Deleting pod pod2 in namespace services-9756
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9756 to expose endpoints map[]
Dec  3 15:00:04.045: INFO: successfully validated that service endpoint-test2 in namespace services-9756 exposes endpoints map[] (89.685283ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:00:04.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9756" for this suite.
Dec  3 15:00:16.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:00:19.638: INFO: namespace services-9756 deletion completed in 15.405357575s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:00:19.638: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7142
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7142.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7142.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 15:00:23.476: INFO: DNS probes using dns-7142/dns-test-fa2489fb-19da-4842-90f7-67a8b8a5f42a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:00:23.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7142" for this suite.
Dec  3 15:00:29.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:00:33.054: INFO: namespace dns-7142 deletion completed in 9.394363726s
â€¢SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:00:33.054: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3558
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-e8a23a06-ba4d-4518-aad5-645e406bdb2c
STEP: Creating a pod to test consume configMaps
Dec  3 15:00:33.875: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-777fc8a4-534b-451a-89e3-1c3bec4a7ca4" in namespace "projected-3558" to be "success or failure"
Dec  3 15:00:33.965: INFO: Pod "pod-projected-configmaps-777fc8a4-534b-451a-89e3-1c3bec4a7ca4": Phase="Pending", Reason="", readiness=false. Elapsed: 89.614953ms
Dec  3 15:00:36.055: INFO: Pod "pod-projected-configmaps-777fc8a4-534b-451a-89e3-1c3bec4a7ca4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179761983s
STEP: Saw pod success
Dec  3 15:00:36.055: INFO: Pod "pod-projected-configmaps-777fc8a4-534b-451a-89e3-1c3bec4a7ca4" satisfied condition "success or failure"
Dec  3 15:00:36.145: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-projected-configmaps-777fc8a4-534b-451a-89e3-1c3bec4a7ca4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:00:36.335: INFO: Waiting for pod pod-projected-configmaps-777fc8a4-534b-451a-89e3-1c3bec4a7ca4 to disappear
Dec  3 15:00:36.424: INFO: Pod pod-projected-configmaps-777fc8a4-534b-451a-89e3-1c3bec4a7ca4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:00:36.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3558" for this suite.
Dec  3 15:00:42.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:00:45.919: INFO: namespace projected-3558 deletion completed in 9.402854449s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:00:45.919: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2751
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-8731
STEP: Creating secret with name secret-test-1fe6c002-4675-4a30-b673-f4149cebf740
STEP: Creating a pod to test consume secrets
Dec  3 15:00:47.376: INFO: Waiting up to 5m0s for pod "pod-secrets-1f952bbe-48ca-4861-92ba-04d25f96156c" in namespace "secrets-2751" to be "success or failure"
Dec  3 15:00:47.466: INFO: Pod "pod-secrets-1f952bbe-48ca-4861-92ba-04d25f96156c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.69319ms
Dec  3 15:00:49.556: INFO: Pod "pod-secrets-1f952bbe-48ca-4861-92ba-04d25f96156c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179853372s
STEP: Saw pod success
Dec  3 15:00:49.556: INFO: Pod "pod-secrets-1f952bbe-48ca-4861-92ba-04d25f96156c" satisfied condition "success or failure"
Dec  3 15:00:49.646: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-secrets-1f952bbe-48ca-4861-92ba-04d25f96156c container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:00:49.838: INFO: Waiting for pod pod-secrets-1f952bbe-48ca-4861-92ba-04d25f96156c to disappear
Dec  3 15:00:49.927: INFO: Pod pod-secrets-1f952bbe-48ca-4861-92ba-04d25f96156c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:00:49.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2751" for this suite.
Dec  3 15:00:56.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:00:59.447: INFO: namespace secrets-2751 deletion completed in 9.427405083s
STEP: Destroying namespace "secret-namespace-8731" for this suite.
Dec  3 15:01:05.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:01:08.857: INFO: namespace secret-namespace-8731 deletion completed in 9.410676309s
â€¢SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:01:08.858: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-11
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec  3 15:01:12.722: INFO: Successfully updated pod "annotationupdatecbfe5fbb-6215-42c5-b0c7-a9e098e1a269"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:01:14.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-11" for this suite.
Dec  3 15:01:27.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:01:30.409: INFO: namespace projected-11 deletion completed in 15.406339642s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:01:30.410: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Dec  3 15:01:31.056: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec  3 15:01:31.056: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5672'
Dec  3 15:01:32.009: INFO: stderr: ""
Dec  3 15:01:32.009: INFO: stdout: "service/redis-slave created\n"
Dec  3 15:01:32.009: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec  3 15:01:32.009: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5672'
Dec  3 15:01:32.965: INFO: stderr: ""
Dec  3 15:01:32.965: INFO: stdout: "service/redis-master created\n"
Dec  3 15:01:32.966: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec  3 15:01:32.966: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5672'
Dec  3 15:01:33.920: INFO: stderr: ""
Dec  3 15:01:33.920: INFO: stdout: "service/frontend created\n"
Dec  3 15:01:33.920: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec  3 15:01:33.920: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5672'
Dec  3 15:01:34.873: INFO: stderr: ""
Dec  3 15:01:34.873: INFO: stdout: "deployment.apps/frontend created\n"
Dec  3 15:01:34.873: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec  3 15:01:34.873: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5672'
Dec  3 15:01:35.476: INFO: stderr: ""
Dec  3 15:01:35.476: INFO: stdout: "deployment.apps/redis-master created\n"
Dec  3 15:01:35.476: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec  3 15:01:35.476: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5672'
Dec  3 15:01:36.418: INFO: stderr: ""
Dec  3 15:01:36.418: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Dec  3 15:01:36.418: INFO: Waiting for all frontend pods to be Running.
Dec  3 15:01:56.519: INFO: Waiting for frontend to serve content.
Dec  3 15:01:56.698: INFO: Trying to add a new entry to the guestbook.
Dec  3 15:01:56.878: INFO: Verifying that added entry can be retrieved.
Dec  3 15:01:57.058: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec  3 15:02:02.239: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec  3 15:02:07.419: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec  3 15:02:12.598: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec  3 15:02:17.778: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec  3 15:02:22.958: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec  3 15:02:28.138: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec  3 15:02:33.318: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec  3 15:02:38.498: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec  3 15:02:43.678: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Dec  3 15:02:48.858: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Dec  3 15:02:54.039: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5672'
Dec  3 15:02:55.086: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 15:02:55.086: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 15:02:55.086: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5672'
Dec  3 15:02:55.607: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 15:02:55.607: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 15:02:55.608: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5672'
Dec  3 15:02:56.129: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 15:02:56.129: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 15:02:56.129: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5672'
Dec  3 15:02:56.637: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 15:02:56.637: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 15:02:56.638: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5672'
Dec  3 15:02:57.154: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 15:02:57.154: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  3 15:02:57.154: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-5672'
Dec  3 15:02:57.666: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 15:02:57.666: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:02:57.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5672" for this suite.
Dec  3 15:03:04.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:03:07.165: INFO: namespace kubectl-5672 deletion completed in 9.40798705s
â€¢SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:03:07.165: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2672
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  3 15:03:07.895: INFO: Waiting up to 5m0s for pod "pod-b97aa371-2efa-4afc-8cc8-015ba0e26587" in namespace "emptydir-2672" to be "success or failure"
Dec  3 15:03:07.985: INFO: Pod "pod-b97aa371-2efa-4afc-8cc8-015ba0e26587": Phase="Pending", Reason="", readiness=false. Elapsed: 89.549087ms
Dec  3 15:03:10.075: INFO: Pod "pod-b97aa371-2efa-4afc-8cc8-015ba0e26587": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179780826s
STEP: Saw pod success
Dec  3 15:03:10.075: INFO: Pod "pod-b97aa371-2efa-4afc-8cc8-015ba0e26587" satisfied condition "success or failure"
Dec  3 15:03:10.165: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-b97aa371-2efa-4afc-8cc8-015ba0e26587 container test-container: <nil>
STEP: delete the pod
Dec  3 15:03:10.493: INFO: Waiting for pod pod-b97aa371-2efa-4afc-8cc8-015ba0e26587 to disappear
Dec  3 15:03:10.582: INFO: Pod pod-b97aa371-2efa-4afc-8cc8-015ba0e26587 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:03:10.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2672" for this suite.
Dec  3 15:03:16.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:03:20.078: INFO: namespace emptydir-2672 deletion completed in 9.405349616s
â€¢SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:03:20.079: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6385
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  3 15:03:21.440: INFO: Number of nodes with available pods: 0
Dec  3 15:03:21.440: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:03:22.621: INFO: Number of nodes with available pods: 1
Dec  3 15:03:22.621: INFO: Node ip-10-250-27-214.ec2.internal is running more than one daemon pod
Dec  3 15:03:23.620: INFO: Number of nodes with available pods: 2
Dec  3 15:03:23.620: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec  3 15:03:24.071: INFO: Number of nodes with available pods: 1
Dec  3 15:03:24.071: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:03:25.259: INFO: Number of nodes with available pods: 1
Dec  3 15:03:25.259: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:03:26.255: INFO: Number of nodes with available pods: 1
Dec  3 15:03:26.255: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:03:27.252: INFO: Number of nodes with available pods: 1
Dec  3 15:03:27.252: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:03:28.252: INFO: Number of nodes with available pods: 1
Dec  3 15:03:28.252: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:03:29.253: INFO: Number of nodes with available pods: 1
Dec  3 15:03:29.253: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:03:30.252: INFO: Number of nodes with available pods: 1
Dec  3 15:03:30.252: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:03:31.251: INFO: Number of nodes with available pods: 1
Dec  3 15:03:31.251: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:03:32.251: INFO: Number of nodes with available pods: 1
Dec  3 15:03:32.251: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:03:33.252: INFO: Number of nodes with available pods: 1
Dec  3 15:03:33.252: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:03:34.252: INFO: Number of nodes with available pods: 1
Dec  3 15:03:34.252: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:03:35.251: INFO: Number of nodes with available pods: 2
Dec  3 15:03:35.251: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6385, will wait for the garbage collector to delete the pods
Dec  3 15:03:35.623: INFO: Deleting DaemonSet.extensions daemon-set took: 91.322226ms
Dec  3 15:03:36.023: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.360342ms
Dec  3 15:03:43.113: INFO: Number of nodes with available pods: 0
Dec  3 15:03:43.113: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 15:03:43.204: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6385/daemonsets","resourceVersion":"6821"},"items":null}

Dec  3 15:03:43.294: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6385/pods","resourceVersion":"6822"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:03:43.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6385" for this suite.
Dec  3 15:03:49.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:03:53.063: INFO: namespace daemonsets-6385 deletion completed in 9.409584965s
â€¢SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:03:53.064: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-4745
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec  3 15:03:54.823: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982234, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982234, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982234, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982234, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 15:03:58.009: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 15:03:58.099: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:04:00.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4745" for this suite.
Dec  3 15:04:06.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:04:09.876: INFO: namespace crd-webhook-4745 deletion completed in 9.398943708s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
â€¢SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:04:10.236: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6671
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 15:04:11.321: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec  3 15:04:11.501: INFO: Number of nodes with available pods: 0
Dec  3 15:04:11.501: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec  3 15:04:11.860: INFO: Number of nodes with available pods: 0
Dec  3 15:04:11.860: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:04:12.950: INFO: Number of nodes with available pods: 0
Dec  3 15:04:12.950: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:04:13.950: INFO: Number of nodes with available pods: 1
Dec  3 15:04:13.950: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec  3 15:04:14.310: INFO: Number of nodes with available pods: 0
Dec  3 15:04:14.310: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec  3 15:04:14.493: INFO: Number of nodes with available pods: 0
Dec  3 15:04:14.493: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:04:15.584: INFO: Number of nodes with available pods: 0
Dec  3 15:04:15.584: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:04:16.584: INFO: Number of nodes with available pods: 0
Dec  3 15:04:16.584: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:04:17.584: INFO: Number of nodes with available pods: 0
Dec  3 15:04:17.584: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:04:18.583: INFO: Number of nodes with available pods: 0
Dec  3 15:04:18.584: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:04:19.584: INFO: Number of nodes with available pods: 1
Dec  3 15:04:19.584: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6671, will wait for the garbage collector to delete the pods
Dec  3 15:04:20.046: INFO: Deleting DaemonSet.extensions daemon-set took: 91.78864ms
Dec  3 15:04:20.146: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.771144ms
Dec  3 15:04:33.136: INFO: Number of nodes with available pods: 0
Dec  3 15:04:33.136: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 15:04:33.226: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6671/daemonsets","resourceVersion":"7030"},"items":null}

Dec  3 15:04:33.317: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6671/pods","resourceVersion":"7031"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:04:33.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6671" for this suite.
Dec  3 15:04:40.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:04:43.173: INFO: namespace daemonsets-6671 deletion completed in 9.399390805s
â€¢SSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:04:43.174: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6157
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-6157
I1203 15:04:44.186382    5076 runners.go:184] Created replication controller with name: externalname-service, namespace: services-6157, replica count: 2
I1203 15:04:47.287036    5076 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  3 15:04:47.287: INFO: Creating new exec pod
Dec  3 15:04:50.648: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-6157 execpod5j74m -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec  3 15:04:51.960: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec  3 15:04:51.960: INFO: stdout: ""
Dec  3 15:04:51.960: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-6157 execpod5j74m -- /bin/sh -x -c nc -zv -t -w 2 100.107.71.251 80'
Dec  3 15:04:53.218: INFO: stderr: "+ nc -zv -t -w 2 100.107.71.251 80\nConnection to 100.107.71.251 80 port [tcp/http] succeeded!\n"
Dec  3 15:04:53.218: INFO: stdout: ""
Dec  3 15:04:53.218: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-6157 execpod5j74m -- /bin/sh -x -c nc -zv -t -w 2 10.250.10.98 30168'
Dec  3 15:04:54.520: INFO: stderr: "+ nc -zv -t -w 2 10.250.10.98 30168\nConnection to 10.250.10.98 30168 port [tcp/30168] succeeded!\n"
Dec  3 15:04:54.520: INFO: stdout: ""
Dec  3 15:04:54.520: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-6157 execpod5j74m -- /bin/sh -x -c nc -zv -t -w 2 10.250.27.214 30168'
Dec  3 15:04:55.780: INFO: stderr: "+ nc -zv -t -w 2 10.250.27.214 30168\nConnection to 10.250.27.214 30168 port [tcp/30168] succeeded!\n"
Dec  3 15:04:55.780: INFO: stdout: ""
Dec  3 15:04:55.780: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:04:55.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6157" for this suite.
Dec  3 15:05:02.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:05:05.371: INFO: namespace services-6157 deletion completed in 9.403222492s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95
â€¢SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:05:05.372: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4844
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 15:05:06.290: INFO: (0) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 189.242019ms)
Dec  3 15:05:06.383: INFO: (1) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 93.35977ms)
Dec  3 15:05:06.476: INFO: (2) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.465438ms)
Dec  3 15:05:06.568: INFO: (3) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.084918ms)
Dec  3 15:05:06.660: INFO: (4) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.458151ms)
Dec  3 15:05:06.753: INFO: (5) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.311508ms)
Dec  3 15:05:06.845: INFO: (6) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.358103ms)
Dec  3 15:05:06.938: INFO: (7) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.840899ms)
Dec  3 15:05:07.030: INFO: (8) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.14136ms)
Dec  3 15:05:07.123: INFO: (9) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.465864ms)
Dec  3 15:05:07.215: INFO: (10) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.460776ms)
Dec  3 15:05:07.308: INFO: (11) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.273997ms)
Dec  3 15:05:07.399: INFO: (12) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.529988ms)
Dec  3 15:05:07.492: INFO: (13) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.844617ms)
Dec  3 15:05:07.585: INFO: (14) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.603843ms)
Dec  3 15:05:07.677: INFO: (15) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.16796ms)
Dec  3 15:05:07.769: INFO: (16) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.118918ms)
Dec  3 15:05:07.861: INFO: (17) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.8771ms)
Dec  3 15:05:07.954: INFO: (18) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.412843ms)
Dec  3 15:05:08.046: INFO: (19) /api/v1/nodes/ip-10-250-10-98.ec2.internal:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.152207ms)
[AfterEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:05:08.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4844" for this suite.
Dec  3 15:05:14.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:05:17.532: INFO: namespace proxy-4844 deletion completed in 9.395521685s
â€¢SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:05:17.532: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5841
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  3 15:05:18.261: INFO: Waiting up to 5m0s for pod "pod-a01a9057-2a06-43f9-bace-7e50cd053162" in namespace "emptydir-5841" to be "success or failure"
Dec  3 15:05:18.351: INFO: Pod "pod-a01a9057-2a06-43f9-bace-7e50cd053162": Phase="Pending", Reason="", readiness=false. Elapsed: 89.873263ms
Dec  3 15:05:20.441: INFO: Pod "pod-a01a9057-2a06-43f9-bace-7e50cd053162": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180361268s
STEP: Saw pod success
Dec  3 15:05:20.441: INFO: Pod "pod-a01a9057-2a06-43f9-bace-7e50cd053162" satisfied condition "success or failure"
Dec  3 15:05:20.531: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-a01a9057-2a06-43f9-bace-7e50cd053162 container test-container: <nil>
STEP: delete the pod
Dec  3 15:05:20.723: INFO: Waiting for pod pod-a01a9057-2a06-43f9-bace-7e50cd053162 to disappear
Dec  3 15:05:20.812: INFO: Pod pod-a01a9057-2a06-43f9-bace-7e50cd053162 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:05:20.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5841" for this suite.
Dec  3 15:05:27.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:05:30.324: INFO: namespace emptydir-5841 deletion completed in 9.421001955s
â€¢
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:05:30.324: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-125
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  3 15:05:31.054: INFO: Waiting up to 5m0s for pod "pod-c23f0aa1-fa22-4c2a-9728-8075d60c94ea" in namespace "emptydir-125" to be "success or failure"
Dec  3 15:05:31.144: INFO: Pod "pod-c23f0aa1-fa22-4c2a-9728-8075d60c94ea": Phase="Pending", Reason="", readiness=false. Elapsed: 89.776042ms
Dec  3 15:05:33.234: INFO: Pod "pod-c23f0aa1-fa22-4c2a-9728-8075d60c94ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179867362s
STEP: Saw pod success
Dec  3 15:05:33.234: INFO: Pod "pod-c23f0aa1-fa22-4c2a-9728-8075d60c94ea" satisfied condition "success or failure"
Dec  3 15:05:33.324: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-c23f0aa1-fa22-4c2a-9728-8075d60c94ea container test-container: <nil>
STEP: delete the pod
Dec  3 15:05:33.514: INFO: Waiting for pod pod-c23f0aa1-fa22-4c2a-9728-8075d60c94ea to disappear
Dec  3 15:05:33.604: INFO: Pod pod-c23f0aa1-fa22-4c2a-9728-8075d60c94ea no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:05:33.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-125" for this suite.
Dec  3 15:05:39.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:05:43.093: INFO: namespace emptydir-125 deletion completed in 9.398993929s
â€¢SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:05:43.093: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5534
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:05:48.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5534" for this suite.
Dec  3 15:05:54.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:05:57.495: INFO: namespace kubelet-test-5534 deletion completed in 9.401747465s
â€¢SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:05:57.495: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1663
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:05:58.225: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f696fb4e-7627-4d80-a041-835dacf8c89a" in namespace "projected-1663" to be "success or failure"
Dec  3 15:05:58.315: INFO: Pod "downwardapi-volume-f696fb4e-7627-4d80-a041-835dacf8c89a": Phase="Pending", Reason="", readiness=false. Elapsed: 89.697541ms
Dec  3 15:06:00.405: INFO: Pod "downwardapi-volume-f696fb4e-7627-4d80-a041-835dacf8c89a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179544065s
STEP: Saw pod success
Dec  3 15:06:00.405: INFO: Pod "downwardapi-volume-f696fb4e-7627-4d80-a041-835dacf8c89a" satisfied condition "success or failure"
Dec  3 15:06:00.494: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-f696fb4e-7627-4d80-a041-835dacf8c89a container client-container: <nil>
STEP: delete the pod
Dec  3 15:06:00.683: INFO: Waiting for pod downwardapi-volume-f696fb4e-7627-4d80-a041-835dacf8c89a to disappear
Dec  3 15:06:00.773: INFO: Pod downwardapi-volume-f696fb4e-7627-4d80-a041-835dacf8c89a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:06:00.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1663" for this suite.
Dec  3 15:06:07.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:06:10.267: INFO: namespace projected-1663 deletion completed in 9.403643104s
â€¢SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:06:10.267: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8286
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 15:06:12.027: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982371, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982371, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982371, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710982371, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 15:06:15.212: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:06:16.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8286" for this suite.
Dec  3 15:06:23.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:06:26.320: INFO: namespace webhook-8286 deletion completed in 9.400918894s
STEP: Destroying namespace "webhook-8286-markers" for this suite.
Dec  3 15:06:32.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:06:35.719: INFO: namespace webhook-8286-markers deletion completed in 9.398118602s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:06:36.079: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9000
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 15:06:36.716: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec  3 15:06:40.869: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9000 create -f -'
Dec  3 15:06:42.357: INFO: stderr: ""
Dec  3 15:06:42.357: INFO: stdout: "e2e-test-crd-publish-openapi-8043-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec  3 15:06:42.357: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9000 delete e2e-test-crd-publish-openapi-8043-crds test-cr'
Dec  3 15:06:42.911: INFO: stderr: ""
Dec  3 15:06:42.911: INFO: stdout: "e2e-test-crd-publish-openapi-8043-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec  3 15:06:42.911: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9000 apply -f -'
Dec  3 15:06:43.683: INFO: stderr: ""
Dec  3 15:06:43.683: INFO: stdout: "e2e-test-crd-publish-openapi-8043-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec  3 15:06:43.683: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-9000 delete e2e-test-crd-publish-openapi-8043-crds test-cr'
Dec  3 15:06:44.204: INFO: stderr: ""
Dec  3 15:06:44.204: INFO: stdout: "e2e-test-crd-publish-openapi-8043-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec  3 15:06:44.205: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config explain e2e-test-crd-publish-openapi-8043-crds'
Dec  3 15:06:44.720: INFO: stderr: ""
Dec  3 15:06:44.720: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8043-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:06:48.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9000" for this suite.
Dec  3 15:06:55.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:06:58.282: INFO: namespace crd-publish-openapi-9000 deletion completed in 9.409500229s
â€¢SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:06:58.282: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9750
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-a73499d3-f1cc-41e6-9989-976b5fbd681c
STEP: Creating secret with name s-test-opt-upd-af583725-213a-4f23-8b90-53cea730d545
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a73499d3-f1cc-41e6-9989-976b5fbd681c
STEP: Updating secret s-test-opt-upd-af583725-213a-4f23-8b90-53cea730d545
STEP: Creating secret with name s-test-opt-create-418adc51-5373-4358-b90f-f4e84ecae259
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:07:06.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9750" for this suite.
Dec  3 15:07:24.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:07:28.096: INFO: namespace projected-9750 deletion completed in 21.403254154s
â€¢SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:07:28.096: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1511
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1511
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Dec  3 15:07:29.005: INFO: Found 1 stateful pods, waiting for 3
Dec  3 15:07:39.095: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:07:39.095: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:07:39.096: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:07:39.365: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1511 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  3 15:07:40.656: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  3 15:07:40.656: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  3 15:07:40.656: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec  3 15:07:51.206: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec  3 15:07:51.476: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1511 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 15:07:52.746: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  3 15:07:52.746: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  3 15:07:52.746: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
Dec  3 15:08:23.286: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1511 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  3 15:08:24.579: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  3 15:08:24.579: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  3 15:08:24.579: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  3 15:08:24.949: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec  3 15:08:25.218: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1511 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 15:08:26.504: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  3 15:08:26.504: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  3 15:08:26.504: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  3 15:08:26.863: INFO: Waiting for StatefulSet statefulset-1511/ss2 to complete update
Dec  3 15:08:26.863: INFO: Waiting for Pod statefulset-1511/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec  3 15:08:26.863: INFO: Waiting for Pod statefulset-1511/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec  3 15:08:26.863: INFO: Waiting for Pod statefulset-1511/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec  3 15:08:37.045: INFO: Waiting for StatefulSet statefulset-1511/ss2 to complete update
Dec  3 15:08:37.045: INFO: Waiting for Pod statefulset-1511/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec  3 15:08:37.045: INFO: Waiting for Pod statefulset-1511/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec  3 15:08:47.044: INFO: Waiting for StatefulSet statefulset-1511/ss2 to complete update
Dec  3 15:08:47.044: INFO: Waiting for Pod statefulset-1511/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec  3 15:08:57.044: INFO: Deleting all statefulset in ns statefulset-1511
Dec  3 15:08:57.133: INFO: Scaling statefulset ss2 to 0
Dec  3 15:09:07.494: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 15:09:07.584: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:09:07.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1511" for this suite.
Dec  3 15:09:14.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:09:17.402: INFO: namespace statefulset-1511 deletion completed in 9.457417352s
â€¢SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:09:17.402: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8062
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec  3 15:09:18.040: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  3 15:09:18.310: INFO: Waiting for terminating namespaces to be deleted...
Dec  3 15:09:18.399: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-10-98.ec2.internal before test
Dec  3 15:09:18.600: INFO: calico-node-sjfqj from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.600: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:09:18.600: INFO: node-exporter-bt9wb from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.600: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 15:09:18.600: INFO: kube-proxy-877v6 from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.600: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:09:18.600: INFO: node-problem-detector-m2t47 from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.600: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 15:09:18.600: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-27-214.ec2.internal before test
Dec  3 15:09:18.707: INFO: calico-typha-deploy-9f6b455c4-xv9k8 from kube-system started at 2019-12-03 14:33:07 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.707: INFO: 	Container calico-typha ready: true, restart count 0
Dec  3 15:09:18.707: INFO: calico-node-drs2g from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.707: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:09:18.707: INFO: calico-typha-vertical-autoscaler-847d859f8c-m9mnl from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.707: INFO: 	Container autoscaler ready: true, restart count 4
Dec  3 15:09:18.707: INFO: vpn-shoot-57c94d6f78-cmlmw from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.707: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  3 15:09:18.707: INFO: metrics-server-778877cf87-5dxgx from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.707: INFO: 	Container metrics-server ready: true, restart count 0
Dec  3 15:09:18.707: INFO: addons-nginx-ingress-controller-7c75bb76db-pr4bf from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.707: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  3 15:09:18.707: INFO: addons-kubernetes-dashboard-78954cc66b-fj8m4 from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.707: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  3 15:09:18.707: INFO: calico-typha-horizontal-autoscaler-69df649c59-9tbvm from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.707: INFO: 	Container autoscaler ready: true, restart count 0
Dec  3 15:09:18.707: INFO: coredns-59c969ffb8-pbnhz from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.707: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:09:18.707: INFO: kube-proxy-pcff8 from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.707: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:09:18.707: INFO: node-problem-detector-j6tww from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.707: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 15:09:18.707: INFO: coredns-59c969ffb8-6vlmd from kube-system started at 2019-12-03 14:30:07 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.707: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:09:18.707: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-95f65778d-d2nzf from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.707: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  3 15:09:18.707: INFO: blackbox-exporter-7bd7b55dfc-nqf6r from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.707: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  3 15:09:18.707: INFO: node-exporter-f2j7c from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.707: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 15:09:18.707: INFO: calico-kube-controllers-79bcd784b6-w4fv5 from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:09:18.707: INFO: 	Container calico-kube-controllers ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c801196c-dba3-4dab-9fe1-cc052ca02002 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-c801196c-dba3-4dab-9fe1-cc052ca02002 off the node ip-10-250-10-98.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c801196c-dba3-4dab-9fe1-cc052ca02002
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:14:24.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8062" for this suite.
Dec  3 15:14:32.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:14:35.746: INFO: namespace sched-pred-8062 deletion completed in 11.406279589s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:318.344 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:14:35.746: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6341
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  3 15:14:36.477: INFO: Waiting up to 5m0s for pod "pod-ee6c98cf-1d0f-4e0b-bedf-11f5154c134c" in namespace "emptydir-6341" to be "success or failure"
Dec  3 15:14:36.567: INFO: Pod "pod-ee6c98cf-1d0f-4e0b-bedf-11f5154c134c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.28238ms
Dec  3 15:14:38.657: INFO: Pod "pod-ee6c98cf-1d0f-4e0b-bedf-11f5154c134c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17924332s
STEP: Saw pod success
Dec  3 15:14:38.657: INFO: Pod "pod-ee6c98cf-1d0f-4e0b-bedf-11f5154c134c" satisfied condition "success or failure"
Dec  3 15:14:38.749: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-ee6c98cf-1d0f-4e0b-bedf-11f5154c134c container test-container: <nil>
STEP: delete the pod
Dec  3 15:14:39.076: INFO: Waiting for pod pod-ee6c98cf-1d0f-4e0b-bedf-11f5154c134c to disappear
Dec  3 15:14:39.165: INFO: Pod pod-ee6c98cf-1d0f-4e0b-bedf-11f5154c134c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:14:39.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6341" for this suite.
Dec  3 15:14:45.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:14:48.650: INFO: namespace emptydir-6341 deletion completed in 9.395101643s
â€¢SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:14:48.651: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5145
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-5145
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5145
STEP: creating replication controller externalsvc in namespace services-5145
I1203 15:14:49.566111    5076 runners.go:184] Created replication controller with name: externalsvc, namespace: services-5145, replica count: 2
I1203 15:14:52.666717    5076 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec  3 15:14:52.940: INFO: Creating new exec pod
Dec  3 15:14:55.210: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-5145 execpodg5hvs -- /bin/sh -x -c nslookup clusterip-service'
Dec  3 15:14:56.487: INFO: stderr: "+ nslookup clusterip-service\n"
Dec  3 15:14:56.487: INFO: stdout: "Server:\t\t100.104.0.10\nAddress:\t100.104.0.10#53\n\nclusterip-service.services-5145.svc.cluster.local\tcanonical name = externalsvc.services-5145.svc.cluster.local.\nName:\texternalsvc.services-5145.svc.cluster.local\nAddress: 100.107.88.183\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5145, will wait for the garbage collector to delete the pods
Dec  3 15:14:56.768: INFO: Deleting ReplicationController externalsvc took: 91.204989ms
Dec  3 15:14:56.868: INFO: Terminating ReplicationController externalsvc pods took: 100.352672ms
Dec  3 15:15:03.164: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:15:03.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5145" for this suite.
Dec  3 15:15:09.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:15:12.739: INFO: namespace services-5145 deletion completed in 9.391932231s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:15:12.740: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4285
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 15:15:13.912: INFO: Create a RollingUpdate DaemonSet
Dec  3 15:15:14.001: INFO: Check that daemon pods launch on every node of the cluster
Dec  3 15:15:14.180: INFO: Number of nodes with available pods: 0
Dec  3 15:15:14.180: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 15:15:15.361: INFO: Number of nodes with available pods: 2
Dec  3 15:15:15.361: INFO: Number of running nodes: 2, number of available pods: 2
Dec  3 15:15:15.361: INFO: Update the DaemonSet to trigger a rollout
Dec  3 15:15:15.540: INFO: Updating DaemonSet daemon-set
Dec  3 15:15:23.900: INFO: Roll back the DaemonSet before rollout is complete
Dec  3 15:15:24.079: INFO: Updating DaemonSet daemon-set
Dec  3 15:15:24.079: INFO: Make sure DaemonSet rollback is complete
Dec  3 15:15:24.169: INFO: Wrong image for pod: daemon-set-kj4nv. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec  3 15:15:24.169: INFO: Pod daemon-set-kj4nv is not available
Dec  3 15:15:25.348: INFO: Wrong image for pod: daemon-set-kj4nv. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec  3 15:15:25.348: INFO: Pod daemon-set-kj4nv is not available
Dec  3 15:15:26.349: INFO: Pod daemon-set-mkgtf is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4285, will wait for the garbage collector to delete the pods
Dec  3 15:15:26.899: INFO: Deleting DaemonSet.extensions daemon-set took: 90.840152ms
Dec  3 15:15:26.999: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.303897ms
Dec  3 15:15:33.089: INFO: Number of nodes with available pods: 0
Dec  3 15:15:33.089: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 15:15:33.178: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4285/daemonsets","resourceVersion":"9060"},"items":null}

Dec  3 15:15:33.268: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4285/pods","resourceVersion":"9060"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:15:33.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4285" for this suite.
Dec  3 15:15:39.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:15:43.027: INFO: namespace daemonsets-4285 deletion completed in 9.399723501s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:15:43.027: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9173
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-btgz
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 15:15:43.936: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-btgz" in namespace "subpath-9173" to be "success or failure"
Dec  3 15:15:44.026: INFO: Pod "pod-subpath-test-projected-btgz": Phase="Pending", Reason="", readiness=false. Elapsed: 89.385875ms
Dec  3 15:15:46.116: INFO: Pod "pod-subpath-test-projected-btgz": Phase="Running", Reason="", readiness=true. Elapsed: 2.179520896s
Dec  3 15:15:48.205: INFO: Pod "pod-subpath-test-projected-btgz": Phase="Running", Reason="", readiness=true. Elapsed: 4.26906238s
Dec  3 15:15:50.295: INFO: Pod "pod-subpath-test-projected-btgz": Phase="Running", Reason="", readiness=true. Elapsed: 6.358995958s
Dec  3 15:15:52.385: INFO: Pod "pod-subpath-test-projected-btgz": Phase="Running", Reason="", readiness=true. Elapsed: 8.449032624s
Dec  3 15:15:54.475: INFO: Pod "pod-subpath-test-projected-btgz": Phase="Running", Reason="", readiness=true. Elapsed: 10.538992183s
Dec  3 15:15:56.565: INFO: Pod "pod-subpath-test-projected-btgz": Phase="Running", Reason="", readiness=true. Elapsed: 12.629000245s
Dec  3 15:15:58.656: INFO: Pod "pod-subpath-test-projected-btgz": Phase="Running", Reason="", readiness=true. Elapsed: 14.719311356s
Dec  3 15:16:00.746: INFO: Pod "pod-subpath-test-projected-btgz": Phase="Running", Reason="", readiness=true. Elapsed: 16.809738923s
Dec  3 15:16:02.836: INFO: Pod "pod-subpath-test-projected-btgz": Phase="Running", Reason="", readiness=true. Elapsed: 18.89985266s
Dec  3 15:16:04.926: INFO: Pod "pod-subpath-test-projected-btgz": Phase="Running", Reason="", readiness=true. Elapsed: 20.989727498s
Dec  3 15:16:07.016: INFO: Pod "pod-subpath-test-projected-btgz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.079828127s
STEP: Saw pod success
Dec  3 15:16:07.016: INFO: Pod "pod-subpath-test-projected-btgz" satisfied condition "success or failure"
Dec  3 15:16:07.106: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-subpath-test-projected-btgz container test-container-subpath-projected-btgz: <nil>
STEP: delete the pod
Dec  3 15:16:07.302: INFO: Waiting for pod pod-subpath-test-projected-btgz to disappear
Dec  3 15:16:07.391: INFO: Pod pod-subpath-test-projected-btgz no longer exists
STEP: Deleting pod pod-subpath-test-projected-btgz
Dec  3 15:16:07.391: INFO: Deleting pod "pod-subpath-test-projected-btgz" in namespace "subpath-9173"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:16:07.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9173" for this suite.
Dec  3 15:16:13.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:16:16.965: INFO: namespace subpath-9173 deletion completed in 9.390125471s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:16:16.966: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9679
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:16:17.697: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5465376c-5564-4967-8ade-dd33c7174791" in namespace "downward-api-9679" to be "success or failure"
Dec  3 15:16:17.786: INFO: Pod "downwardapi-volume-5465376c-5564-4967-8ade-dd33c7174791": Phase="Pending", Reason="", readiness=false. Elapsed: 89.475358ms
Dec  3 15:16:19.877: INFO: Pod "downwardapi-volume-5465376c-5564-4967-8ade-dd33c7174791": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180191506s
STEP: Saw pod success
Dec  3 15:16:19.877: INFO: Pod "downwardapi-volume-5465376c-5564-4967-8ade-dd33c7174791" satisfied condition "success or failure"
Dec  3 15:16:19.973: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-5465376c-5564-4967-8ade-dd33c7174791 container client-container: <nil>
STEP: delete the pod
Dec  3 15:16:20.164: INFO: Waiting for pod downwardapi-volume-5465376c-5564-4967-8ade-dd33c7174791 to disappear
Dec  3 15:16:20.253: INFO: Pod downwardapi-volume-5465376c-5564-4967-8ade-dd33c7174791 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:16:20.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9679" for this suite.
Dec  3 15:16:26.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:16:29.741: INFO: namespace downward-api-9679 deletion completed in 9.397599318s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:16:29.742: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3203
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Dec  3 15:16:30.470: INFO: Waiting up to 5m0s for pod "var-expansion-5c2f050e-af59-4830-b50c-e8007f6f2b06" in namespace "var-expansion-3203" to be "success or failure"
Dec  3 15:16:30.561: INFO: Pod "var-expansion-5c2f050e-af59-4830-b50c-e8007f6f2b06": Phase="Pending", Reason="", readiness=false. Elapsed: 90.1434ms
Dec  3 15:16:32.650: INFO: Pod "var-expansion-5c2f050e-af59-4830-b50c-e8007f6f2b06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179921995s
STEP: Saw pod success
Dec  3 15:16:32.650: INFO: Pod "var-expansion-5c2f050e-af59-4830-b50c-e8007f6f2b06" satisfied condition "success or failure"
Dec  3 15:16:32.740: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod var-expansion-5c2f050e-af59-4830-b50c-e8007f6f2b06 container dapi-container: <nil>
STEP: delete the pod
Dec  3 15:16:32.929: INFO: Waiting for pod var-expansion-5c2f050e-af59-4830-b50c-e8007f6f2b06 to disappear
Dec  3 15:16:33.018: INFO: Pod var-expansion-5c2f050e-af59-4830-b50c-e8007f6f2b06 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:16:33.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3203" for this suite.
Dec  3 15:16:39.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:16:42.504: INFO: namespace var-expansion-3203 deletion completed in 9.396555984s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:16:42.505: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9792
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec  3 15:16:43.234: INFO: Waiting up to 5m0s for pod "downward-api-4ae0deeb-3ebd-43cf-833e-9f8b28d164ca" in namespace "downward-api-9792" to be "success or failure"
Dec  3 15:16:43.324: INFO: Pod "downward-api-4ae0deeb-3ebd-43cf-833e-9f8b28d164ca": Phase="Pending", Reason="", readiness=false. Elapsed: 89.82358ms
Dec  3 15:16:45.414: INFO: Pod "downward-api-4ae0deeb-3ebd-43cf-833e-9f8b28d164ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179828926s
STEP: Saw pod success
Dec  3 15:16:45.414: INFO: Pod "downward-api-4ae0deeb-3ebd-43cf-833e-9f8b28d164ca" satisfied condition "success or failure"
Dec  3 15:16:45.504: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downward-api-4ae0deeb-3ebd-43cf-833e-9f8b28d164ca container dapi-container: <nil>
STEP: delete the pod
Dec  3 15:16:45.696: INFO: Waiting for pod downward-api-4ae0deeb-3ebd-43cf-833e-9f8b28d164ca to disappear
Dec  3 15:16:45.785: INFO: Pod downward-api-4ae0deeb-3ebd-43cf-833e-9f8b28d164ca no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:16:45.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9792" for this suite.
Dec  3 15:16:52.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:16:55.268: INFO: namespace downward-api-9792 deletion completed in 9.392728407s
â€¢SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:16:55.268: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8345
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec  3 15:16:55.904: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:16:59.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8345" for this suite.
Dec  3 15:17:28.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:17:31.206: INFO: namespace init-container-8345 deletion completed in 31.442299672s
â€¢SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:17:31.208: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5114
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5114
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5114
STEP: Creating statefulset with conflicting port in namespace statefulset-5114
STEP: Waiting until pod test-pod will start running in namespace statefulset-5114
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5114
Dec  3 15:17:34.479: INFO: Observed stateful pod in namespace: statefulset-5114, name: ss-0, uid: 561ee3f1-a677-4e3b-8b64-4222b51ee8a9, status phase: Failed. Waiting for statefulset controller to delete.
Dec  3 15:17:34.515: INFO: Observed stateful pod in namespace: statefulset-5114, name: ss-0, uid: 561ee3f1-a677-4e3b-8b64-4222b51ee8a9, status phase: Failed. Waiting for statefulset controller to delete.
Dec  3 15:17:34.516: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5114
STEP: Removing pod with conflicting port in namespace statefulset-5114
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5114 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec  3 15:17:36.789: INFO: Deleting all statefulset in ns statefulset-5114
Dec  3 15:17:36.879: INFO: Scaling statefulset ss to 0
Dec  3 15:17:47.239: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 15:17:47.328: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:17:47.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5114" for this suite.
Dec  3 15:17:53.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:17:57.087: INFO: namespace statefulset-5114 deletion completed in 9.397468624s
â€¢SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:17:57.087: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7518
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-7518
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  3 15:17:57.723: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  3 15:18:17.253: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.85:8080/dial?request=hostName&protocol=udp&host=100.64.0.30&port=8081&tries=1'] Namespace:pod-network-test-7518 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:18:17.253: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:18:18.134: INFO: Waiting for endpoints: map[]
Dec  3 15:18:18.224: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.85:8080/dial?request=hostName&protocol=udp&host=100.64.1.84&port=8081&tries=1'] Namespace:pod-network-test-7518 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:18:18.224: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:18:19.090: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:18:19.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7518" for this suite.
Dec  3 15:18:31.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:18:34.584: INFO: namespace pod-network-test-7518 deletion completed in 15.404319358s
â€¢SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:18:34.584: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5815
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 15:18:35.222: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec  3 15:18:39.433: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5815 create -f -'
Dec  3 15:18:40.904: INFO: stderr: ""
Dec  3 15:18:40.904: INFO: stdout: "e2e-test-crd-publish-openapi-2966-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec  3 15:18:40.904: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5815 delete e2e-test-crd-publish-openapi-2966-crds test-cr'
Dec  3 15:18:41.415: INFO: stderr: ""
Dec  3 15:18:41.415: INFO: stdout: "e2e-test-crd-publish-openapi-2966-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec  3 15:18:41.415: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5815 apply -f -'
Dec  3 15:18:42.557: INFO: stderr: ""
Dec  3 15:18:42.557: INFO: stdout: "e2e-test-crd-publish-openapi-2966-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec  3 15:18:42.557: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5815 delete e2e-test-crd-publish-openapi-2966-crds test-cr'
Dec  3 15:18:43.074: INFO: stderr: ""
Dec  3 15:18:43.074: INFO: stdout: "e2e-test-crd-publish-openapi-2966-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec  3 15:18:43.074: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config explain e2e-test-crd-publish-openapi-2966-crds'
Dec  3 15:18:43.922: INFO: stderr: ""
Dec  3 15:18:43.922: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2966-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:18:47.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5815" for this suite.
Dec  3 15:18:54.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:18:57.459: INFO: namespace crd-publish-openapi-5815 deletion completed in 9.394889349s
â€¢SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:18:57.460: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 15:18:59.205: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983138, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983138, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983138, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983138, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:19:01.295: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983138, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983138, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983138, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983138, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 15:19:04.390: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:19:04.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3771" for this suite.
Dec  3 15:19:11.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:19:14.224: INFO: namespace webhook-3771 deletion completed in 9.387374071s
STEP: Destroying namespace "webhook-3771-markers" for this suite.
Dec  3 15:19:20.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:19:23.619: INFO: namespace webhook-3771-markers deletion completed in 9.395042947s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:19:23.980: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 15:19:27.162: INFO: Waiting up to 5m0s for pod "client-envvars-ae4b5ffe-9033-41c9-b01d-cc9228dcb87d" in namespace "pods-4672" to be "success or failure"
Dec  3 15:19:27.252: INFO: Pod "client-envvars-ae4b5ffe-9033-41c9-b01d-cc9228dcb87d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.35919ms
Dec  3 15:19:29.341: INFO: Pod "client-envvars-ae4b5ffe-9033-41c9-b01d-cc9228dcb87d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179035001s
STEP: Saw pod success
Dec  3 15:19:29.341: INFO: Pod "client-envvars-ae4b5ffe-9033-41c9-b01d-cc9228dcb87d" satisfied condition "success or failure"
Dec  3 15:19:29.431: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod client-envvars-ae4b5ffe-9033-41c9-b01d-cc9228dcb87d container env3cont: <nil>
STEP: delete the pod
Dec  3 15:19:29.714: INFO: Waiting for pod client-envvars-ae4b5ffe-9033-41c9-b01d-cc9228dcb87d to disappear
Dec  3 15:19:29.804: INFO: Pod client-envvars-ae4b5ffe-9033-41c9-b01d-cc9228dcb87d no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:19:29.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4672" for this suite.
Dec  3 15:19:58.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:20:01.403: INFO: namespace pods-4672 deletion completed in 31.508748408s
â€¢SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:20:01.404: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1686
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:20:10.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1686" for this suite.
Dec  3 15:20:16.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:20:19.723: INFO: namespace job-1686 deletion completed in 9.391277003s
â€¢SS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:20:19.724: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6641
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec  3 15:20:20.454: INFO: Waiting up to 5m0s for pod "downward-api-6b8df5fe-d259-4d26-9d2b-a1226ef26ef9" in namespace "downward-api-6641" to be "success or failure"
Dec  3 15:20:20.544: INFO: Pod "downward-api-6b8df5fe-d259-4d26-9d2b-a1226ef26ef9": Phase="Pending", Reason="", readiness=false. Elapsed: 89.233021ms
Dec  3 15:20:22.633: INFO: Pod "downward-api-6b8df5fe-d259-4d26-9d2b-a1226ef26ef9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179042393s
STEP: Saw pod success
Dec  3 15:20:22.633: INFO: Pod "downward-api-6b8df5fe-d259-4d26-9d2b-a1226ef26ef9" satisfied condition "success or failure"
Dec  3 15:20:22.725: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downward-api-6b8df5fe-d259-4d26-9d2b-a1226ef26ef9 container dapi-container: <nil>
STEP: delete the pod
Dec  3 15:20:23.026: INFO: Waiting for pod downward-api-6b8df5fe-d259-4d26-9d2b-a1226ef26ef9 to disappear
Dec  3 15:20:23.115: INFO: Pod downward-api-6b8df5fe-d259-4d26-9d2b-a1226ef26ef9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:20:23.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6641" for this suite.
Dec  3 15:20:29.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:20:32.613: INFO: namespace downward-api-6641 deletion completed in 9.407171543s
â€¢SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:20:32.613: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4028
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Dec  3 15:20:33.342: INFO: Waiting up to 5m0s for pod "var-expansion-ca59029f-6202-41dd-99e0-024bdca5462d" in namespace "var-expansion-4028" to be "success or failure"
Dec  3 15:20:33.432: INFO: Pod "var-expansion-ca59029f-6202-41dd-99e0-024bdca5462d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.21633ms
Dec  3 15:20:35.522: INFO: Pod "var-expansion-ca59029f-6202-41dd-99e0-024bdca5462d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179018084s
STEP: Saw pod success
Dec  3 15:20:35.522: INFO: Pod "var-expansion-ca59029f-6202-41dd-99e0-024bdca5462d" satisfied condition "success or failure"
Dec  3 15:20:35.611: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod var-expansion-ca59029f-6202-41dd-99e0-024bdca5462d container dapi-container: <nil>
STEP: delete the pod
Dec  3 15:20:35.800: INFO: Waiting for pod var-expansion-ca59029f-6202-41dd-99e0-024bdca5462d to disappear
Dec  3 15:20:35.889: INFO: Pod var-expansion-ca59029f-6202-41dd-99e0-024bdca5462d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:20:35.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4028" for this suite.
Dec  3 15:20:42.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:20:45.379: INFO: namespace var-expansion-4028 deletion completed in 9.399658022s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:20:45.380: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-451
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec  3 15:20:46.202: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:21:03.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-451" for this suite.
Dec  3 15:21:09.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:21:12.563: INFO: namespace pods-451 deletion completed in 9.404992198s
â€¢SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:21:12.563: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8361
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-8361/secret-test-51293674-2c48-4ff2-94e4-b0ed71247c13
STEP: Creating a pod to test consume secrets
Dec  3 15:21:13.381: INFO: Waiting up to 5m0s for pod "pod-configmaps-95131147-0dc4-44b6-83a4-6ae9c7e03751" in namespace "secrets-8361" to be "success or failure"
Dec  3 15:21:13.470: INFO: Pod "pod-configmaps-95131147-0dc4-44b6-83a4-6ae9c7e03751": Phase="Pending", Reason="", readiness=false. Elapsed: 88.962645ms
Dec  3 15:21:15.560: INFO: Pod "pod-configmaps-95131147-0dc4-44b6-83a4-6ae9c7e03751": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178497466s
STEP: Saw pod success
Dec  3 15:21:15.560: INFO: Pod "pod-configmaps-95131147-0dc4-44b6-83a4-6ae9c7e03751" satisfied condition "success or failure"
Dec  3 15:21:15.649: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-configmaps-95131147-0dc4-44b6-83a4-6ae9c7e03751 container env-test: <nil>
STEP: delete the pod
Dec  3 15:21:15.836: INFO: Waiting for pod pod-configmaps-95131147-0dc4-44b6-83a4-6ae9c7e03751 to disappear
Dec  3 15:21:15.926: INFO: Pod pod-configmaps-95131147-0dc4-44b6-83a4-6ae9c7e03751 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:21:15.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8361" for this suite.
Dec  3 15:21:22.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:21:25.461: INFO: namespace secrets-8361 deletion completed in 9.445449344s
â€¢SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:21:25.462: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8257
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Dec  3 15:21:26.101: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-8257 -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec  3 15:21:26.544: INFO: stderr: ""
Dec  3 15:21:26.544: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Dec  3 15:21:26.544: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec  3 15:21:26.544: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8257" to be "running and ready, or succeeded"
Dec  3 15:21:26.634: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 89.883974ms
Dec  3 15:21:28.724: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.179891062s
Dec  3 15:21:28.724: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec  3 15:21:28.724: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec  3 15:21:28.724: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs logs-generator logs-generator --namespace=kubectl-8257'
Dec  3 15:21:29.244: INFO: stderr: ""
Dec  3 15:21:29.244: INFO: stdout: "I1203 15:21:27.265650       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/b9gl 557\nI1203 15:21:27.465764       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/h7c 303\nI1203 15:21:27.665768       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/6h7 262\nI1203 15:21:27.865768       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/hlxs 407\nI1203 15:21:28.065759       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/wjv6 223\nI1203 15:21:28.265780       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/jmr 467\nI1203 15:21:28.465766       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/wwb 306\nI1203 15:21:28.665771       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/72mr 342\nI1203 15:21:28.865772       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/525l 466\nI1203 15:21:29.065774       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/qq6 367\n"
STEP: limiting log lines
Dec  3 15:21:29.245: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs logs-generator logs-generator --namespace=kubectl-8257 --tail=1'
Dec  3 15:21:29.904: INFO: stderr: ""
Dec  3 15:21:29.904: INFO: stdout: "I1203 15:21:29.665756       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/9lk 323\n"
STEP: limiting log bytes
Dec  3 15:21:29.904: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs logs-generator logs-generator --namespace=kubectl-8257 --limit-bytes=1'
Dec  3 15:21:30.427: INFO: stderr: ""
Dec  3 15:21:30.427: INFO: stdout: "I"
STEP: exposing timestamps
Dec  3 15:21:30.428: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs logs-generator logs-generator --namespace=kubectl-8257 --tail=1 --timestamps'
Dec  3 15:21:30.949: INFO: stderr: ""
Dec  3 15:21:30.949: INFO: stdout: "2019-12-03T15:21:30.86591011Z I1203 15:21:30.865784       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/xsc 407\n"
STEP: restricting to a time range
Dec  3 15:21:33.449: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs logs-generator logs-generator --namespace=kubectl-8257 --since=1s'
Dec  3 15:21:33.973: INFO: stderr: ""
Dec  3 15:21:33.973: INFO: stdout: "I1203 15:21:33.065759       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/bzdh 302\nI1203 15:21:33.265783       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/rn72 470\nI1203 15:21:33.465818       1 logs_generator.go:76] 31 GET /api/v1/namespaces/ns/pods/h24g 461\nI1203 15:21:33.665790       1 logs_generator.go:76] 32 GET /api/v1/namespaces/kube-system/pods/p2j8 453\nI1203 15:21:33.865779       1 logs_generator.go:76] 33 POST /api/v1/namespaces/kube-system/pods/pf92 374\n"
Dec  3 15:21:33.973: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs logs-generator logs-generator --namespace=kubectl-8257 --since=24h'
Dec  3 15:21:34.591: INFO: stderr: ""
Dec  3 15:21:34.591: INFO: stdout: "I1203 15:21:27.265650       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/kube-system/pods/b9gl 557\nI1203 15:21:27.465764       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/h7c 303\nI1203 15:21:27.665768       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/6h7 262\nI1203 15:21:27.865768       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/hlxs 407\nI1203 15:21:28.065759       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/wjv6 223\nI1203 15:21:28.265780       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/jmr 467\nI1203 15:21:28.465766       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/wwb 306\nI1203 15:21:28.665771       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/72mr 342\nI1203 15:21:28.865772       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/525l 466\nI1203 15:21:29.065774       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/qq6 367\nI1203 15:21:29.265727       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/bz9g 511\nI1203 15:21:29.465765       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/cgj 303\nI1203 15:21:29.665756       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/9lk 323\nI1203 15:21:29.865777       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/default/pods/6w4 322\nI1203 15:21:30.065788       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/bl2 300\nI1203 15:21:30.265730       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/7sr 239\nI1203 15:21:30.465777       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/bssw 514\nI1203 15:21:30.665760       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/j4f6 399\nI1203 15:21:30.865784       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/xsc 407\nI1203 15:21:31.065817       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/lckm 515\nI1203 15:21:31.265826       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/76f 452\nI1203 15:21:31.465766       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/xgkk 229\nI1203 15:21:31.665779       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/kb4 522\nI1203 15:21:31.865773       1 logs_generator.go:76] 23 GET /api/v1/namespaces/ns/pods/2hd 533\nI1203 15:21:32.065811       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/662 500\nI1203 15:21:32.265791       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/4tsv 477\nI1203 15:21:32.465812       1 logs_generator.go:76] 26 GET /api/v1/namespaces/default/pods/hvq5 353\nI1203 15:21:32.665837       1 logs_generator.go:76] 27 POST /api/v1/namespaces/kube-system/pods/fkz 310\nI1203 15:21:32.865766       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/gv7 372\nI1203 15:21:33.065759       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/bzdh 302\nI1203 15:21:33.265783       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/ns/pods/rn72 470\nI1203 15:21:33.465818       1 logs_generator.go:76] 31 GET /api/v1/namespaces/ns/pods/h24g 461\nI1203 15:21:33.665790       1 logs_generator.go:76] 32 GET /api/v1/namespaces/kube-system/pods/p2j8 453\nI1203 15:21:33.865779       1 logs_generator.go:76] 33 POST /api/v1/namespaces/kube-system/pods/pf92 374\nI1203 15:21:34.065811       1 logs_generator.go:76] 34 GET /api/v1/namespaces/kube-system/pods/nzb8 335\nI1203 15:21:34.265873       1 logs_generator.go:76] 35 POST /api/v1/namespaces/kube-system/pods/mv4 464\nI1203 15:21:34.465777       1 logs_generator.go:76] 36 PUT /api/v1/namespaces/kube-system/pods/756 286\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Dec  3 15:21:34.592: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pod logs-generator --namespace=kubectl-8257'
Dec  3 15:21:37.153: INFO: stderr: ""
Dec  3 15:21:37.153: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:21:37.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8257" for this suite.
Dec  3 15:21:43.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:21:46.633: INFO: namespace kubectl-8257 deletion completed in 9.389978757s
â€¢SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:21:46.634: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9734
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec  3 15:21:47.271: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9734'
Dec  3 15:21:47.705: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 15:21:47.705: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Dec  3 15:21:47.794: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete jobs e2e-test-httpd-job --namespace=kubectl-9734'
Dec  3 15:21:48.304: INFO: stderr: ""
Dec  3 15:21:48.304: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:21:48.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9734" for this suite.
Dec  3 15:21:54.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:21:57.789: INFO: namespace kubectl-9734 deletion completed in 9.394690818s
â€¢
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:21:57.789: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8340
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:22:15.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8340" for this suite.
Dec  3 15:22:21.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:22:24.638: INFO: namespace resourcequota-8340 deletion completed in 9.398151582s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:22:24.638: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-872
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 15:22:25.276: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec  3 15:22:25.813: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:22:25.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-872" for this suite.
Dec  3 15:22:32.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:22:35.382: INFO: namespace replication-controller-872 deletion completed in 9.389043327s
â€¢SSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:22:35.382: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Dec  3 15:22:38.560: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec  3 15:22:54.075: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:22:54.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6736" for this suite.
Dec  3 15:23:00.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:23:03.656: INFO: namespace pods-6736 deletion completed in 9.400981728s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:23:03.656: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4325
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  3 15:23:09.103: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:23:09.193: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:23:11.193: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:23:11.282: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:23:13.193: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:23:13.283: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:23:15.193: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:23:15.283: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:23:17.193: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:23:17.283: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:23:19.193: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:23:19.283: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:23:21.193: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:23:21.283: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  3 15:23:23.193: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  3 15:23:23.283: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:23:23.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4325" for this suite.
Dec  3 15:23:35.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:23:38.869: INFO: namespace container-lifecycle-hook-4325 deletion completed in 15.399383773s
â€¢SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:23:38.869: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7477
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:23:39.600: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2a22d30a-05db-449c-a088-89fa135c02df" in namespace "projected-7477" to be "success or failure"
Dec  3 15:23:39.690: INFO: Pod "downwardapi-volume-2a22d30a-05db-449c-a088-89fa135c02df": Phase="Pending", Reason="", readiness=false. Elapsed: 89.819258ms
Dec  3 15:23:41.779: INFO: Pod "downwardapi-volume-2a22d30a-05db-449c-a088-89fa135c02df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179762112s
STEP: Saw pod success
Dec  3 15:23:41.780: INFO: Pod "downwardapi-volume-2a22d30a-05db-449c-a088-89fa135c02df" satisfied condition "success or failure"
Dec  3 15:23:41.869: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-2a22d30a-05db-449c-a088-89fa135c02df container client-container: <nil>
STEP: delete the pod
Dec  3 15:23:42.058: INFO: Waiting for pod downwardapi-volume-2a22d30a-05db-449c-a088-89fa135c02df to disappear
Dec  3 15:23:42.147: INFO: Pod downwardapi-volume-2a22d30a-05db-449c-a088-89fa135c02df no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:23:42.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7477" for this suite.
Dec  3 15:23:48.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:23:51.636: INFO: namespace projected-7477 deletion completed in 9.39816231s
â€¢SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:23:51.636: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7411
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-83c9cd1b-5809-49ed-9972-d87ee39a26fd in namespace container-probe-7411
Dec  3 15:23:54.546: INFO: Started pod busybox-83c9cd1b-5809-49ed-9972-d87ee39a26fd in namespace container-probe-7411
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 15:23:54.636: INFO: Initial restart count of pod busybox-83c9cd1b-5809-49ed-9972-d87ee39a26fd is 0
Dec  3 15:24:42.797: INFO: Restart count of pod container-probe-7411/busybox-83c9cd1b-5809-49ed-9972-d87ee39a26fd is now 1 (48.160840471s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:24:42.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7411" for this suite.
Dec  3 15:24:49.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:24:52.388: INFO: namespace container-probe-7411 deletion completed in 9.406792955s
â€¢SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:24:52.388: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1992
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec  3 15:24:55.660: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:24:55.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1992" for this suite.
Dec  3 15:25:24.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:25:27.418: INFO: namespace replicaset-1992 deletion completed in 31.397200353s
â€¢SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:25:27.418: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5989
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 15:25:28.058: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Dec  3 15:25:32.190: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5989 create -f -'
Dec  3 15:25:33.669: INFO: stderr: ""
Dec  3 15:25:33.670: INFO: stdout: "e2e-test-crd-publish-openapi-8058-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec  3 15:25:33.670: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5989 delete e2e-test-crd-publish-openapi-8058-crds test-foo'
Dec  3 15:25:34.207: INFO: stderr: ""
Dec  3 15:25:34.207: INFO: stdout: "e2e-test-crd-publish-openapi-8058-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec  3 15:25:34.207: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5989 apply -f -'
Dec  3 15:25:35.330: INFO: stderr: ""
Dec  3 15:25:35.330: INFO: stdout: "e2e-test-crd-publish-openapi-8058-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec  3 15:25:35.330: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5989 delete e2e-test-crd-publish-openapi-8058-crds test-foo'
Dec  3 15:25:35.843: INFO: stderr: ""
Dec  3 15:25:35.843: INFO: stdout: "e2e-test-crd-publish-openapi-8058-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec  3 15:25:35.843: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5989 create -f -'
Dec  3 15:25:36.339: INFO: rc: 1
Dec  3 15:25:36.339: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5989 apply -f -'
Dec  3 15:25:37.190: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Dec  3 15:25:37.190: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5989 create -f -'
Dec  3 15:25:38.039: INFO: rc: 1
Dec  3 15:25:38.039: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=crd-publish-openapi-5989 apply -f -'
Dec  3 15:25:38.538: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec  3 15:25:38.538: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config explain e2e-test-crd-publish-openapi-8058-crds'
Dec  3 15:25:39.040: INFO: stderr: ""
Dec  3 15:25:39.040: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8058-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec  3 15:25:39.040: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config explain e2e-test-crd-publish-openapi-8058-crds.metadata'
Dec  3 15:25:39.542: INFO: stderr: ""
Dec  3 15:25:39.542: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8058-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec  3 15:25:39.542: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config explain e2e-test-crd-publish-openapi-8058-crds.spec'
Dec  3 15:25:40.059: INFO: stderr: ""
Dec  3 15:25:40.059: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8058-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec  3 15:25:40.059: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config explain e2e-test-crd-publish-openapi-8058-crds.spec.bars'
Dec  3 15:25:40.905: INFO: stderr: ""
Dec  3 15:25:40.905: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8058-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec  3 15:25:40.905: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config explain e2e-test-crd-publish-openapi-8058-crds.spec.bars2'
Dec  3 15:25:41.759: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:25:45.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5989" for this suite.
Dec  3 15:25:52.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:25:55.267: INFO: namespace crd-publish-openapi-5989 deletion completed in 9.400493962s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:25:55.268: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-862
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-f80cb8b4-192e-4aed-b223-f6ee5f0b2d50
STEP: Creating a pod to test consume configMaps
Dec  3 15:25:56.088: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-db737fdf-24e5-4ba4-b33b-849ebaa7c90c" in namespace "projected-862" to be "success or failure"
Dec  3 15:25:56.178: INFO: Pod "pod-projected-configmaps-db737fdf-24e5-4ba4-b33b-849ebaa7c90c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.785942ms
Dec  3 15:25:58.268: INFO: Pod "pod-projected-configmaps-db737fdf-24e5-4ba4-b33b-849ebaa7c90c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179750201s
STEP: Saw pod success
Dec  3 15:25:58.268: INFO: Pod "pod-projected-configmaps-db737fdf-24e5-4ba4-b33b-849ebaa7c90c" satisfied condition "success or failure"
Dec  3 15:25:58.358: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-projected-configmaps-db737fdf-24e5-4ba4-b33b-849ebaa7c90c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:25:58.684: INFO: Waiting for pod pod-projected-configmaps-db737fdf-24e5-4ba4-b33b-849ebaa7c90c to disappear
Dec  3 15:25:58.774: INFO: Pod pod-projected-configmaps-db737fdf-24e5-4ba4-b33b-849ebaa7c90c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:25:58.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-862" for this suite.
Dec  3 15:26:05.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:26:08.267: INFO: namespace projected-862 deletion completed in 9.402007636s
â€¢SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:26:08.268: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4858
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-05acb72f-ff05-413b-9b99-74cdcc853b15
STEP: Creating a pod to test consume secrets
Dec  3 15:26:09.088: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6beb695f-3d61-4ee7-a12b-110f4957378d" in namespace "projected-4858" to be "success or failure"
Dec  3 15:26:09.178: INFO: Pod "pod-projected-secrets-6beb695f-3d61-4ee7-a12b-110f4957378d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.642271ms
Dec  3 15:26:11.267: INFO: Pod "pod-projected-secrets-6beb695f-3d61-4ee7-a12b-110f4957378d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179234364s
STEP: Saw pod success
Dec  3 15:26:11.268: INFO: Pod "pod-projected-secrets-6beb695f-3d61-4ee7-a12b-110f4957378d" satisfied condition "success or failure"
Dec  3 15:26:11.357: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-projected-secrets-6beb695f-3d61-4ee7-a12b-110f4957378d container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:26:11.544: INFO: Waiting for pod pod-projected-secrets-6beb695f-3d61-4ee7-a12b-110f4957378d to disappear
Dec  3 15:26:11.633: INFO: Pod pod-projected-secrets-6beb695f-3d61-4ee7-a12b-110f4957378d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:26:11.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4858" for this suite.
Dec  3 15:26:17.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:26:21.128: INFO: namespace projected-4858 deletion completed in 9.404154715s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:26:21.128: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3447
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:26:21.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3447" for this suite.
Dec  3 15:26:34.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:26:37.445: INFO: namespace pods-3447 deletion completed in 15.404338551s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:26:37.445: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7480
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-92a68be7-5aa0-43e1-b5ef-e3f93b87aade
STEP: Creating a pod to test consume secrets
Dec  3 15:26:38.263: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-105577ed-2c1e-4ad8-a0d5-6b0080cc3ab8" in namespace "projected-7480" to be "success or failure"
Dec  3 15:26:38.353: INFO: Pod "pod-projected-secrets-105577ed-2c1e-4ad8-a0d5-6b0080cc3ab8": Phase="Pending", Reason="", readiness=false. Elapsed: 89.78094ms
Dec  3 15:26:40.443: INFO: Pod "pod-projected-secrets-105577ed-2c1e-4ad8-a0d5-6b0080cc3ab8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179672062s
STEP: Saw pod success
Dec  3 15:26:40.443: INFO: Pod "pod-projected-secrets-105577ed-2c1e-4ad8-a0d5-6b0080cc3ab8" satisfied condition "success or failure"
Dec  3 15:26:40.532: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-projected-secrets-105577ed-2c1e-4ad8-a0d5-6b0080cc3ab8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:26:40.728: INFO: Waiting for pod pod-projected-secrets-105577ed-2c1e-4ad8-a0d5-6b0080cc3ab8 to disappear
Dec  3 15:26:40.817: INFO: Pod pod-projected-secrets-105577ed-2c1e-4ad8-a0d5-6b0080cc3ab8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:26:40.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7480" for this suite.
Dec  3 15:26:47.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:26:50.305: INFO: namespace projected-7480 deletion completed in 9.396897265s
â€¢SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:26:50.305: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4869
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4869
I1203 15:26:51.306928    5076 runners.go:184] Created replication controller with name: externalname-service, namespace: services-4869, replica count: 2
Dec  3 15:26:54.407: INFO: Creating new exec pod
I1203 15:26:54.407648    5076 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  3 15:26:57.679: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-4869 execpoddcrpw -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec  3 15:26:58.988: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec  3 15:26:58.988: INFO: stdout: ""
Dec  3 15:26:58.988: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-4869 execpoddcrpw -- /bin/sh -x -c nc -zv -t -w 2 100.106.105.17 80'
Dec  3 15:27:00.283: INFO: stderr: "+ nc -zv -t -w 2 100.106.105.17 80\nConnection to 100.106.105.17 80 port [tcp/http] succeeded!\n"
Dec  3 15:27:00.283: INFO: stdout: ""
Dec  3 15:27:00.283: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:27:00.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4869" for this suite.
Dec  3 15:27:06.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:27:09.858: INFO: namespace services-4869 deletion completed in 9.38898574s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95
â€¢SSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:27:09.858: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-9118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Dec  3 15:27:10.498: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Dec  3 15:27:11.967: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:27:14.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:27:16.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:27:18.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:27:20.059: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710983631, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 15:27:24.157: INFO: Waited 2.008575423s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:27:27.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9118" for this suite.
Dec  3 15:27:33.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:27:36.708: INFO: namespace aggregator-9118 deletion completed in 9.401959965s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:27:36.708: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-7618
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Dec  3 15:27:38.299: INFO: created pod pod-service-account-defaultsa
Dec  3 15:27:38.299: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec  3 15:27:38.390: INFO: created pod pod-service-account-mountsa
Dec  3 15:27:38.390: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec  3 15:27:38.479: INFO: created pod pod-service-account-nomountsa
Dec  3 15:27:38.479: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec  3 15:27:38.569: INFO: created pod pod-service-account-defaultsa-mountspec
Dec  3 15:27:38.569: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec  3 15:27:38.659: INFO: created pod pod-service-account-mountsa-mountspec
Dec  3 15:27:38.659: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec  3 15:27:38.750: INFO: created pod pod-service-account-nomountsa-mountspec
Dec  3 15:27:38.750: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec  3 15:27:38.841: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec  3 15:27:38.841: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec  3 15:27:38.932: INFO: created pod pod-service-account-mountsa-nomountspec
Dec  3 15:27:38.932: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec  3 15:27:39.024: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec  3 15:27:39.024: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:27:39.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7618" for this suite.
Dec  3 15:27:45.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:27:48.512: INFO: namespace svcaccounts-7618 deletion completed in 9.397547285s
â€¢SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:27:48.512: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3402
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec  3 15:27:49.242: INFO: Waiting up to 5m0s for pod "downward-api-f299579c-fd8a-4b82-80a1-25c37302dd53" in namespace "downward-api-3402" to be "success or failure"
Dec  3 15:27:49.332: INFO: Pod "downward-api-f299579c-fd8a-4b82-80a1-25c37302dd53": Phase="Pending", Reason="", readiness=false. Elapsed: 89.257395ms
Dec  3 15:27:51.421: INFO: Pod "downward-api-f299579c-fd8a-4b82-80a1-25c37302dd53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179038455s
STEP: Saw pod success
Dec  3 15:27:51.421: INFO: Pod "downward-api-f299579c-fd8a-4b82-80a1-25c37302dd53" satisfied condition "success or failure"
Dec  3 15:27:51.511: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downward-api-f299579c-fd8a-4b82-80a1-25c37302dd53 container dapi-container: <nil>
STEP: delete the pod
Dec  3 15:27:51.697: INFO: Waiting for pod downward-api-f299579c-fd8a-4b82-80a1-25c37302dd53 to disappear
Dec  3 15:27:51.786: INFO: Pod downward-api-f299579c-fd8a-4b82-80a1-25c37302dd53 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:27:51.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3402" for this suite.
Dec  3 15:27:58.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:28:01.272: INFO: namespace downward-api-3402 deletion completed in 9.395824546s
â€¢SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:28:01.273: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Dec  3 15:28:32.675: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1203 15:28:32.675348    5076 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:28:32.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8937" for this suite.
Dec  3 15:28:39.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:28:42.159: INFO: namespace gc-8937 deletion completed in 9.39456292s
â€¢SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:28:42.160: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-400
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 15:28:42.797: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:28:43.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-400" for this suite.
Dec  3 15:28:49.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:28:53.007: INFO: namespace custom-resource-definition-400 deletion completed in 9.399491324s
â€¢SSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:28:53.007: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-7431
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:28:53.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7431" for this suite.
Dec  3 15:29:00.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:29:03.313: INFO: namespace tables-7431 deletion completed in 9.401028543s
â€¢SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:29:03.313: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9533
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:29:04.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9533" for this suite.
Dec  3 15:29:10.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:29:13.702: INFO: namespace custom-resource-definition-9533 deletion completed in 9.396802749s
â€¢SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:29:13.703: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6663
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  3 15:29:14.431: INFO: Waiting up to 5m0s for pod "pod-34e7bb90-af69-42c0-9550-d6d913d25f6d" in namespace "emptydir-6663" to be "success or failure"
Dec  3 15:29:14.520: INFO: Pod "pod-34e7bb90-af69-42c0-9550-d6d913d25f6d": Phase="Pending", Reason="", readiness=false. Elapsed: 89.334295ms
Dec  3 15:29:16.610: INFO: Pod "pod-34e7bb90-af69-42c0-9550-d6d913d25f6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179157724s
STEP: Saw pod success
Dec  3 15:29:16.610: INFO: Pod "pod-34e7bb90-af69-42c0-9550-d6d913d25f6d" satisfied condition "success or failure"
Dec  3 15:29:16.699: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-34e7bb90-af69-42c0-9550-d6d913d25f6d container test-container: <nil>
STEP: delete the pod
Dec  3 15:29:16.890: INFO: Waiting for pod pod-34e7bb90-af69-42c0-9550-d6d913d25f6d to disappear
Dec  3 15:29:16.980: INFO: Pod pod-34e7bb90-af69-42c0-9550-d6d913d25f6d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:29:16.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6663" for this suite.
Dec  3 15:29:23.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:29:26.470: INFO: namespace emptydir-6663 deletion completed in 9.399452598s
â€¢SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:29:26.470: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6956
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-c28ee844-34d5-44f0-9834-0b6b4d916125
STEP: Creating a pod to test consume configMaps
Dec  3 15:29:27.289: INFO: Waiting up to 5m0s for pod "pod-configmaps-590804ad-ff1a-4a70-bc69-4dc07e9984de" in namespace "configmap-6956" to be "success or failure"
Dec  3 15:29:27.379: INFO: Pod "pod-configmaps-590804ad-ff1a-4a70-bc69-4dc07e9984de": Phase="Pending", Reason="", readiness=false. Elapsed: 89.434376ms
Dec  3 15:29:29.468: INFO: Pod "pod-configmaps-590804ad-ff1a-4a70-bc69-4dc07e9984de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179216705s
STEP: Saw pod success
Dec  3 15:29:29.469: INFO: Pod "pod-configmaps-590804ad-ff1a-4a70-bc69-4dc07e9984de" satisfied condition "success or failure"
Dec  3 15:29:29.558: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-configmaps-590804ad-ff1a-4a70-bc69-4dc07e9984de container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:29:29.747: INFO: Waiting for pod pod-configmaps-590804ad-ff1a-4a70-bc69-4dc07e9984de to disappear
Dec  3 15:29:29.837: INFO: Pod pod-configmaps-590804ad-ff1a-4a70-bc69-4dc07e9984de no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:29:29.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6956" for this suite.
Dec  3 15:29:36.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:29:39.322: INFO: namespace configmap-6956 deletion completed in 9.394884878s
â€¢SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:29:39.322: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3965
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec  3 15:29:39.960: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  3 15:29:40.233: INFO: Waiting for terminating namespaces to be deleted...
Dec  3 15:29:40.322: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-10-98.ec2.internal before test
Dec  3 15:29:40.417: INFO: calico-node-sjfqj from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.417: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:29:40.417: INFO: node-exporter-bt9wb from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.417: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 15:29:40.417: INFO: kube-proxy-877v6 from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.417: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:29:40.417: INFO: node-problem-detector-m2t47 from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.417: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 15:29:40.417: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-27-214.ec2.internal before test
Dec  3 15:29:40.521: INFO: calico-kube-controllers-79bcd784b6-w4fv5 from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.521: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  3 15:29:40.521: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-95f65778d-d2nzf from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.521: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  3 15:29:40.521: INFO: blackbox-exporter-7bd7b55dfc-nqf6r from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.521: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  3 15:29:40.521: INFO: node-exporter-f2j7c from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.521: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 15:29:40.521: INFO: vpn-shoot-57c94d6f78-cmlmw from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.521: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  3 15:29:40.521: INFO: calico-typha-deploy-9f6b455c4-xv9k8 from kube-system started at 2019-12-03 14:33:07 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.521: INFO: 	Container calico-typha ready: true, restart count 0
Dec  3 15:29:40.521: INFO: calico-node-drs2g from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.521: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 15:29:40.521: INFO: calico-typha-vertical-autoscaler-847d859f8c-m9mnl from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.521: INFO: 	Container autoscaler ready: true, restart count 4
Dec  3 15:29:40.521: INFO: addons-kubernetes-dashboard-78954cc66b-fj8m4 from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.521: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  3 15:29:40.521: INFO: metrics-server-778877cf87-5dxgx from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.521: INFO: 	Container metrics-server ready: true, restart count 0
Dec  3 15:29:40.521: INFO: addons-nginx-ingress-controller-7c75bb76db-pr4bf from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.521: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  3 15:29:40.521: INFO: coredns-59c969ffb8-6vlmd from kube-system started at 2019-12-03 14:30:07 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.521: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:29:40.521: INFO: calico-typha-horizontal-autoscaler-69df649c59-9tbvm from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.521: INFO: 	Container autoscaler ready: true, restart count 0
Dec  3 15:29:40.521: INFO: coredns-59c969ffb8-pbnhz from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.521: INFO: 	Container coredns ready: true, restart count 0
Dec  3 15:29:40.521: INFO: kube-proxy-pcff8 from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.521: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 15:29:40.521: INFO: node-problem-detector-j6tww from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 15:29:40.521: INFO: 	Container node-problem-detector ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-771b5235-acfa-441d-b78e-d62624a3f7c0 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-771b5235-acfa-441d-b78e-d62624a3f7c0 off the node ip-10-250-10-98.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-771b5235-acfa-441d-b78e-d62624a3f7c0
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:29:45.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3965" for this suite.
Dec  3 15:29:56.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:29:59.283: INFO: namespace sched-pred-3965 deletion completed in 13.408023339s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
â€¢S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:29:59.283: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1981
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:30:02.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1981" for this suite.
Dec  3 15:30:10.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:30:13.784: INFO: namespace containers-1981 deletion completed in 11.391504207s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:30:13.784: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2616
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-47eb84c7-7fcb-4989-b87a-1e7f210d762c in namespace container-probe-2616
Dec  3 15:30:16.694: INFO: Started pod test-webserver-47eb84c7-7fcb-4989-b87a-1e7f210d762c in namespace container-probe-2616
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 15:30:16.784: INFO: Initial restart count of pod test-webserver-47eb84c7-7fcb-4989-b87a-1e7f210d762c is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:34:17.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2616" for this suite.
Dec  3 15:34:23.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:34:26.709: INFO: namespace container-probe-2616 deletion completed in 9.396173032s
â€¢SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:34:26.710: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-493
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 15:34:27.348: INFO: Creating deployment "webserver-deployment"
Dec  3 15:34:27.438: INFO: Waiting for observed generation 1
Dec  3 15:34:27.527: INFO: Waiting for all required pods to come up
Dec  3 15:34:27.617: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec  3 15:34:31.797: INFO: Waiting for deployment "webserver-deployment" to complete
Dec  3 15:34:31.976: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec  3 15:34:32.156: INFO: Updating deployment webserver-deployment
Dec  3 15:34:32.156: INFO: Waiting for observed generation 2
Dec  3 15:34:32.246: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec  3 15:34:32.335: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec  3 15:34:32.425: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec  3 15:34:32.694: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec  3 15:34:32.694: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec  3 15:34:32.783: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec  3 15:34:32.962: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec  3 15:34:32.962: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec  3 15:34:33.141: INFO: Updating deployment webserver-deployment
Dec  3 15:34:33.141: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec  3 15:34:33.319: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec  3 15:34:33.409: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec  3 15:34:33.588: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-493 /apis/apps/v1/namespaces/deployment-493/deployments/webserver-deployment 83c53c32-7960-4f32-bf0c-825392dba568 12895 3 2019-12-03 15:34:27 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006b92158 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-12-03 15:34:33 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2019-12-03 15:34:33 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec  3 15:34:33.678: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-493 /apis/apps/v1/namespaces/deployment-493/replicasets/webserver-deployment-c7997dcc8 c454a63e-e57c-4855-9f2e-55f49c3fd17a 12894 3 2019-12-03 15:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 83c53c32-7960-4f32-bf0c-825392dba568 0xc006b92657 0xc006b92658}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006b926c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  3 15:34:33.678: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec  3 15:34:33.678: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-493 /apis/apps/v1/namespaces/deployment-493/replicasets/webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 12874 3 2019-12-03 15:34:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 83c53c32-7960-4f32-bf0c-825392dba568 0xc006b92597 0xc006b92598}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006b925f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec  3 15:34:33.770: INFO: Pod "webserver-deployment-595b5b9587-2gzgz" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2gzgz webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-2gzgz b9c51fef-0d5c-4090-8287-973e4553da41 12890 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc006b92ba7 0xc006b92ba8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-27-214.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.27.214,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.770: INFO: Pod "webserver-deployment-595b5b9587-4m27b" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4m27b webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-4m27b 86bfeb00-a7dc-4c19-87cc-0ef7a4d64ef8 12884 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc006b92cf7 0xc006b92cf8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-27-214.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.27.214,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.770: INFO: Pod "webserver-deployment-595b5b9587-65hcc" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-65hcc webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-65hcc 964785f5-2cc2-43f6-940d-de784f518dd7 12788 0 2019-12-03 15:34:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.64.1.139/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc006b92e57 0xc006b92e58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:100.64.1.139,StartTime:2019-12-03 15:34:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-03 15:34:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://a4cfb5ab45a8a60c16d1f4642291c8ac79b83d6dd96de40a26278fe259c69756,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.771: INFO: Pod "webserver-deployment-595b5b9587-6nl6g" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6nl6g webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-6nl6g 070ff09c-004d-4fbc-8546-5d9eb17ab61e 12904 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc006b92fc7 0xc006b92fc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.771: INFO: Pod "webserver-deployment-595b5b9587-6qqw9" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6qqw9 webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-6qqw9 2d565cc8-8581-4160-8e0e-ec33e91264f6 12906 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc006b93117 0xc006b93118}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.771: INFO: Pod "webserver-deployment-595b5b9587-6r6h7" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6r6h7 webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-6r6h7 b846d903-e587-4d7c-af8f-a60dce2d0702 12754 0 2019-12-03 15:34:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.64.0.31/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc006b93277 0xc006b93278}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-27-214.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.27.214,PodIP:100.64.0.31,StartTime:2019-12-03 15:34:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-03 15:34:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://52055a085368f1ac0ca74a0843ab4c6e522fe1702761a7064ed4eb3866832d26,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.771: INFO: Pod "webserver-deployment-595b5b9587-75wkj" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-75wkj webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-75wkj 7a59e0ab-eec6-402e-8554-6bef9c7715a8 12907 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc006b933e0 0xc006b933e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.771: INFO: Pod "webserver-deployment-595b5b9587-dg9bv" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dg9bv webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-dg9bv a55c4179-1bf2-4c0c-80c3-e7d7a154947c 12891 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc006b93527 0xc006b93528}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-27-214.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.27.214,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.771: INFO: Pod "webserver-deployment-595b5b9587-fwjbv" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-fwjbv webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-fwjbv 6ac5a7f9-79f5-4789-9b6b-db7650c84a19 12757 0 2019-12-03 15:34:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.64.0.32/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc006b93687 0xc006b93688}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-27-214.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.27.214,PodIP:100.64.0.32,StartTime:2019-12-03 15:34:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-03 15:34:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://75c2929c0eb38dd1711be84ab8f8c7404de49d8687b536506e7d2c285aa7a9ab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.0.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.771: INFO: Pod "webserver-deployment-595b5b9587-jbqht" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jbqht webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-jbqht baae960b-c72f-4cf6-a5a8-bd41e4f6fe9a 12903 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc006b937f0 0xc006b937f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.772: INFO: Pod "webserver-deployment-595b5b9587-jzhd7" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jzhd7 webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-jzhd7 1074d4e3-dc86-410b-8548-48a847279514 12896 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc006b93937 0xc006b93938}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-27-214.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.27.214,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.772: INFO: Pod "webserver-deployment-595b5b9587-l78k5" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-l78k5 webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-l78k5 6c4f621b-3859-4974-8885-9c2912cec1fe 12779 0 2019-12-03 15:34:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.64.1.140/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc006b93a97 0xc006b93a98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:100.64.1.140,StartTime:2019-12-03 15:34:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-03 15:34:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://56a8f0d5d089a152996a191a7761f2f09b81336cc2525b7749b59f86e80f0a4c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.772: INFO: Pod "webserver-deployment-595b5b9587-lr6ph" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lr6ph webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-lr6ph e45e16d3-183c-4ae7-b818-89b5f381a70a 12905 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc006b93c07 0xc006b93c08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.772: INFO: Pod "webserver-deployment-595b5b9587-mb8c6" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mb8c6 webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-mb8c6 6678367c-4b9d-40de-a48b-dadb32815d4d 12770 0 2019-12-03 15:34:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.64.1.136/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc006b93d67 0xc006b93d68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:100.64.1.136,StartTime:2019-12-03 15:34:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-03 15:34:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://9aaeb286d41daa9c9effab342755dd5880e010e63278a90a9fd47a8cefb318d7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.772: INFO: Pod "webserver-deployment-595b5b9587-mt867" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mt867 webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-mt867 fbf131a2-1e70-4232-8cf8-6b2e872938d1 12782 0 2019-12-03 15:34:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.64.1.143/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc006b93ee7 0xc006b93ee8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:100.64.1.143,StartTime:2019-12-03 15:34:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-03 15:34:30 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b51784f848568077a4b4125ad1a0f57af5a1759f1ca0e622a7a430bf719a223b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.773: INFO: Pod "webserver-deployment-595b5b9587-nqv9k" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-nqv9k webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-nqv9k e1d5f85a-66d6-4336-8e38-8f740f1036d2 12888 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc003a80077 0xc003a80078}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-27-214.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.27.214,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.773: INFO: Pod "webserver-deployment-595b5b9587-vfgmh" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vfgmh webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-vfgmh 4d3f5c68-0e85-4572-8432-e71527cd526a 12878 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc003a801c7 0xc003a801c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.773: INFO: Pod "webserver-deployment-595b5b9587-vw5pv" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-vw5pv webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-vw5pv 6136ff70-de08-4b09-86e7-7685e5132547 12785 0 2019-12-03 15:34:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.64.1.141/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc003a80327 0xc003a80328}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:100.64.1.141,StartTime:2019-12-03 15:34:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-03 15:34:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://2cdf9c92fe3c84c37b6990ab9da7383a5a8bfd062932df1ddaad446dad3cc67a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.773: INFO: Pod "webserver-deployment-595b5b9587-xldh4" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xldh4 webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-xldh4 be84a2b0-9595-4756-8f58-34394fb62554 12889 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc003a80497 0xc003a80498}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-27-214.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.27.214,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.773: INFO: Pod "webserver-deployment-595b5b9587-zskpk" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zskpk webserver-deployment-595b5b9587- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-595b5b9587-zskpk c5b9ef58-4d78-49cc-a6f3-75b6b8751158 12773 0 2019-12-03 15:34:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:100.64.1.138/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 3357b36d-d85e-4a51-b853-d80b64bc567e 0xc003a805f7 0xc003a805f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:100.64.1.138,StartTime:2019-12-03 15:34:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-03 15:34:29 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://4ff203f38c36be4304075cadda22d4542b043c26f960e8a9c6745baea96b5118,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.773: INFO: Pod "webserver-deployment-c7997dcc8-5nnxl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-5nnxl webserver-deployment-c7997dcc8- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-c7997dcc8-5nnxl a3f196b7-d9c5-4259-8cca-f7ce2deaf7a4 12897 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c454a63e-e57c-4855-9f2e-55f49c3fd17a 0xc003a80777 0xc003a80778}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.774: INFO: Pod "webserver-deployment-c7997dcc8-5st7z" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-5st7z webserver-deployment-c7997dcc8- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-c7997dcc8-5st7z 37dcf6b1-7b48-41f5-9560-390699c290de 12900 0 2019-12-03 15:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.64.1.146/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c454a63e-e57c-4855-9f2e-55f49c3fd17a 0xc003a808f7 0xc003a808f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:,StartTime:2019-12-03 15:34:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.774: INFO: Pod "webserver-deployment-c7997dcc8-6gch2" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-6gch2 webserver-deployment-c7997dcc8- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-c7997dcc8-6gch2 448dbfa1-30f9-42d8-99b0-b32115cbe98a 12876 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c454a63e-e57c-4855-9f2e-55f49c3fd17a 0xc003a80a67 0xc003a80a68}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-27-214.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.27.214,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.774: INFO: Pod "webserver-deployment-c7997dcc8-96rtn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-96rtn webserver-deployment-c7997dcc8- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-c7997dcc8-96rtn 2190c51a-68db-4488-89f6-c413b5a3a54a 12908 0 2019-12-03 15:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.64.1.144/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c454a63e-e57c-4855-9f2e-55f49c3fd17a 0xc003a80be7 0xc003a80be8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:100.64.1.144,StartTime:2019-12-03 15:34:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login',},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.144,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.775: INFO: Pod "webserver-deployment-c7997dcc8-9jtk8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-9jtk8 webserver-deployment-c7997dcc8- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-c7997dcc8-9jtk8 2eabccae-7f85-4083-9b54-8193acc7ab74 12898 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c454a63e-e57c-4855-9f2e-55f49c3fd17a 0xc003a80d87 0xc003a80d88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.775: INFO: Pod "webserver-deployment-c7997dcc8-bvbtl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-bvbtl webserver-deployment-c7997dcc8- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-c7997dcc8-bvbtl f2c26df6-e19c-49c8-9e16-90d4ca134a10 12893 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c454a63e-e57c-4855-9f2e-55f49c3fd17a 0xc003a80ef7 0xc003a80ef8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-27-214.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.27.214,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.775: INFO: Pod "webserver-deployment-c7997dcc8-ft4gd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ft4gd webserver-deployment-c7997dcc8- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-c7997dcc8-ft4gd 12c862ab-2d62-4267-bc39-71d2d49827ee 12901 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c454a63e-e57c-4855-9f2e-55f49c3fd17a 0xc003a81067 0xc003a81068}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.775: INFO: Pod "webserver-deployment-c7997dcc8-hzmm2" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-hzmm2 webserver-deployment-c7997dcc8- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-c7997dcc8-hzmm2 37d99f3d-722d-4268-9a59-204d2dc7bb88 12832 0 2019-12-03 15:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.64.0.34/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c454a63e-e57c-4855-9f2e-55f49c3fd17a 0xc003a811e7 0xc003a811e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-27-214.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.27.214,PodIP:,StartTime:2019-12-03 15:34:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.775: INFO: Pod "webserver-deployment-c7997dcc8-k8zg5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-k8zg5 webserver-deployment-c7997dcc8- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-c7997dcc8-k8zg5 86aaef52-0044-4d19-961d-49381102a553 12831 0 2019-12-03 15:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.64.0.33/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c454a63e-e57c-4855-9f2e-55f49c3fd17a 0xc003a81367 0xc003a81368}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-27-214.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.27.214,PodIP:,StartTime:2019-12-03 15:34:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.775: INFO: Pod "webserver-deployment-c7997dcc8-lb7tl" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-lb7tl webserver-deployment-c7997dcc8- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-c7997dcc8-lb7tl 3041c360-3845-4734-b06d-b48b729ef5bf 12886 0 2019-12-03 15:34:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:100.64.1.145/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c454a63e-e57c-4855-9f2e-55f49c3fd17a 0xc003a814e7 0xc003a814e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:,StartTime:2019-12-03 15:34:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.776: INFO: Pod "webserver-deployment-c7997dcc8-qxzsm" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qxzsm webserver-deployment-c7997dcc8- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-c7997dcc8-qxzsm 008134d9-fd31-4418-a3b1-ce91f7d492ad 12887 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c454a63e-e57c-4855-9f2e-55f49c3fd17a 0xc003a81657 0xc003a81658}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-27-214.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.27.214,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.776: INFO: Pod "webserver-deployment-c7997dcc8-qz9jw" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qz9jw webserver-deployment-c7997dcc8- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-c7997dcc8-qz9jw 2e97136c-69f5-4358-8c45-a471d34562d7 12892 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c454a63e-e57c-4855-9f2e-55f49c3fd17a 0xc003a817c7 0xc003a817c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec  3 15:34:33.776: INFO: Pod "webserver-deployment-c7997dcc8-zk8bk" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zk8bk webserver-deployment-c7997dcc8- deployment-493 /api/v1/namespaces/deployment-493/pods/webserver-deployment-c7997dcc8-zk8bk 1b186b13-9956-4409-a797-858b3a5221fb 12899 0 2019-12-03 15:34:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 c454a63e-e57c-4855-9f2e-55f49c3fd17a 0xc003a81937 0xc003a81938}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wgd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wgd7d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wgd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 15:34:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:,StartTime:2019-12-03 15:34:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:34:33.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-493" for this suite.
Dec  3 15:34:40.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:34:43.266: INFO: namespace deployment-493 deletion completed in 9.399377414s
â€¢SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:34:43.266: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6962
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Dec  3 15:34:43.910: INFO: namespace kubectl-6962
Dec  3 15:34:43.910: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-6962'
Dec  3 15:34:44.908: INFO: stderr: ""
Dec  3 15:34:44.908: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  3 15:34:45.999: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:34:45.999: INFO: Found 0 / 1
Dec  3 15:34:46.998: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:34:46.998: INFO: Found 1 / 1
Dec  3 15:34:46.998: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  3 15:34:47.091: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 15:34:47.091: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  3 15:34:47.091: INFO: wait on redis-master startup in kubectl-6962 
Dec  3 15:34:47.091: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-w8f5c redis-master --namespace=kubectl-6962'
Dec  3 15:34:47.724: INFO: stderr: ""
Dec  3 15:34:47.724: INFO: stdout: "1:C 03 Dec 2019 15:34:45.645 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 03 Dec 2019 15:34:45.645 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 03 Dec 2019 15:34:45.645 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 03 Dec 2019 15:34:45.647 * Running mode=standalone, port=6379.\n1:M 03 Dec 2019 15:34:45.647 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 Dec 2019 15:34:45.647 # Server initialized\n1:M 03 Dec 2019 15:34:45.647 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 03 Dec 2019 15:34:45.647 * Ready to accept connections\n"
STEP: exposing RC
Dec  3 15:34:47.725: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-6962'
Dec  3 15:34:48.266: INFO: stderr: ""
Dec  3 15:34:48.266: INFO: stdout: "service/rm2 exposed\n"
Dec  3 15:34:48.355: INFO: Service rm2 in namespace kubectl-6962 found.
STEP: exposing service
Dec  3 15:34:50.535: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-6962'
Dec  3 15:34:51.065: INFO: stderr: ""
Dec  3 15:34:51.066: INFO: stdout: "service/rm3 exposed\n"
Dec  3 15:34:51.155: INFO: Service rm3 in namespace kubectl-6962 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:34:53.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6962" for this suite.
Dec  3 15:35:05.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:35:08.820: INFO: namespace kubectl-6962 deletion completed in 15.394683422s
â€¢SS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:35:08.821: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2597
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-83c61d5e-3c94-4724-a41f-1409dae1210e in namespace container-probe-2597
Dec  3 15:35:11.728: INFO: Started pod liveness-83c61d5e-3c94-4724-a41f-1409dae1210e in namespace container-probe-2597
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 15:35:11.817: INFO: Initial restart count of pod liveness-83c61d5e-3c94-4724-a41f-1409dae1210e is 0
Dec  3 15:35:26.538: INFO: Restart count of pod container-probe-2597/liveness-83c61d5e-3c94-4724-a41f-1409dae1210e is now 1 (14.720530505s elapsed)
Dec  3 15:35:47.436: INFO: Restart count of pod container-probe-2597/liveness-83c61d5e-3c94-4724-a41f-1409dae1210e is now 2 (35.618064143s elapsed)
Dec  3 15:36:06.250: INFO: Restart count of pod container-probe-2597/liveness-83c61d5e-3c94-4724-a41f-1409dae1210e is now 3 (54.43234199s elapsed)
Dec  3 15:36:25.057: INFO: Restart count of pod container-probe-2597/liveness-83c61d5e-3c94-4724-a41f-1409dae1210e is now 4 (1m13.239348806s elapsed)
Dec  3 15:37:25.660: INFO: Restart count of pod container-probe-2597/liveness-83c61d5e-3c94-4724-a41f-1409dae1210e is now 5 (2m13.842897983s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:37:25.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2597" for this suite.
Dec  3 15:37:32.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:37:35.239: INFO: namespace container-probe-2597 deletion completed in 9.392690572s
â€¢SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:37:35.239: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8692
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-0430b078-2048-47d0-933b-97a08bfe647f
STEP: Creating configMap with name cm-test-opt-upd-21effa08-2fa4-41b7-9599-452ae39f6c37
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-0430b078-2048-47d0-933b-97a08bfe647f
STEP: Updating configmap cm-test-opt-upd-21effa08-2fa4-41b7-9599-452ae39f6c37
STEP: Creating configMap with name cm-test-opt-create-0bd3f028-b153-4aa6-8030-ca445893f1a8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:37:41.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8692" for this suite.
Dec  3 15:37:53.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:37:57.056: INFO: namespace configmap-8692 deletion completed in 15.386657184s
â€¢SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:37:57.056: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5172
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-365
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7197
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:38:05.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5172" for this suite.
Dec  3 15:38:11.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:38:14.719: INFO: namespace namespaces-5172 deletion completed in 9.3835059s
STEP: Destroying namespace "nsdeletetest-365" for this suite.
Dec  3 15:38:14.808: INFO: Namespace nsdeletetest-365 was already deleted
STEP: Destroying namespace "nsdeletetest-7197" for this suite.
Dec  3 15:38:21.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:38:24.201: INFO: namespace nsdeletetest-7197 deletion completed in 9.39297089s
â€¢SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:38:24.201: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:38:27.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5869" for this suite.
Dec  3 15:39:15.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:39:18.781: INFO: namespace kubelet-test-5869 deletion completed in 51.399561887s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:39:18.782: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8444
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8444
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  3 15:39:19.419: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  3 15:39:40.943: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.1.162 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8444 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:39:40.943: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:39:42.787: INFO: Found all expected endpoints: [netserver-0]
Dec  3 15:39:42.877: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.0.44 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8444 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 15:39:42.877: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 15:39:44.720: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:39:44.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8444" for this suite.
Dec  3 15:39:57.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:40:00.201: INFO: namespace pod-network-test-8444 deletion completed in 15.390432537s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:40:00.201: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5950
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-f462f394-0866-4e65-b376-e95eb59e09f2
STEP: Creating a pod to test consume configMaps
Dec  3 15:40:01.021: INFO: Waiting up to 5m0s for pod "pod-configmaps-fa82b38a-3a38-4bc1-bc87-b924770ba26b" in namespace "configmap-5950" to be "success or failure"
Dec  3 15:40:01.133: INFO: Pod "pod-configmaps-fa82b38a-3a38-4bc1-bc87-b924770ba26b": Phase="Pending", Reason="", readiness=false. Elapsed: 112.743936ms
Dec  3 15:40:03.224: INFO: Pod "pod-configmaps-fa82b38a-3a38-4bc1-bc87-b924770ba26b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.203030361s
STEP: Saw pod success
Dec  3 15:40:03.224: INFO: Pod "pod-configmaps-fa82b38a-3a38-4bc1-bc87-b924770ba26b" satisfied condition "success or failure"
Dec  3 15:40:03.313: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-configmaps-fa82b38a-3a38-4bc1-bc87-b924770ba26b container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:40:03.639: INFO: Waiting for pod pod-configmaps-fa82b38a-3a38-4bc1-bc87-b924770ba26b to disappear
Dec  3 15:40:03.729: INFO: Pod pod-configmaps-fa82b38a-3a38-4bc1-bc87-b924770ba26b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:40:03.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5950" for this suite.
Dec  3 15:40:10.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:40:13.209: INFO: namespace configmap-5950 deletion completed in 9.3899121s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:40:13.210: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7959
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec  3 15:40:17.072: INFO: Successfully updated pod "annotationupdate8492cd62-8a58-4983-a480-2ec915a6ad32"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:40:19.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7959" for this suite.
Dec  3 15:40:33.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:40:36.753: INFO: namespace downward-api-7959 deletion completed in 17.398035944s
â€¢S
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:40:36.753: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3520
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-bbd6e0c6-bbf8-447d-ba80-a0254ffe3d21
STEP: Creating a pod to test consume secrets
Dec  3 15:40:37.571: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c2288afa-5695-448e-a429-9b75d708d0bd" in namespace "projected-3520" to be "success or failure"
Dec  3 15:40:37.660: INFO: Pod "pod-projected-secrets-c2288afa-5695-448e-a429-9b75d708d0bd": Phase="Pending", Reason="", readiness=false. Elapsed: 89.344715ms
Dec  3 15:40:39.750: INFO: Pod "pod-projected-secrets-c2288afa-5695-448e-a429-9b75d708d0bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178830103s
STEP: Saw pod success
Dec  3 15:40:39.750: INFO: Pod "pod-projected-secrets-c2288afa-5695-448e-a429-9b75d708d0bd" satisfied condition "success or failure"
Dec  3 15:40:39.839: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-projected-secrets-c2288afa-5695-448e-a429-9b75d708d0bd container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:40:40.030: INFO: Waiting for pod pod-projected-secrets-c2288afa-5695-448e-a429-9b75d708d0bd to disappear
Dec  3 15:40:40.119: INFO: Pod pod-projected-secrets-c2288afa-5695-448e-a429-9b75d708d0bd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:40:40.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3520" for this suite.
Dec  3 15:40:46.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:40:49.605: INFO: namespace projected-3520 deletion completed in 9.395418028s
â€¢SSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:40:49.606: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4829
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-70afdbd9-279e-4896-9a48-a270b4a31ee3
STEP: Creating a pod to test consume secrets
Dec  3 15:40:50.423: INFO: Waiting up to 5m0s for pod "pod-secrets-58a1a206-62c6-4224-8031-35b293d95d99" in namespace "secrets-4829" to be "success or failure"
Dec  3 15:40:50.514: INFO: Pod "pod-secrets-58a1a206-62c6-4224-8031-35b293d95d99": Phase="Pending", Reason="", readiness=false. Elapsed: 90.729974ms
Dec  3 15:40:52.603: INFO: Pod "pod-secrets-58a1a206-62c6-4224-8031-35b293d95d99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.18061348s
STEP: Saw pod success
Dec  3 15:40:52.603: INFO: Pod "pod-secrets-58a1a206-62c6-4224-8031-35b293d95d99" satisfied condition "success or failure"
Dec  3 15:40:52.693: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-secrets-58a1a206-62c6-4224-8031-35b293d95d99 container secret-env-test: <nil>
STEP: delete the pod
Dec  3 15:40:52.881: INFO: Waiting for pod pod-secrets-58a1a206-62c6-4224-8031-35b293d95d99 to disappear
Dec  3 15:40:52.970: INFO: Pod pod-secrets-58a1a206-62c6-4224-8031-35b293d95d99 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:40:52.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4829" for this suite.
Dec  3 15:40:59.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:41:02.468: INFO: namespace secrets-4829 deletion completed in 9.407109623s
â€¢SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:41:02.468: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8796
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 15:41:04.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984463, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984463, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984463, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984463, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 15:41:07.387: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:41:08.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8796" for this suite.
Dec  3 15:41:14.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:41:18.007: INFO: namespace webhook-8796 deletion completed in 9.389662469s
STEP: Destroying namespace "webhook-8796-markers" for this suite.
Dec  3 15:41:24.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:41:27.396: INFO: namespace webhook-8796-markers deletion completed in 9.389771317s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:41:27.758: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8343
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec  3 15:41:35.023: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:41:35.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1203 15:41:35.023282    5076 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-8343" for this suite.
Dec  3 15:41:41.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:41:44.507: INFO: namespace gc-8343 deletion completed in 9.394254756s
â€¢SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:41:44.507: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-288
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec  3 15:41:49.888: INFO: Pod name wrapped-volume-race-e6969440-655a-4e63-bbfe-7238d94dde94: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e6969440-655a-4e63-bbfe-7238d94dde94 in namespace emptydir-wrapper-288, will wait for the garbage collector to delete the pods
Dec  3 15:41:54.708: INFO: Deleting ReplicationController wrapped-volume-race-e6969440-655a-4e63-bbfe-7238d94dde94 took: 91.368958ms
Dec  3 15:41:54.808: INFO: Terminating ReplicationController wrapped-volume-race-e6969440-655a-4e63-bbfe-7238d94dde94 pods took: 100.384975ms
STEP: Creating RC which spawns configmap-volume pods
Dec  3 15:42:33.381: INFO: Pod name wrapped-volume-race-4c43f377-da9a-46be-8ef4-113823571234: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4c43f377-da9a-46be-8ef4-113823571234 in namespace emptydir-wrapper-288, will wait for the garbage collector to delete the pods
Dec  3 15:42:38.202: INFO: Deleting ReplicationController wrapped-volume-race-4c43f377-da9a-46be-8ef4-113823571234 took: 92.291563ms
Dec  3 15:42:38.602: INFO: Terminating ReplicationController wrapped-volume-race-4c43f377-da9a-46be-8ef4-113823571234 pods took: 400.504787ms
STEP: Creating RC which spawns configmap-volume pods
Dec  3 15:43:23.374: INFO: Pod name wrapped-volume-race-f778af89-4060-4bb5-bbf8-bbe187961f57: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f778af89-4060-4bb5-bbf8-bbe187961f57 in namespace emptydir-wrapper-288, will wait for the garbage collector to delete the pods
Dec  3 15:43:28.194: INFO: Deleting ReplicationController wrapped-volume-race-f778af89-4060-4bb5-bbf8-bbe187961f57 took: 91.437017ms
Dec  3 15:43:28.594: INFO: Terminating ReplicationController wrapped-volume-race-f778af89-4060-4bb5-bbf8-bbe187961f57 pods took: 400.372925ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:44:06.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-288" for this suite.
Dec  3 15:44:12.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:44:15.726: INFO: namespace emptydir-wrapper-288 deletion completed in 9.385094882s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:44:15.727: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-467
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:44:30.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-467" for this suite.
Dec  3 15:44:36.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:44:39.649: INFO: namespace watch-467 deletion completed in 9.402164391s
â€¢SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:44:39.650: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-6012
STEP: creating replication controller nodeport-test in namespace services-6012
I1203 15:44:40.474836    5076 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-6012, replica count: 2
Dec  3 15:44:43.575: INFO: Creating new exec pod
I1203 15:44:43.575494    5076 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  3 15:44:46.937: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-6012 execpod46c6p -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Dec  3 15:44:49.279: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec  3 15:44:49.279: INFO: stdout: ""
Dec  3 15:44:49.280: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-6012 execpod46c6p -- /bin/sh -x -c nc -zv -t -w 2 100.105.240.43 80'
Dec  3 15:44:50.592: INFO: stderr: "+ nc -zv -t -w 2 100.105.240.43 80\nConnection to 100.105.240.43 80 port [tcp/http] succeeded!\n"
Dec  3 15:44:50.592: INFO: stdout: ""
Dec  3 15:44:50.592: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-6012 execpod46c6p -- /bin/sh -x -c nc -zv -t -w 2 10.250.10.98 31216'
Dec  3 15:44:51.884: INFO: stderr: "+ nc -zv -t -w 2 10.250.10.98 31216\nConnection to 10.250.10.98 31216 port [tcp/31216] succeeded!\n"
Dec  3 15:44:51.884: INFO: stdout: ""
Dec  3 15:44:51.884: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-6012 execpod46c6p -- /bin/sh -x -c nc -zv -t -w 2 10.250.27.214 31216'
Dec  3 15:44:53.168: INFO: stderr: "+ nc -zv -t -w 2 10.250.27.214 31216\nConnection to 10.250.27.214 31216 port [tcp/31216] succeeded!\n"
Dec  3 15:44:53.168: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:44:53.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6012" for this suite.
Dec  3 15:44:59.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:45:02.655: INFO: namespace services-6012 deletion completed in 9.397606275s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95
â€¢SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:45:02.656: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2075
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Dec  3 15:45:03.385: INFO: Waiting up to 5m0s for pod "client-containers-e2377b55-f111-4eba-acab-c19a5a1e5fee" in namespace "containers-2075" to be "success or failure"
Dec  3 15:45:03.475: INFO: Pod "client-containers-e2377b55-f111-4eba-acab-c19a5a1e5fee": Phase="Pending", Reason="", readiness=false. Elapsed: 89.523163ms
Dec  3 15:45:05.565: INFO: Pod "client-containers-e2377b55-f111-4eba-acab-c19a5a1e5fee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179766627s
STEP: Saw pod success
Dec  3 15:45:05.565: INFO: Pod "client-containers-e2377b55-f111-4eba-acab-c19a5a1e5fee" satisfied condition "success or failure"
Dec  3 15:45:05.654: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod client-containers-e2377b55-f111-4eba-acab-c19a5a1e5fee container test-container: <nil>
STEP: delete the pod
Dec  3 15:45:05.978: INFO: Waiting for pod client-containers-e2377b55-f111-4eba-acab-c19a5a1e5fee to disappear
Dec  3 15:45:06.067: INFO: Pod client-containers-e2377b55-f111-4eba-acab-c19a5a1e5fee no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:45:06.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2075" for this suite.
Dec  3 15:45:12.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:45:15.546: INFO: namespace containers-2075 deletion completed in 9.388879957s
â€¢SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:45:15.546: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2066
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec  3 15:45:16.539: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2066 /api/v1/namespaces/watch-2066/configmaps/e2e-watch-test-watch-closed 9d7b6ee3-936d-4548-b38f-2fda1ce2529f 15298 0 2019-12-03 15:45:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 15:45:16.540: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2066 /api/v1/namespaces/watch-2066/configmaps/e2e-watch-test-watch-closed 9d7b6ee3-936d-4548-b38f-2fda1ce2529f 15299 0 2019-12-03 15:45:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec  3 15:45:16.897: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2066 /api/v1/namespaces/watch-2066/configmaps/e2e-watch-test-watch-closed 9d7b6ee3-936d-4548-b38f-2fda1ce2529f 15300 0 2019-12-03 15:45:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 15:45:16.898: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2066 /api/v1/namespaces/watch-2066/configmaps/e2e-watch-test-watch-closed 9d7b6ee3-936d-4548-b38f-2fda1ce2529f 15302 0 2019-12-03 15:45:16 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:45:16.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2066" for this suite.
Dec  3 15:45:23.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:45:26.381: INFO: namespace watch-2066 deletion completed in 9.393681323s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:45:26.382: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2684
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 15:45:28.548: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984728, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984728, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984728, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984728, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 15:45:31.732: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:45:33.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2684" for this suite.
Dec  3 15:45:39.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:45:43.077: INFO: namespace webhook-2684 deletion completed in 9.410552995s
STEP: Destroying namespace "webhook-2684-markers" for this suite.
Dec  3 15:45:49.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:45:52.467: INFO: namespace webhook-2684-markers deletion completed in 9.390052889s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:45:52.827: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 15:45:55.319: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984755, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984755, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984755, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710984755, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 15:45:58.503: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:45:59.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3012" for this suite.
Dec  3 15:46:06.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:46:09.307: INFO: namespace webhook-3012 deletion completed in 9.393510021s
STEP: Destroying namespace "webhook-3012-markers" for this suite.
Dec  3 15:46:15.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:46:18.700: INFO: namespace webhook-3012-markers deletion completed in 9.392897147s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢SSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:46:19.060: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9273
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Dec  3 15:46:22.147: INFO: Pod pod-hostip-2802f3d0-020d-4a84-8b48-927cf0a39f5c has hostIP: 10.250.10.98
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:46:22.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9273" for this suite.
Dec  3 15:46:34.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:46:37.627: INFO: namespace pods-9273 deletion completed in 15.390944556s
â€¢SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:46:37.628: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4798
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-4798/configmap-test-20fd1429-3c26-4325-9692-ef39542f3570
STEP: Creating a pod to test consume configMaps
Dec  3 15:46:38.452: INFO: Waiting up to 5m0s for pod "pod-configmaps-bb05efdf-352b-49de-85db-4411618a665e" in namespace "configmap-4798" to be "success or failure"
Dec  3 15:46:38.545: INFO: Pod "pod-configmaps-bb05efdf-352b-49de-85db-4411618a665e": Phase="Pending", Reason="", readiness=false. Elapsed: 92.921003ms
Dec  3 15:46:40.635: INFO: Pod "pod-configmaps-bb05efdf-352b-49de-85db-4411618a665e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.182643017s
STEP: Saw pod success
Dec  3 15:46:40.635: INFO: Pod "pod-configmaps-bb05efdf-352b-49de-85db-4411618a665e" satisfied condition "success or failure"
Dec  3 15:46:40.724: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-configmaps-bb05efdf-352b-49de-85db-4411618a665e container env-test: <nil>
STEP: delete the pod
Dec  3 15:46:41.027: INFO: Waiting for pod pod-configmaps-bb05efdf-352b-49de-85db-4411618a665e to disappear
Dec  3 15:46:41.117: INFO: Pod pod-configmaps-bb05efdf-352b-49de-85db-4411618a665e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:46:41.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4798" for this suite.
Dec  3 15:46:47.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:46:50.605: INFO: namespace configmap-4798 deletion completed in 9.397846927s
â€¢SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:46:50.605: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-416
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-058cee6e-aef6-43b2-b1f6-47141d1d51e1
STEP: Creating configMap with name cm-test-opt-upd-ff462b19-022c-47ec-bfec-270aea3e5b88
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-058cee6e-aef6-43b2-b1f6-47141d1d51e1
STEP: Updating configmap cm-test-opt-upd-ff462b19-022c-47ec-bfec-270aea3e5b88
STEP: Creating configMap with name cm-test-opt-create-2560fcf3-ea4c-443f-81dc-6d46c4f183a6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:48:12.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-416" for this suite.
Dec  3 15:48:24.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:48:27.759: INFO: namespace projected-416 deletion completed in 15.3936165s
â€¢SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:48:27.759: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-704
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  3 15:48:28.486: INFO: Waiting up to 5m0s for pod "pod-29b27822-bdcb-4337-a1c1-ab56f9de762c" in namespace "emptydir-704" to be "success or failure"
Dec  3 15:48:28.576: INFO: Pod "pod-29b27822-bdcb-4337-a1c1-ab56f9de762c": Phase="Pending", Reason="", readiness=false. Elapsed: 89.043991ms
Dec  3 15:48:30.665: INFO: Pod "pod-29b27822-bdcb-4337-a1c1-ab56f9de762c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178856143s
STEP: Saw pod success
Dec  3 15:48:30.665: INFO: Pod "pod-29b27822-bdcb-4337-a1c1-ab56f9de762c" satisfied condition "success or failure"
Dec  3 15:48:30.755: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-29b27822-bdcb-4337-a1c1-ab56f9de762c container test-container: <nil>
STEP: delete the pod
Dec  3 15:48:30.942: INFO: Waiting for pod pod-29b27822-bdcb-4337-a1c1-ab56f9de762c to disappear
Dec  3 15:48:31.032: INFO: Pod pod-29b27822-bdcb-4337-a1c1-ab56f9de762c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:48:31.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-704" for this suite.
Dec  3 15:48:37.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:48:40.509: INFO: namespace emptydir-704 deletion completed in 9.3869227s
â€¢SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:48:40.509: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4074
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-8aedd196-6827-4660-8e1b-a781b655c7fb
STEP: Creating a pod to test consume configMaps
Dec  3 15:48:41.327: INFO: Waiting up to 5m0s for pod "pod-configmaps-a14f0c15-c26f-4ae0-aa24-1c689dfac1db" in namespace "configmap-4074" to be "success or failure"
Dec  3 15:48:41.416: INFO: Pod "pod-configmaps-a14f0c15-c26f-4ae0-aa24-1c689dfac1db": Phase="Pending", Reason="", readiness=false. Elapsed: 89.26354ms
Dec  3 15:48:43.506: INFO: Pod "pod-configmaps-a14f0c15-c26f-4ae0-aa24-1c689dfac1db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179167522s
STEP: Saw pod success
Dec  3 15:48:43.506: INFO: Pod "pod-configmaps-a14f0c15-c26f-4ae0-aa24-1c689dfac1db" satisfied condition "success or failure"
Dec  3 15:48:43.596: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-configmaps-a14f0c15-c26f-4ae0-aa24-1c689dfac1db container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:48:43.784: INFO: Waiting for pod pod-configmaps-a14f0c15-c26f-4ae0-aa24-1c689dfac1db to disappear
Dec  3 15:48:43.873: INFO: Pod pod-configmaps-a14f0c15-c26f-4ae0-aa24-1c689dfac1db no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:48:43.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4074" for this suite.
Dec  3 15:48:50.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:48:53.352: INFO: namespace configmap-4074 deletion completed in 9.388479682s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:48:53.352: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4857
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec  3 15:48:54.524: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4857 /api/v1/namespaces/watch-4857/configmaps/e2e-watch-test-label-changed 3a08b232-88a5-4503-afd9-9e375fb3a6ac 16012 0 2019-12-03 15:48:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 15:48:54.524: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4857 /api/v1/namespaces/watch-4857/configmaps/e2e-watch-test-label-changed 3a08b232-88a5-4503-afd9-9e375fb3a6ac 16013 0 2019-12-03 15:48:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  3 15:48:54.524: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4857 /api/v1/namespaces/watch-4857/configmaps/e2e-watch-test-label-changed 3a08b232-88a5-4503-afd9-9e375fb3a6ac 16014 0 2019-12-03 15:48:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec  3 15:49:05.152: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4857 /api/v1/namespaces/watch-4857/configmaps/e2e-watch-test-label-changed 3a08b232-88a5-4503-afd9-9e375fb3a6ac 16037 0 2019-12-03 15:48:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 15:49:05.152: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4857 /api/v1/namespaces/watch-4857/configmaps/e2e-watch-test-label-changed 3a08b232-88a5-4503-afd9-9e375fb3a6ac 16038 0 2019-12-03 15:48:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec  3 15:49:05.152: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-4857 /api/v1/namespaces/watch-4857/configmaps/e2e-watch-test-label-changed 3a08b232-88a5-4503-afd9-9e375fb3a6ac 16039 0 2019-12-03 15:48:54 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:49:05.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4857" for this suite.
Dec  3 15:49:11.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:49:14.624: INFO: namespace watch-4857 deletion completed in 9.382142848s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:49:14.625: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-610
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-edeb6705-9679-4a26-9dc3-064a8e9385de
STEP: Creating a pod to test consume secrets
Dec  3 15:49:15.440: INFO: Waiting up to 5m0s for pod "pod-secrets-51543a5d-bae5-48fb-a6f4-530dc6a4f680" in namespace "secrets-610" to be "success or failure"
Dec  3 15:49:15.529: INFO: Pod "pod-secrets-51543a5d-bae5-48fb-a6f4-530dc6a4f680": Phase="Pending", Reason="", readiness=false. Elapsed: 89.157261ms
Dec  3 15:49:17.619: INFO: Pod "pod-secrets-51543a5d-bae5-48fb-a6f4-530dc6a4f680": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178789103s
STEP: Saw pod success
Dec  3 15:49:17.619: INFO: Pod "pod-secrets-51543a5d-bae5-48fb-a6f4-530dc6a4f680" satisfied condition "success or failure"
Dec  3 15:49:17.708: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-secrets-51543a5d-bae5-48fb-a6f4-530dc6a4f680 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:49:17.898: INFO: Waiting for pod pod-secrets-51543a5d-bae5-48fb-a6f4-530dc6a4f680 to disappear
Dec  3 15:49:17.987: INFO: Pod pod-secrets-51543a5d-bae5-48fb-a6f4-530dc6a4f680 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:49:17.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-610" for this suite.
Dec  3 15:49:24.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:49:27.471: INFO: namespace secrets-610 deletion completed in 9.392595022s
â€¢
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:49:27.471: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6293
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:49:39.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6293" for this suite.
Dec  3 15:49:46.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:49:49.220: INFO: namespace resourcequota-6293 deletion completed in 9.391043838s
â€¢SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:49:49.221: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6093
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  3 15:49:49.948: INFO: Waiting up to 5m0s for pod "pod-9ac30d5d-1f81-4b06-a052-f71a8609b7e7" in namespace "emptydir-6093" to be "success or failure"
Dec  3 15:49:50.037: INFO: Pod "pod-9ac30d5d-1f81-4b06-a052-f71a8609b7e7": Phase="Pending", Reason="", readiness=false. Elapsed: 89.147844ms
Dec  3 15:49:52.127: INFO: Pod "pod-9ac30d5d-1f81-4b06-a052-f71a8609b7e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178681372s
STEP: Saw pod success
Dec  3 15:49:52.127: INFO: Pod "pod-9ac30d5d-1f81-4b06-a052-f71a8609b7e7" satisfied condition "success or failure"
Dec  3 15:49:52.216: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-9ac30d5d-1f81-4b06-a052-f71a8609b7e7 container test-container: <nil>
STEP: delete the pod
Dec  3 15:49:52.404: INFO: Waiting for pod pod-9ac30d5d-1f81-4b06-a052-f71a8609b7e7 to disappear
Dec  3 15:49:52.493: INFO: Pod pod-9ac30d5d-1f81-4b06-a052-f71a8609b7e7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:49:52.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6093" for this suite.
Dec  3 15:49:58.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:50:01.976: INFO: namespace emptydir-6093 deletion completed in 9.391801169s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:50:01.976: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:50:02.706: INFO: Waiting up to 5m0s for pod "downwardapi-volume-77c4158a-da5f-4369-bf19-558decb3b6fc" in namespace "projected-3186" to be "success or failure"
Dec  3 15:50:02.795: INFO: Pod "downwardapi-volume-77c4158a-da5f-4369-bf19-558decb3b6fc": Phase="Pending", Reason="", readiness=false. Elapsed: 89.347421ms
Dec  3 15:50:04.885: INFO: Pod "downwardapi-volume-77c4158a-da5f-4369-bf19-558decb3b6fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179661858s
STEP: Saw pod success
Dec  3 15:50:04.886: INFO: Pod "downwardapi-volume-77c4158a-da5f-4369-bf19-558decb3b6fc" satisfied condition "success or failure"
Dec  3 15:50:04.975: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-77c4158a-da5f-4369-bf19-558decb3b6fc container client-container: <nil>
STEP: delete the pod
Dec  3 15:50:05.165: INFO: Waiting for pod downwardapi-volume-77c4158a-da5f-4369-bf19-558decb3b6fc to disappear
Dec  3 15:50:05.254: INFO: Pod downwardapi-volume-77c4158a-da5f-4369-bf19-558decb3b6fc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:50:05.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3186" for this suite.
Dec  3 15:50:11.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:50:14.739: INFO: namespace projected-3186 deletion completed in 9.394566935s
â€¢SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:50:14.740: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9734
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 15:50:16.988: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985016, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985016, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985016, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985016, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 15:50:20.171: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:50:21.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9734" for this suite.
Dec  3 15:50:27.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:50:30.563: INFO: namespace webhook-9734 deletion completed in 9.394355641s
STEP: Destroying namespace "webhook-9734-markers" for this suite.
Dec  3 15:50:36.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:50:39.954: INFO: namespace webhook-9734-markers deletion completed in 9.390754036s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:50:40.316: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9578
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1748
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8554
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:50:55.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9578" for this suite.
Dec  3 15:51:02.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:05.148: INFO: namespace namespaces-9578 deletion completed in 9.385933631s
STEP: Destroying namespace "nsdeletetest-1748" for this suite.
Dec  3 15:51:05.238: INFO: Namespace nsdeletetest-1748 was already deleted
STEP: Destroying namespace "nsdeletetest-8554" for this suite.
Dec  3 15:51:11.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:14.639: INFO: namespace nsdeletetest-8554 deletion completed in 9.40177627s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:51:14.640: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3412
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 15:51:15.367: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08d19d45-a65b-4a4c-aa1f-cd82afe33361" in namespace "projected-3412" to be "success or failure"
Dec  3 15:51:15.456: INFO: Pod "downwardapi-volume-08d19d45-a65b-4a4c-aa1f-cd82afe33361": Phase="Pending", Reason="", readiness=false. Elapsed: 89.014194ms
Dec  3 15:51:17.546: INFO: Pod "downwardapi-volume-08d19d45-a65b-4a4c-aa1f-cd82afe33361": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178762238s
STEP: Saw pod success
Dec  3 15:51:17.546: INFO: Pod "downwardapi-volume-08d19d45-a65b-4a4c-aa1f-cd82afe33361" satisfied condition "success or failure"
Dec  3 15:51:17.636: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-08d19d45-a65b-4a4c-aa1f-cd82afe33361 container client-container: <nil>
STEP: delete the pod
Dec  3 15:51:17.827: INFO: Waiting for pod downwardapi-volume-08d19d45-a65b-4a4c-aa1f-cd82afe33361 to disappear
Dec  3 15:51:17.916: INFO: Pod downwardapi-volume-08d19d45-a65b-4a4c-aa1f-cd82afe33361 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:51:17.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3412" for this suite.
Dec  3 15:51:24.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:27.395: INFO: namespace projected-3412 deletion completed in 9.388770235s
â€¢SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:51:27.395: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-890
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  3 15:51:30.481: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:51:30.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-890" for this suite.
Dec  3 15:51:37.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:40.164: INFO: namespace container-runtime-890 deletion completed in 9.412437825s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:51:40.165: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-6056
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Dec  3 15:51:40.894: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6056" to be "success or failure"
Dec  3 15:51:40.983: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 89.147041ms
Dec  3 15:51:43.073: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178881839s
STEP: Saw pod success
Dec  3 15:51:43.073: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec  3 15:51:43.163: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec  3 15:51:43.351: INFO: Waiting for pod pod-host-path-test to disappear
Dec  3 15:51:43.440: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:51:43.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6056" for this suite.
Dec  3 15:51:49.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:51:52.933: INFO: namespace hostpath-6056 deletion completed in 9.402099009s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:51:52.934: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5806
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-658b62dd-0971-4860-ad70-8bebceebf975
STEP: Creating a pod to test consume secrets
Dec  3 15:51:53.752: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4a0c4927-99b9-451e-a5c4-8cac08046e59" in namespace "projected-5806" to be "success or failure"
Dec  3 15:51:53.841: INFO: Pod "pod-projected-secrets-4a0c4927-99b9-451e-a5c4-8cac08046e59": Phase="Pending", Reason="", readiness=false. Elapsed: 89.051093ms
Dec  3 15:51:55.931: INFO: Pod "pod-projected-secrets-4a0c4927-99b9-451e-a5c4-8cac08046e59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179075502s
STEP: Saw pod success
Dec  3 15:51:55.931: INFO: Pod "pod-projected-secrets-4a0c4927-99b9-451e-a5c4-8cac08046e59" satisfied condition "success or failure"
Dec  3 15:51:56.021: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-projected-secrets-4a0c4927-99b9-451e-a5c4-8cac08046e59 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:51:56.217: INFO: Waiting for pod pod-projected-secrets-4a0c4927-99b9-451e-a5c4-8cac08046e59 to disappear
Dec  3 15:51:56.306: INFO: Pod pod-projected-secrets-4a0c4927-99b9-451e-a5c4-8cac08046e59 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:51:56.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5806" for this suite.
Dec  3 15:52:02.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:52:05.814: INFO: namespace projected-5806 deletion completed in 9.416283841s
â€¢SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:52:05.814: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9688
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  3 15:52:06.542: INFO: Waiting up to 5m0s for pod "pod-8962c45a-82a2-4aaa-8e03-b0bb47609af7" in namespace "emptydir-9688" to be "success or failure"
Dec  3 15:52:06.635: INFO: Pod "pod-8962c45a-82a2-4aaa-8e03-b0bb47609af7": Phase="Pending", Reason="", readiness=false. Elapsed: 92.984431ms
Dec  3 15:52:08.727: INFO: Pod "pod-8962c45a-82a2-4aaa-8e03-b0bb47609af7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.185030087s
STEP: Saw pod success
Dec  3 15:52:08.727: INFO: Pod "pod-8962c45a-82a2-4aaa-8e03-b0bb47609af7" satisfied condition "success or failure"
Dec  3 15:52:08.828: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-8962c45a-82a2-4aaa-8e03-b0bb47609af7 container test-container: <nil>
STEP: delete the pod
Dec  3 15:52:09.017: INFO: Waiting for pod pod-8962c45a-82a2-4aaa-8e03-b0bb47609af7 to disappear
Dec  3 15:52:09.107: INFO: Pod pod-8962c45a-82a2-4aaa-8e03-b0bb47609af7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:52:09.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9688" for this suite.
Dec  3 15:52:15.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:52:18.598: INFO: namespace emptydir-9688 deletion completed in 9.400010527s
â€¢
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:52:18.598: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2697
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Dec  3 15:52:19.326: INFO: Waiting up to 5m0s for pod "pod-757f0084-f857-4366-8def-be156f42e63f" in namespace "emptydir-2697" to be "success or failure"
Dec  3 15:52:19.416: INFO: Pod "pod-757f0084-f857-4366-8def-be156f42e63f": Phase="Pending", Reason="", readiness=false. Elapsed: 89.387752ms
Dec  3 15:52:21.506: INFO: Pod "pod-757f0084-f857-4366-8def-be156f42e63f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179189036s
STEP: Saw pod success
Dec  3 15:52:21.506: INFO: Pod "pod-757f0084-f857-4366-8def-be156f42e63f" satisfied condition "success or failure"
Dec  3 15:52:21.595: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-757f0084-f857-4366-8def-be156f42e63f container test-container: <nil>
STEP: delete the pod
Dec  3 15:52:21.784: INFO: Waiting for pod pod-757f0084-f857-4366-8def-be156f42e63f to disappear
Dec  3 15:52:21.873: INFO: Pod pod-757f0084-f857-4366-8def-be156f42e63f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:52:21.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2697" for this suite.
Dec  3 15:52:28.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:52:31.347: INFO: namespace emptydir-2697 deletion completed in 9.385061139s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:52:31.348: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-b321f565-cd6b-4330-9d78-cdd32161dfb4
STEP: Creating a pod to test consume configMaps
Dec  3 15:52:32.166: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-59c8216d-4827-4404-85fb-582a2b18d524" in namespace "projected-6158" to be "success or failure"
Dec  3 15:52:32.255: INFO: Pod "pod-projected-configmaps-59c8216d-4827-4404-85fb-582a2b18d524": Phase="Pending", Reason="", readiness=false. Elapsed: 89.026387ms
Dec  3 15:52:34.345: INFO: Pod "pod-projected-configmaps-59c8216d-4827-4404-85fb-582a2b18d524": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17882192s
STEP: Saw pod success
Dec  3 15:52:34.345: INFO: Pod "pod-projected-configmaps-59c8216d-4827-4404-85fb-582a2b18d524" satisfied condition "success or failure"
Dec  3 15:52:34.434: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-projected-configmaps-59c8216d-4827-4404-85fb-582a2b18d524 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:52:34.624: INFO: Waiting for pod pod-projected-configmaps-59c8216d-4827-4404-85fb-582a2b18d524 to disappear
Dec  3 15:52:34.713: INFO: Pod pod-projected-configmaps-59c8216d-4827-4404-85fb-582a2b18d524 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:52:34.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6158" for this suite.
Dec  3 15:52:41.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:52:44.191: INFO: namespace projected-6158 deletion completed in 9.388890437s
â€¢SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:52:44.192: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4622
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-28f26474-4308-4925-917b-1b44648e4d75
STEP: Creating a pod to test consume configMaps
Dec  3 15:52:45.009: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4b21c9d4-7db5-4121-8a75-a524b73355f4" in namespace "projected-4622" to be "success or failure"
Dec  3 15:52:45.098: INFO: Pod "pod-projected-configmaps-4b21c9d4-7db5-4121-8a75-a524b73355f4": Phase="Pending", Reason="", readiness=false. Elapsed: 89.297167ms
Dec  3 15:52:47.188: INFO: Pod "pod-projected-configmaps-4b21c9d4-7db5-4121-8a75-a524b73355f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179410548s
STEP: Saw pod success
Dec  3 15:52:47.188: INFO: Pod "pod-projected-configmaps-4b21c9d4-7db5-4121-8a75-a524b73355f4" satisfied condition "success or failure"
Dec  3 15:52:47.278: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-projected-configmaps-4b21c9d4-7db5-4121-8a75-a524b73355f4 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:52:47.467: INFO: Waiting for pod pod-projected-configmaps-4b21c9d4-7db5-4121-8a75-a524b73355f4 to disappear
Dec  3 15:52:47.556: INFO: Pod pod-projected-configmaps-4b21c9d4-7db5-4121-8a75-a524b73355f4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:52:47.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4622" for this suite.
Dec  3 15:52:53.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:52:57.031: INFO: namespace projected-4622 deletion completed in 9.384941695s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:52:57.032: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Dec  3 15:52:57.668: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:52:58.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8504" for this suite.
Dec  3 15:53:04.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:53:07.563: INFO: namespace kubectl-8504 deletion completed in 9.386461824s
â€¢S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:53:07.564: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5153
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5153
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Dec  3 15:53:08.469: INFO: Found 1 stateful pods, waiting for 3
Dec  3 15:53:18.600: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:53:18.600: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:53:18.600: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec  3 15:53:19.065: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec  3 15:53:19.433: INFO: Updating stateful set ss2
Dec  3 15:53:19.615: INFO: Waiting for Pod statefulset-5153/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Dec  3 15:53:30.094: INFO: Found 2 stateful pods, waiting for 3
Dec  3 15:53:40.185: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:53:40.185: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 15:53:40.185: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec  3 15:53:40.553: INFO: Updating stateful set ss2
Dec  3 15:53:40.732: INFO: Waiting for Pod statefulset-5153/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec  3 15:53:50.914: INFO: Waiting for Pod statefulset-5153/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec  3 15:54:01.102: INFO: Updating stateful set ss2
Dec  3 15:54:01.281: INFO: Waiting for StatefulSet statefulset-5153/ss2 to complete update
Dec  3 15:54:01.281: INFO: Waiting for Pod statefulset-5153/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec  3 15:54:11.461: INFO: Deleting all statefulset in ns statefulset-5153
Dec  3 15:54:11.550: INFO: Scaling statefulset ss2 to 0
Dec  3 15:54:41.909: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 15:54:41.998: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:54:42.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5153" for this suite.
Dec  3 15:54:48.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:54:51.756: INFO: namespace statefulset-5153 deletion completed in 9.397687082s
â€¢S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:54:51.756: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5639
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 15:54:53.553: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985293, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985293, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985293, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985293, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 15:54:56.746: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:54:57.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5639" for this suite.
Dec  3 15:55:04.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:55:07.352: INFO: namespace webhook-5639 deletion completed in 9.384396488s
STEP: Destroying namespace "webhook-5639-markers" for this suite.
Dec  3 15:55:13.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:55:16.738: INFO: namespace webhook-5639-markers deletion completed in 9.385813138s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:55:17.096: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Dec  3 15:55:58.364: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1203 15:55:58.364853    5076 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:55:58.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2856" for this suite.
Dec  3 15:56:04.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:56:07.845: INFO: namespace gc-2856 deletion completed in 9.390579622s
â€¢SS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:56:07.845: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5903
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec  3 15:56:08.578: INFO: Waiting up to 5m0s for pod "downward-api-69e24985-df86-4cac-b2bc-051acce996a6" in namespace "downward-api-5903" to be "success or failure"
Dec  3 15:56:08.667: INFO: Pod "downward-api-69e24985-df86-4cac-b2bc-051acce996a6": Phase="Pending", Reason="", readiness=false. Elapsed: 89.45953ms
Dec  3 15:56:10.757: INFO: Pod "downward-api-69e24985-df86-4cac-b2bc-051acce996a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179228959s
STEP: Saw pod success
Dec  3 15:56:10.757: INFO: Pod "downward-api-69e24985-df86-4cac-b2bc-051acce996a6" satisfied condition "success or failure"
Dec  3 15:56:10.847: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downward-api-69e24985-df86-4cac-b2bc-051acce996a6 container dapi-container: <nil>
STEP: delete the pod
Dec  3 15:56:11.130: INFO: Waiting for pod downward-api-69e24985-df86-4cac-b2bc-051acce996a6 to disappear
Dec  3 15:56:11.219: INFO: Pod downward-api-69e24985-df86-4cac-b2bc-051acce996a6 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:56:11.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5903" for this suite.
Dec  3 15:56:17.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:56:20.699: INFO: namespace downward-api-5903 deletion completed in 9.390680382s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:56:20.700: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4790
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 15:56:21.428: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-bec6c1f7-fb9f-47f0-991e-8d7a2403814a" in namespace "security-context-test-4790" to be "success or failure"
Dec  3 15:56:21.520: INFO: Pod "busybox-privileged-false-bec6c1f7-fb9f-47f0-991e-8d7a2403814a": Phase="Pending", Reason="", readiness=false. Elapsed: 92.297735ms
Dec  3 15:56:23.610: INFO: Pod "busybox-privileged-false-bec6c1f7-fb9f-47f0-991e-8d7a2403814a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.182456707s
Dec  3 15:56:23.610: INFO: Pod "busybox-privileged-false-bec6c1f7-fb9f-47f0-991e-8d7a2403814a" satisfied condition "success or failure"
Dec  3 15:56:23.708: INFO: Got logs for pod "busybox-privileged-false-bec6c1f7-fb9f-47f0-991e-8d7a2403814a": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:56:23.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4790" for this suite.
Dec  3 15:56:30.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:56:33.189: INFO: namespace security-context-test-4790 deletion completed in 9.388035242s
â€¢SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:56:33.189: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3952
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  3 15:56:36.955: INFO: Successfully updated pod "pod-update-activedeadlineseconds-57cde17e-55d4-4aee-a35b-66157d6f00e6"
Dec  3 15:56:36.955: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-57cde17e-55d4-4aee-a35b-66157d6f00e6" in namespace "pods-3952" to be "terminated due to deadline exceeded"
Dec  3 15:56:37.044: INFO: Pod "pod-update-activedeadlineseconds-57cde17e-55d4-4aee-a35b-66157d6f00e6": Phase="Running", Reason="", readiness=true. Elapsed: 89.038783ms
Dec  3 15:56:39.134: INFO: Pod "pod-update-activedeadlineseconds-57cde17e-55d4-4aee-a35b-66157d6f00e6": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.17869406s
Dec  3 15:56:39.134: INFO: Pod "pod-update-activedeadlineseconds-57cde17e-55d4-4aee-a35b-66157d6f00e6" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:56:39.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3952" for this suite.
Dec  3 15:56:45.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:56:48.628: INFO: namespace pods-3952 deletion completed in 9.403402815s
â€¢SSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:56:48.628: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3828
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 15:56:49.266: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:56:52.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3828" for this suite.
Dec  3 15:57:36.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:57:39.656: INFO: namespace pods-3828 deletion completed in 47.389511127s
â€¢SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:57:39.657: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6692
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-0cac5ad5-8b98-4db1-9da2-b597776b0a07
STEP: Creating a pod to test consume secrets
Dec  3 15:57:40.479: INFO: Waiting up to 5m0s for pod "pod-secrets-c4d59052-8d5f-447b-8c9a-f588edf44f04" in namespace "secrets-6692" to be "success or failure"
Dec  3 15:57:40.568: INFO: Pod "pod-secrets-c4d59052-8d5f-447b-8c9a-f588edf44f04": Phase="Pending", Reason="", readiness=false. Elapsed: 89.192566ms
Dec  3 15:57:42.658: INFO: Pod "pod-secrets-c4d59052-8d5f-447b-8c9a-f588edf44f04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179470139s
STEP: Saw pod success
Dec  3 15:57:42.658: INFO: Pod "pod-secrets-c4d59052-8d5f-447b-8c9a-f588edf44f04" satisfied condition "success or failure"
Dec  3 15:57:42.748: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-secrets-c4d59052-8d5f-447b-8c9a-f588edf44f04 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 15:57:42.937: INFO: Waiting for pod pod-secrets-c4d59052-8d5f-447b-8c9a-f588edf44f04 to disappear
Dec  3 15:57:43.026: INFO: Pod pod-secrets-c4d59052-8d5f-447b-8c9a-f588edf44f04 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:57:43.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6692" for this suite.
Dec  3 15:57:49.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:57:52.506: INFO: namespace secrets-6692 deletion completed in 9.390240359s
â€¢SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:57:52.507: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5784
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Dec  3 15:58:03.685: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1203 15:58:03.685185    5076 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:58:03.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5784" for this suite.
Dec  3 15:58:10.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:58:13.170: INFO: namespace gc-5784 deletion completed in 9.394928562s
â€¢SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:58:13.170: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-416
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-6f903dc7-ccc2-4cfb-a1a2-dbefd728c2ee
STEP: Creating a pod to test consume configMaps
Dec  3 15:58:13.985: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9f2b5636-4e2a-433a-a92a-702c7a797855" in namespace "projected-416" to be "success or failure"
Dec  3 15:58:14.074: INFO: Pod "pod-projected-configmaps-9f2b5636-4e2a-433a-a92a-702c7a797855": Phase="Pending", Reason="", readiness=false. Elapsed: 89.069405ms
Dec  3 15:58:16.164: INFO: Pod "pod-projected-configmaps-9f2b5636-4e2a-433a-a92a-702c7a797855": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.178992484s
STEP: Saw pod success
Dec  3 15:58:16.164: INFO: Pod "pod-projected-configmaps-9f2b5636-4e2a-433a-a92a-702c7a797855" satisfied condition "success or failure"
Dec  3 15:58:16.254: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-projected-configmaps-9f2b5636-4e2a-433a-a92a-702c7a797855 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 15:58:16.442: INFO: Waiting for pod pod-projected-configmaps-9f2b5636-4e2a-433a-a92a-702c7a797855 to disappear
Dec  3 15:58:16.531: INFO: Pod pod-projected-configmaps-9f2b5636-4e2a-433a-a92a-702c7a797855 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:58:16.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-416" for this suite.
Dec  3 15:58:22.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:58:26.052: INFO: namespace projected-416 deletion completed in 9.431195131s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:58:26.052: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9458
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-9458.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9458.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-9458.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-9458.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-9458.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9458.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 15:58:29.318: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:29.410: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:29.501: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:29.593: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:29.870: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:29.962: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:30.054: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:30.145: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:30.331: INFO: Lookups using dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9458.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local jessie_udp@dns-test-service-2.dns-9458.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9458.svc.cluster.local]

Dec  3 15:58:35.423: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:35.515: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:35.607: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:35.699: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:35.977: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:36.068: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:36.160: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:36.253: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:36.439: INFO: Lookups using dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9458.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local jessie_udp@dns-test-service-2.dns-9458.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9458.svc.cluster.local]

Dec  3 15:58:40.423: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:40.515: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:40.607: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:40.698: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:40.975: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:41.067: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:41.159: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:41.250: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:41.437: INFO: Lookups using dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9458.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local jessie_udp@dns-test-service-2.dns-9458.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9458.svc.cluster.local]

Dec  3 15:58:45.423: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:45.516: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:45.607: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:45.698: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:45.981: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:46.072: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:46.163: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:46.255: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:46.440: INFO: Lookups using dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9458.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local jessie_udp@dns-test-service-2.dns-9458.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9458.svc.cluster.local]

Dec  3 15:58:50.423: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:50.518: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:50.610: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:50.702: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:50.977: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:51.069: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:51.161: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:51.252: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:51.438: INFO: Lookups using dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9458.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local jessie_udp@dns-test-service-2.dns-9458.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9458.svc.cluster.local]

Dec  3 15:58:55.423: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:55.515: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:55.607: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:55.699: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:55.977: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:56.069: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:56.160: INFO: Unable to read jessie_udp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:56.252: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-9458.svc.cluster.local from pod dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9: the server could not find the requested resource (get pods dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9)
Dec  3 15:58:56.449: INFO: Lookups using dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local wheezy_udp@dns-test-service-2.dns-9458.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-9458.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-9458.svc.cluster.local jessie_udp@dns-test-service-2.dns-9458.svc.cluster.local jessie_tcp@dns-test-service-2.dns-9458.svc.cluster.local]

Dec  3 15:59:01.447: INFO: DNS probes using dns-9458/dns-test-964c8fa3-44aa-4700-be28-3a52b2f2bad9 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:59:01.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9458" for this suite.
Dec  3 15:59:07.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:59:11.118: INFO: namespace dns-9458 deletion completed in 9.393056213s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:59:11.118: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1781
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:59:29.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1781" for this suite.
Dec  3 15:59:35.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:59:38.504: INFO: namespace resourcequota-1781 deletion completed in 9.387465483s
â€¢SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:59:38.504: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3148
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Dec  3 15:59:39.141: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config api-versions'
Dec  3 15:59:39.667: INFO: stderr: ""
Dec  3 15:59:39.667: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncert.gardener.cloud/v1alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 15:59:39.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3148" for this suite.
Dec  3 15:59:46.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 15:59:49.147: INFO: namespace kubectl-3148 deletion completed in 9.387577748s
â€¢SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 15:59:49.147: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7623
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:00:01.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7623" for this suite.
Dec  3 16:00:07.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:00:10.920: INFO: namespace resourcequota-7623 deletion completed in 9.399235391s
â€¢SSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:00:10.920: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-3
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:00:11.649: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-7fb82ebf-5478-448e-9c2f-ed8b7998a6aa" in namespace "security-context-test-3" to be "success or failure"
Dec  3 16:00:11.738: INFO: Pod "busybox-readonly-false-7fb82ebf-5478-448e-9c2f-ed8b7998a6aa": Phase="Pending", Reason="", readiness=false. Elapsed: 89.269444ms
Dec  3 16:00:13.828: INFO: Pod "busybox-readonly-false-7fb82ebf-5478-448e-9c2f-ed8b7998a6aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179030526s
Dec  3 16:00:13.828: INFO: Pod "busybox-readonly-false-7fb82ebf-5478-448e-9c2f-ed8b7998a6aa" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:00:13.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3" for this suite.
Dec  3 16:00:20.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:00:23.315: INFO: namespace security-context-test-3 deletion completed in 9.396180561s
â€¢SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:00:23.315: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7211
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec  3 16:00:23.952: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:00:26.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7211" for this suite.
Dec  3 16:00:33.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:00:36.152: INFO: namespace init-container-7211 deletion completed in 9.400187477s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:00:36.153: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2724
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec  3 16:00:36.799: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  3 16:00:37.068: INFO: Waiting for terminating namespaces to be deleted...
Dec  3 16:00:37.158: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-10-98.ec2.internal before test
Dec  3 16:00:37.355: INFO: node-exporter-bt9wb from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.356: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 16:00:37.356: INFO: node-problem-detector-m2t47 from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.356: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 16:00:37.356: INFO: calico-node-sjfqj from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.356: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 16:00:37.356: INFO: kube-proxy-877v6 from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.356: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 16:00:37.356: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-27-214.ec2.internal before test
Dec  3 16:00:37.458: INFO: metrics-server-778877cf87-5dxgx from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.458: INFO: 	Container metrics-server ready: true, restart count 0
Dec  3 16:00:37.458: INFO: addons-nginx-ingress-controller-7c75bb76db-pr4bf from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.458: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  3 16:00:37.458: INFO: kube-proxy-pcff8 from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.458: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 16:00:37.458: INFO: calico-kube-controllers-79bcd784b6-w4fv5 from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.458: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  3 16:00:37.458: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-95f65778d-d2nzf from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.458: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  3 16:00:37.458: INFO: calico-node-drs2g from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.458: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 16:00:37.458: INFO: calico-typha-vertical-autoscaler-847d859f8c-m9mnl from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.458: INFO: 	Container autoscaler ready: true, restart count 4
Dec  3 16:00:37.458: INFO: vpn-shoot-57c94d6f78-cmlmw from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.458: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  3 16:00:37.458: INFO: addons-kubernetes-dashboard-78954cc66b-fj8m4 from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.458: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  3 16:00:37.458: INFO: node-problem-detector-j6tww from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.458: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 16:00:37.458: INFO: coredns-59c969ffb8-6vlmd from kube-system started at 2019-12-03 14:30:07 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.458: INFO: 	Container coredns ready: true, restart count 0
Dec  3 16:00:37.458: INFO: calico-typha-horizontal-autoscaler-69df649c59-9tbvm from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.458: INFO: 	Container autoscaler ready: true, restart count 0
Dec  3 16:00:37.458: INFO: coredns-59c969ffb8-pbnhz from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.458: INFO: 	Container coredns ready: true, restart count 0
Dec  3 16:00:37.458: INFO: blackbox-exporter-7bd7b55dfc-nqf6r from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.458: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  3 16:00:37.458: INFO: node-exporter-f2j7c from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.458: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 16:00:37.458: INFO: calico-typha-deploy-9f6b455c4-xv9k8 from kube-system started at 2019-12-03 14:33:07 +0000 UTC (1 container statuses recorded)
Dec  3 16:00:37.458: INFO: 	Container calico-typha ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15dce7cd1ad5b715], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:00:38.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2724" for this suite.
Dec  3 16:00:45.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:00:48.407: INFO: namespace sched-pred-2724 deletion completed in 9.402962977s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
â€¢
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:00:48.407: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7821
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f60225f1-e16d-4f91-a88e-0659c8898606
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-f60225f1-e16d-4f91-a88e-0659c8898606
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:00:53.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7821" for this suite.
Dec  3 16:01:06.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:01:09.455: INFO: namespace projected-7821 deletion completed in 15.397710388s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:01:09.456: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1990
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec  3 16:01:10.092: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  3 16:01:10.361: INFO: Waiting for terminating namespaces to be deleted...
Dec  3 16:01:10.451: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-10-98.ec2.internal before test
Dec  3 16:01:10.622: INFO: node-exporter-bt9wb from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.622: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 16:01:10.622: INFO: node-problem-detector-m2t47 from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.622: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 16:01:10.622: INFO: calico-node-sjfqj from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.622: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 16:01:10.622: INFO: kube-proxy-877v6 from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.622: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 16:01:10.622: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-27-214.ec2.internal before test
Dec  3 16:01:10.724: INFO: kube-proxy-pcff8 from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.724: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 16:01:10.724: INFO: calico-kube-controllers-79bcd784b6-w4fv5 from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.724: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  3 16:01:10.724: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-95f65778d-d2nzf from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.724: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  3 16:01:10.724: INFO: calico-node-drs2g from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.724: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 16:01:10.724: INFO: calico-typha-vertical-autoscaler-847d859f8c-m9mnl from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.725: INFO: 	Container autoscaler ready: true, restart count 4
Dec  3 16:01:10.725: INFO: vpn-shoot-57c94d6f78-cmlmw from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.725: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  3 16:01:10.725: INFO: addons-kubernetes-dashboard-78954cc66b-fj8m4 from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.725: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  3 16:01:10.725: INFO: node-problem-detector-j6tww from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.725: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 16:01:10.725: INFO: coredns-59c969ffb8-6vlmd from kube-system started at 2019-12-03 14:30:07 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.725: INFO: 	Container coredns ready: true, restart count 0
Dec  3 16:01:10.725: INFO: calico-typha-horizontal-autoscaler-69df649c59-9tbvm from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.725: INFO: 	Container autoscaler ready: true, restart count 0
Dec  3 16:01:10.725: INFO: coredns-59c969ffb8-pbnhz from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.725: INFO: 	Container coredns ready: true, restart count 0
Dec  3 16:01:10.725: INFO: blackbox-exporter-7bd7b55dfc-nqf6r from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.725: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  3 16:01:10.725: INFO: node-exporter-f2j7c from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.725: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 16:01:10.725: INFO: calico-typha-deploy-9f6b455c4-xv9k8 from kube-system started at 2019-12-03 14:33:07 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.725: INFO: 	Container calico-typha ready: true, restart count 0
Dec  3 16:01:10.725: INFO: metrics-server-778877cf87-5dxgx from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.725: INFO: 	Container metrics-server ready: true, restart count 0
Dec  3 16:01:10.725: INFO: addons-nginx-ingress-controller-7c75bb76db-pr4bf from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:01:10.725: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-aab02322-94e9-41d6-9329-63ab0d01106f 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-aab02322-94e9-41d6-9329-63ab0d01106f off the node ip-10-250-10-98.ec2.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-aab02322-94e9-41d6-9329-63ab0d01106f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:01:20.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1990" for this suite.
Dec  3 16:01:38.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:01:41.916: INFO: namespace sched-pred-1990 deletion completed in 21.393804184s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
â€¢SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:01:41.917: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-556
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  3 16:01:47.367: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  3 16:01:47.457: INFO: Pod pod-with-prestop-http-hook still exists
Dec  3 16:01:49.457: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  3 16:01:49.547: INFO: Pod pod-with-prestop-http-hook still exists
Dec  3 16:01:51.457: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  3 16:01:51.547: INFO: Pod pod-with-prestop-http-hook still exists
Dec  3 16:01:53.457: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  3 16:01:53.547: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:01:53.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-556" for this suite.
Dec  3 16:02:06.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:02:09.133: INFO: namespace container-lifecycle-hook-556 deletion completed in 15.39867788s
â€¢SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:02:09.134: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9432
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:02:09.863: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d55a5739-cb98-4edf-92bb-725d3e81571b" in namespace "downward-api-9432" to be "success or failure"
Dec  3 16:02:09.952: INFO: Pod "downwardapi-volume-d55a5739-cb98-4edf-92bb-725d3e81571b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.659437ms
Dec  3 16:02:12.042: INFO: Pod "downwardapi-volume-d55a5739-cb98-4edf-92bb-725d3e81571b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179603996s
STEP: Saw pod success
Dec  3 16:02:12.042: INFO: Pod "downwardapi-volume-d55a5739-cb98-4edf-92bb-725d3e81571b" satisfied condition "success or failure"
Dec  3 16:02:12.132: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-d55a5739-cb98-4edf-92bb-725d3e81571b container client-container: <nil>
STEP: delete the pod
Dec  3 16:02:12.320: INFO: Waiting for pod downwardapi-volume-d55a5739-cb98-4edf-92bb-725d3e81571b to disappear
Dec  3 16:02:12.409: INFO: Pod downwardapi-volume-d55a5739-cb98-4edf-92bb-725d3e81571b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:02:12.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9432" for this suite.
Dec  3 16:02:18.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:02:21.899: INFO: namespace downward-api-9432 deletion completed in 9.39992139s
â€¢SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:02:21.899: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5420
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:02:22.537: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:02:26.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5420" for this suite.
Dec  3 16:02:33.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:02:36.425: INFO: namespace custom-resource-definition-5420 deletion completed in 9.397925165s
â€¢SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:02:36.426: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  3 16:02:37.780: INFO: Number of nodes with available pods: 0
Dec  3 16:02:37.780: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 16:02:38.960: INFO: Number of nodes with available pods: 1
Dec  3 16:02:38.960: INFO: Node ip-10-250-27-214.ec2.internal is running more than one daemon pod
Dec  3 16:02:39.960: INFO: Number of nodes with available pods: 2
Dec  3 16:02:39.960: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec  3 16:02:40.409: INFO: Number of nodes with available pods: 1
Dec  3 16:02:40.409: INFO: Node ip-10-250-27-214.ec2.internal is running more than one daemon pod
Dec  3 16:02:41.589: INFO: Number of nodes with available pods: 2
Dec  3 16:02:41.589: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2541, will wait for the garbage collector to delete the pods
Dec  3 16:02:42.050: INFO: Deleting DaemonSet.extensions daemon-set took: 93.115481ms
Dec  3 16:02:42.150: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.388176ms
Dec  3 16:02:45.440: INFO: Number of nodes with available pods: 0
Dec  3 16:02:45.440: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 16:02:45.529: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2541/daemonsets","resourceVersion":"18969"},"items":null}

Dec  3 16:02:45.619: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2541/pods","resourceVersion":"18970"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:02:45.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2541" for this suite.
Dec  3 16:02:52.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:02:55.378: INFO: namespace daemonsets-2541 deletion completed in 9.397634886s
â€¢SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:02:55.378: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7962
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:02:56.107: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be3619b2-938d-4fce-9db3-e6da41f6edfd" in namespace "downward-api-7962" to be "success or failure"
Dec  3 16:02:56.196: INFO: Pod "downwardapi-volume-be3619b2-938d-4fce-9db3-e6da41f6edfd": Phase="Pending", Reason="", readiness=false. Elapsed: 89.445686ms
Dec  3 16:02:58.294: INFO: Pod "downwardapi-volume-be3619b2-938d-4fce-9db3-e6da41f6edfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.187022992s
STEP: Saw pod success
Dec  3 16:02:58.294: INFO: Pod "downwardapi-volume-be3619b2-938d-4fce-9db3-e6da41f6edfd" satisfied condition "success or failure"
Dec  3 16:02:58.383: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-be3619b2-938d-4fce-9db3-e6da41f6edfd container client-container: <nil>
STEP: delete the pod
Dec  3 16:02:58.571: INFO: Waiting for pod downwardapi-volume-be3619b2-938d-4fce-9db3-e6da41f6edfd to disappear
Dec  3 16:02:58.660: INFO: Pod downwardapi-volume-be3619b2-938d-4fce-9db3-e6da41f6edfd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:02:58.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7962" for this suite.
Dec  3 16:03:05.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:03:08.146: INFO: namespace downward-api-7962 deletion completed in 9.396918861s
â€¢S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:03:08.147: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2843
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec  3 16:03:08.785: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec  3 16:03:26.658: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:03:30.772: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:03:47.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2843" for this suite.
Dec  3 16:03:54.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:03:57.260: INFO: namespace crd-publish-openapi-2843 deletion completed in 9.400797613s
â€¢SSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:03:57.260: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6740
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6740, will wait for the garbage collector to delete the pods
Dec  3 16:04:00.359: INFO: Deleting Job.batch foo took: 90.844481ms
Dec  3 16:04:00.759: INFO: Terminating Job.batch foo pods took: 400.332569ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:04:43.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6740" for this suite.
Dec  3 16:04:49.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:04:52.637: INFO: namespace job-6740 deletion completed in 9.397717335s
â€¢S
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:04:52.637: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-6295
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6295
I1203 16:04:53.367112    5076 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6295, replica count: 1
I1203 16:04:54.467826    5076 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1203 16:04:55.468090    5076 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  3 16:04:55.662: INFO: Created: latency-svc-8cnj9
Dec  3 16:04:55.664: INFO: Got endpoints: latency-svc-8cnj9 [96.57821ms]
Dec  3 16:04:55.759: INFO: Created: latency-svc-gkm78
Dec  3 16:04:55.763: INFO: Got endpoints: latency-svc-gkm78 [98.332198ms]
Dec  3 16:04:55.763: INFO: Created: latency-svc-nn9ks
Dec  3 16:04:55.767: INFO: Created: latency-svc-68fd4
Dec  3 16:04:55.768: INFO: Got endpoints: latency-svc-nn9ks [103.006213ms]
Dec  3 16:04:55.769: INFO: Got endpoints: latency-svc-68fd4 [103.616126ms]
Dec  3 16:04:55.846: INFO: Created: latency-svc-98t6m
Dec  3 16:04:55.850: INFO: Created: latency-svc-f87s4
Dec  3 16:04:55.850: INFO: Got endpoints: latency-svc-98t6m [185.309548ms]
Dec  3 16:04:55.854: INFO: Got endpoints: latency-svc-f87s4 [189.31413ms]
Dec  3 16:04:55.854: INFO: Created: latency-svc-cwh6x
Dec  3 16:04:55.859: INFO: Got endpoints: latency-svc-cwh6x [193.98159ms]
Dec  3 16:04:55.859: INFO: Created: latency-svc-chggw
Dec  3 16:04:55.861: INFO: Got endpoints: latency-svc-chggw [195.697244ms]
Dec  3 16:04:55.863: INFO: Created: latency-svc-4trrt
Dec  3 16:04:55.867: INFO: Got endpoints: latency-svc-4trrt [202.569432ms]
Dec  3 16:04:55.867: INFO: Created: latency-svc-xf2zn
Dec  3 16:04:55.872: INFO: Got endpoints: latency-svc-xf2zn [206.888701ms]
Dec  3 16:04:55.872: INFO: Created: latency-svc-tnmbp
Dec  3 16:04:55.874: INFO: Got endpoints: latency-svc-tnmbp [208.815676ms]
Dec  3 16:04:55.876: INFO: Created: latency-svc-6ttk5
Dec  3 16:04:55.878: INFO: Got endpoints: latency-svc-6ttk5 [212.863462ms]
Dec  3 16:04:55.881: INFO: Created: latency-svc-j8q98
Dec  3 16:04:55.885: INFO: Got endpoints: latency-svc-j8q98 [220.231558ms]
Dec  3 16:04:55.885: INFO: Created: latency-svc-5zfkt
Dec  3 16:04:55.893: INFO: Got endpoints: latency-svc-5zfkt [227.656014ms]
Dec  3 16:04:55.894: INFO: Created: latency-svc-wsqzd
Dec  3 16:04:55.898: INFO: Created: latency-svc-2qllc
Dec  3 16:04:55.898: INFO: Got endpoints: latency-svc-wsqzd [233.411733ms]
Dec  3 16:04:55.900: INFO: Got endpoints: latency-svc-2qllc [235.276247ms]
Dec  3 16:04:55.905: INFO: Created: latency-svc-5877n
Dec  3 16:04:55.908: INFO: Created: latency-svc-stpmq
Dec  3 16:04:55.908: INFO: Got endpoints: latency-svc-5877n [145.479994ms]
Dec  3 16:04:55.913: INFO: Created: latency-svc-hq556
Dec  3 16:04:55.913: INFO: Got endpoints: latency-svc-stpmq [144.899109ms]
Dec  3 16:04:55.914: INFO: Got endpoints: latency-svc-hq556 [145.77217ms]
Dec  3 16:04:55.945: INFO: Created: latency-svc-v55ww
Dec  3 16:04:55.952: INFO: Got endpoints: latency-svc-v55ww [101.929935ms]
Dec  3 16:04:55.952: INFO: Created: latency-svc-q8vhq
Dec  3 16:04:55.956: INFO: Got endpoints: latency-svc-q8vhq [102.3403ms]
Dec  3 16:04:55.957: INFO: Created: latency-svc-8lxf6
Dec  3 16:04:55.961: INFO: Got endpoints: latency-svc-8lxf6 [101.966597ms]
Dec  3 16:04:55.961: INFO: Created: latency-svc-bjj5g
Dec  3 16:04:55.965: INFO: Got endpoints: latency-svc-bjj5g [104.543732ms]
Dec  3 16:04:55.966: INFO: Created: latency-svc-skmmz
Dec  3 16:04:55.969: INFO: Got endpoints: latency-svc-skmmz [101.950381ms]
Dec  3 16:04:55.970: INFO: Created: latency-svc-92rvm
Dec  3 16:04:55.973: INFO: Got endpoints: latency-svc-92rvm [101.717662ms]
Dec  3 16:04:55.974: INFO: Created: latency-svc-587jd
Dec  3 16:04:55.977: INFO: Got endpoints: latency-svc-587jd [103.818395ms]
Dec  3 16:04:55.978: INFO: Created: latency-svc-b87pw
Dec  3 16:04:55.979: INFO: Got endpoints: latency-svc-b87pw [101.246174ms]
Dec  3 16:04:56.031: INFO: Created: latency-svc-cpj5z
Dec  3 16:04:56.036: INFO: Created: latency-svc-dbw5d
Dec  3 16:04:56.036: INFO: Got endpoints: latency-svc-cpj5z [150.579117ms]
Dec  3 16:04:56.039: INFO: Got endpoints: latency-svc-dbw5d [130.786647ms]
Dec  3 16:04:56.039: INFO: Created: latency-svc-v5prr
Dec  3 16:04:56.044: INFO: Created: latency-svc-tgzg4
Dec  3 16:04:56.044: INFO: Got endpoints: latency-svc-v5prr [145.486068ms]
Dec  3 16:04:56.048: INFO: Got endpoints: latency-svc-tgzg4 [148.305275ms]
Dec  3 16:04:56.048: INFO: Created: latency-svc-sqfcs
Dec  3 16:04:56.053: INFO: Created: latency-svc-2zkkv
Dec  3 16:04:56.053: INFO: Got endpoints: latency-svc-sqfcs [160.116118ms]
Dec  3 16:04:56.057: INFO: Created: latency-svc-q4vsf
Dec  3 16:04:56.057: INFO: Got endpoints: latency-svc-2zkkv [144.388386ms]
Dec  3 16:04:56.059: INFO: Got endpoints: latency-svc-q4vsf [144.843231ms]
Dec  3 16:04:56.063: INFO: Created: latency-svc-nfl7g
Dec  3 16:04:56.064: INFO: Got endpoints: latency-svc-nfl7g [111.73353ms]
Dec  3 16:04:56.067: INFO: Created: latency-svc-t47dv
Dec  3 16:04:56.071: INFO: Got endpoints: latency-svc-t47dv [114.835679ms]
Dec  3 16:04:56.072: INFO: Created: latency-svc-ppndn
Dec  3 16:04:56.073: INFO: Got endpoints: latency-svc-ppndn [112.005789ms]
Dec  3 16:04:56.076: INFO: Created: latency-svc-rjqrr
Dec  3 16:04:56.080: INFO: Created: latency-svc-cljsf
Dec  3 16:04:56.085: INFO: Created: latency-svc-h7lft
Dec  3 16:04:56.089: INFO: Created: latency-svc-hf5pb
Dec  3 16:04:56.093: INFO: Created: latency-svc-rp2bl
Dec  3 16:04:56.112: INFO: Got endpoints: latency-svc-rjqrr [146.296642ms]
Dec  3 16:04:56.129: INFO: Created: latency-svc-r6mwz
Dec  3 16:04:56.134: INFO: Created: latency-svc-ln6cq
Dec  3 16:04:56.138: INFO: Created: latency-svc-2tr72
Dec  3 16:04:56.143: INFO: Created: latency-svc-5ngbj
Dec  3 16:04:56.147: INFO: Created: latency-svc-4shbt
Dec  3 16:04:56.152: INFO: Created: latency-svc-sprx6
Dec  3 16:04:56.157: INFO: Created: latency-svc-7fqmh
Dec  3 16:04:56.161: INFO: Got endpoints: latency-svc-cljsf [191.643919ms]
Dec  3 16:04:56.161: INFO: Created: latency-svc-sc2mf
Dec  3 16:04:56.166: INFO: Created: latency-svc-xfnvk
Dec  3 16:04:56.170: INFO: Created: latency-svc-rphwt
Dec  3 16:04:56.205: INFO: Created: latency-svc-gd6tb
Dec  3 16:04:56.212: INFO: Got endpoints: latency-svc-h7lft [238.679754ms]
Dec  3 16:04:56.260: INFO: Created: latency-svc-x8lpw
Dec  3 16:04:56.261: INFO: Got endpoints: latency-svc-hf5pb [283.344391ms]
Dec  3 16:04:56.307: INFO: Created: latency-svc-n9w7h
Dec  3 16:04:56.311: INFO: Got endpoints: latency-svc-rp2bl [332.20244ms]
Dec  3 16:04:56.356: INFO: Created: latency-svc-f4lgl
Dec  3 16:04:56.361: INFO: Got endpoints: latency-svc-r6mwz [325.298985ms]
Dec  3 16:04:56.405: INFO: Created: latency-svc-vc9xz
Dec  3 16:04:56.411: INFO: Got endpoints: latency-svc-ln6cq [371.988156ms]
Dec  3 16:04:56.456: INFO: Created: latency-svc-zxxst
Dec  3 16:04:56.461: INFO: Got endpoints: latency-svc-2tr72 [417.334826ms]
Dec  3 16:04:56.510: INFO: Created: latency-svc-q5vrg
Dec  3 16:04:56.518: INFO: Got endpoints: latency-svc-5ngbj [469.854615ms]
Dec  3 16:04:56.559: INFO: Created: latency-svc-6v8zb
Dec  3 16:04:56.561: INFO: Got endpoints: latency-svc-4shbt [508.332842ms]
Dec  3 16:04:56.615: INFO: Created: latency-svc-pkg29
Dec  3 16:04:56.615: INFO: Got endpoints: latency-svc-sprx6 [557.642764ms]
Dec  3 16:04:56.655: INFO: Created: latency-svc-w9799
Dec  3 16:04:56.661: INFO: Got endpoints: latency-svc-7fqmh [601.853287ms]
Dec  3 16:04:56.709: INFO: Created: latency-svc-zrzc8
Dec  3 16:04:56.711: INFO: Got endpoints: latency-svc-sc2mf [647.161756ms]
Dec  3 16:04:56.756: INFO: Created: latency-svc-5wk9n
Dec  3 16:04:56.761: INFO: Got endpoints: latency-svc-xfnvk [689.774628ms]
Dec  3 16:04:56.806: INFO: Created: latency-svc-qwnpw
Dec  3 16:04:56.811: INFO: Got endpoints: latency-svc-rphwt [738.254568ms]
Dec  3 16:04:56.862: INFO: Created: latency-svc-kdz69
Dec  3 16:04:56.862: INFO: Got endpoints: latency-svc-gd6tb [750.345931ms]
Dec  3 16:04:56.905: INFO: Created: latency-svc-9tpxc
Dec  3 16:04:56.912: INFO: Got endpoints: latency-svc-x8lpw [750.495585ms]
Dec  3 16:04:56.956: INFO: Created: latency-svc-t72qk
Dec  3 16:04:56.961: INFO: Got endpoints: latency-svc-n9w7h [748.975469ms]
Dec  3 16:04:57.006: INFO: Created: latency-svc-2s2g7
Dec  3 16:04:57.011: INFO: Got endpoints: latency-svc-f4lgl [750.213008ms]
Dec  3 16:04:57.056: INFO: Created: latency-svc-6rx8x
Dec  3 16:04:57.061: INFO: Got endpoints: latency-svc-vc9xz [749.873755ms]
Dec  3 16:04:57.105: INFO: Created: latency-svc-7c5fh
Dec  3 16:04:57.112: INFO: Got endpoints: latency-svc-zxxst [750.336792ms]
Dec  3 16:04:57.160: INFO: Created: latency-svc-g9cs6
Dec  3 16:04:57.161: INFO: Got endpoints: latency-svc-q5vrg [749.788845ms]
Dec  3 16:04:57.211: INFO: Created: latency-svc-vk9d7
Dec  3 16:04:57.212: INFO: Got endpoints: latency-svc-6v8zb [750.31154ms]
Dec  3 16:04:57.259: INFO: Created: latency-svc-fhkd6
Dec  3 16:04:57.261: INFO: Got endpoints: latency-svc-pkg29 [742.885868ms]
Dec  3 16:04:57.305: INFO: Created: latency-svc-l7lbd
Dec  3 16:04:57.311: INFO: Got endpoints: latency-svc-w9799 [750.222667ms]
Dec  3 16:04:57.356: INFO: Created: latency-svc-dpxfp
Dec  3 16:04:57.364: INFO: Got endpoints: latency-svc-zrzc8 [749.159395ms]
Dec  3 16:04:57.405: INFO: Created: latency-svc-stpdf
Dec  3 16:04:57.411: INFO: Got endpoints: latency-svc-5wk9n [749.803891ms]
Dec  3 16:04:57.458: INFO: Created: latency-svc-lbhnv
Dec  3 16:04:57.461: INFO: Got endpoints: latency-svc-qwnpw [749.879358ms]
Dec  3 16:04:57.505: INFO: Created: latency-svc-7dhwl
Dec  3 16:04:57.511: INFO: Got endpoints: latency-svc-kdz69 [749.92089ms]
Dec  3 16:04:57.557: INFO: Created: latency-svc-87qhh
Dec  3 16:04:57.561: INFO: Got endpoints: latency-svc-9tpxc [750.028513ms]
Dec  3 16:04:57.611: INFO: Created: latency-svc-dfdfj
Dec  3 16:04:57.611: INFO: Got endpoints: latency-svc-t72qk [749.235908ms]
Dec  3 16:04:57.655: INFO: Created: latency-svc-wwkr2
Dec  3 16:04:57.661: INFO: Got endpoints: latency-svc-2s2g7 [749.695996ms]
Dec  3 16:04:57.706: INFO: Created: latency-svc-vznlb
Dec  3 16:04:57.711: INFO: Got endpoints: latency-svc-6rx8x [749.669803ms]
Dec  3 16:04:57.755: INFO: Created: latency-svc-n66k7
Dec  3 16:04:57.761: INFO: Got endpoints: latency-svc-7c5fh [750.167655ms]
Dec  3 16:04:57.809: INFO: Created: latency-svc-8668h
Dec  3 16:04:57.813: INFO: Got endpoints: latency-svc-g9cs6 [751.30747ms]
Dec  3 16:04:57.857: INFO: Created: latency-svc-m6qwm
Dec  3 16:04:57.861: INFO: Got endpoints: latency-svc-vk9d7 [749.669579ms]
Dec  3 16:04:57.907: INFO: Created: latency-svc-27dv8
Dec  3 16:04:57.911: INFO: Got endpoints: latency-svc-fhkd6 [749.907909ms]
Dec  3 16:04:57.956: INFO: Created: latency-svc-xknfw
Dec  3 16:04:57.961: INFO: Got endpoints: latency-svc-l7lbd [749.581342ms]
Dec  3 16:04:58.006: INFO: Created: latency-svc-74g9g
Dec  3 16:04:58.011: INFO: Got endpoints: latency-svc-dpxfp [749.925637ms]
Dec  3 16:04:58.055: INFO: Created: latency-svc-7qgxx
Dec  3 16:04:58.061: INFO: Got endpoints: latency-svc-stpdf [749.862164ms]
Dec  3 16:04:58.105: INFO: Created: latency-svc-qxq99
Dec  3 16:04:58.113: INFO: Got endpoints: latency-svc-lbhnv [749.128569ms]
Dec  3 16:04:58.155: INFO: Created: latency-svc-kkszd
Dec  3 16:04:58.161: INFO: Got endpoints: latency-svc-7dhwl [750.106206ms]
Dec  3 16:04:58.207: INFO: Created: latency-svc-c7zlq
Dec  3 16:04:58.211: INFO: Got endpoints: latency-svc-87qhh [750.015392ms]
Dec  3 16:04:58.255: INFO: Created: latency-svc-k9kgf
Dec  3 16:04:58.261: INFO: Got endpoints: latency-svc-dfdfj [750.083306ms]
Dec  3 16:04:58.307: INFO: Created: latency-svc-msthb
Dec  3 16:04:58.311: INFO: Got endpoints: latency-svc-wwkr2 [749.847075ms]
Dec  3 16:04:58.356: INFO: Created: latency-svc-4phk2
Dec  3 16:04:58.361: INFO: Got endpoints: latency-svc-vznlb [749.808896ms]
Dec  3 16:04:58.411: INFO: Created: latency-svc-dwqpk
Dec  3 16:04:58.413: INFO: Got endpoints: latency-svc-n66k7 [751.141801ms]
Dec  3 16:04:58.455: INFO: Created: latency-svc-rjltd
Dec  3 16:04:58.461: INFO: Got endpoints: latency-svc-8668h [750.205606ms]
Dec  3 16:04:58.506: INFO: Created: latency-svc-f7d4f
Dec  3 16:04:58.520: INFO: Got endpoints: latency-svc-m6qwm [758.311033ms]
Dec  3 16:04:58.555: INFO: Created: latency-svc-sr9px
Dec  3 16:04:58.561: INFO: Got endpoints: latency-svc-27dv8 [748.80485ms]
Dec  3 16:04:58.611: INFO: Got endpoints: latency-svc-xknfw [750.070175ms]
Dec  3 16:04:58.615: INFO: Created: latency-svc-hdccv
Dec  3 16:04:58.655: INFO: Created: latency-svc-zbkdd
Dec  3 16:04:58.661: INFO: Got endpoints: latency-svc-74g9g [750.190399ms]
Dec  3 16:04:58.705: INFO: Created: latency-svc-tkvb8
Dec  3 16:04:58.711: INFO: Got endpoints: latency-svc-7qgxx [749.976372ms]
Dec  3 16:04:58.756: INFO: Created: latency-svc-wfvhm
Dec  3 16:04:58.761: INFO: Got endpoints: latency-svc-qxq99 [750.017355ms]
Dec  3 16:04:58.813: INFO: Got endpoints: latency-svc-kkszd [751.264057ms]
Dec  3 16:04:58.813: INFO: Created: latency-svc-q8lmm
Dec  3 16:04:58.855: INFO: Created: latency-svc-vj9xl
Dec  3 16:04:58.861: INFO: Got endpoints: latency-svc-c7zlq [747.912225ms]
Dec  3 16:04:58.906: INFO: Created: latency-svc-fzr4f
Dec  3 16:04:58.911: INFO: Got endpoints: latency-svc-k9kgf [749.905199ms]
Dec  3 16:04:58.955: INFO: Created: latency-svc-m2wh5
Dec  3 16:04:58.961: INFO: Got endpoints: latency-svc-msthb [750.071293ms]
Dec  3 16:04:59.005: INFO: Created: latency-svc-8kbkh
Dec  3 16:04:59.011: INFO: Got endpoints: latency-svc-4phk2 [749.854723ms]
Dec  3 16:04:59.055: INFO: Created: latency-svc-hgnlv
Dec  3 16:04:59.061: INFO: Got endpoints: latency-svc-dwqpk [750.169709ms]
Dec  3 16:04:59.105: INFO: Created: latency-svc-bg5nr
Dec  3 16:04:59.111: INFO: Got endpoints: latency-svc-rjltd [749.858704ms]
Dec  3 16:04:59.156: INFO: Created: latency-svc-9n5x9
Dec  3 16:04:59.161: INFO: Got endpoints: latency-svc-f7d4f [748.575491ms]
Dec  3 16:04:59.205: INFO: Created: latency-svc-fbbh6
Dec  3 16:04:59.212: INFO: Got endpoints: latency-svc-sr9px [750.208412ms]
Dec  3 16:04:59.255: INFO: Created: latency-svc-4rfmf
Dec  3 16:04:59.262: INFO: Got endpoints: latency-svc-hdccv [741.823853ms]
Dec  3 16:04:59.306: INFO: Created: latency-svc-vl96m
Dec  3 16:04:59.311: INFO: Got endpoints: latency-svc-zbkdd [749.646468ms]
Dec  3 16:04:59.356: INFO: Created: latency-svc-l56xx
Dec  3 16:04:59.361: INFO: Got endpoints: latency-svc-tkvb8 [749.650955ms]
Dec  3 16:04:59.406: INFO: Created: latency-svc-lgbgz
Dec  3 16:04:59.411: INFO: Got endpoints: latency-svc-wfvhm [749.892093ms]
Dec  3 16:04:59.457: INFO: Created: latency-svc-4x6vx
Dec  3 16:04:59.461: INFO: Got endpoints: latency-svc-q8lmm [749.778948ms]
Dec  3 16:04:59.505: INFO: Created: latency-svc-fn6wj
Dec  3 16:04:59.512: INFO: Got endpoints: latency-svc-vj9xl [750.338812ms]
Dec  3 16:04:59.561: INFO: Got endpoints: latency-svc-fzr4f [748.17627ms]
Dec  3 16:04:59.561: INFO: Created: latency-svc-9b5kg
Dec  3 16:04:59.605: INFO: Created: latency-svc-8mq2c
Dec  3 16:04:59.611: INFO: Got endpoints: latency-svc-m2wh5 [749.96781ms]
Dec  3 16:04:59.655: INFO: Created: latency-svc-xdjxt
Dec  3 16:04:59.665: INFO: Got endpoints: latency-svc-8kbkh [753.472682ms]
Dec  3 16:04:59.706: INFO: Created: latency-svc-7lnzx
Dec  3 16:04:59.711: INFO: Got endpoints: latency-svc-hgnlv [749.469804ms]
Dec  3 16:04:59.759: INFO: Created: latency-svc-pcck9
Dec  3 16:04:59.761: INFO: Got endpoints: latency-svc-bg5nr [749.810223ms]
Dec  3 16:04:59.805: INFO: Created: latency-svc-jnhq9
Dec  3 16:04:59.811: INFO: Got endpoints: latency-svc-9n5x9 [749.785646ms]
Dec  3 16:04:59.855: INFO: Created: latency-svc-pl5f2
Dec  3 16:04:59.861: INFO: Got endpoints: latency-svc-fbbh6 [750.07762ms]
Dec  3 16:04:59.906: INFO: Created: latency-svc-8cknz
Dec  3 16:04:59.911: INFO: Got endpoints: latency-svc-4rfmf [749.922539ms]
Dec  3 16:04:59.956: INFO: Created: latency-svc-kfp2h
Dec  3 16:04:59.961: INFO: Got endpoints: latency-svc-vl96m [749.6325ms]
Dec  3 16:05:00.006: INFO: Created: latency-svc-cvmlg
Dec  3 16:05:00.011: INFO: Got endpoints: latency-svc-l56xx [749.333678ms]
Dec  3 16:05:00.069: INFO: Got endpoints: latency-svc-lgbgz [758.114368ms]
Dec  3 16:05:00.075: INFO: Created: latency-svc-pzndm
Dec  3 16:05:00.105: INFO: Created: latency-svc-pgnzw
Dec  3 16:05:00.111: INFO: Got endpoints: latency-svc-4x6vx [749.910231ms]
Dec  3 16:05:00.163: INFO: Created: latency-svc-qthnx
Dec  3 16:05:00.163: INFO: Got endpoints: latency-svc-fn6wj [751.90936ms]
Dec  3 16:05:00.205: INFO: Created: latency-svc-z6fg5
Dec  3 16:05:00.211: INFO: Got endpoints: latency-svc-9b5kg [749.955298ms]
Dec  3 16:05:00.257: INFO: Created: latency-svc-6lm7t
Dec  3 16:05:00.261: INFO: Got endpoints: latency-svc-8mq2c [749.630583ms]
Dec  3 16:05:00.305: INFO: Created: latency-svc-rqjvw
Dec  3 16:05:00.311: INFO: Got endpoints: latency-svc-xdjxt [750.09455ms]
Dec  3 16:05:00.355: INFO: Created: latency-svc-9zmcx
Dec  3 16:05:00.361: INFO: Got endpoints: latency-svc-7lnzx [749.939357ms]
Dec  3 16:05:00.405: INFO: Created: latency-svc-7kw46
Dec  3 16:05:00.411: INFO: Got endpoints: latency-svc-pcck9 [746.37103ms]
Dec  3 16:05:00.455: INFO: Created: latency-svc-95fjk
Dec  3 16:05:00.461: INFO: Got endpoints: latency-svc-jnhq9 [750.119781ms]
Dec  3 16:05:00.513: INFO: Got endpoints: latency-svc-pl5f2 [752.015919ms]
Dec  3 16:05:00.519: INFO: Created: latency-svc-ljtzs
Dec  3 16:05:00.563: INFO: Created: latency-svc-f78tn
Dec  3 16:05:00.563: INFO: Got endpoints: latency-svc-8cknz [752.126506ms]
Dec  3 16:05:00.609: INFO: Created: latency-svc-dvjhr
Dec  3 16:05:00.611: INFO: Got endpoints: latency-svc-kfp2h [749.814238ms]
Dec  3 16:05:00.658: INFO: Created: latency-svc-v7dfn
Dec  3 16:05:00.661: INFO: Got endpoints: latency-svc-cvmlg [749.821761ms]
Dec  3 16:05:00.704: INFO: Created: latency-svc-pt4wk
Dec  3 16:05:00.711: INFO: Got endpoints: latency-svc-pzndm [749.768581ms]
Dec  3 16:05:00.756: INFO: Created: latency-svc-km4k5
Dec  3 16:05:00.761: INFO: Got endpoints: latency-svc-pgnzw [750.051801ms]
Dec  3 16:05:00.805: INFO: Created: latency-svc-5cwr7
Dec  3 16:05:00.814: INFO: Got endpoints: latency-svc-qthnx [744.637222ms]
Dec  3 16:05:00.856: INFO: Created: latency-svc-skhmn
Dec  3 16:05:00.862: INFO: Got endpoints: latency-svc-z6fg5 [750.371925ms]
Dec  3 16:05:00.908: INFO: Created: latency-svc-f5d98
Dec  3 16:05:00.911: INFO: Got endpoints: latency-svc-6lm7t [748.083256ms]
Dec  3 16:05:00.955: INFO: Created: latency-svc-cm28d
Dec  3 16:05:00.961: INFO: Got endpoints: latency-svc-rqjvw [750.043604ms]
Dec  3 16:05:01.005: INFO: Created: latency-svc-jjt4z
Dec  3 16:05:01.011: INFO: Got endpoints: latency-svc-9zmcx [749.788321ms]
Dec  3 16:05:01.056: INFO: Created: latency-svc-8vvqp
Dec  3 16:05:01.062: INFO: Got endpoints: latency-svc-7kw46 [750.544775ms]
Dec  3 16:05:01.106: INFO: Created: latency-svc-6nw26
Dec  3 16:05:01.111: INFO: Got endpoints: latency-svc-95fjk [749.94465ms]
Dec  3 16:05:01.157: INFO: Created: latency-svc-sg96c
Dec  3 16:05:01.161: INFO: Got endpoints: latency-svc-ljtzs [749.993883ms]
Dec  3 16:05:01.206: INFO: Created: latency-svc-d4sws
Dec  3 16:05:01.211: INFO: Got endpoints: latency-svc-f78tn [750.023312ms]
Dec  3 16:05:01.259: INFO: Created: latency-svc-5qtnd
Dec  3 16:05:01.261: INFO: Got endpoints: latency-svc-dvjhr [748.015768ms]
Dec  3 16:05:01.305: INFO: Created: latency-svc-rjjlk
Dec  3 16:05:01.313: INFO: Got endpoints: latency-svc-v7dfn [749.14279ms]
Dec  3 16:05:01.364: INFO: Created: latency-svc-vzhrt
Dec  3 16:05:01.364: INFO: Got endpoints: latency-svc-pt4wk [752.562328ms]
Dec  3 16:05:01.408: INFO: Created: latency-svc-blb2p
Dec  3 16:05:01.415: INFO: Got endpoints: latency-svc-km4k5 [753.457953ms]
Dec  3 16:05:01.458: INFO: Created: latency-svc-c8tq6
Dec  3 16:05:01.461: INFO: Got endpoints: latency-svc-5cwr7 [750.077571ms]
Dec  3 16:05:01.509: INFO: Created: latency-svc-hjkqc
Dec  3 16:05:01.511: INFO: Got endpoints: latency-svc-skhmn [749.816249ms]
Dec  3 16:05:01.566: INFO: Got endpoints: latency-svc-f5d98 [751.833386ms]
Dec  3 16:05:01.567: INFO: Created: latency-svc-f9jx4
Dec  3 16:05:01.605: INFO: Created: latency-svc-pt7c5
Dec  3 16:05:01.613: INFO: Got endpoints: latency-svc-cm28d [751.29461ms]
Dec  3 16:05:01.660: INFO: Created: latency-svc-wkppl
Dec  3 16:05:01.661: INFO: Got endpoints: latency-svc-jjt4z [749.742286ms]
Dec  3 16:05:01.712: INFO: Created: latency-svc-tn4dk
Dec  3 16:05:01.713: INFO: Got endpoints: latency-svc-8vvqp [751.208776ms]
Dec  3 16:05:01.757: INFO: Created: latency-svc-zrcsw
Dec  3 16:05:01.761: INFO: Got endpoints: latency-svc-6nw26 [750.160623ms]
Dec  3 16:05:01.807: INFO: Created: latency-svc-rncq5
Dec  3 16:05:01.811: INFO: Got endpoints: latency-svc-sg96c [749.360398ms]
Dec  3 16:05:01.858: INFO: Created: latency-svc-644vn
Dec  3 16:05:01.863: INFO: Got endpoints: latency-svc-d4sws [752.018842ms]
Dec  3 16:05:01.905: INFO: Created: latency-svc-wb6td
Dec  3 16:05:01.911: INFO: Got endpoints: latency-svc-5qtnd [750.004493ms]
Dec  3 16:05:01.958: INFO: Created: latency-svc-bbvzz
Dec  3 16:05:01.961: INFO: Got endpoints: latency-svc-rjjlk [749.946047ms]
Dec  3 16:05:02.006: INFO: Created: latency-svc-8dsnz
Dec  3 16:05:02.011: INFO: Got endpoints: latency-svc-vzhrt [749.979607ms]
Dec  3 16:05:02.055: INFO: Created: latency-svc-qcxvk
Dec  3 16:05:02.061: INFO: Got endpoints: latency-svc-blb2p [748.529629ms]
Dec  3 16:05:02.105: INFO: Created: latency-svc-lngtr
Dec  3 16:05:02.111: INFO: Got endpoints: latency-svc-c8tq6 [747.367012ms]
Dec  3 16:05:02.155: INFO: Created: latency-svc-dj9nm
Dec  3 16:05:02.161: INFO: Got endpoints: latency-svc-hjkqc [746.546161ms]
Dec  3 16:05:02.206: INFO: Created: latency-svc-dwm5q
Dec  3 16:05:02.211: INFO: Got endpoints: latency-svc-f9jx4 [750.062255ms]
Dec  3 16:05:02.257: INFO: Created: latency-svc-lrft4
Dec  3 16:05:02.261: INFO: Got endpoints: latency-svc-pt7c5 [750.007695ms]
Dec  3 16:05:02.305: INFO: Created: latency-svc-rd2dd
Dec  3 16:05:02.311: INFO: Got endpoints: latency-svc-wkppl [745.231697ms]
Dec  3 16:05:02.356: INFO: Created: latency-svc-t5nt2
Dec  3 16:05:02.361: INFO: Got endpoints: latency-svc-tn4dk [748.223417ms]
Dec  3 16:05:02.407: INFO: Created: latency-svc-qxbn4
Dec  3 16:05:02.411: INFO: Got endpoints: latency-svc-zrcsw [749.790296ms]
Dec  3 16:05:02.455: INFO: Created: latency-svc-zprj5
Dec  3 16:05:02.461: INFO: Got endpoints: latency-svc-rncq5 [748.554436ms]
Dec  3 16:05:02.508: INFO: Created: latency-svc-4km7h
Dec  3 16:05:02.511: INFO: Got endpoints: latency-svc-644vn [749.560046ms]
Dec  3 16:05:02.555: INFO: Created: latency-svc-9kxvv
Dec  3 16:05:02.561: INFO: Got endpoints: latency-svc-wb6td [749.970599ms]
Dec  3 16:05:02.606: INFO: Created: latency-svc-snsv5
Dec  3 16:05:02.612: INFO: Got endpoints: latency-svc-bbvzz [748.801131ms]
Dec  3 16:05:02.656: INFO: Created: latency-svc-zgp5j
Dec  3 16:05:02.661: INFO: Got endpoints: latency-svc-8dsnz [749.622194ms]
Dec  3 16:05:02.707: INFO: Created: latency-svc-sts2p
Dec  3 16:05:02.711: INFO: Got endpoints: latency-svc-qcxvk [749.786015ms]
Dec  3 16:05:02.757: INFO: Created: latency-svc-jnn9x
Dec  3 16:05:02.761: INFO: Got endpoints: latency-svc-lngtr [749.731307ms]
Dec  3 16:05:02.805: INFO: Created: latency-svc-4f9c7
Dec  3 16:05:02.811: INFO: Got endpoints: latency-svc-dj9nm [749.613211ms]
Dec  3 16:05:02.856: INFO: Created: latency-svc-7c27b
Dec  3 16:05:02.861: INFO: Got endpoints: latency-svc-dwm5q [749.857676ms]
Dec  3 16:05:02.905: INFO: Created: latency-svc-hfrcr
Dec  3 16:05:02.911: INFO: Got endpoints: latency-svc-lrft4 [749.871315ms]
Dec  3 16:05:02.956: INFO: Created: latency-svc-ngpjc
Dec  3 16:05:02.961: INFO: Got endpoints: latency-svc-rd2dd [749.842697ms]
Dec  3 16:05:03.005: INFO: Created: latency-svc-ns96r
Dec  3 16:05:03.011: INFO: Got endpoints: latency-svc-t5nt2 [749.966982ms]
Dec  3 16:05:03.056: INFO: Created: latency-svc-22lsm
Dec  3 16:05:03.061: INFO: Got endpoints: latency-svc-qxbn4 [749.801288ms]
Dec  3 16:05:03.106: INFO: Created: latency-svc-599km
Dec  3 16:05:03.111: INFO: Got endpoints: latency-svc-zprj5 [749.81237ms]
Dec  3 16:05:03.156: INFO: Created: latency-svc-5rsb4
Dec  3 16:05:03.161: INFO: Got endpoints: latency-svc-4km7h [750.005699ms]
Dec  3 16:05:03.205: INFO: Created: latency-svc-2fg6h
Dec  3 16:05:03.211: INFO: Got endpoints: latency-svc-9kxvv [750.102337ms]
Dec  3 16:05:03.266: INFO: Created: latency-svc-cr67m
Dec  3 16:05:03.266: INFO: Got endpoints: latency-svc-snsv5 [754.829575ms]
Dec  3 16:05:03.305: INFO: Created: latency-svc-tsjz5
Dec  3 16:05:03.311: INFO: Got endpoints: latency-svc-zgp5j [749.876484ms]
Dec  3 16:05:03.361: INFO: Created: latency-svc-rzfzm
Dec  3 16:05:03.361: INFO: Got endpoints: latency-svc-sts2p [748.811641ms]
Dec  3 16:05:03.406: INFO: Created: latency-svc-5qbzn
Dec  3 16:05:03.411: INFO: Got endpoints: latency-svc-jnn9x [750.075764ms]
Dec  3 16:05:03.456: INFO: Created: latency-svc-x2c2j
Dec  3 16:05:03.461: INFO: Got endpoints: latency-svc-4f9c7 [750.018708ms]
Dec  3 16:05:03.505: INFO: Created: latency-svc-tvpb5
Dec  3 16:05:03.511: INFO: Got endpoints: latency-svc-7c27b [750.195756ms]
Dec  3 16:05:03.558: INFO: Created: latency-svc-gszhw
Dec  3 16:05:03.561: INFO: Got endpoints: latency-svc-hfrcr [750.133092ms]
Dec  3 16:05:03.605: INFO: Created: latency-svc-xncpp
Dec  3 16:05:03.612: INFO: Got endpoints: latency-svc-ngpjc [750.478004ms]
Dec  3 16:05:03.662: INFO: Got endpoints: latency-svc-ns96r [750.236983ms]
Dec  3 16:05:03.712: INFO: Got endpoints: latency-svc-22lsm [750.299869ms]
Dec  3 16:05:03.761: INFO: Got endpoints: latency-svc-599km [750.11974ms]
Dec  3 16:05:03.811: INFO: Got endpoints: latency-svc-5rsb4 [750.079442ms]
Dec  3 16:05:03.861: INFO: Got endpoints: latency-svc-2fg6h [750.194116ms]
Dec  3 16:05:03.912: INFO: Got endpoints: latency-svc-cr67m [750.385146ms]
Dec  3 16:05:03.962: INFO: Got endpoints: latency-svc-tsjz5 [751.021574ms]
Dec  3 16:05:04.012: INFO: Got endpoints: latency-svc-rzfzm [745.449638ms]
Dec  3 16:05:04.062: INFO: Got endpoints: latency-svc-5qbzn [750.255694ms]
Dec  3 16:05:04.112: INFO: Got endpoints: latency-svc-x2c2j [750.345961ms]
Dec  3 16:05:04.162: INFO: Got endpoints: latency-svc-tvpb5 [750.39237ms]
Dec  3 16:05:04.212: INFO: Got endpoints: latency-svc-gszhw [750.559342ms]
Dec  3 16:05:04.261: INFO: Got endpoints: latency-svc-xncpp [750.051914ms]
Dec  3 16:05:04.262: INFO: Latencies: [98.332198ms 101.246174ms 101.717662ms 101.929935ms 101.950381ms 101.966597ms 102.3403ms 103.006213ms 103.616126ms 103.818395ms 104.543732ms 111.73353ms 112.005789ms 114.835679ms 130.786647ms 144.388386ms 144.843231ms 144.899109ms 145.479994ms 145.486068ms 145.77217ms 146.296642ms 148.305275ms 150.579117ms 160.116118ms 185.309548ms 189.31413ms 191.643919ms 193.98159ms 195.697244ms 202.569432ms 206.888701ms 208.815676ms 212.863462ms 220.231558ms 227.656014ms 233.411733ms 235.276247ms 238.679754ms 283.344391ms 325.298985ms 332.20244ms 371.988156ms 417.334826ms 469.854615ms 508.332842ms 557.642764ms 601.853287ms 647.161756ms 689.774628ms 738.254568ms 741.823853ms 742.885868ms 744.637222ms 745.231697ms 745.449638ms 746.37103ms 746.546161ms 747.367012ms 747.912225ms 748.015768ms 748.083256ms 748.17627ms 748.223417ms 748.529629ms 748.554436ms 748.575491ms 748.801131ms 748.80485ms 748.811641ms 748.975469ms 749.128569ms 749.14279ms 749.159395ms 749.235908ms 749.333678ms 749.360398ms 749.469804ms 749.560046ms 749.581342ms 749.613211ms 749.622194ms 749.630583ms 749.6325ms 749.646468ms 749.650955ms 749.669579ms 749.669803ms 749.695996ms 749.731307ms 749.742286ms 749.768581ms 749.778948ms 749.785646ms 749.786015ms 749.788321ms 749.788845ms 749.790296ms 749.801288ms 749.803891ms 749.808896ms 749.810223ms 749.81237ms 749.814238ms 749.816249ms 749.821761ms 749.842697ms 749.847075ms 749.854723ms 749.857676ms 749.858704ms 749.862164ms 749.871315ms 749.873755ms 749.876484ms 749.879358ms 749.892093ms 749.905199ms 749.907909ms 749.910231ms 749.92089ms 749.922539ms 749.925637ms 749.939357ms 749.94465ms 749.946047ms 749.955298ms 749.966982ms 749.96781ms 749.970599ms 749.976372ms 749.979607ms 749.993883ms 750.004493ms 750.005699ms 750.007695ms 750.015392ms 750.017355ms 750.018708ms 750.023312ms 750.028513ms 750.043604ms 750.051801ms 750.051914ms 750.062255ms 750.070175ms 750.071293ms 750.075764ms 750.077571ms 750.07762ms 750.079442ms 750.083306ms 750.09455ms 750.102337ms 750.106206ms 750.11974ms 750.119781ms 750.133092ms 750.160623ms 750.167655ms 750.169709ms 750.190399ms 750.194116ms 750.195756ms 750.205606ms 750.208412ms 750.213008ms 750.222667ms 750.236983ms 750.255694ms 750.299869ms 750.31154ms 750.336792ms 750.338812ms 750.345931ms 750.345961ms 750.371925ms 750.385146ms 750.39237ms 750.478004ms 750.495585ms 750.544775ms 750.559342ms 751.021574ms 751.141801ms 751.208776ms 751.264057ms 751.29461ms 751.30747ms 751.833386ms 751.90936ms 752.015919ms 752.018842ms 752.126506ms 752.562328ms 753.457953ms 753.472682ms 754.829575ms 758.114368ms 758.311033ms]
Dec  3 16:05:04.262: INFO: 50 %ile: 749.808896ms
Dec  3 16:05:04.262: INFO: 90 %ile: 750.495585ms
Dec  3 16:05:04.262: INFO: 99 %ile: 758.114368ms
Dec  3 16:05:04.262: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:05:04.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6295" for this suite.
Dec  3 16:05:12.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:05:15.738: INFO: namespace svc-latency-6295 deletion completed in 11.386551729s
â€¢SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:05:15.739: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7747
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:05:16.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7747" for this suite.
Dec  3 16:05:22.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:05:26.037: INFO: namespace kubelet-test-7747 deletion completed in 9.390886839s
â€¢SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:05:26.037: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4417
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-mw9b
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 16:05:26.956: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mw9b" in namespace "subpath-4417" to be "success or failure"
Dec  3 16:05:27.045: INFO: Pod "pod-subpath-test-configmap-mw9b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.076741ms
Dec  3 16:05:29.135: INFO: Pod "pod-subpath-test-configmap-mw9b": Phase="Running", Reason="", readiness=true. Elapsed: 2.179088702s
Dec  3 16:05:31.225: INFO: Pod "pod-subpath-test-configmap-mw9b": Phase="Running", Reason="", readiness=true. Elapsed: 4.269345231s
Dec  3 16:05:33.316: INFO: Pod "pod-subpath-test-configmap-mw9b": Phase="Running", Reason="", readiness=true. Elapsed: 6.359693746s
Dec  3 16:05:35.406: INFO: Pod "pod-subpath-test-configmap-mw9b": Phase="Running", Reason="", readiness=true. Elapsed: 8.449517067s
Dec  3 16:05:37.495: INFO: Pod "pod-subpath-test-configmap-mw9b": Phase="Running", Reason="", readiness=true. Elapsed: 10.539250015s
Dec  3 16:05:39.585: INFO: Pod "pod-subpath-test-configmap-mw9b": Phase="Running", Reason="", readiness=true. Elapsed: 12.629208278s
Dec  3 16:05:41.675: INFO: Pod "pod-subpath-test-configmap-mw9b": Phase="Running", Reason="", readiness=true. Elapsed: 14.718891824s
Dec  3 16:05:43.765: INFO: Pod "pod-subpath-test-configmap-mw9b": Phase="Running", Reason="", readiness=true. Elapsed: 16.808743604s
Dec  3 16:05:45.855: INFO: Pod "pod-subpath-test-configmap-mw9b": Phase="Running", Reason="", readiness=true. Elapsed: 18.898704676s
Dec  3 16:05:47.945: INFO: Pod "pod-subpath-test-configmap-mw9b": Phase="Running", Reason="", readiness=true. Elapsed: 20.989034111s
Dec  3 16:05:50.035: INFO: Pod "pod-subpath-test-configmap-mw9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.079078644s
STEP: Saw pod success
Dec  3 16:05:50.035: INFO: Pod "pod-subpath-test-configmap-mw9b" satisfied condition "success or failure"
Dec  3 16:05:50.125: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-subpath-test-configmap-mw9b container test-container-subpath-configmap-mw9b: <nil>
STEP: delete the pod
Dec  3 16:05:50.409: INFO: Waiting for pod pod-subpath-test-configmap-mw9b to disappear
Dec  3 16:05:50.498: INFO: Pod pod-subpath-test-configmap-mw9b no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mw9b
Dec  3 16:05:50.498: INFO: Deleting pod "pod-subpath-test-configmap-mw9b" in namespace "subpath-4417"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:05:50.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4417" for this suite.
Dec  3 16:05:56.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:06:00.086: INFO: namespace subpath-4417 deletion completed in 9.408136104s
â€¢SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:06:00.086: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6425
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 16:06:02.523: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985962, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985962, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985962, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985962, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 16:06:05.712: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:06:05.801: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7170-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:06:07.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6425" for this suite.
Dec  3 16:06:13.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:06:16.766: INFO: namespace webhook-6425 deletion completed in 9.386037337s
STEP: Destroying namespace "webhook-6425-markers" for this suite.
Dec  3 16:06:23.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:06:26.162: INFO: namespace webhook-6425-markers deletion completed in 9.395524101s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:06:26.696: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 16:06:28.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985988, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985988, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985988, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985988, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:06:30.900: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985988, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985988, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985988, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710985988, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 16:06:33.995: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:06:34.085: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:06:35.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7696" for this suite.
Dec  3 16:06:42.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:06:45.368: INFO: namespace webhook-7696 deletion completed in 9.398078521s
STEP: Destroying namespace "webhook-7696-markers" for this suite.
Dec  3 16:06:51.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:06:54.769: INFO: namespace webhook-7696-markers deletion completed in 9.401014025s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:06:55.129: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1524
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-1ec3fc4c-a3c4-4655-8252-521f126c59dc in namespace container-probe-1524
Dec  3 16:06:58.039: INFO: Started pod busybox-1ec3fc4c-a3c4-4655-8252-521f126c59dc in namespace container-probe-1524
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 16:06:58.129: INFO: Initial restart count of pod busybox-1ec3fc4c-a3c4-4655-8252-521f126c59dc is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:10:58.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1524" for this suite.
Dec  3 16:11:04.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:11:08.093: INFO: namespace container-probe-1524 deletion completed in 9.431518121s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:11:08.094: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8128
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8128.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8128.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8128.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8128.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8128.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8128.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 16:11:11.933: INFO: DNS probes using dns-8128/dns-test-9ad0f2a7-c0b7-423f-9551-8e167bccbc5a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:11:12.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8128" for this suite.
Dec  3 16:11:18.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:11:21.515: INFO: namespace dns-8128 deletion completed in 9.398414194s
â€¢SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:11:21.515: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4174
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 16:11:23.411: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986283, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986283, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986283, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986283, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 16:11:26.596: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:11:38.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4174" for this suite.
Dec  3 16:11:44.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:11:48.089: INFO: namespace webhook-4174 deletion completed in 9.398494781s
STEP: Destroying namespace "webhook-4174-markers" for this suite.
Dec  3 16:11:54.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:11:57.493: INFO: namespace webhook-4174-markers deletion completed in 9.404421977s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:11:57.855: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4965
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  3 16:12:00.945: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:12:01.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4965" for this suite.
Dec  3 16:12:07.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:12:10.623: INFO: namespace container-runtime-4965 deletion completed in 9.405957867s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:12:10.624: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8510
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec  3 16:12:11.355: INFO: Waiting up to 5m0s for pod "downward-api-538978ba-0506-4d96-bd70-edfa2471eeba" in namespace "downward-api-8510" to be "success or failure"
Dec  3 16:12:11.445: INFO: Pod "downward-api-538978ba-0506-4d96-bd70-edfa2471eeba": Phase="Pending", Reason="", readiness=false. Elapsed: 89.373245ms
Dec  3 16:12:13.535: INFO: Pod "downward-api-538978ba-0506-4d96-bd70-edfa2471eeba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179322348s
STEP: Saw pod success
Dec  3 16:12:13.535: INFO: Pod "downward-api-538978ba-0506-4d96-bd70-edfa2471eeba" satisfied condition "success or failure"
Dec  3 16:12:13.624: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downward-api-538978ba-0506-4d96-bd70-edfa2471eeba container dapi-container: <nil>
STEP: delete the pod
Dec  3 16:12:13.909: INFO: Waiting for pod downward-api-538978ba-0506-4d96-bd70-edfa2471eeba to disappear
Dec  3 16:12:13.998: INFO: Pod downward-api-538978ba-0506-4d96-bd70-edfa2471eeba no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:12:13.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8510" for this suite.
Dec  3 16:12:20.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:12:23.700: INFO: namespace downward-api-8510 deletion completed in 9.611360987s
â€¢
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:12:23.700: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec  3 16:12:24.338: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-4356'
Dec  3 16:12:57.988: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 16:12:57.988: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: rolling-update to same image controller
Dec  3 16:12:58.169: INFO: scanned /root for discovery docs: <nil>
Dec  3 16:12:58.169: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-4356'
Dec  3 16:13:11.859: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  3 16:13:11.859: INFO: stdout: "Created e2e-test-httpd-rc-87520af816c9d5f3780f5970ef0dc372\nScaling up e2e-test-httpd-rc-87520af816c9d5f3780f5970ef0dc372 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-87520af816c9d5f3780f5970ef0dc372 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-87520af816c9d5f3780f5970ef0dc372 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Dec  3 16:13:11.859: INFO: stdout: "Created e2e-test-httpd-rc-87520af816c9d5f3780f5970ef0dc372\nScaling up e2e-test-httpd-rc-87520af816c9d5f3780f5970ef0dc372 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-87520af816c9d5f3780f5970ef0dc372 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-87520af816c9d5f3780f5970ef0dc372 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Dec  3 16:13:11.859: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-4356'
Dec  3 16:13:12.294: INFO: stderr: ""
Dec  3 16:13:12.294: INFO: stdout: "e2e-test-httpd-rc-87520af816c9d5f3780f5970ef0dc372-9rwz6 e2e-test-httpd-rc-tgvbv "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
Dec  3 16:13:17.294: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-4356'
Dec  3 16:13:17.729: INFO: stderr: ""
Dec  3 16:13:17.729: INFO: stdout: "e2e-test-httpd-rc-87520af816c9d5f3780f5970ef0dc372-9rwz6 "
Dec  3 16:13:17.729: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-httpd-rc-87520af816c9d5f3780f5970ef0dc372-9rwz6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4356'
Dec  3 16:13:18.178: INFO: stderr: ""
Dec  3 16:13:18.178: INFO: stdout: "true"
Dec  3 16:13:18.178: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-httpd-rc-87520af816c9d5f3780f5970ef0dc372-9rwz6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4356'
Dec  3 16:13:18.632: INFO: stderr: ""
Dec  3 16:13:18.632: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Dec  3 16:13:18.633: INFO: e2e-test-httpd-rc-87520af816c9d5f3780f5970ef0dc372-9rwz6 is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Dec  3 16:13:18.633: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-httpd-rc --namespace=kubectl-4356'
Dec  3 16:13:19.156: INFO: stderr: ""
Dec  3 16:13:19.156: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:13:19.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4356" for this suite.
Dec  3 16:13:25.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:13:28.646: INFO: namespace kubectl-4356 deletion completed in 9.400350789s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:13:28.647: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6191
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 16:13:30.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986410, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986410, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986410, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986410, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 16:13:33.860: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:13:33.950: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-764-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:13:35.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6191" for this suite.
Dec  3 16:13:41.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:13:44.663: INFO: namespace webhook-6191 deletion completed in 9.395712932s
STEP: Destroying namespace "webhook-6191-markers" for this suite.
Dec  3 16:13:50.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:13:54.059: INFO: namespace webhook-6191-markers deletion completed in 9.395955053s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢S
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:13:54.418: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6716
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:13:55.057: INFO: Creating ReplicaSet my-hostname-basic-ab10d726-d7eb-4e4c-9b2a-26a1e6e39cf0
Dec  3 16:13:55.237: INFO: Pod name my-hostname-basic-ab10d726-d7eb-4e4c-9b2a-26a1e6e39cf0: Found 1 pods out of 1
Dec  3 16:13:55.237: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-ab10d726-d7eb-4e4c-9b2a-26a1e6e39cf0" is running
Dec  3 16:13:57.417: INFO: Pod "my-hostname-basic-ab10d726-d7eb-4e4c-9b2a-26a1e6e39cf0-kcmqs" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 16:13:55 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 16:13:55 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ab10d726-d7eb-4e4c-9b2a-26a1e6e39cf0]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 16:13:55 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-ab10d726-d7eb-4e4c-9b2a-26a1e6e39cf0]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 16:13:55 +0000 UTC Reason: Message:}])
Dec  3 16:13:57.417: INFO: Trying to dial the pod
Dec  3 16:14:02.814: INFO: Controller my-hostname-basic-ab10d726-d7eb-4e4c-9b2a-26a1e6e39cf0: Got expected result from replica 1 [my-hostname-basic-ab10d726-d7eb-4e4c-9b2a-26a1e6e39cf0-kcmqs]: "my-hostname-basic-ab10d726-d7eb-4e4c-9b2a-26a1e6e39cf0-kcmqs", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:14:02.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6716" for this suite.
Dec  3 16:14:09.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:14:12.318: INFO: namespace replicaset-6716 deletion completed in 9.413723106s
â€¢S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:14:12.319: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5398
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec  3 16:14:15.413: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:14:15.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5398" for this suite.
Dec  3 16:14:21.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:14:25.098: INFO: namespace container-runtime-5398 deletion completed in 9.411342132s
â€¢SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:14:25.099: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3298
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  3 16:14:25.830: INFO: Waiting up to 5m0s for pod "pod-59f3fb8a-2f4b-4595-90a9-6401bc482f29" in namespace "emptydir-3298" to be "success or failure"
Dec  3 16:14:25.922: INFO: Pod "pod-59f3fb8a-2f4b-4595-90a9-6401bc482f29": Phase="Pending", Reason="", readiness=false. Elapsed: 91.917137ms
Dec  3 16:14:28.012: INFO: Pod "pod-59f3fb8a-2f4b-4595-90a9-6401bc482f29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.182266183s
STEP: Saw pod success
Dec  3 16:14:28.012: INFO: Pod "pod-59f3fb8a-2f4b-4595-90a9-6401bc482f29" satisfied condition "success or failure"
Dec  3 16:14:28.102: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-59f3fb8a-2f4b-4595-90a9-6401bc482f29 container test-container: <nil>
STEP: delete the pod
Dec  3 16:14:28.424: INFO: Waiting for pod pod-59f3fb8a-2f4b-4595-90a9-6401bc482f29 to disappear
Dec  3 16:14:28.513: INFO: Pod pod-59f3fb8a-2f4b-4595-90a9-6401bc482f29 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:14:28.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3298" for this suite.
Dec  3 16:14:34.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:14:38.003: INFO: namespace emptydir-3298 deletion completed in 9.399720887s
â€¢
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:14:38.004: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-9438/configmap-test-e25c1430-d3e2-4d2e-927b-7c46d21f2f1f
STEP: Creating a pod to test consume configMaps
Dec  3 16:14:38.823: INFO: Waiting up to 5m0s for pod "pod-configmaps-690fffd6-2594-40b0-93d3-b4a614597b79" in namespace "configmap-9438" to be "success or failure"
Dec  3 16:14:38.913: INFO: Pod "pod-configmaps-690fffd6-2594-40b0-93d3-b4a614597b79": Phase="Pending", Reason="", readiness=false. Elapsed: 90.128081ms
Dec  3 16:14:41.003: INFO: Pod "pod-configmaps-690fffd6-2594-40b0-93d3-b4a614597b79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179912151s
STEP: Saw pod success
Dec  3 16:14:41.003: INFO: Pod "pod-configmaps-690fffd6-2594-40b0-93d3-b4a614597b79" satisfied condition "success or failure"
Dec  3 16:14:41.093: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-configmaps-690fffd6-2594-40b0-93d3-b4a614597b79 container env-test: <nil>
STEP: delete the pod
Dec  3 16:14:41.281: INFO: Waiting for pod pod-configmaps-690fffd6-2594-40b0-93d3-b4a614597b79 to disappear
Dec  3 16:14:41.370: INFO: Pod pod-configmaps-690fffd6-2594-40b0-93d3-b4a614597b79 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:14:41.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9438" for this suite.
Dec  3 16:14:47.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:14:50.861: INFO: namespace configmap-9438 deletion completed in 9.398636028s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:14:50.862: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2166
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-967eefca-0b39-4382-a389-383ca8e6590d
STEP: Creating a pod to test consume configMaps
Dec  3 16:14:51.683: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5b5938f8-7146-4eb4-86ff-53898627a1ca" in namespace "projected-2166" to be "success or failure"
Dec  3 16:14:51.774: INFO: Pod "pod-projected-configmaps-5b5938f8-7146-4eb4-86ff-53898627a1ca": Phase="Pending", Reason="", readiness=false. Elapsed: 89.859164ms
Dec  3 16:14:53.864: INFO: Pod "pod-projected-configmaps-5b5938f8-7146-4eb4-86ff-53898627a1ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179680093s
STEP: Saw pod success
Dec  3 16:14:53.864: INFO: Pod "pod-projected-configmaps-5b5938f8-7146-4eb4-86ff-53898627a1ca" satisfied condition "success or failure"
Dec  3 16:14:53.954: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-projected-configmaps-5b5938f8-7146-4eb4-86ff-53898627a1ca container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 16:14:54.144: INFO: Waiting for pod pod-projected-configmaps-5b5938f8-7146-4eb4-86ff-53898627a1ca to disappear
Dec  3 16:14:54.233: INFO: Pod pod-projected-configmaps-5b5938f8-7146-4eb4-86ff-53898627a1ca no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:14:54.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2166" for this suite.
Dec  3 16:15:00.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:15:03.726: INFO: namespace projected-2166 deletion completed in 9.402747309s
â€¢SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:15:03.726: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8312
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:15:04.819: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec  3 16:15:05.089: INFO: Number of nodes with available pods: 0
Dec  3 16:15:05.089: INFO: Node ip-10-250-10-98.ec2.internal is running more than one daemon pod
Dec  3 16:15:06.269: INFO: Number of nodes with available pods: 1
Dec  3 16:15:06.269: INFO: Node ip-10-250-27-214.ec2.internal is running more than one daemon pod
Dec  3 16:15:07.269: INFO: Number of nodes with available pods: 2
Dec  3 16:15:07.269: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec  3 16:15:07.900: INFO: Wrong image for pod: daemon-set-2hm75. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:07.900: INFO: Wrong image for pod: daemon-set-xmvzm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:09.080: INFO: Wrong image for pod: daemon-set-2hm75. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:09.080: INFO: Pod daemon-set-2hm75 is not available
Dec  3 16:15:09.080: INFO: Wrong image for pod: daemon-set-xmvzm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:10.080: INFO: Wrong image for pod: daemon-set-2hm75. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:10.080: INFO: Pod daemon-set-2hm75 is not available
Dec  3 16:15:10.080: INFO: Wrong image for pod: daemon-set-xmvzm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:11.080: INFO: Wrong image for pod: daemon-set-2hm75. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:11.080: INFO: Pod daemon-set-2hm75 is not available
Dec  3 16:15:11.080: INFO: Wrong image for pod: daemon-set-xmvzm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:12.080: INFO: Wrong image for pod: daemon-set-2hm75. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:12.080: INFO: Pod daemon-set-2hm75 is not available
Dec  3 16:15:12.080: INFO: Wrong image for pod: daemon-set-xmvzm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:13.079: INFO: Pod daemon-set-27xtv is not available
Dec  3 16:15:13.080: INFO: Wrong image for pod: daemon-set-xmvzm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:14.080: INFO: Wrong image for pod: daemon-set-xmvzm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:15.080: INFO: Wrong image for pod: daemon-set-xmvzm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:15.080: INFO: Pod daemon-set-xmvzm is not available
Dec  3 16:15:16.080: INFO: Wrong image for pod: daemon-set-xmvzm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:16.080: INFO: Pod daemon-set-xmvzm is not available
Dec  3 16:15:17.080: INFO: Wrong image for pod: daemon-set-xmvzm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:17.080: INFO: Pod daemon-set-xmvzm is not available
Dec  3 16:15:18.083: INFO: Wrong image for pod: daemon-set-xmvzm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:18.083: INFO: Pod daemon-set-xmvzm is not available
Dec  3 16:15:19.081: INFO: Wrong image for pod: daemon-set-xmvzm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:19.081: INFO: Pod daemon-set-xmvzm is not available
Dec  3 16:15:20.080: INFO: Wrong image for pod: daemon-set-xmvzm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:20.080: INFO: Pod daemon-set-xmvzm is not available
Dec  3 16:15:21.080: INFO: Wrong image for pod: daemon-set-xmvzm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:21.080: INFO: Pod daemon-set-xmvzm is not available
Dec  3 16:15:22.080: INFO: Wrong image for pod: daemon-set-xmvzm. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec  3 16:15:22.080: INFO: Pod daemon-set-xmvzm is not available
Dec  3 16:15:23.080: INFO: Pod daemon-set-wqt5l is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Dec  3 16:15:23.350: INFO: Number of nodes with available pods: 1
Dec  3 16:15:23.350: INFO: Node ip-10-250-27-214.ec2.internal is running more than one daemon pod
Dec  3 16:15:24.530: INFO: Number of nodes with available pods: 2
Dec  3 16:15:24.530: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8312, will wait for the garbage collector to delete the pods
Dec  3 16:15:25.261: INFO: Deleting DaemonSet.extensions daemon-set took: 91.049879ms
Dec  3 16:15:25.361: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.293301ms
Dec  3 16:15:32.651: INFO: Number of nodes with available pods: 0
Dec  3 16:15:32.651: INFO: Number of running nodes: 0, number of available pods: 0
Dec  3 16:15:32.741: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8312/daemonsets","resourceVersion":"22503"},"items":null}

Dec  3 16:15:32.830: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8312/pods","resourceVersion":"22503"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:15:33.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8312" for this suite.
Dec  3 16:15:39.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:15:42.587: INFO: namespace daemonsets-8312 deletion completed in 9.397793406s
â€¢SSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:15:42.587: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1864
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Dec  3 16:15:43.227: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1864'
Dec  3 16:15:44.347: INFO: stderr: ""
Dec  3 16:15:44.347: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  3 16:15:45.439: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:15:45.439: INFO: Found 1 / 1
Dec  3 16:15:45.439: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec  3 16:15:45.529: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:15:45.529: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  3 16:15:45.529: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config patch pod redis-master-gqqgg --namespace=kubectl-1864 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec  3 16:15:46.123: INFO: stderr: ""
Dec  3 16:15:46.123: INFO: stdout: "pod/redis-master-gqqgg patched\n"
STEP: checking annotations
Dec  3 16:15:46.212: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:15:46.212: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:15:46.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1864" for this suite.
Dec  3 16:15:58.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:16:01.718: INFO: namespace kubectl-1864 deletion completed in 15.415692402s
â€¢SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:16:01.718: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8606
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-1ca29d89-0a25-4a9a-beaf-985e216a1570
STEP: Creating a pod to test consume configMaps
Dec  3 16:16:02.540: INFO: Waiting up to 5m0s for pod "pod-configmaps-98aba355-2095-47a5-8ed1-7942f2837402" in namespace "configmap-8606" to be "success or failure"
Dec  3 16:16:02.629: INFO: Pod "pod-configmaps-98aba355-2095-47a5-8ed1-7942f2837402": Phase="Pending", Reason="", readiness=false. Elapsed: 89.452395ms
Dec  3 16:16:04.719: INFO: Pod "pod-configmaps-98aba355-2095-47a5-8ed1-7942f2837402": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179440993s
STEP: Saw pod success
Dec  3 16:16:04.719: INFO: Pod "pod-configmaps-98aba355-2095-47a5-8ed1-7942f2837402" satisfied condition "success or failure"
Dec  3 16:16:04.809: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-configmaps-98aba355-2095-47a5-8ed1-7942f2837402 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 16:16:04.999: INFO: Waiting for pod pod-configmaps-98aba355-2095-47a5-8ed1-7942f2837402 to disappear
Dec  3 16:16:05.088: INFO: Pod pod-configmaps-98aba355-2095-47a5-8ed1-7942f2837402 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:16:05.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8606" for this suite.
Dec  3 16:16:11.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:16:14.590: INFO: namespace configmap-8606 deletion completed in 9.411897237s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:16:14.591: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7502
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:16:15.236: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7502'
Dec  3 16:16:16.389: INFO: stderr: ""
Dec  3 16:16:16.389: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec  3 16:16:16.389: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7502'
Dec  3 16:16:17.482: INFO: stderr: ""
Dec  3 16:16:17.482: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  3 16:16:18.573: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:16:18.573: INFO: Found 1 / 1
Dec  3 16:16:18.573: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  3 16:16:18.663: INFO: Selector matched 1 pods for map[app:redis]
Dec  3 16:16:18.663: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  3 16:16:18.663: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe pod redis-master-s56tt --namespace=kubectl-7502'
Dec  3 16:16:19.380: INFO: stderr: ""
Dec  3 16:16:19.380: INFO: stdout: "Name:         redis-master-s56tt\nNamespace:    kubectl-7502\nPriority:     0\nNode:         ip-10-250-10-98.ec2.internal/10.250.10.98\nStart Time:   Tue, 03 Dec 2019 16:16:16 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 100.64.1.25/32\n              kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           100.64.1.25\nIPs:\n  IP:           100.64.1.25\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://2aeb78a8ccb4b098bd10b6c7e98f4724f6d36a880aa5883ea1765928586446da\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 03 Dec 2019 16:16:17 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-5l57v (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-5l57v:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-5l57v\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                   Message\n  ----    ------     ----       ----                                   -------\n  Normal  Scheduled  <unknown>  default-scheduler                      Successfully assigned kubectl-7502/redis-master-s56tt to ip-10-250-10-98.ec2.internal\n  Normal  Pulled     3s         kubelet, ip-10-250-10-98.ec2.internal  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    2s         kubelet, ip-10-250-10-98.ec2.internal  Created container redis-master\n  Normal  Started    2s         kubelet, ip-10-250-10-98.ec2.internal  Started container redis-master\n"
Dec  3 16:16:19.380: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe rc redis-master --namespace=kubectl-7502'
Dec  3 16:16:20.179: INFO: stderr: ""
Dec  3 16:16:20.179: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-7502\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-s56tt\n"
Dec  3 16:16:20.179: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe service redis-master --namespace=kubectl-7502'
Dec  3 16:16:20.945: INFO: stderr: ""
Dec  3 16:16:20.945: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-7502\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.105.58.63\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.64.1.25:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec  3 16:16:21.036: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe node ip-10-250-10-98.ec2.internal'
Dec  3 16:16:21.959: INFO: stderr: ""
Dec  3 16:16:21.959: INFO: stdout: "Name:               ip-10-250-10-98.ec2.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.large\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1c\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-250-10-98.ec2.internal\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/role=node\n                    worker.garden.sapcloud.io/group=worker-1\n                    worker.gardener.cloud/pool=worker-1\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.10.98/19\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.64.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 03 Dec 2019 14:29:53 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  FrequentContainerdRestart     False   Tue, 03 Dec 2019 16:16:02 +0000   Tue, 03 Dec 2019 14:30:58 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  KernelDeadlock                False   Tue, 03 Dec 2019 16:16:02 +0000   Tue, 03 Dec 2019 14:30:58 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Tue, 03 Dec 2019 16:16:02 +0000   Tue, 03 Dec 2019 14:30:58 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  CorruptDockerOverlay2         False   Tue, 03 Dec 2019 16:16:02 +0000   Tue, 03 Dec 2019 14:30:59 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  FrequentUnregisterNetDevice   False   Tue, 03 Dec 2019 16:16:02 +0000   Tue, 03 Dec 2019 14:30:58 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  FrequentKubeletRestart        False   Tue, 03 Dec 2019 16:16:02 +0000   Tue, 03 Dec 2019 14:30:58 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Tue, 03 Dec 2019 16:16:02 +0000   Tue, 03 Dec 2019 14:30:58 +0000   NoFrequentDockerRestart         docker is functioning properly\n  NetworkUnavailable            False   Tue, 03 Dec 2019 14:30:12 +0000   Tue, 03 Dec 2019 14:30:12 +0000   CalicoIsUp                      Calico is running on this node\n  MemoryPressure                False   Tue, 03 Dec 2019 16:16:20 +0000   Tue, 03 Dec 2019 14:29:53 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Tue, 03 Dec 2019 16:16:20 +0000   Tue, 03 Dec 2019 14:29:53 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Tue, 03 Dec 2019 16:16:20 +0000   Tue, 03 Dec 2019 14:29:53 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Tue, 03 Dec 2019 16:16:20 +0000   Tue, 03 Dec 2019 14:30:03 +0000   KubeletReady                    kubelet is posting ready status\nAddresses:\n  InternalIP:   10.250.10.98\n  Hostname:     ip-10-250-10-98.ec2.internal\n  InternalDNS:  ip-10-250-10-98.ec2.internal\nCapacity:\n attachable-volumes-aws-ebs:  25\n cpu:                         2\n ephemeral-storage:           28056816Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      7865420Ki\n pods:                        110\nAllocatable:\n attachable-volumes-aws-ebs:  25\n cpu:                         1920m\n ephemeral-storage:           27293670584\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      6577738746\n pods:                        110\nSystem Info:\n Machine ID:                 ec2907348ca4a25ef3c316a6bcb96180\n System UUID:                ec290734-8ca4-a25e-f3c3-16a6bcb96180\n Boot ID:                    1bd000ef-d804-4ca0-9f1b-34a6b547bec9\n Kernel Version:             4.19.56-coreos-r1\n OS Image:                   Container Linux by CoreOS 2135.6.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.16.3\n Kube-Proxy Version:         v1.16.3\nPodCIDR:                     100.64.1.0/24\nPodCIDRs:                    100.64.1.0/24\nProviderID:                  aws:///us-east-1c/i-01b99062aef973f59\nNon-terminated Pods:         (5 in total)\n  Namespace                  Name                           CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                           ------------  ----------  ---------------  -------------  ---\n  kube-system                calico-node-sjfqj              100m (5%)     500m (26%)  100Mi (1%)       700Mi (11%)    106m\n  kube-system                kube-proxy-877v6               20m (1%)      0 (0%)      64Mi (1%)        0 (0%)         106m\n  kube-system                node-exporter-bt9wb            5m (0%)       25m (1%)    10Mi (0%)        100Mi (1%)     106m\n  kube-system                node-problem-detector-m2t47    20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)     106m\n  kubectl-7502               redis-master-s56tt             0 (0%)        0 (0%)      0 (0%)           0 (0%)         5s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         145m (7%)   725m (37%)\n  memory                      194Mi (3%)  900Mi (14%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\nEvents:                       <none>\n"
Dec  3 16:16:21.959: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe namespace kubectl-7502'
Dec  3 16:16:22.713: INFO: stderr: ""
Dec  3 16:16:22.713: INFO: stdout: "Name:         kubectl-7502\nLabels:       e2e-framework=kubectl\n              e2e-run=3efac81b-2654-45a6-ada6-042aa5d94e8d\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:16:22.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7502" for this suite.
Dec  3 16:16:35.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:16:38.203: INFO: namespace kubectl-7502 deletion completed in 15.398648261s
â€¢SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:16:38.203: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3068
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-2b97be15-0a0b-4c6e-8324-cb935839b0ec
STEP: Creating a pod to test consume secrets
Dec  3 16:16:39.027: INFO: Waiting up to 5m0s for pod "pod-secrets-e0cf3d43-0c49-4464-9c2f-a12e3d56e002" in namespace "secrets-3068" to be "success or failure"
Dec  3 16:16:39.116: INFO: Pod "pod-secrets-e0cf3d43-0c49-4464-9c2f-a12e3d56e002": Phase="Pending", Reason="", readiness=false. Elapsed: 89.5193ms
Dec  3 16:16:41.206: INFO: Pod "pod-secrets-e0cf3d43-0c49-4464-9c2f-a12e3d56e002": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179440541s
STEP: Saw pod success
Dec  3 16:16:41.206: INFO: Pod "pod-secrets-e0cf3d43-0c49-4464-9c2f-a12e3d56e002" satisfied condition "success or failure"
Dec  3 16:16:41.299: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-secrets-e0cf3d43-0c49-4464-9c2f-a12e3d56e002 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 16:16:41.490: INFO: Waiting for pod pod-secrets-e0cf3d43-0c49-4464-9c2f-a12e3d56e002 to disappear
Dec  3 16:16:41.580: INFO: Pod pod-secrets-e0cf3d43-0c49-4464-9c2f-a12e3d56e002 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:16:41.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3068" for this suite.
Dec  3 16:16:47.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:16:51.062: INFO: namespace secrets-3068 deletion completed in 9.391755431s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:16:51.062: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-7340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-7340
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7340
STEP: Deleting pre-stop pod
Dec  3 16:17:01.602: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:17:01.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7340" for this suite.
Dec  3 16:17:46.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:17:49.187: INFO: namespace prestop-7340 deletion completed in 47.40328988s
â€¢SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:17:49.187: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8396
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec  3 16:17:52.966: INFO: Successfully updated pod "labelsupdatebbccb205-644c-4165-a14d-1438145d9d5d"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:17:55.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8396" for this suite.
Dec  3 16:18:17.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:18:20.678: INFO: namespace projected-8396 deletion completed in 25.429778789s
â€¢SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:18:20.679: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8794
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec  3 16:18:21.317: INFO: PodSpec: initContainers in spec.initContainers
Dec  3 16:19:02.469: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-2e125dce-1a30-436e-bcaa-fb11dd5ade08", GenerateName:"", Namespace:"init-container-8794", SelfLink:"/api/v1/namespaces/init-container-8794/pods/pod-init-2e125dce-1a30-436e-bcaa-fb11dd5ade08", UID:"a3e8d44f-4caf-41aa-8237-8d4f2b239c2b", ResourceVersion:"23113", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63710986701, loc:(*time.Location)(0x84bfb00)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"317405352"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.64.1.30/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-wp6pp", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0040ff5c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wp6pp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wp6pp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wp6pp", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0032d0b78), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-250-10-98.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002a181e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0032d0c90)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0032d0cb0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0032d0cb8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0032d0cbc), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986701, loc:(*time.Location)(0x84bfb00)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986701, loc:(*time.Location)(0x84bfb00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986701, loc:(*time.Location)(0x84bfb00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986701, loc:(*time.Location)(0x84bfb00)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.10.98", PodIP:"100.64.1.30", PodIPs:[]v1.PodIP{v1.PodIP{IP:"100.64.1.30"}}, StartTime:(*v1.Time)(0xc002f219a0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001f49420)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001f49500)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://427d249dcf113ae04fde3dbfb5dffbb67c543454c125f6ddbe02597d82ba1e08", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f21c60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f219e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc0032d0d6f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:19:02.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8794" for this suite.
Dec  3 16:19:14.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:19:17.953: INFO: namespace init-container-8794 deletion completed in 15.394058718s
â€¢SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:19:17.954: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:19:18.592: INFO: Creating deployment "test-recreate-deployment"
Dec  3 16:19:18.682: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec  3 16:19:18.861: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec  3 16:19:18.954: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986758, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986758, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986758, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986758, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:19:21.044: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec  3 16:19:21.225: INFO: Updating deployment test-recreate-deployment
Dec  3 16:19:21.225: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec  3 16:19:21.403: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4869 /apis/apps/v1/namespaces/deployment-4869/deployments/test-recreate-deployment c6503acb-126f-4c3a-b0b8-87a0f6622c13 23196 2 2019-12-03 16:19:18 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003d7c1d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-12-03 16:19:21 +0000 UTC,LastTransitionTime:2019-12-03 16:19:21 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2019-12-03 16:19:21 +0000 UTC,LastTransitionTime:2019-12-03 16:19:18 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec  3 16:19:21.493: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-4869 /apis/apps/v1/namespaces/deployment-4869/replicasets/test-recreate-deployment-5f94c574ff e0f6d41f-f9e6-4b74-9b73-91e81b6276a5 23195 1 2019-12-03 16:19:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment c6503acb-126f-4c3a-b0b8-87a0f6622c13 0xc0050c5c77 0xc0050c5c78}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0050c5ce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  3 16:19:21.493: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec  3 16:19:21.494: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-4869 /apis/apps/v1/namespaces/deployment-4869/replicasets/test-recreate-deployment-68fc85c7bb ecf7b3ea-a6c6-4b72-b1fc-b6b0df5a0e8a 23188 2 2019-12-03 16:19:18 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment c6503acb-126f-4c3a-b0b8-87a0f6622c13 0xc0050c5d57 0xc0050c5d58}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0050c5db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  3 16:19:21.585: INFO: Pod "test-recreate-deployment-5f94c574ff-298l8" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-298l8 test-recreate-deployment-5f94c574ff- deployment-4869 /api/v1/namespaces/deployment-4869/pods/test-recreate-deployment-5f94c574ff-298l8 6df7e935-eddf-4fa7-aff7-1159a54e7886 23197 0 2019-12-03 16:19:21 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff e0f6d41f-f9e6-4b74-9b73-91e81b6276a5 0xc003bf8217 0xc003bf8218}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-npcvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-npcvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-npcvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:19:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:19:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:19:21 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:19:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:,StartTime:2019-12-03 16:19:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:19:21.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4869" for this suite.
Dec  3 16:19:27.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:19:31.072: INFO: namespace deployment-4869 deletion completed in 9.397484621s
â€¢
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:19:31.073: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-842
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:19:53.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-842" for this suite.
Dec  3 16:19:59.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:20:03.011: INFO: namespace container-runtime-842 deletion completed in 9.41585993s
â€¢SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:20:03.011: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8877
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-9skn
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 16:20:03.931: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9skn" in namespace "subpath-8877" to be "success or failure"
Dec  3 16:20:04.022: INFO: Pod "pod-subpath-test-configmap-9skn": Phase="Pending", Reason="", readiness=false. Elapsed: 90.339333ms
Dec  3 16:20:06.112: INFO: Pod "pod-subpath-test-configmap-9skn": Phase="Running", Reason="", readiness=true. Elapsed: 2.180454258s
Dec  3 16:20:08.202: INFO: Pod "pod-subpath-test-configmap-9skn": Phase="Running", Reason="", readiness=true. Elapsed: 4.270297124s
Dec  3 16:20:10.291: INFO: Pod "pod-subpath-test-configmap-9skn": Phase="Running", Reason="", readiness=true. Elapsed: 6.360068521s
Dec  3 16:20:12.381: INFO: Pod "pod-subpath-test-configmap-9skn": Phase="Running", Reason="", readiness=true. Elapsed: 8.449817358s
Dec  3 16:20:14.472: INFO: Pod "pod-subpath-test-configmap-9skn": Phase="Running", Reason="", readiness=true. Elapsed: 10.540755782s
Dec  3 16:20:16.563: INFO: Pod "pod-subpath-test-configmap-9skn": Phase="Running", Reason="", readiness=true. Elapsed: 12.632157439s
Dec  3 16:20:18.653: INFO: Pod "pod-subpath-test-configmap-9skn": Phase="Running", Reason="", readiness=true. Elapsed: 14.722028061s
Dec  3 16:20:20.747: INFO: Pod "pod-subpath-test-configmap-9skn": Phase="Running", Reason="", readiness=true. Elapsed: 16.815807661s
Dec  3 16:20:22.838: INFO: Pod "pod-subpath-test-configmap-9skn": Phase="Running", Reason="", readiness=true. Elapsed: 18.90709291s
Dec  3 16:20:24.929: INFO: Pod "pod-subpath-test-configmap-9skn": Phase="Running", Reason="", readiness=true. Elapsed: 20.997476838s
Dec  3 16:20:27.019: INFO: Pod "pod-subpath-test-configmap-9skn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.087590285s
STEP: Saw pod success
Dec  3 16:20:27.019: INFO: Pod "pod-subpath-test-configmap-9skn" satisfied condition "success or failure"
Dec  3 16:20:27.109: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-subpath-test-configmap-9skn container test-container-subpath-configmap-9skn: <nil>
STEP: delete the pod
Dec  3 16:20:27.398: INFO: Waiting for pod pod-subpath-test-configmap-9skn to disappear
Dec  3 16:20:27.487: INFO: Pod pod-subpath-test-configmap-9skn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9skn
Dec  3 16:20:27.488: INFO: Deleting pod "pod-subpath-test-configmap-9skn" in namespace "subpath-8877"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:20:27.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8877" for this suite.
Dec  3 16:20:33.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:20:37.069: INFO: namespace subpath-8877 deletion completed in 9.401822063s
â€¢S
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:20:37.070: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7868
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec  3 16:20:48.992: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1203 16:20:48.992603    5076 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:20:48.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7868" for this suite.
Dec  3 16:20:55.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:20:58.498: INFO: namespace gc-7868 deletion completed in 9.404422282s
â€¢SSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:20:58.499: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5475
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:20:59.137: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec  3 16:20:59.317: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  3 16:21:01.499: INFO: Creating deployment "test-rolling-update-deployment"
Dec  3 16:21:01.590: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec  3 16:21:01.805: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec  3 16:21:01.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986861, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986861, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986861, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710986861, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:21:03.989: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec  3 16:21:04.259: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5475 /apis/apps/v1/namespaces/deployment-5475/deployments/test-rolling-update-deployment c8ae36ad-f6e5-4286-a453-c4598bbe0fdd 23637 1 2019-12-03 16:21:01 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0074a26c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-03 16:21:01 +0000 UTC,LastTransitionTime:2019-12-03 16:21:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2019-12-03 16:21:02 +0000 UTC,LastTransitionTime:2019-12-03 16:21:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec  3 16:21:04.349: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-5475 /apis/apps/v1/namespaces/deployment-5475/replicasets/test-rolling-update-deployment-55d946486 58fe31f1-6322-4059-a145-e6a3c2a97b78 23630 1 2019-12-03 16:21:01 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment c8ae36ad-f6e5-4286-a453-c4598bbe0fdd 0xc0074a2b90 0xc0074a2b91}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0074a2bf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  3 16:21:04.349: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec  3 16:21:04.349: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5475 /apis/apps/v1/namespaces/deployment-5475/replicasets/test-rolling-update-controller f0dc9f13-0e83-4823-9cb6-d634b596274a 23636 2 2019-12-03 16:20:59 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment c8ae36ad-f6e5-4286-a453-c4598bbe0fdd 0xc0074a2ac7 0xc0074a2ac8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0074a2b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  3 16:21:04.440: INFO: Pod "test-rolling-update-deployment-55d946486-k2zbc" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-k2zbc test-rolling-update-deployment-55d946486- deployment-5475 /api/v1/namespaces/deployment-5475/pods/test-rolling-update-deployment-55d946486-k2zbc ca703466-05a7-4dab-ac47-83cd1d3445a9 23629 0 2019-12-03 16:21:01 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:100.64.1.46/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 58fe31f1-6322-4059-a145-e6a3c2a97b78 0xc0074a3060 0xc0074a3061}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fxkpw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fxkpw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fxkpw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:21:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:21:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:21:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:21:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:100.64.1.46,StartTime:2019-12-03 16:21:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-03 16:21:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://26011961b76b4c036855a0a4bd1396eb7a3e5fa02d1dabb546f05cb7f6abbad0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.46,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:21:04.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5475" for this suite.
Dec  3 16:21:10.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:21:13.922: INFO: namespace deployment-5475 deletion completed in 9.39236819s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:21:13.923: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8002
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:21:14.653: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b42a036-d06f-4eec-a523-f725d534f28f" in namespace "downward-api-8002" to be "success or failure"
Dec  3 16:21:14.743: INFO: Pod "downwardapi-volume-7b42a036-d06f-4eec-a523-f725d534f28f": Phase="Pending", Reason="", readiness=false. Elapsed: 89.876393ms
Dec  3 16:21:16.833: INFO: Pod "downwardapi-volume-7b42a036-d06f-4eec-a523-f725d534f28f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179743567s
STEP: Saw pod success
Dec  3 16:21:16.833: INFO: Pod "downwardapi-volume-7b42a036-d06f-4eec-a523-f725d534f28f" satisfied condition "success or failure"
Dec  3 16:21:16.923: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-7b42a036-d06f-4eec-a523-f725d534f28f container client-container: <nil>
STEP: delete the pod
Dec  3 16:21:17.112: INFO: Waiting for pod downwardapi-volume-7b42a036-d06f-4eec-a523-f725d534f28f to disappear
Dec  3 16:21:17.201: INFO: Pod downwardapi-volume-7b42a036-d06f-4eec-a523-f725d534f28f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:21:17.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8002" for this suite.
Dec  3 16:21:23.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:21:26.699: INFO: namespace downward-api-8002 deletion completed in 9.407192852s
â€¢SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:21:26.699: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9324
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-c82b5478-e6b5-4a5d-b877-5a2a64a52e0f
STEP: Creating a pod to test consume secrets
Dec  3 16:21:27.520: INFO: Waiting up to 5m0s for pod "pod-secrets-7b1f9fb1-97d0-43ae-8a3f-8318d3850d19" in namespace "secrets-9324" to be "success or failure"
Dec  3 16:21:27.610: INFO: Pod "pod-secrets-7b1f9fb1-97d0-43ae-8a3f-8318d3850d19": Phase="Pending", Reason="", readiness=false. Elapsed: 89.529289ms
Dec  3 16:21:29.701: INFO: Pod "pod-secrets-7b1f9fb1-97d0-43ae-8a3f-8318d3850d19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.181265226s
STEP: Saw pod success
Dec  3 16:21:29.702: INFO: Pod "pod-secrets-7b1f9fb1-97d0-43ae-8a3f-8318d3850d19" satisfied condition "success or failure"
Dec  3 16:21:29.791: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-secrets-7b1f9fb1-97d0-43ae-8a3f-8318d3850d19 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 16:21:29.982: INFO: Waiting for pod pod-secrets-7b1f9fb1-97d0-43ae-8a3f-8318d3850d19 to disappear
Dec  3 16:21:30.071: INFO: Pod pod-secrets-7b1f9fb1-97d0-43ae-8a3f-8318d3850d19 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:21:30.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9324" for this suite.
Dec  3 16:21:36.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:21:39.557: INFO: namespace secrets-9324 deletion completed in 9.395531142s
â€¢SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:21:39.557: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5466
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5466
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-5466
Dec  3 16:21:40.466: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Dec  3 16:21:50.557: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec  3 16:21:51.006: INFO: Deleting all statefulset in ns statefulset-5466
Dec  3 16:21:51.098: INFO: Scaling statefulset ss to 0
Dec  3 16:22:11.458: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 16:22:11.548: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:22:11.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5466" for this suite.
Dec  3 16:22:18.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:22:21.307: INFO: namespace statefulset-5466 deletion completed in 9.398669398s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:22:21.308: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5684
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-2fc977e2-3f87-4e2e-b26b-b672b33810c8
STEP: Creating a pod to test consume secrets
Dec  3 16:22:22.128: INFO: Waiting up to 5m0s for pod "pod-secrets-7b0672a6-16f4-48ff-91ef-0e1c1183effa" in namespace "secrets-5684" to be "success or failure"
Dec  3 16:22:22.218: INFO: Pod "pod-secrets-7b0672a6-16f4-48ff-91ef-0e1c1183effa": Phase="Pending", Reason="", readiness=false. Elapsed: 89.833684ms
Dec  3 16:22:24.308: INFO: Pod "pod-secrets-7b0672a6-16f4-48ff-91ef-0e1c1183effa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179866279s
STEP: Saw pod success
Dec  3 16:22:24.308: INFO: Pod "pod-secrets-7b0672a6-16f4-48ff-91ef-0e1c1183effa" satisfied condition "success or failure"
Dec  3 16:22:24.398: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-secrets-7b0672a6-16f4-48ff-91ef-0e1c1183effa container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 16:22:24.588: INFO: Waiting for pod pod-secrets-7b0672a6-16f4-48ff-91ef-0e1c1183effa to disappear
Dec  3 16:22:24.677: INFO: Pod pod-secrets-7b0672a6-16f4-48ff-91ef-0e1c1183effa no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:22:24.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5684" for this suite.
Dec  3 16:22:31.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:22:34.165: INFO: namespace secrets-5684 deletion completed in 9.397024046s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:22:34.165: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4078
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:22:34.897: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7400bd8b-0308-483d-8ef0-c6a7f0d4bb04" in namespace "projected-4078" to be "success or failure"
Dec  3 16:22:34.987: INFO: Pod "downwardapi-volume-7400bd8b-0308-483d-8ef0-c6a7f0d4bb04": Phase="Pending", Reason="", readiness=false. Elapsed: 89.622015ms
Dec  3 16:22:37.077: INFO: Pod "downwardapi-volume-7400bd8b-0308-483d-8ef0-c6a7f0d4bb04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179628012s
STEP: Saw pod success
Dec  3 16:22:37.077: INFO: Pod "downwardapi-volume-7400bd8b-0308-483d-8ef0-c6a7f0d4bb04" satisfied condition "success or failure"
Dec  3 16:22:37.167: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-7400bd8b-0308-483d-8ef0-c6a7f0d4bb04 container client-container: <nil>
STEP: delete the pod
Dec  3 16:22:37.356: INFO: Waiting for pod downwardapi-volume-7400bd8b-0308-483d-8ef0-c6a7f0d4bb04 to disappear
Dec  3 16:22:37.446: INFO: Pod downwardapi-volume-7400bd8b-0308-483d-8ef0-c6a7f0d4bb04 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:22:37.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4078" for this suite.
Dec  3 16:22:43.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:22:46.939: INFO: namespace projected-4078 deletion completed in 9.402758883s
â€¢SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:22:46.939: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6624
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec  3 16:22:50.119: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec pod-sharedvolume-8924e40a-de5f-4c4c-ba4e-2494ba95ea67 -c busybox-main-container --namespace=emptydir-6624 -- cat /usr/share/volumeshare/shareddata.txt'
Dec  3 16:22:51.703: INFO: stderr: ""
Dec  3 16:22:51.703: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:22:51.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6624" for this suite.
Dec  3 16:22:58.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:23:01.196: INFO: namespace emptydir-6624 deletion completed in 9.402289699s
â€¢SSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:23:01.196: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-308
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-9d4fbe05-5e07-45e8-b546-6dffde95fe0f
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:23:01.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-308" for this suite.
Dec  3 16:23:08.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:23:11.411: INFO: namespace secrets-308 deletion completed in 9.395416352s
â€¢SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:23:11.411: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9238
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec  3 16:23:12.052: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  3 16:23:12.322: INFO: Waiting for terminating namespaces to be deleted...
Dec  3 16:23:12.411: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-10-98.ec2.internal before test
Dec  3 16:23:12.512: INFO: node-problem-detector-m2t47 from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.512: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 16:23:12.512: INFO: calico-node-sjfqj from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.512: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 16:23:12.512: INFO: kube-proxy-877v6 from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.512: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 16:23:12.512: INFO: node-exporter-bt9wb from kube-system started at 2019-12-03 14:29:53 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.512: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 16:23:12.512: INFO: 
Logging pods the kubelet thinks is on node ip-10-250-27-214.ec2.internal before test
Dec  3 16:23:12.712: INFO: calico-typha-horizontal-autoscaler-69df649c59-9tbvm from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.712: INFO: 	Container autoscaler ready: true, restart count 0
Dec  3 16:23:12.712: INFO: coredns-59c969ffb8-pbnhz from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.712: INFO: 	Container coredns ready: true, restart count 0
Dec  3 16:23:12.712: INFO: node-problem-detector-j6tww from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.712: INFO: 	Container node-problem-detector ready: true, restart count 0
Dec  3 16:23:12.712: INFO: coredns-59c969ffb8-6vlmd from kube-system started at 2019-12-03 14:30:07 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.712: INFO: 	Container coredns ready: true, restart count 0
Dec  3 16:23:12.712: INFO: blackbox-exporter-7bd7b55dfc-nqf6r from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.712: INFO: 	Container blackbox-exporter ready: true, restart count 0
Dec  3 16:23:12.712: INFO: node-exporter-f2j7c from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.712: INFO: 	Container node-exporter ready: true, restart count 0
Dec  3 16:23:12.712: INFO: calico-typha-deploy-9f6b455c4-xv9k8 from kube-system started at 2019-12-03 14:33:07 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.712: INFO: 	Container calico-typha ready: true, restart count 0
Dec  3 16:23:12.712: INFO: metrics-server-778877cf87-5dxgx from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.712: INFO: 	Container metrics-server ready: true, restart count 0
Dec  3 16:23:12.712: INFO: addons-nginx-ingress-controller-7c75bb76db-pr4bf from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.712: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  3 16:23:12.712: INFO: kube-proxy-pcff8 from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.712: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  3 16:23:12.712: INFO: calico-kube-controllers-79bcd784b6-w4fv5 from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.712: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec  3 16:23:12.712: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-95f65778d-d2nzf from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.712: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Dec  3 16:23:12.712: INFO: vpn-shoot-57c94d6f78-cmlmw from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.712: INFO: 	Container vpn-shoot ready: true, restart count 0
Dec  3 16:23:12.712: INFO: calico-node-drs2g from kube-system started at 2019-12-03 14:29:42 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.712: INFO: 	Container calico-node ready: true, restart count 0
Dec  3 16:23:12.712: INFO: calico-typha-vertical-autoscaler-847d859f8c-m9mnl from kube-system started at 2019-12-03 14:30:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.712: INFO: 	Container autoscaler ready: true, restart count 4
Dec  3 16:23:12.712: INFO: addons-kubernetes-dashboard-78954cc66b-fj8m4 from kube-system started at 2019-12-03 14:56:13 +0000 UTC (1 container statuses recorded)
Dec  3 16:23:12.712: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node ip-10-250-10-98.ec2.internal
STEP: verifying the node has the label node ip-10-250-27-214.ec2.internal
Dec  3 16:23:13.259: INFO: Pod addons-kubernetes-dashboard-78954cc66b-fj8m4 requesting resource cpu=50m on Node ip-10-250-27-214.ec2.internal
Dec  3 16:23:13.259: INFO: Pod addons-nginx-ingress-controller-7c75bb76db-pr4bf requesting resource cpu=100m on Node ip-10-250-27-214.ec2.internal
Dec  3 16:23:13.259: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-95f65778d-d2nzf requesting resource cpu=0m on Node ip-10-250-27-214.ec2.internal
Dec  3 16:23:13.259: INFO: Pod blackbox-exporter-7bd7b55dfc-nqf6r requesting resource cpu=5m on Node ip-10-250-27-214.ec2.internal
Dec  3 16:23:13.259: INFO: Pod calico-kube-controllers-79bcd784b6-w4fv5 requesting resource cpu=0m on Node ip-10-250-27-214.ec2.internal
Dec  3 16:23:13.259: INFO: Pod calico-node-drs2g requesting resource cpu=100m on Node ip-10-250-27-214.ec2.internal
Dec  3 16:23:13.259: INFO: Pod calico-node-sjfqj requesting resource cpu=100m on Node ip-10-250-10-98.ec2.internal
Dec  3 16:23:13.259: INFO: Pod calico-typha-deploy-9f6b455c4-xv9k8 requesting resource cpu=0m on Node ip-10-250-27-214.ec2.internal
Dec  3 16:23:13.259: INFO: Pod calico-typha-horizontal-autoscaler-69df649c59-9tbvm requesting resource cpu=10m on Node ip-10-250-27-214.ec2.internal
Dec  3 16:23:13.259: INFO: Pod calico-typha-vertical-autoscaler-847d859f8c-m9mnl requesting resource cpu=0m on Node ip-10-250-27-214.ec2.internal
Dec  3 16:23:13.259: INFO: Pod coredns-59c969ffb8-6vlmd requesting resource cpu=50m on Node ip-10-250-27-214.ec2.internal
Dec  3 16:23:13.259: INFO: Pod coredns-59c969ffb8-pbnhz requesting resource cpu=50m on Node ip-10-250-27-214.ec2.internal
Dec  3 16:23:13.259: INFO: Pod kube-proxy-877v6 requesting resource cpu=20m on Node ip-10-250-10-98.ec2.internal
Dec  3 16:23:13.259: INFO: Pod kube-proxy-pcff8 requesting resource cpu=20m on Node ip-10-250-27-214.ec2.internal
Dec  3 16:23:13.259: INFO: Pod metrics-server-778877cf87-5dxgx requesting resource cpu=20m on Node ip-10-250-27-214.ec2.internal
Dec  3 16:23:13.259: INFO: Pod node-exporter-bt9wb requesting resource cpu=5m on Node ip-10-250-10-98.ec2.internal
Dec  3 16:23:13.259: INFO: Pod node-exporter-f2j7c requesting resource cpu=5m on Node ip-10-250-27-214.ec2.internal
Dec  3 16:23:13.259: INFO: Pod node-problem-detector-j6tww requesting resource cpu=20m on Node ip-10-250-27-214.ec2.internal
Dec  3 16:23:13.259: INFO: Pod node-problem-detector-m2t47 requesting resource cpu=20m on Node ip-10-250-10-98.ec2.internal
Dec  3 16:23:13.259: INFO: Pod vpn-shoot-57c94d6f78-cmlmw requesting resource cpu=100m on Node ip-10-250-27-214.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU.
Dec  3 16:23:13.259: INFO: Creating a pod which consumes cpu=1242m on Node ip-10-250-10-98.ec2.internal
Dec  3 16:23:13.350: INFO: Creating a pod which consumes cpu=973m on Node ip-10-250-27-214.ec2.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05a79a8f-48fc-4710-93f3-e342d7d24282.15dce908b6ccd93e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9238/filler-pod-05a79a8f-48fc-4710-93f3-e342d7d24282 to ip-10-250-10-98.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05a79a8f-48fc-4710-93f3-e342d7d24282.15dce908dc1f3b69], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05a79a8f-48fc-4710-93f3-e342d7d24282.15dce908df416ce5], Reason = [Created], Message = [Created container filler-pod-05a79a8f-48fc-4710-93f3-e342d7d24282]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05a79a8f-48fc-4710-93f3-e342d7d24282.15dce908e59eb08e], Reason = [Started], Message = [Started container filler-pod-05a79a8f-48fc-4710-93f3-e342d7d24282]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-eb08a766-0c84-4dac-834f-7c21346349da.15dce908bc310713], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9238/filler-pod-eb08a766-0c84-4dac-834f-7c21346349da to ip-10-250-27-214.ec2.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-eb08a766-0c84-4dac-834f-7c21346349da.15dce908e53298ed], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-eb08a766-0c84-4dac-834f-7c21346349da.15dce908e7e52b75], Reason = [Created], Message = [Created container filler-pod-eb08a766-0c84-4dac-834f-7c21346349da]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-eb08a766-0c84-4dac-834f-7c21346349da.15dce908ed655cb1], Reason = [Started], Message = [Started container filler-pod-eb08a766-0c84-4dac-834f-7c21346349da]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15dce909533c055a], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node ip-10-250-10-98.ec2.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-250-27-214.ec2.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:23:17.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9238" for this suite.
Dec  3 16:23:23.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:23:27.101: INFO: namespace sched-pred-9238 deletion completed in 9.398307563s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:23:27.102: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4740
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec  3 16:23:30.781: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-4740 pod-service-account-04b11386-7626-41a4-bff4-1192bed1c7b2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec  3 16:23:32.443: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-4740 pod-service-account-04b11386-7626-41a4-bff4-1192bed1c7b2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec  3 16:23:33.751: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-4740 pod-service-account-04b11386-7626-41a4-bff4-1192bed1c7b2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:23:35.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4740" for this suite.
Dec  3 16:23:41.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:23:44.631: INFO: namespace svcaccounts-4740 deletion completed in 9.4087866s
â€¢S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:23:44.632: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4783
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-cwgv
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 16:23:45.544: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-cwgv" in namespace "subpath-4783" to be "success or failure"
Dec  3 16:23:45.633: INFO: Pod "pod-subpath-test-downwardapi-cwgv": Phase="Pending", Reason="", readiness=false. Elapsed: 89.479493ms
Dec  3 16:23:47.723: INFO: Pod "pod-subpath-test-downwardapi-cwgv": Phase="Running", Reason="", readiness=true. Elapsed: 2.179312291s
Dec  3 16:23:49.813: INFO: Pod "pod-subpath-test-downwardapi-cwgv": Phase="Running", Reason="", readiness=true. Elapsed: 4.269113619s
Dec  3 16:23:51.903: INFO: Pod "pod-subpath-test-downwardapi-cwgv": Phase="Running", Reason="", readiness=true. Elapsed: 6.358925393s
Dec  3 16:23:53.992: INFO: Pod "pod-subpath-test-downwardapi-cwgv": Phase="Running", Reason="", readiness=true. Elapsed: 8.448757404s
Dec  3 16:23:56.083: INFO: Pod "pod-subpath-test-downwardapi-cwgv": Phase="Running", Reason="", readiness=true. Elapsed: 10.539098645s
Dec  3 16:23:58.173: INFO: Pod "pod-subpath-test-downwardapi-cwgv": Phase="Running", Reason="", readiness=true. Elapsed: 12.629128372s
Dec  3 16:24:00.263: INFO: Pod "pod-subpath-test-downwardapi-cwgv": Phase="Running", Reason="", readiness=true. Elapsed: 14.719167108s
Dec  3 16:24:02.356: INFO: Pod "pod-subpath-test-downwardapi-cwgv": Phase="Running", Reason="", readiness=true. Elapsed: 16.812305838s
Dec  3 16:24:04.449: INFO: Pod "pod-subpath-test-downwardapi-cwgv": Phase="Running", Reason="", readiness=true. Elapsed: 18.905041848s
Dec  3 16:24:06.539: INFO: Pod "pod-subpath-test-downwardapi-cwgv": Phase="Running", Reason="", readiness=true. Elapsed: 20.994906922s
Dec  3 16:24:08.628: INFO: Pod "pod-subpath-test-downwardapi-cwgv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.084591974s
STEP: Saw pod success
Dec  3 16:24:08.628: INFO: Pod "pod-subpath-test-downwardapi-cwgv" satisfied condition "success or failure"
Dec  3 16:24:08.718: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-subpath-test-downwardapi-cwgv container test-container-subpath-downwardapi-cwgv: <nil>
STEP: delete the pod
Dec  3 16:24:08.916: INFO: Waiting for pod pod-subpath-test-downwardapi-cwgv to disappear
Dec  3 16:24:09.006: INFO: Pod pod-subpath-test-downwardapi-cwgv no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-cwgv
Dec  3 16:24:09.006: INFO: Deleting pod "pod-subpath-test-downwardapi-cwgv" in namespace "subpath-4783"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:24:09.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4783" for this suite.
Dec  3 16:24:15.454: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:24:18.576: INFO: namespace subpath-4783 deletion completed in 9.39131832s
â€¢
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:24:18.577: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-721
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:24:19.250: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating first CR 
Dec  3 16:24:20.186: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-03T16:24:20Z generation:1 name:name1 resourceVersion:24311 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:3e0b3ac3-2adb-4f22-91b5-10badac9cb72] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec  3 16:24:30.279: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-03T16:24:30Z generation:1 name:name2 resourceVersion:24332 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1e52c86f-bfd1-4c9c-8524-4f5c786c644a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec  3 16:24:40.370: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-03T16:24:20Z generation:2 name:name1 resourceVersion:24356 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:3e0b3ac3-2adb-4f22-91b5-10badac9cb72] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec  3 16:24:50.461: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-03T16:24:30Z generation:2 name:name2 resourceVersion:24378 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1e52c86f-bfd1-4c9c-8524-4f5c786c644a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec  3 16:25:00.555: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-03T16:24:20Z generation:2 name:name1 resourceVersion:24401 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:3e0b3ac3-2adb-4f22-91b5-10badac9cb72] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec  3 16:25:10.646: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-03T16:24:30Z generation:2 name:name2 resourceVersion:24423 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:1e52c86f-bfd1-4c9c-8524-4f5c786c644a] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:25:20.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-721" for this suite.
Dec  3 16:25:27.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:25:30.312: INFO: namespace crd-watch-721 deletion completed in 9.395937457s
â€¢SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:25:30.313: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:25:31.142: INFO: (0) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 94.402902ms)
Dec  3 16:25:31.234: INFO: (1) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.224044ms)
Dec  3 16:25:31.326: INFO: (2) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.805678ms)
Dec  3 16:25:31.419: INFO: (3) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.386644ms)
Dec  3 16:25:31.510: INFO: (4) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.556104ms)
Dec  3 16:25:31.602: INFO: (5) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.418208ms)
Dec  3 16:25:31.694: INFO: (6) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 92.423618ms)
Dec  3 16:25:31.786: INFO: (7) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.955718ms)
Dec  3 16:25:31.878: INFO: (8) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.544783ms)
Dec  3 16:25:31.970: INFO: (9) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.555012ms)
Dec  3 16:25:32.061: INFO: (10) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.687178ms)
Dec  3 16:25:32.153: INFO: (11) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.341336ms)
Dec  3 16:25:32.245: INFO: (12) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.854978ms)
Dec  3 16:25:32.337: INFO: (13) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.772021ms)
Dec  3 16:25:32.430: INFO: (14) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 93.418947ms)
Dec  3 16:25:32.522: INFO: (15) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.803106ms)
Dec  3 16:25:32.614: INFO: (16) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.613843ms)
Dec  3 16:25:32.706: INFO: (17) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.684421ms)
Dec  3 16:25:32.798: INFO: (18) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.948304ms)
Dec  3 16:25:32.889: INFO: (19) /api/v1/nodes/ip-10-250-10-98.ec2.internal/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 91.514799ms)
[AfterEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:25:32.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-909" for this suite.
Dec  3 16:25:39.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:25:42.377: INFO: namespace proxy-909 deletion completed in 9.397539797s
â€¢SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:25:42.377: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-181
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-97e5e1d5-ac8a-4f40-8643-3b156e8b587d
STEP: Creating a pod to test consume secrets
Dec  3 16:25:43.196: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-734d998f-9189-45b0-95e4-024078cb917a" in namespace "projected-181" to be "success or failure"
Dec  3 16:25:43.286: INFO: Pod "pod-projected-secrets-734d998f-9189-45b0-95e4-024078cb917a": Phase="Pending", Reason="", readiness=false. Elapsed: 89.988475ms
Dec  3 16:25:45.377: INFO: Pod "pod-projected-secrets-734d998f-9189-45b0-95e4-024078cb917a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180574045s
STEP: Saw pod success
Dec  3 16:25:45.377: INFO: Pod "pod-projected-secrets-734d998f-9189-45b0-95e4-024078cb917a" satisfied condition "success or failure"
Dec  3 16:25:45.467: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-projected-secrets-734d998f-9189-45b0-95e4-024078cb917a container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  3 16:25:45.656: INFO: Waiting for pod pod-projected-secrets-734d998f-9189-45b0-95e4-024078cb917a to disappear
Dec  3 16:25:45.746: INFO: Pod pod-projected-secrets-734d998f-9189-45b0-95e4-024078cb917a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:25:45.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-181" for this suite.
Dec  3 16:25:52.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:25:55.232: INFO: namespace projected-181 deletion completed in 9.395198052s
â€¢SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:25:55.233: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3572
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Dec  3 16:25:55.871: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3572'
Dec  3 16:25:56.937: INFO: stderr: ""
Dec  3 16:25:56.937: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 16:25:56.937: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3572'
Dec  3 16:25:57.432: INFO: stderr: ""
Dec  3 16:25:57.432: INFO: stdout: "update-demo-nautilus-8kqhf update-demo-nautilus-qwrjh "
Dec  3 16:25:57.432: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8kqhf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3572'
Dec  3 16:25:57.870: INFO: stderr: ""
Dec  3 16:25:57.870: INFO: stdout: ""
Dec  3 16:25:57.870: INFO: update-demo-nautilus-8kqhf is created but not running
Dec  3 16:26:02.870: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3572'
Dec  3 16:26:03.333: INFO: stderr: ""
Dec  3 16:26:03.333: INFO: stdout: "update-demo-nautilus-8kqhf update-demo-nautilus-qwrjh "
Dec  3 16:26:03.333: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8kqhf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3572'
Dec  3 16:26:03.785: INFO: stderr: ""
Dec  3 16:26:03.785: INFO: stdout: "true"
Dec  3 16:26:03.785: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-8kqhf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3572'
Dec  3 16:26:04.308: INFO: stderr: ""
Dec  3 16:26:04.308: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:26:04.308: INFO: validating pod update-demo-nautilus-8kqhf
Dec  3 16:26:04.487: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:26:04.487: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:26:04.487: INFO: update-demo-nautilus-8kqhf is verified up and running
Dec  3 16:26:04.487: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-qwrjh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3572'
Dec  3 16:26:05.027: INFO: stderr: ""
Dec  3 16:26:05.027: INFO: stdout: "true"
Dec  3 16:26:05.027: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-qwrjh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3572'
Dec  3 16:26:05.520: INFO: stderr: ""
Dec  3 16:26:05.521: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:26:05.521: INFO: validating pod update-demo-nautilus-qwrjh
Dec  3 16:26:05.698: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:26:05.698: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:26:05.698: INFO: update-demo-nautilus-qwrjh is verified up and running
STEP: rolling-update to new replication controller
Dec  3 16:26:05.701: INFO: scanned /root for discovery docs: <nil>
Dec  3 16:26:05.701: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-3572'
Dec  3 16:26:21.048: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  3 16:26:21.048: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 16:26:21.049: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3572'
Dec  3 16:26:21.503: INFO: stderr: ""
Dec  3 16:26:21.503: INFO: stdout: "update-demo-kitten-8htrw update-demo-kitten-tzp87 "
Dec  3 16:26:21.503: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-8htrw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3572'
Dec  3 16:26:21.960: INFO: stderr: ""
Dec  3 16:26:21.960: INFO: stdout: "true"
Dec  3 16:26:21.960: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-8htrw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3572'
Dec  3 16:26:22.450: INFO: stderr: ""
Dec  3 16:26:22.450: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  3 16:26:22.450: INFO: validating pod update-demo-kitten-8htrw
Dec  3 16:26:22.630: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  3 16:26:22.630: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  3 16:26:22.630: INFO: update-demo-kitten-8htrw is verified up and running
Dec  3 16:26:22.630: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-tzp87 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3572'
Dec  3 16:26:23.102: INFO: stderr: ""
Dec  3 16:26:23.102: INFO: stdout: "true"
Dec  3 16:26:23.102: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-tzp87 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3572'
Dec  3 16:26:23.594: INFO: stderr: ""
Dec  3 16:26:23.594: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  3 16:26:23.594: INFO: validating pod update-demo-kitten-tzp87
Dec  3 16:26:23.774: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  3 16:26:23.774: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  3 16:26:23.774: INFO: update-demo-kitten-tzp87 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:26:23.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3572" for this suite.
Dec  3 16:26:36.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:26:39.277: INFO: namespace kubectl-3572 deletion completed in 15.412642812s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:26:39.278: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3820
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3820
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3820
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3820
Dec  3 16:26:40.278: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Dec  3 16:26:50.368: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec  3 16:26:50.459: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  3 16:26:51.786: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  3 16:26:51.786: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  3 16:26:51.786: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  3 16:26:51.876: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  3 16:27:01.967: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 16:27:01.967: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 16:27:02.328: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999004s
Dec  3 16:27:03.418: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.908304501s
Dec  3 16:27:04.508: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.817885277s
Dec  3 16:27:05.598: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.728013012s
Dec  3 16:27:06.688: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.637961573s
Dec  3 16:27:07.778: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.547501209s
Dec  3 16:27:08.869: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.457368942s
Dec  3 16:27:09.959: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.367094994s
Dec  3 16:27:11.049: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.277048779s
Dec  3 16:27:12.139: INFO: Verifying statefulset ss doesn't scale past 1 for another 186.615384ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3820
Dec  3 16:27:13.232: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:27:14.541: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  3 16:27:14.541: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  3 16:27:14.541: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  3 16:27:14.630: INFO: Found 1 stateful pods, waiting for 3
Dec  3 16:27:24.721: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 16:27:24.721: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  3 16:27:24.721: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec  3 16:27:24.900: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  3 16:27:26.241: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  3 16:27:26.242: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  3 16:27:26.242: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  3 16:27:26.242: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  3 16:27:27.601: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  3 16:27:27.601: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  3 16:27:27.601: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  3 16:27:27.601: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec  3 16:27:28.956: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec  3 16:27:28.956: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec  3 16:27:28.956: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec  3 16:27:28.956: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 16:27:29.046: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec  3 16:27:39.226: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 16:27:39.227: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 16:27:39.227: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  3 16:27:39.497: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999383s
Dec  3 16:27:40.588: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.909744392s
Dec  3 16:27:41.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.819051098s
Dec  3 16:27:42.772: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.728995173s
Dec  3 16:27:43.862: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.634709348s
Dec  3 16:27:44.953: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.544623375s
Dec  3 16:27:46.043: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.454499343s
Dec  3 16:27:47.133: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.364178337s
Dec  3 16:27:48.223: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.274140167s
Dec  3 16:27:49.313: INFO: Verifying statefulset ss doesn't scale past 3 for another 184.11526ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3820
Dec  3 16:27:50.404: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:27:51.721: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  3 16:27:51.721: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  3 16:27:51.721: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  3 16:27:51.721: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:27:53.148: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec  3 16:27:53.148: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec  3 16:27:53.148: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec  3 16:27:53.149: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:27:54.101: INFO: rc: 1
Dec  3 16:27:54.101: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc001ab26f0 exit status 1 <nil> <nil> true [0xc0024d0000 0xc0024d0018 0xc0024d0030] [0xc0024d0000 0xc0024d0018 0xc0024d0030] [0xc0024d0010 0xc0024d0028] [0x10efe30 0x10efe30] 0xc002d082a0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Dec  3 16:28:04.101: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:28:04.723: INFO: rc: 1
Dec  3 16:28:04.724: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003796390 exit status 1 <nil> <nil> true [0xc0075b2148 0xc0075b2160 0xc0075b2178] [0xc0075b2148 0xc0075b2160 0xc0075b2178] [0xc0075b2158 0xc0075b2170] [0x10efe30 0x10efe30] 0xc00358d200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:28:14.724: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:28:15.304: INFO: rc: 1
Dec  3 16:28:15.304: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0036945d0 exit status 1 <nil> <nil> true [0xc001d70018 0xc001d70108 0xc001d70150] [0xc001d70018 0xc001d70108 0xc001d70150] [0xc001d70098 0xc001d70140] [0x10efe30 0x10efe30] 0xc005abe420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:28:25.304: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:28:25.854: INFO: rc: 1
Dec  3 16:28:25.854: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032505a0 exit status 1 <nil> <nil> true [0xc001d3c060 0xc001d3c538 0xc001d3c710] [0xc001d3c060 0xc001d3c538 0xc001d3c710] [0xc001d3c460 0xc001d3c668] [0x10efe30 0x10efe30] 0xc00331e240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:28:35.855: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:28:36.429: INFO: rc: 1
Dec  3 16:28:36.429: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001a4c5a0 exit status 1 <nil> <nil> true [0xc00056bda0 0xc000010758 0xc000011420] [0xc00056bda0 0xc000010758 0xc000011420] [0xc000010708 0xc000011280] [0x10efe30 0x10efe30] 0xc0025aa1e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:28:46.429: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:28:47.003: INFO: rc: 1
Dec  3 16:28:47.003: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003694c00 exit status 1 <nil> <nil> true [0xc001d70188 0xc001d701d0 0xc001d70250] [0xc001d70188 0xc001d701d0 0xc001d70250] [0xc001d701b8 0xc001d70238] [0x10efe30 0x10efe30] 0xc005abe780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:28:57.004: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:28:57.585: INFO: rc: 1
Dec  3 16:28:57.586: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003250d50 exit status 1 <nil> <nil> true [0xc001d3c800 0xc001d3ca58 0xc001d3cbe8] [0xc001d3c800 0xc001d3ca58 0xc001d3cbe8] [0xc001d3c9f0 0xc001d3cb98] [0x10efe30 0x10efe30] 0xc00331e540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:29:07.586: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:29:08.234: INFO: rc: 1
Dec  3 16:29:08.235: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003251320 exit status 1 <nil> <nil> true [0xc001d3cca8 0xc001d3cf00 0xc001d3d328] [0xc001d3cca8 0xc001d3cf00 0xc001d3d328] [0xc001d3ce30 0xc001d3d2c0] [0x10efe30 0x10efe30] 0xc00331e840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:29:18.235: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:29:18.790: INFO: rc: 1
Dec  3 16:29:18.791: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003695260 exit status 1 <nil> <nil> true [0xc001d70268 0xc001d702f8 0xc001d70388] [0xc001d70268 0xc001d702f8 0xc001d70388] [0xc001d702d0 0xc001d70358] [0x10efe30 0x10efe30] 0xc005abeae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:29:28.791: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:29:29.344: INFO: rc: 1
Dec  3 16:29:29.344: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc005d285d0 exit status 1 <nil> <nil> true [0xc0018ae0c0 0xc0018ae0e8 0xc0018ae208] [0xc0018ae0c0 0xc0018ae0e8 0xc0018ae208] [0xc0018ae0e0 0xc0018ae1b0] [0x10efe30 0x10efe30] 0xc0048c83c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:29:39.344: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:29:39.909: INFO: rc: 1
Dec  3 16:29:39.909: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003695d10 exit status 1 <nil> <nil> true [0xc001d703b8 0xc001d703f0 0xc001d70450] [0xc001d703b8 0xc001d703f0 0xc001d70450] [0xc001d703e8 0xc001d70438] [0x10efe30 0x10efe30] 0xc005abeea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:29:49.910: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:29:50.452: INFO: rc: 1
Dec  3 16:29:50.452: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032519b0 exit status 1 <nil> <nil> true [0xc001d3d370 0xc001d3d498 0xc001d3d988] [0xc001d3d370 0xc001d3d498 0xc001d3d988] [0xc001d3d458 0xc001d3d8b8] [0x10efe30 0x10efe30] 0xc00261e000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:30:00.453: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:30:01.012: INFO: rc: 1
Dec  3 16:30:01.012: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001a4cbd0 exit status 1 <nil> <nil> true [0xc000011498 0xc000011650 0xc0000118e8] [0xc000011498 0xc000011650 0xc0000118e8] [0xc000011610 0xc000011780] [0x10efe30 0x10efe30] 0xc0025aa4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:30:11.013: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:30:11.580: INFO: rc: 1
Dec  3 16:30:11.580: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc005d285a0 exit status 1 <nil> <nil> true [0xc00056bda0 0xc0018ae0e0 0xc0018ae1b0] [0xc00056bda0 0xc0018ae0e0 0xc0018ae1b0] [0xc0018ae0d8 0xc0018ae128] [0x10efe30 0x10efe30] 0xc00331e180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:30:21.581: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:30:22.206: INFO: rc: 1
Dec  3 16:30:22.207: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003694600 exit status 1 <nil> <nil> true [0xc001d3c060 0xc001d3c538 0xc001d3c710] [0xc001d3c060 0xc001d3c538 0xc001d3c710] [0xc001d3c460 0xc001d3c668] [0x10efe30 0x10efe30] 0xc0048c83c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:30:32.207: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:30:32.783: INFO: rc: 1
Dec  3 16:30:32.783: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001a4c5d0 exit status 1 <nil> <nil> true [0xc001d70018 0xc001d70108 0xc001d70150] [0xc001d70018 0xc001d70108 0xc001d70150] [0xc001d70098 0xc001d70140] [0x10efe30 0x10efe30] 0xc00261fbc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:30:42.783: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:30:43.352: INFO: rc: 1
Dec  3 16:30:43.352: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003250630 exit status 1 <nil> <nil> true [0xc000010708 0xc000011280 0xc0000115c8] [0xc000010708 0xc000011280 0xc0000115c8] [0xc000010ee0 0xc000011498] [0x10efe30 0x10efe30] 0xc0025aa2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:30:53.352: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:30:53.923: INFO: rc: 1
Dec  3 16:30:53.924: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001a4cc00 exit status 1 <nil> <nil> true [0xc001d70188 0xc001d701d0 0xc001d70250] [0xc001d70188 0xc001d701d0 0xc001d70250] [0xc001d701b8 0xc001d70238] [0x10efe30 0x10efe30] 0xc005abe480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:31:03.924: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:31:04.482: INFO: rc: 1
Dec  3 16:31:04.482: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003694c30 exit status 1 <nil> <nil> true [0xc001d3c800 0xc001d3ca58 0xc001d3cbe8] [0xc001d3c800 0xc001d3ca58 0xc001d3cbe8] [0xc001d3c9f0 0xc001d3cb98] [0x10efe30 0x10efe30] 0xc0048c86c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:31:14.482: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:31:15.040: INFO: rc: 1
Dec  3 16:31:15.040: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003250e40 exit status 1 <nil> <nil> true [0xc000011610 0xc000011780 0xc000011a00] [0xc000011610 0xc000011780 0xc000011a00] [0xc000011698 0xc000011938] [0x10efe30 0x10efe30] 0xc0025aa5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:31:25.040: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:31:25.585: INFO: rc: 1
Dec  3 16:31:25.585: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0032514d0 exit status 1 <nil> <nil> true [0xc000011a30 0xc000011af8 0xc000011bf0] [0xc000011a30 0xc000011af8 0xc000011bf0] [0xc000011ae8 0xc000011b88] [0x10efe30 0x10efe30] 0xc0025aa8a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:31:35.586: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:31:36.155: INFO: rc: 1
Dec  3 16:31:36.155: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003251bc0 exit status 1 <nil> <nil> true [0xc000011d78 0xc000011de0 0xc000011e98] [0xc000011d78 0xc000011de0 0xc000011e98] [0xc000011db0 0xc000011e40] [0x10efe30 0x10efe30] 0xc0025aad20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:31:46.155: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:31:46.720: INFO: rc: 1
Dec  3 16:31:46.720: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001a0a1b0 exit status 1 <nil> <nil> true [0xc000011ed8 0xc000011f08 0xc000011fa0] [0xc000011ed8 0xc000011f08 0xc000011fa0] [0xc000011ee8 0xc000011f50] [0x10efe30 0x10efe30] 0xc0025ab2c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:31:56.720: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:31:57.293: INFO: rc: 1
Dec  3 16:31:57.293: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc005d28d50 exit status 1 <nil> <nil> true [0xc0018ae208 0xc0018ae3e8 0xc0018ae590] [0xc0018ae208 0xc0018ae3e8 0xc0018ae590] [0xc0018ae360 0xc0018ae560] [0x10efe30 0x10efe30] 0xc00331e480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:32:07.294: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:32:07.855: INFO: rc: 1
Dec  3 16:32:07.855: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc005d29380 exit status 1 <nil> <nil> true [0xc0018ae728 0xc0018ae7b0 0xc0018ae7f8] [0xc0018ae728 0xc0018ae7b0 0xc0018ae7f8] [0xc0018ae7a0 0xc0018ae7c8] [0x10efe30 0x10efe30] 0xc00331e780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:32:17.856: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:32:18.373: INFO: rc: 1
Dec  3 16:32:18.373: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001a4c5a0 exit status 1 <nil> <nil> true [0xc00056bda0 0xc001d70060 0xc001d70128] [0xc00056bda0 0xc001d70060 0xc001d70128] [0xc001d70018 0xc001d70108] [0x10efe30 0x10efe30] 0xc00261fbc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:32:28.374: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:32:28.917: INFO: rc: 1
Dec  3 16:32:28.917: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001a4cbd0 exit status 1 <nil> <nil> true [0xc001d70140 0xc001d70198 0xc001d701e8] [0xc001d70140 0xc001d70198 0xc001d701e8] [0xc001d70188 0xc001d701d0] [0x10efe30 0x10efe30] 0xc005abe3c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:32:38.917: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:32:39.435: INFO: rc: 1
Dec  3 16:32:39.435: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001a4d290 exit status 1 <nil> <nil> true [0xc001d70238 0xc001d70288 0xc001d70320] [0xc001d70238 0xc001d70288 0xc001d70320] [0xc001d70268 0xc001d702f8] [0x10efe30 0x10efe30] 0xc005abe720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:32:49.436: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:32:49.948: INFO: rc: 1
Dec  3 16:32:49.948: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001a0a600 exit status 1 <nil> <nil> true [0xc000010708 0xc000011280 0xc0000115c8] [0xc000010708 0xc000011280 0xc0000115c8] [0xc000010ee0 0xc000011498] [0x10efe30 0x10efe30] 0xc0025aa2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Dec  3 16:32:59.949: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-3820 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec  3 16:33:00.794: INFO: rc: 1
Dec  3 16:33:00.794: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Dec  3 16:33:00.794: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec  3 16:33:01.065: INFO: Deleting all statefulset in ns statefulset-3820
Dec  3 16:33:01.154: INFO: Scaling statefulset ss to 0
Dec  3 16:33:01.423: INFO: Waiting for statefulset status.replicas updated to 0
Dec  3 16:33:01.514: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:33:01.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3820" for this suite.
Dec  3 16:33:08.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:33:11.281: INFO: namespace statefulset-3820 deletion completed in 9.401413686s

â€¢ [SLOW TEST:392.003 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:33:11.281: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6680
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec  3 16:33:11.921: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:33:16.212: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:33:34.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6680" for this suite.
Dec  3 16:33:40.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:33:43.628: INFO: namespace crd-publish-openapi-6680 deletion completed in 9.394323704s
â€¢SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:33:43.628: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6845
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 16:33:45.615: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987625, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987625, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987625, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710987625, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 16:33:48.800: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:33:48.890: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1663-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:33:49.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6845" for this suite.
Dec  3 16:33:56.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:33:59.403: INFO: namespace webhook-6845 deletion completed in 9.395709484s
STEP: Destroying namespace "webhook-6845-markers" for this suite.
Dec  3 16:34:05.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:34:08.801: INFO: namespace webhook-6845-markers deletion completed in 9.397627247s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:34:09.160: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9430
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Dec  3 16:34:09.798: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:34:37.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9430" for this suite.
Dec  3 16:34:43.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:34:47.067: INFO: namespace crd-publish-openapi-9430 deletion completed in 9.402929836s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:34:47.068: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2370
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:35:01.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2370" for this suite.
Dec  3 16:35:08.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:35:11.189: INFO: namespace resourcequota-2370 deletion completed in 9.394796404s
â€¢
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:35:11.190: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5693
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-7e22a41c-963f-4bbb-85ab-772035d027d7
Dec  3 16:35:12.008: INFO: Pod name my-hostname-basic-7e22a41c-963f-4bbb-85ab-772035d027d7: Found 1 pods out of 1
Dec  3 16:35:12.008: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-7e22a41c-963f-4bbb-85ab-772035d027d7" are running
Dec  3 16:35:14.187: INFO: Pod "my-hostname-basic-7e22a41c-963f-4bbb-85ab-772035d027d7-d9t68" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 16:35:11 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 16:35:11 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-7e22a41c-963f-4bbb-85ab-772035d027d7]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 16:35:11 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-7e22a41c-963f-4bbb-85ab-772035d027d7]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-03 16:35:11 +0000 UTC Reason: Message:}])
Dec  3 16:35:14.187: INFO: Trying to dial the pod
Dec  3 16:35:19.546: INFO: Controller my-hostname-basic-7e22a41c-963f-4bbb-85ab-772035d027d7: Got expected result from replica 1 [my-hostname-basic-7e22a41c-963f-4bbb-85ab-772035d027d7-d9t68]: "my-hostname-basic-7e22a41c-963f-4bbb-85ab-772035d027d7-d9t68", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:35:19.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5693" for this suite.
Dec  3 16:35:25.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:35:29.041: INFO: namespace replication-controller-5693 deletion completed in 9.403408486s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:35:29.042: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7920
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Dec  3 16:35:29.770: INFO: Waiting up to 5m0s for pod "var-expansion-b561dd42-4e5f-40da-96b9-279d72e5c869" in namespace "var-expansion-7920" to be "success or failure"
Dec  3 16:35:29.859: INFO: Pod "var-expansion-b561dd42-4e5f-40da-96b9-279d72e5c869": Phase="Pending", Reason="", readiness=false. Elapsed: 89.334193ms
Dec  3 16:35:31.949: INFO: Pod "var-expansion-b561dd42-4e5f-40da-96b9-279d72e5c869": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17915129s
STEP: Saw pod success
Dec  3 16:35:31.949: INFO: Pod "var-expansion-b561dd42-4e5f-40da-96b9-279d72e5c869" satisfied condition "success or failure"
Dec  3 16:35:32.038: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod var-expansion-b561dd42-4e5f-40da-96b9-279d72e5c869 container dapi-container: <nil>
STEP: delete the pod
Dec  3 16:35:32.324: INFO: Waiting for pod var-expansion-b561dd42-4e5f-40da-96b9-279d72e5c869 to disappear
Dec  3 16:35:32.414: INFO: Pod var-expansion-b561dd42-4e5f-40da-96b9-279d72e5c869 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:35:32.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7920" for this suite.
Dec  3 16:35:38.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:35:41.899: INFO: namespace var-expansion-7920 deletion completed in 9.395244031s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:35:41.900: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6372
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-7fee5208-6e7e-47a2-80a9-771afed6ee4e
STEP: Creating a pod to test consume configMaps
Dec  3 16:35:42.721: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-89af5fd6-d3e3-4360-8c24-5f716a08e596" in namespace "projected-6372" to be "success or failure"
Dec  3 16:35:42.811: INFO: Pod "pod-projected-configmaps-89af5fd6-d3e3-4360-8c24-5f716a08e596": Phase="Pending", Reason="", readiness=false. Elapsed: 89.683822ms
Dec  3 16:35:44.901: INFO: Pod "pod-projected-configmaps-89af5fd6-d3e3-4360-8c24-5f716a08e596": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180475882s
STEP: Saw pod success
Dec  3 16:35:44.901: INFO: Pod "pod-projected-configmaps-89af5fd6-d3e3-4360-8c24-5f716a08e596" satisfied condition "success or failure"
Dec  3 16:35:44.991: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-projected-configmaps-89af5fd6-d3e3-4360-8c24-5f716a08e596 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 16:35:45.179: INFO: Waiting for pod pod-projected-configmaps-89af5fd6-d3e3-4360-8c24-5f716a08e596 to disappear
Dec  3 16:35:45.268: INFO: Pod pod-projected-configmaps-89af5fd6-d3e3-4360-8c24-5f716a08e596 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:35:45.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6372" for this suite.
Dec  3 16:35:51.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:35:54.752: INFO: namespace projected-6372 deletion completed in 9.393745868s
â€¢SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:35:54.752: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  3 16:35:55.483: INFO: Waiting up to 5m0s for pod "pod-a1df672a-78f4-4b87-bf5e-c9ddf1fe153f" in namespace "emptydir-6339" to be "success or failure"
Dec  3 16:35:55.572: INFO: Pod "pod-a1df672a-78f4-4b87-bf5e-c9ddf1fe153f": Phase="Pending", Reason="", readiness=false. Elapsed: 89.500407ms
Dec  3 16:35:57.662: INFO: Pod "pod-a1df672a-78f4-4b87-bf5e-c9ddf1fe153f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.17932535s
STEP: Saw pod success
Dec  3 16:35:57.662: INFO: Pod "pod-a1df672a-78f4-4b87-bf5e-c9ddf1fe153f" satisfied condition "success or failure"
Dec  3 16:35:57.751: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-a1df672a-78f4-4b87-bf5e-c9ddf1fe153f container test-container: <nil>
STEP: delete the pod
Dec  3 16:35:57.939: INFO: Waiting for pod pod-a1df672a-78f4-4b87-bf5e-c9ddf1fe153f to disappear
Dec  3 16:35:58.028: INFO: Pod pod-a1df672a-78f4-4b87-bf5e-c9ddf1fe153f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:35:58.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6339" for this suite.
Dec  3 16:36:04.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:36:07.514: INFO: namespace emptydir-6339 deletion completed in 9.395414706s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:36:07.514: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4750
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:36:08.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4750" for this suite.
Dec  3 16:36:14.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:36:17.721: INFO: namespace services-4750 deletion completed in 9.389077528s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95
â€¢SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:36:17.722: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6347
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-849a3100-79f5-4438-8b10-c8afb3c16e64
STEP: Creating a pod to test consume configMaps
Dec  3 16:36:18.540: INFO: Waiting up to 5m0s for pod "pod-configmaps-aeaef282-dc17-47c8-a0a0-afac235f5f96" in namespace "configmap-6347" to be "success or failure"
Dec  3 16:36:18.629: INFO: Pod "pod-configmaps-aeaef282-dc17-47c8-a0a0-afac235f5f96": Phase="Pending", Reason="", readiness=false. Elapsed: 89.300564ms
Dec  3 16:36:20.719: INFO: Pod "pod-configmaps-aeaef282-dc17-47c8-a0a0-afac235f5f96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179149489s
STEP: Saw pod success
Dec  3 16:36:20.719: INFO: Pod "pod-configmaps-aeaef282-dc17-47c8-a0a0-afac235f5f96" satisfied condition "success or failure"
Dec  3 16:36:20.808: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-configmaps-aeaef282-dc17-47c8-a0a0-afac235f5f96 container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 16:36:20.997: INFO: Waiting for pod pod-configmaps-aeaef282-dc17-47c8-a0a0-afac235f5f96 to disappear
Dec  3 16:36:21.087: INFO: Pod pod-configmaps-aeaef282-dc17-47c8-a0a0-afac235f5f96 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:36:21.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6347" for this suite.
Dec  3 16:36:27.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:36:30.568: INFO: namespace configmap-6347 deletion completed in 9.391055448s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:36:30.569: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7863
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec  3 16:36:31.860: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7863 /api/v1/namespaces/watch-7863/configmaps/e2e-watch-test-resource-version 9484d3d8-3b67-4399-9e5d-e42a5befb446 26319 0 2019-12-03 16:36:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 16:36:31.860: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7863 /api/v1/namespaces/watch-7863/configmaps/e2e-watch-test-resource-version 9484d3d8-3b67-4399-9e5d-e42a5befb446 26320 0 2019-12-03 16:36:31 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:36:31.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7863" for this suite.
Dec  3 16:36:38.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:36:41.343: INFO: namespace watch-7863 deletion completed in 9.392943143s
â€¢SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:36:41.344: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3025
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:36:42.347: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"afe9bde2-5fda-42a1-8a0e-963e1b3879ae", Controller:(*bool)(0xc0045ba5ca), BlockOwnerDeletion:(*bool)(0xc0045ba5cb)}}
Dec  3 16:36:42.437: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f1d69397-261f-4143-9dbd-18618a049391", Controller:(*bool)(0xc0033b4266), BlockOwnerDeletion:(*bool)(0xc0033b4267)}}
Dec  3 16:36:42.528: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"c4e73307-5910-42b1-8e48-0be9c837ad7c", Controller:(*bool)(0xc003bf9426), BlockOwnerDeletion:(*bool)(0xc003bf9427)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:36:47.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3025" for this suite.
Dec  3 16:36:54.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:36:57.201: INFO: namespace gc-3025 deletion completed in 9.401865424s
â€¢SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:36:57.202: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3865
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:37:05.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3865" for this suite.
Dec  3 16:37:11.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:37:14.602: INFO: namespace resourcequota-3865 deletion completed in 9.397207541s
â€¢SSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:37:14.602: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7128
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:37:17.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7128" for this suite.
Dec  3 16:37:46.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:37:49.266: INFO: namespace replication-controller-7128 deletion completed in 31.394318577s
â€¢SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:37:49.266: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec  3 16:37:49.908: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-9387'
Dec  3 16:37:50.767: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 16:37:50.767: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Dec  3 16:37:50.857: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-httpd-deployment --namespace=kubectl-9387'
Dec  3 16:37:51.394: INFO: stderr: ""
Dec  3 16:37:51.394: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:37:51.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9387" for this suite.
Dec  3 16:39:17.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:39:20.877: INFO: namespace kubectl-9387 deletion completed in 1m29.392567327s
â€¢SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:39:20.877: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2990
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:39:22.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2990" for this suite.
Dec  3 16:39:28.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:39:31.541: INFO: namespace resourcequota-2990 deletion completed in 9.397607855s
â€¢
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:39:31.542: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6760
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-0495b662-ad20-443b-a66e-5b70ebaf8a3f
STEP: Creating a pod to test consume secrets
Dec  3 16:39:32.361: INFO: Waiting up to 5m0s for pod "pod-secrets-cee397e9-4563-458e-8c6a-7dc8af7effc0" in namespace "secrets-6760" to be "success or failure"
Dec  3 16:39:32.451: INFO: Pod "pod-secrets-cee397e9-4563-458e-8c6a-7dc8af7effc0": Phase="Pending", Reason="", readiness=false. Elapsed: 89.637675ms
Dec  3 16:39:34.541: INFO: Pod "pod-secrets-cee397e9-4563-458e-8c6a-7dc8af7effc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179528808s
STEP: Saw pod success
Dec  3 16:39:34.541: INFO: Pod "pod-secrets-cee397e9-4563-458e-8c6a-7dc8af7effc0" satisfied condition "success or failure"
Dec  3 16:39:34.630: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-secrets-cee397e9-4563-458e-8c6a-7dc8af7effc0 container secret-volume-test: <nil>
STEP: delete the pod
Dec  3 16:39:34.952: INFO: Waiting for pod pod-secrets-cee397e9-4563-458e-8c6a-7dc8af7effc0 to disappear
Dec  3 16:39:35.041: INFO: Pod pod-secrets-cee397e9-4563-458e-8c6a-7dc8af7effc0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:39:35.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6760" for this suite.
Dec  3 16:39:41.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:39:44.524: INFO: namespace secrets-6760 deletion completed in 9.392854069s
â€¢SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:39:44.524: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1058
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-1058
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  3 16:39:45.162: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  3 16:40:08.777: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.1.76:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1058 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:40:08.777: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:40:09.618: INFO: Found all expected endpoints: [netserver-0]
Dec  3 16:40:09.707: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.0.59:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1058 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:40:09.707: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:40:10.534: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:40:10.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1058" for this suite.
Dec  3 16:40:22.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:40:26.017: INFO: namespace pod-network-test-1058 deletion completed in 15.393482389s
â€¢SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:40:26.018: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7967
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:40:26.748: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9fcaa56a-1a5a-4fe9-b771-1c2e4d6f42f0" in namespace "projected-7967" to be "success or failure"
Dec  3 16:40:26.839: INFO: Pod "downwardapi-volume-9fcaa56a-1a5a-4fe9-b771-1c2e4d6f42f0": Phase="Pending", Reason="", readiness=false. Elapsed: 90.994588ms
Dec  3 16:40:28.929: INFO: Pod "downwardapi-volume-9fcaa56a-1a5a-4fe9-b771-1c2e4d6f42f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180778884s
STEP: Saw pod success
Dec  3 16:40:28.929: INFO: Pod "downwardapi-volume-9fcaa56a-1a5a-4fe9-b771-1c2e4d6f42f0" satisfied condition "success or failure"
Dec  3 16:40:29.018: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-9fcaa56a-1a5a-4fe9-b771-1c2e4d6f42f0 container client-container: <nil>
STEP: delete the pod
Dec  3 16:40:29.207: INFO: Waiting for pod downwardapi-volume-9fcaa56a-1a5a-4fe9-b771-1c2e4d6f42f0 to disappear
Dec  3 16:40:29.296: INFO: Pod downwardapi-volume-9fcaa56a-1a5a-4fe9-b771-1c2e4d6f42f0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:40:29.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7967" for this suite.
Dec  3 16:40:35.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:40:38.793: INFO: namespace projected-7967 deletion completed in 9.406118489s
â€¢SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:40:38.793: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3276
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-7e755f93-7b54-4d59-9294-d35b9392b505 in namespace container-probe-3276
Dec  3 16:40:41.702: INFO: Started pod liveness-7e755f93-7b54-4d59-9294-d35b9392b505 in namespace container-probe-3276
STEP: checking the pod's current state and verifying that restartCount is present
Dec  3 16:40:41.791: INFO: Initial restart count of pod liveness-7e755f93-7b54-4d59-9294-d35b9392b505 is 0
Dec  3 16:40:56.509: INFO: Restart count of pod container-probe-3276/liveness-7e755f93-7b54-4d59-9294-d35b9392b505 is now 1 (14.717908121s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:40:56.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3276" for this suite.
Dec  3 16:41:02.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:41:06.091: INFO: namespace container-probe-3276 deletion completed in 9.399216805s
â€¢SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:41:06.092: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6433
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec  3 16:41:06.909: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:41:07.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6433" for this suite.
Dec  3 16:41:13.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:41:16.658: INFO: namespace replication-controller-6433 deletion completed in 9.388847858s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:41:16.659: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-5359
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec  3 16:41:20.248: INFO: Successfully updated pod "adopt-release-f27v7"
STEP: Checking that the Job readopts the Pod
Dec  3 16:41:20.248: INFO: Waiting up to 15m0s for pod "adopt-release-f27v7" in namespace "job-5359" to be "adopted"
Dec  3 16:41:20.337: INFO: Pod "adopt-release-f27v7": Phase="Running", Reason="", readiness=true. Elapsed: 89.264008ms
Dec  3 16:41:20.337: INFO: Pod "adopt-release-f27v7" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec  3 16:41:21.017: INFO: Successfully updated pod "adopt-release-f27v7"
STEP: Checking that the Job releases the Pod
Dec  3 16:41:21.018: INFO: Waiting up to 15m0s for pod "adopt-release-f27v7" in namespace "job-5359" to be "released"
Dec  3 16:41:21.118: INFO: Pod "adopt-release-f27v7": Phase="Running", Reason="", readiness=true. Elapsed: 100.7233ms
Dec  3 16:41:21.118: INFO: Pod "adopt-release-f27v7" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:41:21.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5359" for this suite.
Dec  3 16:42:05.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:42:08.613: INFO: namespace job-5359 deletion completed in 47.404345057s
â€¢SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:42:08.613: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-205
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-205.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-205.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-205.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-205.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-205.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-205.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-205.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-205.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-205.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-205.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-205.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 246.24.109.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.109.24.246_udp@PTR;check="$$(dig +tcp +noall +answer +search 246.24.109.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.109.24.246_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-205.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-205.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-205.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-205.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-205.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-205.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-205.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-205.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-205.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-205.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-205.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 246.24.109.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.109.24.246_udp@PTR;check="$$(dig +tcp +noall +answer +search 246.24.109.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.109.24.246_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 16:42:12.042: INFO: Unable to read wheezy_udp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:12.135: INFO: Unable to read wheezy_tcp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:12.227: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:12.321: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:12.973: INFO: Unable to read jessie_udp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:13.064: INFO: Unable to read jessie_tcp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:13.155: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:13.247: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:13.803: INFO: Lookups using dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb failed for: [wheezy_udp@dns-test-service.dns-205.svc.cluster.local wheezy_tcp@dns-test-service.dns-205.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local jessie_udp@dns-test-service.dns-205.svc.cluster.local jessie_tcp@dns-test-service.dns-205.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local]

Dec  3 16:42:18.895: INFO: Unable to read wheezy_udp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:18.987: INFO: Unable to read wheezy_tcp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:19.079: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:19.174: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:19.824: INFO: Unable to read jessie_udp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:19.916: INFO: Unable to read jessie_tcp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:20.008: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:20.100: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:20.659: INFO: Lookups using dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb failed for: [wheezy_udp@dns-test-service.dns-205.svc.cluster.local wheezy_tcp@dns-test-service.dns-205.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local jessie_udp@dns-test-service.dns-205.svc.cluster.local jessie_tcp@dns-test-service.dns-205.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local]

Dec  3 16:42:23.895: INFO: Unable to read wheezy_udp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:23.987: INFO: Unable to read wheezy_tcp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:24.078: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:24.169: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:24.819: INFO: Unable to read jessie_udp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:24.911: INFO: Unable to read jessie_tcp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:25.002: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:25.094: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:25.651: INFO: Lookups using dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb failed for: [wheezy_udp@dns-test-service.dns-205.svc.cluster.local wheezy_tcp@dns-test-service.dns-205.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local jessie_udp@dns-test-service.dns-205.svc.cluster.local jessie_tcp@dns-test-service.dns-205.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local]

Dec  3 16:42:28.895: INFO: Unable to read wheezy_udp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:28.987: INFO: Unable to read wheezy_tcp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:29.079: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:29.170: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:29.821: INFO: Unable to read jessie_udp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:29.913: INFO: Unable to read jessie_tcp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:30.005: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:30.096: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:30.656: INFO: Lookups using dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb failed for: [wheezy_udp@dns-test-service.dns-205.svc.cluster.local wheezy_tcp@dns-test-service.dns-205.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local jessie_udp@dns-test-service.dns-205.svc.cluster.local jessie_tcp@dns-test-service.dns-205.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local]

Dec  3 16:42:33.896: INFO: Unable to read wheezy_udp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:33.987: INFO: Unable to read wheezy_tcp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:34.079: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:34.214: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:34.865: INFO: Unable to read jessie_udp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:34.956: INFO: Unable to read jessie_tcp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:35.048: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:35.140: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:35.698: INFO: Lookups using dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb failed for: [wheezy_udp@dns-test-service.dns-205.svc.cluster.local wheezy_tcp@dns-test-service.dns-205.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local jessie_udp@dns-test-service.dns-205.svc.cluster.local jessie_tcp@dns-test-service.dns-205.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local]

Dec  3 16:42:38.895: INFO: Unable to read wheezy_udp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:38.987: INFO: Unable to read wheezy_tcp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:39.078: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:39.169: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:39.818: INFO: Unable to read jessie_udp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:39.910: INFO: Unable to read jessie_tcp@dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:40.002: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:40.093: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local from pod dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb: the server could not find the requested resource (get pods dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb)
Dec  3 16:42:40.650: INFO: Lookups using dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb failed for: [wheezy_udp@dns-test-service.dns-205.svc.cluster.local wheezy_tcp@dns-test-service.dns-205.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local jessie_udp@dns-test-service.dns-205.svc.cluster.local jessie_tcp@dns-test-service.dns-205.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-205.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-205.svc.cluster.local]

Dec  3 16:42:45.659: INFO: DNS probes using dns-205/dns-test-f98f5322-0e7e-4f07-aab7-1c0f21a4b2bb succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:42:45.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-205" for this suite.
Dec  3 16:42:52.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:42:55.442: INFO: namespace dns-205 deletion completed in 9.402291336s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:42:55.443: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5074
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec  3 16:42:59.350: INFO: Successfully updated pod "labelsupdate9dba3b10-633e-44d4-ae9a-63917207a5a3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:43:01.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5074" for this suite.
Dec  3 16:43:13.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:43:17.034: INFO: namespace downward-api-5074 deletion completed in 15.396901643s
â€¢SSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:43:17.035: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3103
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec  3 16:43:20.123: INFO: &Pod{ObjectMeta:{send-events-9d0464b0-1487-41d5-b57a-07240d55dd1c  events-3103 /api/v1/namespaces/events-3103/pods/send-events-9d0464b0-1487-41d5-b57a-07240d55dd1c 1fd0355b-df59-49fb-8ba8-7a1048d05c13 27539 0 2019-12-03 16:43:17 +0000 UTC <nil> <nil> map[name:foo time:672892611] map[cni.projectcalico.org/podIP:100.64.1.87/32 kubernetes.io/psp:e2e-test-privileged-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zjsw2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zjsw2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zjsw2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:43:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:43:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:43:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:43:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:100.64.1.87,StartTime:2019-12-03 16:43:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-03 16:43:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://5bfa322548c5d3b62a6837f4b84a28601d2752f8a496de0880e3d157692e8502,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.87,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Dec  3 16:43:22.213: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec  3 16:43:24.303: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:43:24.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3103" for this suite.
Dec  3 16:44:08.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:44:11.875: INFO: namespace events-3103 deletion completed in 47.391236787s
â€¢
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:44:11.875: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:44:15.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9169" for this suite.
Dec  3 16:44:21.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:44:24.806: INFO: namespace emptydir-wrapper-9169 deletion completed in 9.391010198s
â€¢SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:44:24.806: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7218
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Dec  3 16:44:25.444: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7218'
Dec  3 16:44:26.675: INFO: stderr: ""
Dec  3 16:44:26.675: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 16:44:26.675: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7218'
Dec  3 16:44:27.104: INFO: stderr: ""
Dec  3 16:44:27.104: INFO: stdout: "update-demo-nautilus-gc8xp update-demo-nautilus-p99lv "
Dec  3 16:44:27.104: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-gc8xp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7218'
Dec  3 16:44:27.531: INFO: stderr: ""
Dec  3 16:44:27.531: INFO: stdout: ""
Dec  3 16:44:27.531: INFO: update-demo-nautilus-gc8xp is created but not running
Dec  3 16:44:32.532: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7218'
Dec  3 16:44:32.957: INFO: stderr: ""
Dec  3 16:44:32.957: INFO: stdout: "update-demo-nautilus-gc8xp update-demo-nautilus-p99lv "
Dec  3 16:44:32.957: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-gc8xp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7218'
Dec  3 16:44:33.381: INFO: stderr: ""
Dec  3 16:44:33.381: INFO: stdout: "true"
Dec  3 16:44:33.381: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-gc8xp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7218'
Dec  3 16:44:33.801: INFO: stderr: ""
Dec  3 16:44:33.801: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:44:33.801: INFO: validating pod update-demo-nautilus-gc8xp
Dec  3 16:44:33.982: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:44:33.982: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:44:33.982: INFO: update-demo-nautilus-gc8xp is verified up and running
Dec  3 16:44:33.982: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-p99lv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7218'
Dec  3 16:44:34.406: INFO: stderr: ""
Dec  3 16:44:34.406: INFO: stdout: "true"
Dec  3 16:44:34.406: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-p99lv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7218'
Dec  3 16:44:34.833: INFO: stderr: ""
Dec  3 16:44:34.833: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:44:34.833: INFO: validating pod update-demo-nautilus-p99lv
Dec  3 16:44:35.010: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:44:35.010: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:44:35.010: INFO: update-demo-nautilus-p99lv is verified up and running
STEP: scaling down the replication controller
Dec  3 16:44:35.013: INFO: scanned /root for discovery docs: <nil>
Dec  3 16:44:35.013: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7218'
Dec  3 16:44:35.620: INFO: stderr: ""
Dec  3 16:44:35.620: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 16:44:35.620: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7218'
Dec  3 16:44:36.051: INFO: stderr: ""
Dec  3 16:44:36.051: INFO: stdout: "update-demo-nautilus-gc8xp update-demo-nautilus-p99lv "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  3 16:44:41.052: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7218'
Dec  3 16:44:41.472: INFO: stderr: ""
Dec  3 16:44:41.472: INFO: stdout: "update-demo-nautilus-p99lv "
Dec  3 16:44:41.472: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-p99lv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7218'
Dec  3 16:44:41.982: INFO: stderr: ""
Dec  3 16:44:41.982: INFO: stdout: "true"
Dec  3 16:44:41.983: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-p99lv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7218'
Dec  3 16:44:42.406: INFO: stderr: ""
Dec  3 16:44:42.406: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:44:42.406: INFO: validating pod update-demo-nautilus-p99lv
Dec  3 16:44:42.499: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:44:42.499: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:44:42.499: INFO: update-demo-nautilus-p99lv is verified up and running
STEP: scaling up the replication controller
Dec  3 16:44:42.501: INFO: scanned /root for discovery docs: <nil>
Dec  3 16:44:42.502: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7218'
Dec  3 16:44:43.109: INFO: stderr: ""
Dec  3 16:44:43.109: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  3 16:44:43.109: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7218'
Dec  3 16:44:43.535: INFO: stderr: ""
Dec  3 16:44:43.535: INFO: stdout: "update-demo-nautilus-fpncz update-demo-nautilus-p99lv "
Dec  3 16:44:43.535: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-fpncz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7218'
Dec  3 16:44:43.955: INFO: stderr: ""
Dec  3 16:44:43.955: INFO: stdout: ""
Dec  3 16:44:43.955: INFO: update-demo-nautilus-fpncz is created but not running
Dec  3 16:44:48.955: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7218'
Dec  3 16:44:49.384: INFO: stderr: ""
Dec  3 16:44:49.384: INFO: stdout: "update-demo-nautilus-fpncz update-demo-nautilus-p99lv "
Dec  3 16:44:49.384: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-fpncz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7218'
Dec  3 16:44:49.807: INFO: stderr: ""
Dec  3 16:44:49.807: INFO: stdout: "true"
Dec  3 16:44:49.808: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-fpncz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7218'
Dec  3 16:44:50.231: INFO: stderr: ""
Dec  3 16:44:50.231: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:44:50.231: INFO: validating pod update-demo-nautilus-fpncz
Dec  3 16:44:50.410: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:44:50.410: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:44:50.410: INFO: update-demo-nautilus-fpncz is verified up and running
Dec  3 16:44:50.410: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-p99lv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7218'
Dec  3 16:44:50.831: INFO: stderr: ""
Dec  3 16:44:50.832: INFO: stdout: "true"
Dec  3 16:44:50.832: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-p99lv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7218'
Dec  3 16:44:51.260: INFO: stderr: ""
Dec  3 16:44:51.260: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  3 16:44:51.260: INFO: validating pod update-demo-nautilus-p99lv
Dec  3 16:44:51.356: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  3 16:44:51.356: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  3 16:44:51.356: INFO: update-demo-nautilus-p99lv is verified up and running
STEP: using delete to clean up resources
Dec  3 16:44:51.356: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-7218'
Dec  3 16:44:51.867: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  3 16:44:51.867: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  3 16:44:51.867: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7218'
Dec  3 16:44:52.381: INFO: stderr: "No resources found in kubectl-7218 namespace.\n"
Dec  3 16:44:52.381: INFO: stdout: ""
Dec  3 16:44:52.381: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-7218 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  3 16:44:52.808: INFO: stderr: ""
Dec  3 16:44:52.808: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:44:52.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7218" for this suite.
Dec  3 16:44:59.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:45:02.320: INFO: namespace kubectl-7218 deletion completed in 9.421068156s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:45:02.320: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3467
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:45:03.063: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9362d9e1-a605-464f-b61e-be005f2cb1f8" in namespace "downward-api-3467" to be "success or failure"
Dec  3 16:45:03.176: INFO: Pod "downwardapi-volume-9362d9e1-a605-464f-b61e-be005f2cb1f8": Phase="Pending", Reason="", readiness=false. Elapsed: 112.762347ms
Dec  3 16:45:05.358: INFO: Pod "downwardapi-volume-9362d9e1-a605-464f-b61e-be005f2cb1f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.294540592s
STEP: Saw pod success
Dec  3 16:45:05.358: INFO: Pod "downwardapi-volume-9362d9e1-a605-464f-b61e-be005f2cb1f8" satisfied condition "success or failure"
Dec  3 16:45:05.546: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-9362d9e1-a605-464f-b61e-be005f2cb1f8 container client-container: <nil>
STEP: delete the pod
Dec  3 16:45:06.059: INFO: Waiting for pod downwardapi-volume-9362d9e1-a605-464f-b61e-be005f2cb1f8 to disappear
Dec  3 16:45:06.633: INFO: Pod downwardapi-volume-9362d9e1-a605-464f-b61e-be005f2cb1f8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:45:06.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3467" for this suite.
Dec  3 16:45:13.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:45:19.817: INFO: namespace downward-api-3467 deletion completed in 13.002692097s
â€¢SSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:45:19.817: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3074
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Dec  3 16:45:21.214: INFO: Waiting up to 5m0s for pod "client-containers-b1557016-fb58-486a-9e60-7b73dd05a1ce" in namespace "containers-3074" to be "success or failure"
Dec  3 16:45:21.401: INFO: Pod "client-containers-b1557016-fb58-486a-9e60-7b73dd05a1ce": Phase="Pending", Reason="", readiness=false. Elapsed: 186.846856ms
Dec  3 16:45:23.583: INFO: Pod "client-containers-b1557016-fb58-486a-9e60-7b73dd05a1ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.368768299s
STEP: Saw pod success
Dec  3 16:45:23.583: INFO: Pod "client-containers-b1557016-fb58-486a-9e60-7b73dd05a1ce" satisfied condition "success or failure"
Dec  3 16:45:23.767: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod client-containers-b1557016-fb58-486a-9e60-7b73dd05a1ce container test-container: <nil>
STEP: delete the pod
Dec  3 16:45:24.150: INFO: Waiting for pod client-containers-b1557016-fb58-486a-9e60-7b73dd05a1ce to disappear
Dec  3 16:45:24.338: INFO: Pod client-containers-b1557016-fb58-486a-9e60-7b73dd05a1ce no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:45:24.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3074" for this suite.
Dec  3 16:45:31.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:45:37.444: INFO: namespace containers-3074 deletion completed in 12.920154256s
â€¢SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:45:37.444: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6653
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-2e215cce-1de5-4fee-9807-f7417756af4b
STEP: Creating secret with name secret-projected-all-test-volume-6da23473-7a74-46bd-b4a9-fe3768b96bdc
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec  3 16:45:39.121: INFO: Waiting up to 5m0s for pod "projected-volume-38aa43ea-a6e3-45d5-b2b7-c55cfa976a8b" in namespace "projected-6653" to be "success or failure"
Dec  3 16:45:39.295: INFO: Pod "projected-volume-38aa43ea-a6e3-45d5-b2b7-c55cfa976a8b": Phase="Pending", Reason="", readiness=false. Elapsed: 173.980893ms
Dec  3 16:45:41.466: INFO: Pod "projected-volume-38aa43ea-a6e3-45d5-b2b7-c55cfa976a8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.345100032s
STEP: Saw pod success
Dec  3 16:45:41.466: INFO: Pod "projected-volume-38aa43ea-a6e3-45d5-b2b7-c55cfa976a8b" satisfied condition "success or failure"
Dec  3 16:45:41.631: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod projected-volume-38aa43ea-a6e3-45d5-b2b7-c55cfa976a8b container projected-all-volume-test: <nil>
STEP: delete the pod
Dec  3 16:45:41.967: INFO: Waiting for pod projected-volume-38aa43ea-a6e3-45d5-b2b7-c55cfa976a8b to disappear
Dec  3 16:45:42.127: INFO: Pod projected-volume-38aa43ea-a6e3-45d5-b2b7-c55cfa976a8b no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:45:42.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6653" for this suite.
Dec  3 16:45:48.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:45:51.862: INFO: namespace projected-6653 deletion completed in 9.572557033s
â€¢SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:45:51.862: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9729
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:45:52.500: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config version'
Dec  3 16:45:52.927: INFO: stderr: ""
Dec  3 16:45:52.927: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.3\", GitCommit:\"b3cbbae08ec52a7fc73d334838e18d17e8512749\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:23:11Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.3\", GitCommit:\"b3cbbae08ec52a7fc73d334838e18d17e8512749\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:13:49Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:45:52.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9729" for this suite.
Dec  3 16:45:59.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:46:02.413: INFO: namespace kubectl-9729 deletion completed in 9.395797795s
â€¢SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:46:02.413: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-3410
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:46:03.230: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  3 16:46:05.410: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec  3 16:46:07.500: INFO: Creating deployment "test-rollover-deployment"
Dec  3 16:46:07.679: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec  3 16:46:07.769: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec  3 16:46:07.948: INFO: Ensure that both replica sets have 1 created replica
Dec  3 16:46:08.128: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec  3 16:46:08.307: INFO: Updating deployment test-rollover-deployment
Dec  3 16:46:08.307: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec  3 16:46:08.396: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec  3 16:46:08.576: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec  3 16:46:08.756: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 16:46:08.756: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988368, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:46:10.935: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 16:46:10.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988369, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:46:12.935: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 16:46:12.935: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988369, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:46:14.935: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 16:46:14.935: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988369, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:46:16.936: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 16:46:16.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988369, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:46:18.936: INFO: all replica sets need to contain the pod-template-hash label
Dec  3 16:46:18.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988369, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988367, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:46:20.938: INFO: 
Dec  3 16:46:20.938: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec  3 16:46:21.207: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3410 /apis/apps/v1/namespaces/deployment-3410/deployments/test-rollover-deployment fdecd88d-6013-48a0-abfd-855e4149ef69 28124 2 2019-12-03 16:46:07 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003283028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-03 16:46:07 +0000 UTC,LastTransitionTime:2019-12-03 16:46:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2019-12-03 16:46:19 +0000 UTC,LastTransitionTime:2019-12-03 16:46:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec  3 16:46:21.297: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-3410 /apis/apps/v1/namespaces/deployment-3410/replicasets/test-rollover-deployment-7d7dc6548c debababb-d00c-42cb-9bad-4fe3d85ddbed 28117 2 2019-12-03 16:46:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment fdecd88d-6013-48a0-abfd-855e4149ef69 0xc003283c47 0xc003283c48}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003283de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  3 16:46:21.297: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec  3 16:46:21.297: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3410 /apis/apps/v1/namespaces/deployment-3410/replicasets/test-rollover-controller dd8ab796-83f6-4164-b945-7a62f041bd74 28123 2 2019-12-03 16:46:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment fdecd88d-6013-48a0-abfd-855e4149ef69 0xc003283a57 0xc003283a58}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003283b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  3 16:46:21.297: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-3410 /apis/apps/v1/namespaces/deployment-3410/replicasets/test-rollover-deployment-f6c94f66c ad5a3a7e-5c13-42d4-af23-81d50a56ea6e 28086 2 2019-12-03 16:46:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment fdecd88d-6013-48a0-abfd-855e4149ef69 0xc003283f40 0xc003283f41}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003283fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec  3 16:46:21.388: INFO: Pod "test-rollover-deployment-7d7dc6548c-rmflh" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-rmflh test-rollover-deployment-7d7dc6548c- deployment-3410 /api/v1/namespaces/deployment-3410/pods/test-rollover-deployment-7d7dc6548c-rmflh e2379fc8-438c-4d98-9b0a-32af13da4076 28093 0 2019-12-03 16:46:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:100.64.1.97/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c debababb-d00c-42cb-9bad-4fe3d85ddbed 0xc003e40617 0xc003e40618}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-w2qmp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-w2qmp,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-w2qmp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:46:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:46:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:46:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:46:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:100.64.1.97,StartTime:2019-12-03 16:46:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-03 16:46:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://d83d4e48ebff8df594191fc07008b25d67502cf375de583344fd84dacce95ab3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.97,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:46:21.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3410" for this suite.
Dec  3 16:46:27.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:46:30.890: INFO: namespace deployment-3410 deletion completed in 9.412565734s
â€¢SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:46:30.891: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7906
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Dec  3 16:46:31.531: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:46:55.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7906" for this suite.
Dec  3 16:47:02.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:47:05.392: INFO: namespace crd-publish-openapi-7906 deletion completed in 9.406571481s
â€¢
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:47:05.392: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-fskb
STEP: Creating a pod to test atomic-volume-subpath
Dec  3 16:47:06.303: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-fskb" in namespace "subpath-5498" to be "success or failure"
Dec  3 16:47:06.393: INFO: Pod "pod-subpath-test-secret-fskb": Phase="Pending", Reason="", readiness=false. Elapsed: 89.618784ms
Dec  3 16:47:08.483: INFO: Pod "pod-subpath-test-secret-fskb": Phase="Running", Reason="", readiness=true. Elapsed: 2.17952863s
Dec  3 16:47:10.573: INFO: Pod "pod-subpath-test-secret-fskb": Phase="Running", Reason="", readiness=true. Elapsed: 4.269301292s
Dec  3 16:47:12.663: INFO: Pod "pod-subpath-test-secret-fskb": Phase="Running", Reason="", readiness=true. Elapsed: 6.359091349s
Dec  3 16:47:14.752: INFO: Pod "pod-subpath-test-secret-fskb": Phase="Running", Reason="", readiness=true. Elapsed: 8.448749882s
Dec  3 16:47:16.842: INFO: Pod "pod-subpath-test-secret-fskb": Phase="Running", Reason="", readiness=true. Elapsed: 10.538654218s
Dec  3 16:47:18.932: INFO: Pod "pod-subpath-test-secret-fskb": Phase="Running", Reason="", readiness=true. Elapsed: 12.628623471s
Dec  3 16:47:21.022: INFO: Pod "pod-subpath-test-secret-fskb": Phase="Running", Reason="", readiness=true. Elapsed: 14.718910826s
Dec  3 16:47:23.118: INFO: Pod "pod-subpath-test-secret-fskb": Phase="Running", Reason="", readiness=true. Elapsed: 16.814050162s
Dec  3 16:47:25.208: INFO: Pod "pod-subpath-test-secret-fskb": Phase="Running", Reason="", readiness=true. Elapsed: 18.904315157s
Dec  3 16:47:27.298: INFO: Pod "pod-subpath-test-secret-fskb": Phase="Running", Reason="", readiness=true. Elapsed: 20.994831621s
Dec  3 16:47:29.388: INFO: Pod "pod-subpath-test-secret-fskb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 23.084413848s
STEP: Saw pod success
Dec  3 16:47:29.388: INFO: Pod "pod-subpath-test-secret-fskb" satisfied condition "success or failure"
Dec  3 16:47:29.478: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-subpath-test-secret-fskb container test-container-subpath-secret-fskb: <nil>
STEP: delete the pod
Dec  3 16:47:29.762: INFO: Waiting for pod pod-subpath-test-secret-fskb to disappear
Dec  3 16:47:29.851: INFO: Pod pod-subpath-test-secret-fskb no longer exists
STEP: Deleting pod pod-subpath-test-secret-fskb
Dec  3 16:47:29.851: INFO: Deleting pod "pod-subpath-test-secret-fskb" in namespace "subpath-5498"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:47:29.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5498" for this suite.
Dec  3 16:47:36.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:47:39.428: INFO: namespace subpath-5498 deletion completed in 9.39731844s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:47:39.429: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2813
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-2813
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2813 to expose endpoints map[]
Dec  3 16:47:40.253: INFO: successfully validated that service multi-endpoint-test in namespace services-2813 exposes endpoints map[] (91.037514ms elapsed)
STEP: Creating pod pod1 in namespace services-2813
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2813 to expose endpoints map[pod1:[100]]
Dec  3 16:47:42.883: INFO: successfully validated that service multi-endpoint-test in namespace services-2813 exposes endpoints map[pod1:[100]] (2.537580751s elapsed)
STEP: Creating pod pod2 in namespace services-2813
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2813 to expose endpoints map[pod1:[100] pod2:[101]]
Dec  3 16:47:44.512: INFO: successfully validated that service multi-endpoint-test in namespace services-2813 exposes endpoints map[pod1:[100] pod2:[101]] (1.538167403s elapsed)
STEP: Deleting pod pod1 in namespace services-2813
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2813 to expose endpoints map[pod2:[101]]
Dec  3 16:47:44.782: INFO: successfully validated that service multi-endpoint-test in namespace services-2813 exposes endpoints map[pod2:[101]] (179.05966ms elapsed)
STEP: Deleting pod pod2 in namespace services-2813
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2813 to expose endpoints map[]
Dec  3 16:47:44.962: INFO: successfully validated that service multi-endpoint-test in namespace services-2813 exposes endpoints map[] (89.323487ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:47:45.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2813" for this suite.
Dec  3 16:47:57.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:48:00.548: INFO: namespace services-2813 deletion completed in 15.397893619s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95
â€¢SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:48:00.549: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7880
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  3 16:48:01.282: INFO: Waiting up to 5m0s for pod "pod-131c9ff5-a5e8-4280-905e-1d3ee724edad" in namespace "emptydir-7880" to be "success or failure"
Dec  3 16:48:01.372: INFO: Pod "pod-131c9ff5-a5e8-4280-905e-1d3ee724edad": Phase="Pending", Reason="", readiness=false. Elapsed: 89.670901ms
Dec  3 16:48:03.462: INFO: Pod "pod-131c9ff5-a5e8-4280-905e-1d3ee724edad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179579789s
STEP: Saw pod success
Dec  3 16:48:03.462: INFO: Pod "pod-131c9ff5-a5e8-4280-905e-1d3ee724edad" satisfied condition "success or failure"
Dec  3 16:48:03.551: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-131c9ff5-a5e8-4280-905e-1d3ee724edad container test-container: <nil>
STEP: delete the pod
Dec  3 16:48:03.752: INFO: Waiting for pod pod-131c9ff5-a5e8-4280-905e-1d3ee724edad to disappear
Dec  3 16:48:03.842: INFO: Pod pod-131c9ff5-a5e8-4280-905e-1d3ee724edad no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:48:03.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7880" for this suite.
Dec  3 16:48:10.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:48:13.331: INFO: namespace emptydir-7880 deletion completed in 9.397827802s
â€¢S
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:48:13.331: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-6842
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6842.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6842.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6842.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6842.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6842.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6842.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  3 16:48:17.250: INFO: DNS probes using dns-6842/dns-test-3d32acd4-c128-4e92-9e17-4ff032b3a8ab succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:48:17.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6842" for this suite.
Dec  3 16:48:23.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:48:26.925: INFO: namespace dns-6842 deletion completed in 9.393707785s
â€¢SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:48:26.925: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9730
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  3 16:48:32.472: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 16:48:32.562: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 16:48:34.562: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 16:48:34.652: INFO: Pod pod-with-poststart-http-hook still exists
Dec  3 16:48:36.562: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  3 16:48:36.652: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:48:36.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9730" for this suite.
Dec  3 16:48:49.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:48:52.165: INFO: namespace container-lifecycle-hook-9730 deletion completed in 15.422323022s
â€¢SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:48:52.165: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2791
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec  3 16:48:52.898: INFO: Waiting up to 5m0s for pod "pod-72b73214-87a2-4723-8063-9d6f746ef34b" in namespace "emptydir-2791" to be "success or failure"
Dec  3 16:48:52.988: INFO: Pod "pod-72b73214-87a2-4723-8063-9d6f746ef34b": Phase="Pending", Reason="", readiness=false. Elapsed: 89.531529ms
Dec  3 16:48:55.078: INFO: Pod "pod-72b73214-87a2-4723-8063-9d6f746ef34b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179464431s
STEP: Saw pod success
Dec  3 16:48:55.078: INFO: Pod "pod-72b73214-87a2-4723-8063-9d6f746ef34b" satisfied condition "success or failure"
Dec  3 16:48:55.167: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-72b73214-87a2-4723-8063-9d6f746ef34b container test-container: <nil>
STEP: delete the pod
Dec  3 16:48:55.355: INFO: Waiting for pod pod-72b73214-87a2-4723-8063-9d6f746ef34b to disappear
Dec  3 16:48:55.445: INFO: Pod pod-72b73214-87a2-4723-8063-9d6f746ef34b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:48:55.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2791" for this suite.
Dec  3 16:49:01.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:49:04.931: INFO: namespace emptydir-2791 deletion completed in 9.395887819s
â€¢S
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:49:04.932: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7796
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec  3 16:49:05.926: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7796 /api/v1/namespaces/watch-7796/configmaps/e2e-watch-test-configmap-a 5f5b6376-39ad-4867-bebd-0489fd1ff340 28665 0 2019-12-03 16:49:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 16:49:05.926: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7796 /api/v1/namespaces/watch-7796/configmaps/e2e-watch-test-configmap-a 5f5b6376-39ad-4867-bebd-0489fd1ff340 28665 0 2019-12-03 16:49:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec  3 16:49:16.105: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7796 /api/v1/namespaces/watch-7796/configmaps/e2e-watch-test-configmap-a 5f5b6376-39ad-4867-bebd-0489fd1ff340 28686 0 2019-12-03 16:49:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  3 16:49:16.105: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7796 /api/v1/namespaces/watch-7796/configmaps/e2e-watch-test-configmap-a 5f5b6376-39ad-4867-bebd-0489fd1ff340 28686 0 2019-12-03 16:49:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec  3 16:49:26.285: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7796 /api/v1/namespaces/watch-7796/configmaps/e2e-watch-test-configmap-a 5f5b6376-39ad-4867-bebd-0489fd1ff340 28709 0 2019-12-03 16:49:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 16:49:26.285: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7796 /api/v1/namespaces/watch-7796/configmaps/e2e-watch-test-configmap-a 5f5b6376-39ad-4867-bebd-0489fd1ff340 28709 0 2019-12-03 16:49:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec  3 16:49:36.378: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7796 /api/v1/namespaces/watch-7796/configmaps/e2e-watch-test-configmap-a 5f5b6376-39ad-4867-bebd-0489fd1ff340 28731 0 2019-12-03 16:49:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  3 16:49:36.378: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-7796 /api/v1/namespaces/watch-7796/configmaps/e2e-watch-test-configmap-a 5f5b6376-39ad-4867-bebd-0489fd1ff340 28731 0 2019-12-03 16:49:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec  3 16:49:46.470: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7796 /api/v1/namespaces/watch-7796/configmaps/e2e-watch-test-configmap-b 0d6ec246-e9ed-4efa-aa01-493427ba7616 28752 0 2019-12-03 16:49:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 16:49:46.470: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7796 /api/v1/namespaces/watch-7796/configmaps/e2e-watch-test-configmap-b 0d6ec246-e9ed-4efa-aa01-493427ba7616 28752 0 2019-12-03 16:49:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec  3 16:49:56.562: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7796 /api/v1/namespaces/watch-7796/configmaps/e2e-watch-test-configmap-b 0d6ec246-e9ed-4efa-aa01-493427ba7616 28774 0 2019-12-03 16:49:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  3 16:49:56.562: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-7796 /api/v1/namespaces/watch-7796/configmaps/e2e-watch-test-configmap-b 0d6ec246-e9ed-4efa-aa01-493427ba7616 28774 0 2019-12-03 16:49:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:50:06.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7796" for this suite.
Dec  3 16:50:12.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:50:16.045: INFO: namespace watch-7796 deletion completed in 9.392394612s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:50:16.046: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1997
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:50:16.776: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15567a8a-9fb4-4451-902d-56e03f38fe7a" in namespace "projected-1997" to be "success or failure"
Dec  3 16:50:16.866: INFO: Pod "downwardapi-volume-15567a8a-9fb4-4451-902d-56e03f38fe7a": Phase="Pending", Reason="", readiness=false. Elapsed: 89.358899ms
Dec  3 16:50:18.959: INFO: Pod "downwardapi-volume-15567a8a-9fb4-4451-902d-56e03f38fe7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.182356096s
STEP: Saw pod success
Dec  3 16:50:18.959: INFO: Pod "downwardapi-volume-15567a8a-9fb4-4451-902d-56e03f38fe7a" satisfied condition "success or failure"
Dec  3 16:50:19.048: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-15567a8a-9fb4-4451-902d-56e03f38fe7a container client-container: <nil>
STEP: delete the pod
Dec  3 16:50:19.271: INFO: Waiting for pod downwardapi-volume-15567a8a-9fb4-4451-902d-56e03f38fe7a to disappear
Dec  3 16:50:19.361: INFO: Pod downwardapi-volume-15567a8a-9fb4-4451-902d-56e03f38fe7a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:50:19.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1997" for this suite.
Dec  3 16:50:25.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:50:28.847: INFO: namespace projected-1997 deletion completed in 9.396411398s
â€¢
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:50:28.848: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6657
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec  3 16:50:29.486: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6657'
Dec  3 16:50:30.265: INFO: stderr: ""
Dec  3 16:50:30.265: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Dec  3 16:50:30.354: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-httpd-pod --namespace=kubectl-6657'
Dec  3 16:50:33.175: INFO: stderr: ""
Dec  3 16:50:33.175: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:50:33.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6657" for this suite.
Dec  3 16:50:39.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:50:42.660: INFO: namespace kubectl-6657 deletion completed in 9.395065647s
â€¢S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:50:42.661: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:50:43.392: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70a86dd0-91e3-4bca-864e-837b76893e97" in namespace "projected-9115" to be "success or failure"
Dec  3 16:50:43.481: INFO: Pod "downwardapi-volume-70a86dd0-91e3-4bca-864e-837b76893e97": Phase="Pending", Reason="", readiness=false. Elapsed: 89.712346ms
Dec  3 16:50:45.571: INFO: Pod "downwardapi-volume-70a86dd0-91e3-4bca-864e-837b76893e97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179594375s
STEP: Saw pod success
Dec  3 16:50:45.571: INFO: Pod "downwardapi-volume-70a86dd0-91e3-4bca-864e-837b76893e97" satisfied condition "success or failure"
Dec  3 16:50:45.661: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-70a86dd0-91e3-4bca-864e-837b76893e97 container client-container: <nil>
STEP: delete the pod
Dec  3 16:50:45.851: INFO: Waiting for pod downwardapi-volume-70a86dd0-91e3-4bca-864e-837b76893e97 to disappear
Dec  3 16:50:45.941: INFO: Pod downwardapi-volume-70a86dd0-91e3-4bca-864e-837b76893e97 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:50:45.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9115" for this suite.
Dec  3 16:50:52.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:50:55.428: INFO: namespace projected-9115 deletion completed in 9.396687505s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:50:55.429: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec  3 16:50:56.068: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-6310'
Dec  3 16:50:56.509: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  3 16:50:56.509: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Dec  3 16:50:58.691: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-httpd-deployment --namespace=kubectl-6310'
Dec  3 16:50:59.217: INFO: stderr: ""
Dec  3 16:50:59.217: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:50:59.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6310" for this suite.
Dec  3 16:51:05.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:51:08.707: INFO: namespace kubectl-6310 deletion completed in 9.399678058s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:51:08.708: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8676
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Dec  3 16:51:09.347: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config cluster-info'
Dec  3 16:51:09.768: INFO: stderr: ""
Dec  3 16:51:09.769: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.tmayn-bao.it.internal.staging.k8s.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.tmayn-bao.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.tmayn-bao.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:51:09.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8676" for this suite.
Dec  3 16:51:16.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:51:19.263: INFO: namespace kubectl-8676 deletion completed in 9.40377329s
â€¢SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:51:19.263: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-9415
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:51:37.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9415" for this suite.
Dec  3 16:51:43.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:51:47.114: INFO: namespace resourcequota-9415 deletion completed in 9.395279316s
â€¢SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:51:47.114: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-6874
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec  3 16:51:49.098: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988708, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988708, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988708, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988708, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  3 16:51:51.188: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988708, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988708, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988708, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988708, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 16:51:54.282: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:51:54.372: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:51:55.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6874" for this suite.
Dec  3 16:52:01.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:52:05.066: INFO: namespace crd-webhook-6874 deletion completed in 9.425015694s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:52:05.426: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9683
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:52:06.065: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:52:06.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9683" for this suite.
Dec  3 16:52:12.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:52:16.003: INFO: namespace custom-resource-definition-9683 deletion completed in 9.399342568s
â€¢SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:52:16.003: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3504
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3504
STEP: creating replication controller externalsvc in namespace services-3504
I1203 16:52:16.922829    5076 runners.go:184] Created replication controller with name: externalsvc, namespace: services-3504, replica count: 2
I1203 16:52:20.023404    5076 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec  3 16:52:20.298: INFO: Creating new exec pod
Dec  3 16:52:22.570: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=services-3504 execpodpqrps -- /bin/sh -x -c nslookup nodeport-service'
Dec  3 16:52:23.832: INFO: stderr: "+ nslookup nodeport-service\n"
Dec  3 16:52:23.832: INFO: stdout: "Server:\t\t100.104.0.10\nAddress:\t100.104.0.10#53\n\nnodeport-service.services-3504.svc.cluster.local\tcanonical name = externalsvc.services-3504.svc.cluster.local.\nName:\texternalsvc.services-3504.svc.cluster.local\nAddress: 100.109.76.234\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3504, will wait for the garbage collector to delete the pods
Dec  3 16:52:24.114: INFO: Deleting ReplicationController externalsvc took: 91.47899ms
Dec  3 16:52:24.214: INFO: Terminating ReplicationController externalsvc pods took: 100.410294ms
Dec  3 16:52:33.110: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:52:33.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3504" for this suite.
Dec  3 16:52:39.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:52:42.705: INFO: namespace services-3504 deletion completed in 9.410530927s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95
â€¢SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:52:42.705: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8258
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:52:43.436: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fabd5433-6a8e-4dfa-b997-3bde3f39a568" in namespace "downward-api-8258" to be "success or failure"
Dec  3 16:52:43.526: INFO: Pod "downwardapi-volume-fabd5433-6a8e-4dfa-b997-3bde3f39a568": Phase="Pending", Reason="", readiness=false. Elapsed: 89.987051ms
Dec  3 16:52:45.616: INFO: Pod "downwardapi-volume-fabd5433-6a8e-4dfa-b997-3bde3f39a568": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180232308s
STEP: Saw pod success
Dec  3 16:52:45.617: INFO: Pod "downwardapi-volume-fabd5433-6a8e-4dfa-b997-3bde3f39a568" satisfied condition "success or failure"
Dec  3 16:52:45.706: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-fabd5433-6a8e-4dfa-b997-3bde3f39a568 container client-container: <nil>
STEP: delete the pod
Dec  3 16:52:46.031: INFO: Waiting for pod downwardapi-volume-fabd5433-6a8e-4dfa-b997-3bde3f39a568 to disappear
Dec  3 16:52:46.120: INFO: Pod downwardapi-volume-fabd5433-6a8e-4dfa-b997-3bde3f39a568 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:52:46.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8258" for this suite.
Dec  3 16:52:52.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:52:55.606: INFO: namespace downward-api-8258 deletion completed in 9.3960648s
â€¢SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:52:55.607: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3799
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-9f066d62-72e3-4f11-b87d-15639166b40a
STEP: Creating a pod to test consume configMaps
Dec  3 16:52:56.432: INFO: Waiting up to 5m0s for pod "pod-configmaps-39f2d73d-f8a5-4e44-901d-29b8c685d23a" in namespace "configmap-3799" to be "success or failure"
Dec  3 16:52:56.521: INFO: Pod "pod-configmaps-39f2d73d-f8a5-4e44-901d-29b8c685d23a": Phase="Pending", Reason="", readiness=false. Elapsed: 89.733516ms
Dec  3 16:52:58.611: INFO: Pod "pod-configmaps-39f2d73d-f8a5-4e44-901d-29b8c685d23a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.179735485s
STEP: Saw pod success
Dec  3 16:52:58.611: INFO: Pod "pod-configmaps-39f2d73d-f8a5-4e44-901d-29b8c685d23a" satisfied condition "success or failure"
Dec  3 16:52:58.701: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod pod-configmaps-39f2d73d-f8a5-4e44-901d-29b8c685d23a container configmap-volume-test: <nil>
STEP: delete the pod
Dec  3 16:52:58.891: INFO: Waiting for pod pod-configmaps-39f2d73d-f8a5-4e44-901d-29b8c685d23a to disappear
Dec  3 16:52:58.980: INFO: Pod pod-configmaps-39f2d73d-f8a5-4e44-901d-29b8c685d23a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:52:58.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3799" for this suite.
Dec  3 16:53:05.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:53:08.472: INFO: namespace configmap-3799 deletion completed in 9.40155055s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:53:08.472: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4951
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec  3 16:53:10.374: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988790, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988790, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988790, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710988790, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec  3 16:53:13.560: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec  3 16:53:16.238: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config attach --namespace=webhook-4951 to-be-attached-pod -i -c=container1'
Dec  3 16:53:17.030: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:53:17.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4951" for this suite.
Dec  3 16:53:45.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:53:48.610: INFO: namespace webhook-4951 deletion completed in 31.39913721s
STEP: Destroying namespace "webhook-4951-markers" for this suite.
Dec  3 16:53:54.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:53:58.012: INFO: namespace webhook-4951-markers deletion completed in 9.402183872s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:53:58.372: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5883
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec  3 16:53:59.102: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7e284574-ea6d-4b54-9faf-61879e5b67c3" in namespace "projected-5883" to be "success or failure"
Dec  3 16:53:59.192: INFO: Pod "downwardapi-volume-7e284574-ea6d-4b54-9faf-61879e5b67c3": Phase="Pending", Reason="", readiness=false. Elapsed: 89.941388ms
Dec  3 16:54:01.282: INFO: Pod "downwardapi-volume-7e284574-ea6d-4b54-9faf-61879e5b67c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.180121251s
STEP: Saw pod success
Dec  3 16:54:01.282: INFO: Pod "downwardapi-volume-7e284574-ea6d-4b54-9faf-61879e5b67c3" satisfied condition "success or failure"
Dec  3 16:54:01.372: INFO: Trying to get logs from node ip-10-250-10-98.ec2.internal pod downwardapi-volume-7e284574-ea6d-4b54-9faf-61879e5b67c3 container client-container: <nil>
STEP: delete the pod
Dec  3 16:54:01.564: INFO: Waiting for pod downwardapi-volume-7e284574-ea6d-4b54-9faf-61879e5b67c3 to disappear
Dec  3 16:54:01.654: INFO: Pod downwardapi-volume-7e284574-ea6d-4b54-9faf-61879e5b67c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:54:01.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5883" for this suite.
Dec  3 16:54:08.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:54:11.140: INFO: namespace projected-5883 deletion completed in 9.394983951s
â€¢SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:54:11.140: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7029
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-7029
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  3 16:54:11.778: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  3 16:54:29.310: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.120:8080/dial?request=hostName&protocol=http&host=100.64.1.119&port=8080&tries=1'] Namespace:pod-network-test-7029 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:54:29.310: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:54:30.205: INFO: Waiting for endpoints: map[]
Dec  3 16:54:30.295: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.1.120:8080/dial?request=hostName&protocol=http&host=100.64.0.60&port=8080&tries=1'] Namespace:pod-network-test-7029 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  3 16:54:30.295: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:54:31.118: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:54:31.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7029" for this suite.
Dec  3 16:54:43.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:54:46.607: INFO: namespace pod-network-test-7029 deletion completed in 15.398647938s
â€¢SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:54:46.607: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9193
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-e7b0bccf-c18f-4dc3-b098-50b6efc96a6e
STEP: Creating secret with name s-test-opt-upd-a6938718-233e-4de0-b869-c6af52e9f11f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e7b0bccf-c18f-4dc3-b098-50b6efc96a6e
STEP: Updating secret s-test-opt-upd-a6938718-233e-4de0-b869-c6af52e9f11f
STEP: Creating secret with name s-test-opt-create-29593177-5ba5-4121-8321-c34b70df56bd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:54:52.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9193" for this suite.
Dec  3 16:55:05.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:55:08.318: INFO: namespace secrets-9193 deletion completed in 15.399639794s
â€¢SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:55:08.318: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9718
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Dec  3 16:55:08.957: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-9718 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec  3 16:55:11.964: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec  3 16:55:11.996: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:55:14.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9718" for this suite.
Dec  3 16:55:20.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:55:23.671: INFO: namespace kubectl-9718 deletion completed in 9.405709442s
â€¢SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:55:23.671: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9124
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec  3 16:55:24.310: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Dec  3 16:55:28.619: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:55:46.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9124" for this suite.
Dec  3 16:55:52.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:55:55.832: INFO: namespace crd-publish-openapi-9124 deletion completed in 9.402011104s
â€¢
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:55:55.832: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-6041
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Dec  3 16:55:56.471: INFO: Waiting up to 1m0s for all nodes to be ready
Dec  3 16:56:57.009: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:56:57.099: INFO: Starting informer...
STEP: Starting pods...
Dec  3 16:56:57.375: INFO: Pod1 is running on ip-10-250-10-98.ec2.internal. Tainting Node
Dec  3 16:56:59.822: INFO: Pod2 is running on ip-10-250-10-98.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Dec  3 16:57:07.315: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec  3 16:57:27.438: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:57:27.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6041" for this suite.
Dec  3 16:57:34.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:57:37.202: INFO: namespace taint-multiple-pods-6041 deletion completed in 9.403381612s
â€¢SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:57:37.202: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7405
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Dec  3 16:57:37.841: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tmayn-bao.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy --unix-socket=/tmp/kubectl-proxy-unix990780420/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:57:37.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7405" for this suite.
Dec  3 16:57:44.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:57:47.405: INFO: namespace kubectl-7405 deletion completed in 9.413002648s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:57:47.405: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2298
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-fe02d6cf-5494-4767-9a73-758a5d2d3820
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:57:50.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2298" for this suite.
Dec  3 16:58:03.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:58:06.362: INFO: namespace configmap-2298 deletion completed in 15.398979379s
â€¢SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:58:06.363: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec  3 16:58:07.001: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:58:10.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6481" for this suite.
Dec  3 16:58:17.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:58:20.252: INFO: namespace init-container-6481 deletion completed in 9.459872732s
â€¢SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:58:20.253: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9934
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  3 16:58:25.798: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 16:58:25.888: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 16:58:27.888: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 16:58:27.978: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 16:58:29.888: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 16:58:29.978: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 16:58:31.888: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 16:58:31.978: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  3 16:58:33.888: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  3 16:58:33.978: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:58:33.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9934" for this suite.
Dec  3 16:58:46.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:58:49.539: INFO: namespace container-lifecycle-hook-9934 deletion completed in 15.470937646s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec  3 16:58:49.539: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8159
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec  3 16:58:50.356: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  3 16:58:52.536: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec  3 16:58:55.253: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8159 /apis/apps/v1/namespaces/deployment-8159/deployments/test-cleanup-deployment 43c5a58d-f1b7-4aa2-8124-b3194753cf17 30533 1 2019-12-03 16:58:52 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003ad83c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-03 16:58:52 +0000 UTC,LastTransitionTime:2019-12-03 16:58:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-65db99849b" has successfully progressed.,LastUpdateTime:2019-12-03 16:58:54 +0000 UTC,LastTransitionTime:2019-12-03 16:58:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec  3 16:58:55.344: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-8159 /apis/apps/v1/namespaces/deployment-8159/replicasets/test-cleanup-deployment-65db99849b 7e681a88-a02c-4d3b-aac2-5735fee577cf 30526 1 2019-12-03 16:58:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 43c5a58d-f1b7-4aa2-8124-b3194753cf17 0xc003ad87c7 0xc003ad87c8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003ad8828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec  3 16:58:55.434: INFO: Pod "test-cleanup-deployment-65db99849b-pjc6l" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-pjc6l test-cleanup-deployment-65db99849b- deployment-8159 /api/v1/namespaces/deployment-8159/pods/test-cleanup-deployment-65db99849b-pjc6l b99b2c07-ec99-4932-b0e5-1c77a59b8792 30525 0 2019-12-03 16:58:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[cni.projectcalico.org/podIP:100.64.1.130/32 kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b 7e681a88-a02c-4d3b-aac2-5735fee577cf 0xc003ad8bc7 0xc003ad8bc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-t4mcx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-t4mcx,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-t4mcx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-250-10-98.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:58:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:58:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-03 16:58:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.250.10.98,PodIP:100.64.1.130,StartTime:2019-12-03 16:58:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-03 16:58:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://0ad033dd45f99b42efe4a7f72d3909350fc228716c22c48cd1c2b76acdae75c4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:100.64.1.130,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec  3 16:58:55.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8159" for this suite.
Dec  3 16:59:01.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  3 16:59:04.930: INFO: namespace deployment-8159 deletion completed in 9.406503546s
â€¢SDec  3 16:59:04.930: INFO: Running AfterSuite actions on all nodes
Dec  3 16:59:04.931: INFO: Running AfterSuite actions on node 1
Dec  3 16:59:04.931: INFO: Skipping dumping logs from cluster

Ran 276 of 4732 Specs in 8341.512 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Flaked | 0 Pending | 4456 Skipped
PASS

Ginkgo ran 1 suite in 2h19m3.385914698s
Test Suite Passed
