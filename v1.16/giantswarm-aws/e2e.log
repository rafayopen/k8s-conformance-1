I0616 15:27:02.055449      24 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-790258633
I0616 15:27:02.055560      24 e2e.go:92] Starting e2e run "ff4c0d7c-7e88-4536-b290-a17ec70fa0af" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1592321220 - Will randomize all specs
Will run 276 of 4732 specs

Jun 16 15:27:02.076: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 15:27:02.079: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 16 15:27:02.089: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 16 15:27:02.124: INFO: The status of Pod aws-cni-restarter-1592320200-mxgxv is Succeeded, skipping waiting
Jun 16 15:27:02.124: INFO: The status of Pod aws-cni-restarter-1592320500-vct6w is Succeeded, skipping waiting
Jun 16 15:27:02.124: INFO: The status of Pod aws-cni-restarter-1592320800-mj8sv is Succeeded, skipping waiting
Jun 16 15:27:02.124: INFO: The status of Pod aws-cni-restarter-1592321100-flg75 is Succeeded, skipping waiting
Jun 16 15:27:02.124: INFO: 40 / 44 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 16 15:27:02.124: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Jun 16 15:27:02.124: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 16 15:27:02.132: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'aws-node' (0 seconds elapsed)
Jun 16 15:27:02.132: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jun 16 15:27:02.132: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'cert-exporter' (0 seconds elapsed)
Jun 16 15:27:02.132: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kiam-agent' (0 seconds elapsed)
Jun 16 15:27:02.132: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'kiam-server' (0 seconds elapsed)
Jun 16 15:27:02.132: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun 16 15:27:02.132: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'net-exporter' (0 seconds elapsed)
Jun 16 15:27:02.132: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Jun 16 15:27:02.133: INFO: e2e test version: v1.16.9
Jun 16 15:27:02.133: INFO: kube-apiserver version: v1.16.9
Jun 16 15:27:02.133: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 15:27:02.137: INFO: Cluster IP family: ipv4
SSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:27:02.137: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename deployment
Jun 16 15:27:02.155: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jun 16 15:27:02.161: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5821
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 15:27:02.267: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 16 15:27:02.270: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 16 15:27:07.273: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 16 15:27:09.276: INFO: Creating deployment "test-rolling-update-deployment"
Jun 16 15:27:09.278: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 16 15:27:09.281: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 16 15:27:11.285: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 16 15:27:11.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918029, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918029, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918029, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918029, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 16 15:27:13.288: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 16 15:27:13.293: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5821 /apis/apps/v1/namespaces/deployment-5821/deployments/test-rolling-update-deployment d4f4b30d-cc09-4811-ab2d-62a853deebda 4497 1 2020-06-16 15:27:09 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0022ca268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-06-16 15:27:09 +0000 UTC,LastTransitionTime:2020-06-16 15:27:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-06-16 15:27:12 +0000 UTC,LastTransitionTime:2020-06-16 15:27:09 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 16 15:27:13.294: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-5821 /apis/apps/v1/namespaces/deployment-5821/replicasets/test-rolling-update-deployment-55d946486 b37f8da8-4c65-48c4-bd59-6593a6521c3f 4487 1 2020-06-16 15:27:09 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d4f4b30d-cc09-4811-ab2d-62a853deebda 0xc0022ca740 0xc0022ca741}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0022ca7a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 16 15:27:13.294: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 16 15:27:13.294: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5821 /apis/apps/v1/namespaces/deployment-5821/replicasets/test-rolling-update-controller a4ddd034-8d4d-4c3e-a70a-baeb7154cc0f 4496 2 2020-06-16 15:27:02 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d4f4b30d-cc09-4811-ab2d-62a853deebda 0xc0022ca66f 0xc0022ca680}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0022ca6e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 16 15:27:13.296: INFO: Pod "test-rolling-update-deployment-55d946486-b7sgf" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-b7sgf test-rolling-update-deployment-55d946486- deployment-5821 /api/v1/namespaces/deployment-5821/pods/test-rolling-update-deployment-55d946486-b7sgf 5eb8c4c7-3407-4947-9db7-2fa2fb4a44fa 4486 0 2020-06-16 15:27:09 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 b37f8da8-4c65-48c4-bd59-6593a6521c3f 0xc0022cac10 0xc0022cac11}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6hfxd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6hfxd,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6hfxd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-6.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:27:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:27:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:27:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:27:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.6,PodIP:172.18.139.28,StartTime:2020-06-16 15:27:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-16 15:27:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://0e55b77e145dd61414a4d6c901a1615053ce5061dc3b98a3cf61e5a3a4d27ec6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.18.139.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:27:13.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5821" for this suite.
Jun 16 15:27:19.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:27:19.355: INFO: namespace deployment-5821 deletion completed in 6.056654949s

• [SLOW TEST:17.218 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:27:19.355: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-301
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jun 16 15:27:19.475: INFO: PodSpec: initContainers in spec.initContainers
Jun 16 15:28:06.468: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9eed11ff-e6b3-42e9-b6f2-24d3f3236628", GenerateName:"", Namespace:"init-container-301", SelfLink:"/api/v1/namespaces/init-container-301/pods/pod-init-9eed11ff-e6b3-42e9-b6f2-24d3f3236628", UID:"8bc86b67-703e-4fd1-893c-e4932efe9b1a", ResourceVersion:"4679", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63727918039, loc:(*time.Location)(0x78a2900)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"475656202"}, Annotations:map[string]string{"kubernetes.io/psp":"cert-exporter-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-hwll6", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002836ac0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hwll6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hwll6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-hwll6", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0028609b8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-19-65-6.eu-west-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0025f3c20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002860a30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002860a50)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002860a58), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002860a5c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918039, loc:(*time.Location)(0x78a2900)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918039, loc:(*time.Location)(0x78a2900)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918039, loc:(*time.Location)(0x78a2900)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918039, loc:(*time.Location)(0x78a2900)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.19.65.6", PodIP:"172.18.134.112", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.18.134.112"}}, StartTime:(*v1.Time)(0xc002932240), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003029180)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0030291f0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://3ea35001661a14d52132db9125cdcf769308dba412711e678037a162f85b1143", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002932280), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002932260), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc002860adf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:28:06.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-301" for this suite.
Jun 16 15:28:34.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:28:34.526: INFO: namespace init-container-301 deletion completed in 28.054717255s

• [SLOW TEST:75.171 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:28:34.526: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5246
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-5d7fd6dd-9b47-4c7f-8367-21207ea29006
STEP: Creating a pod to test consume secrets
Jun 16 15:28:34.653: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e20ec31d-aaa1-402e-a21f-b0cc1263ba9e" in namespace "projected-5246" to be "success or failure"
Jun 16 15:28:34.654: INFO: Pod "pod-projected-secrets-e20ec31d-aaa1-402e-a21f-b0cc1263ba9e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.668986ms
Jun 16 15:28:36.657: INFO: Pod "pod-projected-secrets-e20ec31d-aaa1-402e-a21f-b0cc1263ba9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004007879s
Jun 16 15:28:38.659: INFO: Pod "pod-projected-secrets-e20ec31d-aaa1-402e-a21f-b0cc1263ba9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006111231s
STEP: Saw pod success
Jun 16 15:28:38.659: INFO: Pod "pod-projected-secrets-e20ec31d-aaa1-402e-a21f-b0cc1263ba9e" satisfied condition "success or failure"
Jun 16 15:28:38.660: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-projected-secrets-e20ec31d-aaa1-402e-a21f-b0cc1263ba9e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 16 15:28:38.671: INFO: Waiting for pod pod-projected-secrets-e20ec31d-aaa1-402e-a21f-b0cc1263ba9e to disappear
Jun 16 15:28:38.675: INFO: Pod pod-projected-secrets-e20ec31d-aaa1-402e-a21f-b0cc1263ba9e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:28:38.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5246" for this suite.
Jun 16 15:28:44.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:28:44.732: INFO: namespace projected-5246 deletion completed in 6.054894786s

• [SLOW TEST:10.206 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:28:44.732: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4587
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jun 16 15:28:45.874: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0616 15:28:45.874955      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:28:45.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4587" for this suite.
Jun 16 15:28:51.882: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:28:51.931: INFO: namespace gc-4587 deletion completed in 6.054486683s

• [SLOW TEST:7.199 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:28:51.931: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9555
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Jun 16 15:28:52.055: INFO: Waiting up to 5m0s for pod "var-expansion-16b054b5-7de5-48f1-ad5d-d6ec4d84cf31" in namespace "var-expansion-9555" to be "success or failure"
Jun 16 15:28:52.056: INFO: Pod "var-expansion-16b054b5-7de5-48f1-ad5d-d6ec4d84cf31": Phase="Pending", Reason="", readiness=false. Elapsed: 1.571745ms
Jun 16 15:28:54.059: INFO: Pod "var-expansion-16b054b5-7de5-48f1-ad5d-d6ec4d84cf31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00372444s
STEP: Saw pod success
Jun 16 15:28:54.059: INFO: Pod "var-expansion-16b054b5-7de5-48f1-ad5d-d6ec4d84cf31" satisfied condition "success or failure"
Jun 16 15:28:54.060: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod var-expansion-16b054b5-7de5-48f1-ad5d-d6ec4d84cf31 container dapi-container: <nil>
STEP: delete the pod
Jun 16 15:28:54.071: INFO: Waiting for pod var-expansion-16b054b5-7de5-48f1-ad5d-d6ec4d84cf31 to disappear
Jun 16 15:28:54.073: INFO: Pod var-expansion-16b054b5-7de5-48f1-ad5d-d6ec4d84cf31 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:28:54.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9555" for this suite.
Jun 16 15:29:00.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:29:00.135: INFO: namespace var-expansion-9555 deletion completed in 6.05984105s

• [SLOW TEST:8.204 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:29:00.135: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2404
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-zqr8
STEP: Creating a pod to test atomic-volume-subpath
Jun 16 15:29:00.263: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-zqr8" in namespace "subpath-2404" to be "success or failure"
Jun 16 15:29:00.265: INFO: Pod "pod-subpath-test-secret-zqr8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024848ms
Jun 16 15:29:02.272: INFO: Pod "pod-subpath-test-secret-zqr8": Phase="Running", Reason="", readiness=true. Elapsed: 2.009353029s
Jun 16 15:29:04.274: INFO: Pod "pod-subpath-test-secret-zqr8": Phase="Running", Reason="", readiness=true. Elapsed: 4.011466109s
Jun 16 15:29:06.277: INFO: Pod "pod-subpath-test-secret-zqr8": Phase="Running", Reason="", readiness=true. Elapsed: 6.013946446s
Jun 16 15:29:08.279: INFO: Pod "pod-subpath-test-secret-zqr8": Phase="Running", Reason="", readiness=true. Elapsed: 8.016059402s
Jun 16 15:29:10.281: INFO: Pod "pod-subpath-test-secret-zqr8": Phase="Running", Reason="", readiness=true. Elapsed: 10.018055474s
Jun 16 15:29:12.283: INFO: Pod "pod-subpath-test-secret-zqr8": Phase="Running", Reason="", readiness=true. Elapsed: 12.020149463s
Jun 16 15:29:14.285: INFO: Pod "pod-subpath-test-secret-zqr8": Phase="Running", Reason="", readiness=true. Elapsed: 14.02227065s
Jun 16 15:29:16.288: INFO: Pod "pod-subpath-test-secret-zqr8": Phase="Running", Reason="", readiness=true. Elapsed: 16.025215608s
Jun 16 15:29:18.290: INFO: Pod "pod-subpath-test-secret-zqr8": Phase="Running", Reason="", readiness=true. Elapsed: 18.027333009s
Jun 16 15:29:20.292: INFO: Pod "pod-subpath-test-secret-zqr8": Phase="Running", Reason="", readiness=true. Elapsed: 20.029342525s
Jun 16 15:29:22.294: INFO: Pod "pod-subpath-test-secret-zqr8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.031661939s
STEP: Saw pod success
Jun 16 15:29:22.294: INFO: Pod "pod-subpath-test-secret-zqr8" satisfied condition "success or failure"
Jun 16 15:29:22.296: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-subpath-test-secret-zqr8 container test-container-subpath-secret-zqr8: <nil>
STEP: delete the pod
Jun 16 15:29:22.305: INFO: Waiting for pod pod-subpath-test-secret-zqr8 to disappear
Jun 16 15:29:22.307: INFO: Pod pod-subpath-test-secret-zqr8 no longer exists
STEP: Deleting pod pod-subpath-test-secret-zqr8
Jun 16 15:29:22.307: INFO: Deleting pod "pod-subpath-test-secret-zqr8" in namespace "subpath-2404"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:29:22.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2404" for this suite.
Jun 16 15:29:28.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:29:28.365: INFO: namespace subpath-2404 deletion completed in 6.054576381s

• [SLOW TEST:28.230 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:29:28.365: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5276
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-f56bfaaa-e1df-4d3d-a62a-22084fe408d0
STEP: Creating configMap with name cm-test-opt-upd-13339ec0-6a17-4a20-ad4a-1df6bd54b626
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-f56bfaaa-e1df-4d3d-a62a-22084fe408d0
STEP: Updating configmap cm-test-opt-upd-13339ec0-6a17-4a20-ad4a-1df6bd54b626
STEP: Creating configMap with name cm-test-opt-create-2af2f0be-db64-4854-a870-50fb51df71bd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:29:34.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5276" for this suite.
Jun 16 15:29:46.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:29:46.602: INFO: namespace configmap-5276 deletion completed in 12.05821685s

• [SLOW TEST:18.236 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:29:46.602: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-980
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 15:29:46.731: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-72258c3d-0cad-43e8-80cd-b164e754d396" in namespace "security-context-test-980" to be "success or failure"
Jun 16 15:29:46.733: INFO: Pod "busybox-readonly-false-72258c3d-0cad-43e8-80cd-b164e754d396": Phase="Pending", Reason="", readiness=false. Elapsed: 1.852068ms
Jun 16 15:29:48.738: INFO: Pod "busybox-readonly-false-72258c3d-0cad-43e8-80cd-b164e754d396": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007562849s
Jun 16 15:29:48.739: INFO: Pod "busybox-readonly-false-72258c3d-0cad-43e8-80cd-b164e754d396" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:29:48.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-980" for this suite.
Jun 16 15:29:54.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:29:54.797: INFO: namespace security-context-test-980 deletion completed in 6.056825085s

• [SLOW TEST:8.195 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:29:54.797: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6606
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 15:29:54.920: INFO: Waiting up to 5m0s for pod "downwardapi-volume-64afff90-a4dc-4353-bb50-ee8df37d9cb1" in namespace "downward-api-6606" to be "success or failure"
Jun 16 15:29:54.924: INFO: Pod "downwardapi-volume-64afff90-a4dc-4353-bb50-ee8df37d9cb1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.623351ms
Jun 16 15:29:56.926: INFO: Pod "downwardapi-volume-64afff90-a4dc-4353-bb50-ee8df37d9cb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005494073s
STEP: Saw pod success
Jun 16 15:29:56.926: INFO: Pod "downwardapi-volume-64afff90-a4dc-4353-bb50-ee8df37d9cb1" satisfied condition "success or failure"
Jun 16 15:29:56.927: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod downwardapi-volume-64afff90-a4dc-4353-bb50-ee8df37d9cb1 container client-container: <nil>
STEP: delete the pod
Jun 16 15:29:56.937: INFO: Waiting for pod downwardapi-volume-64afff90-a4dc-4353-bb50-ee8df37d9cb1 to disappear
Jun 16 15:29:56.939: INFO: Pod downwardapi-volume-64afff90-a4dc-4353-bb50-ee8df37d9cb1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:29:56.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6606" for this suite.
Jun 16 15:30:02.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:30:02.995: INFO: namespace downward-api-6606 deletion completed in 6.054066127s

• [SLOW TEST:8.197 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:30:02.995: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 15:30:03.641: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jun 16 15:30:05.646: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918203, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918203, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918203, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918203, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 16 15:30:07.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918203, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918203, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918203, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918203, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 15:30:10.653: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:30:10.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3115" for this suite.
Jun 16 15:30:16.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:30:16.778: INFO: namespace webhook-3115 deletion completed in 6.056028719s
STEP: Destroying namespace "webhook-3115-markers" for this suite.
Jun 16 15:30:22.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:30:22.882: INFO: namespace webhook-3115-markers deletion completed in 6.104500387s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:20.037 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:30:23.032: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7648
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 15:30:23.156: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74fe8119-b0cf-49ec-9507-5a7c833743de" in namespace "downward-api-7648" to be "success or failure"
Jun 16 15:30:23.158: INFO: Pod "downwardapi-volume-74fe8119-b0cf-49ec-9507-5a7c833743de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019924ms
Jun 16 15:30:25.160: INFO: Pod "downwardapi-volume-74fe8119-b0cf-49ec-9507-5a7c833743de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004217251s
STEP: Saw pod success
Jun 16 15:30:25.160: INFO: Pod "downwardapi-volume-74fe8119-b0cf-49ec-9507-5a7c833743de" satisfied condition "success or failure"
Jun 16 15:30:25.162: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod downwardapi-volume-74fe8119-b0cf-49ec-9507-5a7c833743de container client-container: <nil>
STEP: delete the pod
Jun 16 15:30:25.171: INFO: Waiting for pod downwardapi-volume-74fe8119-b0cf-49ec-9507-5a7c833743de to disappear
Jun 16 15:30:25.176: INFO: Pod downwardapi-volume-74fe8119-b0cf-49ec-9507-5a7c833743de no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:30:25.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7648" for this suite.
Jun 16 15:30:31.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:30:31.238: INFO: namespace downward-api-7648 deletion completed in 6.060722791s

• [SLOW TEST:8.207 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:30:31.239: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8710
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-8fbc3ac2-e961-489b-8299-b4d4ef874289
STEP: Creating a pod to test consume configMaps
Jun 16 15:30:31.364: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a941cd21-cfa3-423e-8e2f-67a4509fc555" in namespace "projected-8710" to be "success or failure"
Jun 16 15:30:31.366: INFO: Pod "pod-projected-configmaps-a941cd21-cfa3-423e-8e2f-67a4509fc555": Phase="Pending", Reason="", readiness=false. Elapsed: 1.965636ms
Jun 16 15:30:33.368: INFO: Pod "pod-projected-configmaps-a941cd21-cfa3-423e-8e2f-67a4509fc555": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004160519s
STEP: Saw pod success
Jun 16 15:30:33.368: INFO: Pod "pod-projected-configmaps-a941cd21-cfa3-423e-8e2f-67a4509fc555" satisfied condition "success or failure"
Jun 16 15:30:33.370: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-projected-configmaps-a941cd21-cfa3-423e-8e2f-67a4509fc555 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 16 15:30:33.381: INFO: Waiting for pod pod-projected-configmaps-a941cd21-cfa3-423e-8e2f-67a4509fc555 to disappear
Jun 16 15:30:33.384: INFO: Pod pod-projected-configmaps-a941cd21-cfa3-423e-8e2f-67a4509fc555 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:30:33.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8710" for this suite.
Jun 16 15:30:39.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:30:39.443: INFO: namespace projected-8710 deletion completed in 6.057493253s

• [SLOW TEST:8.204 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:30:39.443: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9318
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-91b966a4-cfa7-4eb3-90f3-23712a64acce
STEP: Creating a pod to test consume configMaps
Jun 16 15:30:39.571: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f2031b1f-d07e-4352-a2eb-d7f44acd6370" in namespace "projected-9318" to be "success or failure"
Jun 16 15:30:39.573: INFO: Pod "pod-projected-configmaps-f2031b1f-d07e-4352-a2eb-d7f44acd6370": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081151ms
Jun 16 15:30:41.575: INFO: Pod "pod-projected-configmaps-f2031b1f-d07e-4352-a2eb-d7f44acd6370": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004179202s
STEP: Saw pod success
Jun 16 15:30:41.575: INFO: Pod "pod-projected-configmaps-f2031b1f-d07e-4352-a2eb-d7f44acd6370" satisfied condition "success or failure"
Jun 16 15:30:41.576: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-projected-configmaps-f2031b1f-d07e-4352-a2eb-d7f44acd6370 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 16 15:30:41.586: INFO: Waiting for pod pod-projected-configmaps-f2031b1f-d07e-4352-a2eb-d7f44acd6370 to disappear
Jun 16 15:30:41.588: INFO: Pod pod-projected-configmaps-f2031b1f-d07e-4352-a2eb-d7f44acd6370 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:30:41.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9318" for this suite.
Jun 16 15:30:47.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:30:47.644: INFO: namespace projected-9318 deletion completed in 6.05492347s

• [SLOW TEST:8.201 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:30:47.645: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9391
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-be9965fe-c22e-4918-971c-de9b7465a30e
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:30:49.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9391" for this suite.
Jun 16 15:31:13.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:31:13.861: INFO: namespace configmap-9391 deletion completed in 24.057892576s

• [SLOW TEST:26.216 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:31:13.861: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2931
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-5b6d286c-2afe-4d1f-83df-e85783d8046c
STEP: Creating a pod to test consume configMaps
Jun 16 15:31:13.988: INFO: Waiting up to 5m0s for pod "pod-configmaps-7d19a6ed-1718-4a96-97d6-61bd04a20391" in namespace "configmap-2931" to be "success or failure"
Jun 16 15:31:13.990: INFO: Pod "pod-configmaps-7d19a6ed-1718-4a96-97d6-61bd04a20391": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037115ms
Jun 16 15:31:15.993: INFO: Pod "pod-configmaps-7d19a6ed-1718-4a96-97d6-61bd04a20391": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004426678s
STEP: Saw pod success
Jun 16 15:31:15.993: INFO: Pod "pod-configmaps-7d19a6ed-1718-4a96-97d6-61bd04a20391" satisfied condition "success or failure"
Jun 16 15:31:15.995: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-configmaps-7d19a6ed-1718-4a96-97d6-61bd04a20391 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 16 15:31:16.006: INFO: Waiting for pod pod-configmaps-7d19a6ed-1718-4a96-97d6-61bd04a20391 to disappear
Jun 16 15:31:16.008: INFO: Pod pod-configmaps-7d19a6ed-1718-4a96-97d6-61bd04a20391 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:31:16.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2931" for this suite.
Jun 16 15:31:22.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:31:22.072: INFO: namespace configmap-2931 deletion completed in 6.061875319s

• [SLOW TEST:8.211 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:31:22.072: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3150
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-31504fd4-8887-4d73-829c-4e9f9564699b
STEP: Creating a pod to test consume secrets
Jun 16 15:31:22.197: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a98ecb98-58f7-4c0b-8c10-85f9c77183cc" in namespace "projected-3150" to be "success or failure"
Jun 16 15:31:22.199: INFO: Pod "pod-projected-secrets-a98ecb98-58f7-4c0b-8c10-85f9c77183cc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.91104ms
Jun 16 15:31:24.201: INFO: Pod "pod-projected-secrets-a98ecb98-58f7-4c0b-8c10-85f9c77183cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004355412s
STEP: Saw pod success
Jun 16 15:31:24.201: INFO: Pod "pod-projected-secrets-a98ecb98-58f7-4c0b-8c10-85f9c77183cc" satisfied condition "success or failure"
Jun 16 15:31:24.203: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-projected-secrets-a98ecb98-58f7-4c0b-8c10-85f9c77183cc container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 16 15:31:24.214: INFO: Waiting for pod pod-projected-secrets-a98ecb98-58f7-4c0b-8c10-85f9c77183cc to disappear
Jun 16 15:31:24.217: INFO: Pod pod-projected-secrets-a98ecb98-58f7-4c0b-8c10-85f9c77183cc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:31:24.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3150" for this suite.
Jun 16 15:31:30.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:31:30.275: INFO: namespace projected-3150 deletion completed in 6.056343396s

• [SLOW TEST:8.203 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:31:30.275: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9667
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Jun 16 15:31:30.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 create -f - --namespace=kubectl-9667'
Jun 16 15:31:30.708: INFO: stderr: ""
Jun 16 15:31:30.708: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 16 15:31:30.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9667'
Jun 16 15:31:30.772: INFO: stderr: ""
Jun 16 15:31:30.772: INFO: stdout: "update-demo-nautilus-4jd72 update-demo-nautilus-fr6x2 "
Jun 16 15:31:30.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-4jd72 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9667'
Jun 16 15:31:30.828: INFO: stderr: ""
Jun 16 15:31:30.828: INFO: stdout: ""
Jun 16 15:31:30.828: INFO: update-demo-nautilus-4jd72 is created but not running
Jun 16 15:31:35.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9667'
Jun 16 15:31:35.886: INFO: stderr: ""
Jun 16 15:31:35.886: INFO: stdout: "update-demo-nautilus-4jd72 update-demo-nautilus-fr6x2 "
Jun 16 15:31:35.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-4jd72 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9667'
Jun 16 15:31:35.941: INFO: stderr: ""
Jun 16 15:31:35.941: INFO: stdout: "true"
Jun 16 15:31:35.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-4jd72 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9667'
Jun 16 15:31:35.995: INFO: stderr: ""
Jun 16 15:31:35.995: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 16 15:31:35.995: INFO: validating pod update-demo-nautilus-4jd72
Jun 16 15:31:35.998: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 16 15:31:35.998: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 16 15:31:35.998: INFO: update-demo-nautilus-4jd72 is verified up and running
Jun 16 15:31:35.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-fr6x2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9667'
Jun 16 15:31:36.052: INFO: stderr: ""
Jun 16 15:31:36.052: INFO: stdout: "true"
Jun 16 15:31:36.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-fr6x2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9667'
Jun 16 15:31:36.106: INFO: stderr: ""
Jun 16 15:31:36.106: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 16 15:31:36.106: INFO: validating pod update-demo-nautilus-fr6x2
Jun 16 15:31:36.109: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 16 15:31:36.109: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 16 15:31:36.109: INFO: update-demo-nautilus-fr6x2 is verified up and running
STEP: using delete to clean up resources
Jun 16 15:31:36.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete --grace-period=0 --force -f - --namespace=kubectl-9667'
Jun 16 15:31:36.163: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 16 15:31:36.163: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 16 15:31:36.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9667'
Jun 16 15:31:36.222: INFO: stderr: "No resources found in kubectl-9667 namespace.\n"
Jun 16 15:31:36.222: INFO: stdout: ""
Jun 16 15:31:36.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -l name=update-demo --namespace=kubectl-9667 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 16 15:31:36.279: INFO: stderr: ""
Jun 16 15:31:36.279: INFO: stdout: "update-demo-nautilus-4jd72\nupdate-demo-nautilus-fr6x2\n"
Jun 16 15:31:36.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9667'
Jun 16 15:31:36.839: INFO: stderr: "No resources found in kubectl-9667 namespace.\n"
Jun 16 15:31:36.839: INFO: stdout: ""
Jun 16 15:31:36.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -l name=update-demo --namespace=kubectl-9667 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 16 15:31:36.897: INFO: stderr: ""
Jun 16 15:31:36.897: INFO: stdout: "update-demo-nautilus-4jd72\nupdate-demo-nautilus-fr6x2\n"
Jun 16 15:31:37.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9667'
Jun 16 15:31:37.369: INFO: stderr: "No resources found in kubectl-9667 namespace.\n"
Jun 16 15:31:37.369: INFO: stdout: ""
Jun 16 15:31:37.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -l name=update-demo --namespace=kubectl-9667 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 16 15:31:37.451: INFO: stderr: ""
Jun 16 15:31:37.451: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:31:37.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9667" for this suite.
Jun 16 15:31:43.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:31:43.571: INFO: namespace kubectl-9667 deletion completed in 6.117784353s

• [SLOW TEST:13.296 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:31:43.571: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-3269
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9079
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1485
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:31:56.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3269" for this suite.
Jun 16 15:32:02.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:32:03.005: INFO: namespace namespaces-3269 deletion completed in 6.054408757s
STEP: Destroying namespace "nsdeletetest-9079" for this suite.
Jun 16 15:32:03.006: INFO: Namespace nsdeletetest-9079 was already deleted
STEP: Destroying namespace "nsdeletetest-1485" for this suite.
Jun 16 15:32:09.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:32:09.158: INFO: namespace nsdeletetest-1485 deletion completed in 6.151843141s

• [SLOW TEST:25.587 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:32:09.158: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1491
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jun 16 15:32:09.280: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 16 15:32:09.286: INFO: Waiting for terminating namespaces to be deleted...
Jun 16 15:32:09.288: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-122.eu-west-1.compute.internal before test
Jun 16 15:32:09.292: INFO: kiam-agent-x89wb from kube-system started at 2020-06-16 15:10:38 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.292: INFO: 	Container kiam-agent ready: true, restart count 3
Jun 16 15:32:09.292: INFO: external-dns-776fc66667-24mls from kube-system started at 2020-06-16 15:10:42 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.292: INFO: 	Container external-dns ready: true, restart count 0
Jun 16 15:32:09.292: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-6qlkz from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 15:32:09.292: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 15:32:09.292: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 15:32:09.292: INFO: kube-proxy-xdgfc from kube-system started at 2020-06-16 15:06:53 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.292: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 15:32:09.292: INFO: node-exporter-5vz2p from kube-system started at 2020-06-16 15:10:31 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.292: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 15:32:09.292: INFO: cert-exporter-sgwbs from kube-system started at 2020-06-16 15:10:32 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.292: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 15:32:09.292: INFO: coredns-6d56c484c-wcs6v from kube-system started at 2020-06-16 15:10:34 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.292: INFO: 	Container coredns ready: true, restart count 0
Jun 16 15:32:09.292: INFO: aws-node-jbq4c from kube-system started at 2020-06-16 15:10:43 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.292: INFO: 	Container aws-node ready: true, restart count 0
Jun 16 15:32:09.292: INFO: kube-state-metrics-6d998ffd8b-jpbpj from kube-system started at 2020-06-16 15:10:43 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.292: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 16 15:32:09.292: INFO: net-exporter-ltbgf from kube-system started at 2020-06-16 15:14:31 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.292: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 15:32:09.292: INFO: calico-node-sk9rd from kube-system started at 2020-06-16 15:06:53 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.292: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 15:32:09.292: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-219.eu-west-1.compute.internal before test
Jun 16 15:32:09.296: INFO: aws-node-kmx2q from kube-system started at 2020-06-16 15:10:51 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.296: INFO: 	Container aws-node ready: true, restart count 0
Jun 16 15:32:09.296: INFO: kube-proxy-r2g9b from kube-system started at 2020-06-16 15:06:51 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.296: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 15:32:09.296: INFO: calico-node-xpkvc from kube-system started at 2020-06-16 15:06:51 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.296: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 15:32:09.296: INFO: node-exporter-pqhv4 from kube-system started at 2020-06-16 15:10:31 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.296: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 15:32:09.296: INFO: cert-exporter-fthcq from kube-system started at 2020-06-16 15:10:32 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.296: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 15:32:09.296: INFO: net-exporter-p7cpr from kube-system started at 2020-06-16 15:14:31 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.296: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 15:32:09.296: INFO: sonobuoy-e2e-job-40469eeb076c4210 from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 15:32:09.296: INFO: 	Container e2e ready: true, restart count 0
Jun 16 15:32:09.296: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 15:32:09.296: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-bbzmx from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 15:32:09.296: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 15:32:09.296: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 15:32:09.296: INFO: tiller-deploy-684c6b545b-xfl8b from giantswarm started at 2020-06-16 15:09:33 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.296: INFO: 	Container tiller ready: true, restart count 0
Jun 16 15:32:09.296: INFO: coredns-6d56c484c-l7qk9 from kube-system started at 2020-06-16 15:10:34 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.296: INFO: 	Container coredns ready: true, restart count 0
Jun 16 15:32:09.296: INFO: sonobuoy from sonobuoy started at 2020-06-16 15:26:40 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.296: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 16 15:32:09.296: INFO: kiam-agent-6q62w from kube-system started at 2020-06-16 15:10:38 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.296: INFO: 	Container kiam-agent ready: true, restart count 3
Jun 16 15:32:09.296: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-6.eu-west-1.compute.internal before test
Jun 16 15:32:09.300: INFO: kube-proxy-g6cmw from kube-system started at 2020-06-16 15:06:46 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.300: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 15:32:09.300: INFO: calico-node-ddwlg from kube-system started at 2020-06-16 15:06:46 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.300: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 15:32:09.300: INFO: aws-node-vjj2v from kube-system started at 2020-06-16 15:10:39 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.300: INFO: 	Container aws-node ready: true, restart count 0
Jun 16 15:32:09.300: INFO: node-exporter-z96vr from kube-system started at 2020-06-16 15:10:31 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.300: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 15:32:09.300: INFO: coredns-6d56c484c-8z6gz from kube-system started at 2020-06-16 15:10:34 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.300: INFO: 	Container coredns ready: true, restart count 0
Jun 16 15:32:09.300: INFO: cert-exporter-2k4kc from kube-system started at 2020-06-16 15:10:32 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.300: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 15:32:09.300: INFO: cert-manager-67dfd96fcd-n8nrk from kube-system started at 2020-06-16 15:10:36 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.300: INFO: 	Container cert-manager ready: true, restart count 0
Jun 16 15:32:09.300: INFO: kiam-agent-5xldj from kube-system started at 2020-06-16 15:10:38 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.300: INFO: 	Container kiam-agent ready: true, restart count 3
Jun 16 15:32:09.300: INFO: metrics-server-66df9f5b56-mgt2q from kube-system started at 2020-06-16 15:10:44 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.300: INFO: 	Container metrics-server ready: true, restart count 0
Jun 16 15:32:09.300: INFO: net-exporter-5wtl4 from kube-system started at 2020-06-16 15:14:31 +0000 UTC (1 container statuses recorded)
Jun 16 15:32:09.300: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 15:32:09.300: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-87l2v from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 15:32:09.300: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 15:32:09.300: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node ip-172-19-65-122.eu-west-1.compute.internal
STEP: verifying the node has the label node ip-172-19-65-219.eu-west-1.compute.internal
STEP: verifying the node has the label node ip-172-19-65-6.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod tiller-deploy-684c6b545b-xfl8b requesting resource cpu=0m on Node ip-172-19-65-219.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod aws-node-jbq4c requesting resource cpu=30m on Node ip-172-19-65-122.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod aws-node-kmx2q requesting resource cpu=30m on Node ip-172-19-65-219.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod aws-node-vjj2v requesting resource cpu=30m on Node ip-172-19-65-6.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod calico-node-ddwlg requesting resource cpu=250m on Node ip-172-19-65-6.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod calico-node-sk9rd requesting resource cpu=250m on Node ip-172-19-65-122.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod calico-node-xpkvc requesting resource cpu=250m on Node ip-172-19-65-219.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod cert-exporter-2k4kc requesting resource cpu=50m on Node ip-172-19-65-6.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod cert-exporter-fthcq requesting resource cpu=50m on Node ip-172-19-65-219.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod cert-exporter-sgwbs requesting resource cpu=50m on Node ip-172-19-65-122.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod cert-manager-67dfd96fcd-n8nrk requesting resource cpu=50m on Node ip-172-19-65-6.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod coredns-6d56c484c-8z6gz requesting resource cpu=250m on Node ip-172-19-65-6.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod coredns-6d56c484c-l7qk9 requesting resource cpu=250m on Node ip-172-19-65-219.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod coredns-6d56c484c-wcs6v requesting resource cpu=250m on Node ip-172-19-65-122.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod external-dns-776fc66667-24mls requesting resource cpu=50m on Node ip-172-19-65-122.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod kiam-agent-5xldj requesting resource cpu=50m on Node ip-172-19-65-6.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod kiam-agent-6q62w requesting resource cpu=50m on Node ip-172-19-65-219.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod kiam-agent-x89wb requesting resource cpu=50m on Node ip-172-19-65-122.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod kube-proxy-g6cmw requesting resource cpu=75m on Node ip-172-19-65-6.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod kube-proxy-r2g9b requesting resource cpu=75m on Node ip-172-19-65-219.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod kube-proxy-xdgfc requesting resource cpu=75m on Node ip-172-19-65-122.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod kube-state-metrics-6d998ffd8b-jpbpj requesting resource cpu=500m on Node ip-172-19-65-122.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod metrics-server-66df9f5b56-mgt2q requesting resource cpu=0m on Node ip-172-19-65-6.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod net-exporter-5wtl4 requesting resource cpu=50m on Node ip-172-19-65-6.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod net-exporter-ltbgf requesting resource cpu=50m on Node ip-172-19-65-122.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod net-exporter-p7cpr requesting resource cpu=50m on Node ip-172-19-65-219.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod node-exporter-5vz2p requesting resource cpu=75m on Node ip-172-19-65-122.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod node-exporter-pqhv4 requesting resource cpu=75m on Node ip-172-19-65-219.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod node-exporter-z96vr requesting resource cpu=75m on Node ip-172-19-65-6.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-19-65-219.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod sonobuoy-e2e-job-40469eeb076c4210 requesting resource cpu=0m on Node ip-172-19-65-219.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-6qlkz requesting resource cpu=0m on Node ip-172-19-65-122.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-87l2v requesting resource cpu=0m on Node ip-172-19-65-6.eu-west-1.compute.internal
Jun 16 15:32:09.343: INFO: Pod sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-bbzmx requesting resource cpu=0m on Node ip-172-19-65-219.eu-west-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
Jun 16 15:32:09.343: INFO: Creating a pod which consumes cpu=1484m on Node ip-172-19-65-122.eu-west-1.compute.internal
Jun 16 15:32:09.347: INFO: Creating a pod which consumes cpu=1869m on Node ip-172-19-65-219.eu-west-1.compute.internal
Jun 16 15:32:09.353: INFO: Creating a pod which consumes cpu=1834m on Node ip-172-19-65-6.eu-west-1.compute.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-16ee8bdd-337e-4d49-a3c1-c7ee3d11363d.16190ffe953fb35e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1491/filler-pod-16ee8bdd-337e-4d49-a3c1-c7ee3d11363d to ip-172-19-65-122.eu-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-16ee8bdd-337e-4d49-a3c1-c7ee3d11363d.16190ffeb547a8ae], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-16ee8bdd-337e-4d49-a3c1-c7ee3d11363d.16190ffeb6dae3fb], Reason = [Created], Message = [Created container filler-pod-16ee8bdd-337e-4d49-a3c1-c7ee3d11363d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-16ee8bdd-337e-4d49-a3c1-c7ee3d11363d.16190ffebc70fb78], Reason = [Started], Message = [Started container filler-pod-16ee8bdd-337e-4d49-a3c1-c7ee3d11363d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-51ab1500-2aca-4ef2-9837-8439d0b6cbe4.16190ffe95dda314], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1491/filler-pod-51ab1500-2aca-4ef2-9837-8439d0b6cbe4 to ip-172-19-65-219.eu-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-51ab1500-2aca-4ef2-9837-8439d0b6cbe4.16190ffeb6375e4c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-51ab1500-2aca-4ef2-9837-8439d0b6cbe4.16190ffeb8324980], Reason = [Created], Message = [Created container filler-pod-51ab1500-2aca-4ef2-9837-8439d0b6cbe4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-51ab1500-2aca-4ef2-9837-8439d0b6cbe4.16190ffebde9c4ac], Reason = [Started], Message = [Started container filler-pod-51ab1500-2aca-4ef2-9837-8439d0b6cbe4]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca80ec5b-96d1-4a41-bda9-b8b3ddfe9225.16190ffe95e84ab9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1491/filler-pod-ca80ec5b-96d1-4a41-bda9-b8b3ddfe9225 to ip-172-19-65-6.eu-west-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca80ec5b-96d1-4a41-bda9-b8b3ddfe9225.16190ffeb6d45916], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca80ec5b-96d1-4a41-bda9-b8b3ddfe9225.16190ffeb8a8978b], Reason = [Created], Message = [Created container filler-pod-ca80ec5b-96d1-4a41-bda9-b8b3ddfe9225]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ca80ec5b-96d1-4a41-bda9-b8b3ddfe9225.16190ffebddfc215], Reason = [Started], Message = [Started container filler-pod-ca80ec5b-96d1-4a41-bda9-b8b3ddfe9225]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16190fff0dc3be7c], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node ip-172-19-65-6.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-19-65-122.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-19-65-219.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:32:12.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1491" for this suite.
Jun 16 15:32:18.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:32:18.465: INFO: namespace sched-pred-1491 deletion completed in 6.062446472s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:9.307 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:32:18.465: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3048
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 16 15:32:18.592: INFO: Waiting up to 5m0s for pod "pod-24fe8112-a35f-4f62-86cd-ade542f77e00" in namespace "emptydir-3048" to be "success or failure"
Jun 16 15:32:18.594: INFO: Pod "pod-24fe8112-a35f-4f62-86cd-ade542f77e00": Phase="Pending", Reason="", readiness=false. Elapsed: 1.795961ms
Jun 16 15:32:20.596: INFO: Pod "pod-24fe8112-a35f-4f62-86cd-ade542f77e00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004077962s
STEP: Saw pod success
Jun 16 15:32:20.596: INFO: Pod "pod-24fe8112-a35f-4f62-86cd-ade542f77e00" satisfied condition "success or failure"
Jun 16 15:32:20.598: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-24fe8112-a35f-4f62-86cd-ade542f77e00 container test-container: <nil>
STEP: delete the pod
Jun 16 15:32:20.608: INFO: Waiting for pod pod-24fe8112-a35f-4f62-86cd-ade542f77e00 to disappear
Jun 16 15:32:20.609: INFO: Pod pod-24fe8112-a35f-4f62-86cd-ade542f77e00 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:32:20.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3048" for this suite.
Jun 16 15:32:26.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:32:26.674: INFO: namespace emptydir-3048 deletion completed in 6.057675516s

• [SLOW TEST:8.209 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:32:26.674: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-4458
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 16 15:32:26.934: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun 16 15:32:28.939: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918346, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918346, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918346, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727918346, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 15:32:31.946: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 15:32:31.947: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:32:32.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4458" for this suite.
Jun 16 15:32:38.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:32:38.685: INFO: namespace crd-webhook-4458 deletion completed in 6.056791955s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:12.162 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:32:38.837: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4624
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 16 15:32:41.470: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4624 pod-service-account-38a888dc-4288-427c-8a10-0f9b6eeec589 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 16 15:32:41.615: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4624 pod-service-account-38a888dc-4288-427c-8a10-0f9b6eeec589 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 16 15:32:41.741: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4624 pod-service-account-38a888dc-4288-427c-8a10-0f9b6eeec589 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:32:41.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4624" for this suite.
Jun 16 15:32:47.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:32:47.946: INFO: namespace svcaccounts-4624 deletion completed in 6.053237788s

• [SLOW TEST:9.109 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:32:47.946: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3760
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jun 16 15:32:50.587: INFO: Successfully updated pod "annotationupdatea46f3b45-d12e-4699-b706-4fc0bf968b45"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:32:54.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3760" for this suite.
Jun 16 15:33:08.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:33:08.662: INFO: namespace downward-api-3760 deletion completed in 14.054834348s

• [SLOW TEST:20.716 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:33:08.662: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-282
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 16 15:33:08.786: INFO: Waiting up to 5m0s for pod "pod-ec791f9d-ca28-45c7-8143-da0f2e09cb9a" in namespace "emptydir-282" to be "success or failure"
Jun 16 15:33:08.788: INFO: Pod "pod-ec791f9d-ca28-45c7-8143-da0f2e09cb9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.436906ms
Jun 16 15:33:10.790: INFO: Pod "pod-ec791f9d-ca28-45c7-8143-da0f2e09cb9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004568768s
STEP: Saw pod success
Jun 16 15:33:10.790: INFO: Pod "pod-ec791f9d-ca28-45c7-8143-da0f2e09cb9a" satisfied condition "success or failure"
Jun 16 15:33:10.792: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-ec791f9d-ca28-45c7-8143-da0f2e09cb9a container test-container: <nil>
STEP: delete the pod
Jun 16 15:33:10.802: INFO: Waiting for pod pod-ec791f9d-ca28-45c7-8143-da0f2e09cb9a to disappear
Jun 16 15:33:10.803: INFO: Pod pod-ec791f9d-ca28-45c7-8143-da0f2e09cb9a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:33:10.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-282" for this suite.
Jun 16 15:33:16.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:33:16.860: INFO: namespace emptydir-282 deletion completed in 6.054428173s

• [SLOW TEST:8.198 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:33:16.860: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8057
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Jun 16 15:33:16.985: INFO: Waiting up to 5m0s for pod "var-expansion-1e178f05-bee6-4efe-b7f2-4d874012cc05" in namespace "var-expansion-8057" to be "success or failure"
Jun 16 15:33:16.987: INFO: Pod "var-expansion-1e178f05-bee6-4efe-b7f2-4d874012cc05": Phase="Pending", Reason="", readiness=false. Elapsed: 1.984902ms
Jun 16 15:33:18.989: INFO: Pod "var-expansion-1e178f05-bee6-4efe-b7f2-4d874012cc05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004149967s
STEP: Saw pod success
Jun 16 15:33:18.989: INFO: Pod "var-expansion-1e178f05-bee6-4efe-b7f2-4d874012cc05" satisfied condition "success or failure"
Jun 16 15:33:18.991: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod var-expansion-1e178f05-bee6-4efe-b7f2-4d874012cc05 container dapi-container: <nil>
STEP: delete the pod
Jun 16 15:33:19.000: INFO: Waiting for pod var-expansion-1e178f05-bee6-4efe-b7f2-4d874012cc05 to disappear
Jun 16 15:33:19.002: INFO: Pod var-expansion-1e178f05-bee6-4efe-b7f2-4d874012cc05 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:33:19.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8057" for this suite.
Jun 16 15:33:25.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:33:25.060: INFO: namespace var-expansion-8057 deletion completed in 6.056088452s

• [SLOW TEST:8.200 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:33:25.060: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6714
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-6714
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6714 to expose endpoints map[]
Jun 16 15:33:25.188: INFO: Get endpoints failed (1.588438ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jun 16 15:33:26.190: INFO: successfully validated that service endpoint-test2 in namespace services-6714 exposes endpoints map[] (1.003636682s elapsed)
STEP: Creating pod pod1 in namespace services-6714
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6714 to expose endpoints map[pod1:[80]]
Jun 16 15:33:27.201: INFO: successfully validated that service endpoint-test2 in namespace services-6714 exposes endpoints map[pod1:[80]] (1.006548552s elapsed)
STEP: Creating pod pod2 in namespace services-6714
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6714 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 16 15:33:31.232: INFO: Unexpected endpoints: found map[9e289e55-bfca-4fed-a04d-a6772279b968:[80]], expected map[pod1:[80] pod2:[80]] (4.028015202s elapsed, will retry)
Jun 16 15:33:32.238: INFO: successfully validated that service endpoint-test2 in namespace services-6714 exposes endpoints map[pod1:[80] pod2:[80]] (5.034501699s elapsed)
STEP: Deleting pod pod1 in namespace services-6714
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6714 to expose endpoints map[pod2:[80]]
Jun 16 15:33:33.253: INFO: successfully validated that service endpoint-test2 in namespace services-6714 exposes endpoints map[pod2:[80]] (1.012029983s elapsed)
STEP: Deleting pod pod2 in namespace services-6714
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6714 to expose endpoints map[]
Jun 16 15:33:34.260: INFO: successfully validated that service endpoint-test2 in namespace services-6714 exposes endpoints map[] (1.004050058s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:33:34.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6714" for this suite.
Jun 16 15:33:40.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:33:40.329: INFO: namespace services-6714 deletion completed in 6.056993638s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:15.269 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:33:40.329: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8696
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jun 16 15:33:42.970: INFO: Successfully updated pod "labelsupdatedc720c98-8eeb-4107-a09a-9e2fe2448f50"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:33:46.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8696" for this suite.
Jun 16 15:33:58.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:33:59.044: INFO: namespace downward-api-8696 deletion completed in 12.056653149s

• [SLOW TEST:18.716 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:33:59.045: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2717
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-2717
STEP: creating replication controller nodeport-test in namespace services-2717
I0616 15:33:59.174497      24 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-2717, replica count: 2
Jun 16 15:34:02.224: INFO: Creating new exec pod
I0616 15:34:02.224791      24 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 16 15:34:05.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=services-2717 execpoddxmqr -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Jun 16 15:34:05.393: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 16 15:34:05.393: INFO: stdout: ""
Jun 16 15:34:05.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=services-2717 execpoddxmqr -- /bin/sh -x -c nc -zv -t -w 2 172.31.76.43 80'
Jun 16 15:34:05.531: INFO: stderr: "+ nc -zv -t -w 2 172.31.76.43 80\nConnection to 172.31.76.43 80 port [tcp/http] succeeded!\n"
Jun 16 15:34:05.531: INFO: stdout: ""
Jun 16 15:34:05.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=services-2717 execpoddxmqr -- /bin/sh -x -c nc -zv -t -w 2 172.19.65.122 30209'
Jun 16 15:34:05.670: INFO: stderr: "+ nc -zv -t -w 2 172.19.65.122 30209\nConnection to 172.19.65.122 30209 port [tcp/30209] succeeded!\n"
Jun 16 15:34:05.670: INFO: stdout: ""
Jun 16 15:34:05.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=services-2717 execpoddxmqr -- /bin/sh -x -c nc -zv -t -w 2 172.19.65.219 30209'
Jun 16 15:34:05.808: INFO: stderr: "+ nc -zv -t -w 2 172.19.65.219 30209\nConnection to 172.19.65.219 30209 port [tcp/30209] succeeded!\n"
Jun 16 15:34:05.808: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:34:05.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2717" for this suite.
Jun 16 15:34:11.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:34:11.866: INFO: namespace services-2717 deletion completed in 6.056044667s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:12.822 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:34:11.867: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7294
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:34:14.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7294" for this suite.
Jun 16 15:34:58.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:34:58.059: INFO: namespace kubelet-test-7294 deletion completed in 44.053772765s

• [SLOW TEST:46.192 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:34:58.059: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6730
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 15:34:58.741: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 15:35:01.752: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:35:01.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6730" for this suite.
Jun 16 15:35:07.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:35:07.850: INFO: namespace webhook-6730 deletion completed in 6.066919459s
STEP: Destroying namespace "webhook-6730-markers" for this suite.
Jun 16 15:35:13.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:35:13.946: INFO: namespace webhook-6730-markers deletion completed in 6.09607973s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.037 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:35:14.096: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8265
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-a63770d4-b923-4917-9deb-4daebc99e4c5
STEP: Creating a pod to test consume configMaps
Jun 16 15:35:14.229: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2ed7ad02-5456-47d7-8b5f-f7738965d39d" in namespace "projected-8265" to be "success or failure"
Jun 16 15:35:14.230: INFO: Pod "pod-projected-configmaps-2ed7ad02-5456-47d7-8b5f-f7738965d39d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.52862ms
Jun 16 15:35:16.232: INFO: Pod "pod-projected-configmaps-2ed7ad02-5456-47d7-8b5f-f7738965d39d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003707288s
STEP: Saw pod success
Jun 16 15:35:16.232: INFO: Pod "pod-projected-configmaps-2ed7ad02-5456-47d7-8b5f-f7738965d39d" satisfied condition "success or failure"
Jun 16 15:35:16.234: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-projected-configmaps-2ed7ad02-5456-47d7-8b5f-f7738965d39d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 16 15:35:16.244: INFO: Waiting for pod pod-projected-configmaps-2ed7ad02-5456-47d7-8b5f-f7738965d39d to disappear
Jun 16 15:35:16.246: INFO: Pod pod-projected-configmaps-2ed7ad02-5456-47d7-8b5f-f7738965d39d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:35:16.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8265" for this suite.
Jun 16 15:35:22.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:35:22.305: INFO: namespace projected-8265 deletion completed in 6.055294018s

• [SLOW TEST:8.210 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:35:22.306: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9973
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 15:35:22.433: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:35:23.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9973" for this suite.
Jun 16 15:35:29.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:35:29.503: INFO: namespace custom-resource-definition-9973 deletion completed in 6.056691522s

• [SLOW TEST:7.198 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:35:29.504: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4755
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4755
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4755
STEP: creating replication controller externalsvc in namespace services-4755
I0616 15:35:29.637991      24 runners.go:184] Created replication controller with name: externalsvc, namespace: services-4755, replica count: 2
I0616 15:35:32.688289      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jun 16 15:35:32.696: INFO: Creating new exec pod
Jun 16 15:35:34.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=services-4755 execpod5ztnk -- /bin/sh -x -c nslookup clusterip-service'
Jun 16 15:35:34.863: INFO: stderr: "+ nslookup clusterip-service\n"
Jun 16 15:35:34.863: INFO: stdout: "Server:\t\t172.31.0.10\nAddress:\t172.31.0.10#53\n\nclusterip-service.services-4755.svc.cluster.local\tcanonical name = externalsvc.services-4755.svc.cluster.local.\nName:\texternalsvc.services-4755.svc.cluster.local\nAddress: 172.31.140.246\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4755, will wait for the garbage collector to delete the pods
Jun 16 15:35:34.918: INFO: Deleting ReplicationController externalsvc took: 3.392091ms
Jun 16 15:35:35.018: INFO: Terminating ReplicationController externalsvc pods took: 100.146405ms
Jun 16 15:35:46.825: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:35:46.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4755" for this suite.
Jun 16 15:35:52.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:35:52.888: INFO: namespace services-4755 deletion completed in 6.05541785s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:23.384 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:35:52.888: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9042
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 16 15:35:53.023: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:35:53.024: INFO: Number of nodes with available pods: 0
Jun 16 15:35:53.024: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:35:54.027: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:35:54.029: INFO: Number of nodes with available pods: 0
Jun 16 15:35:54.029: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:35:55.027: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:35:55.028: INFO: Number of nodes with available pods: 1
Jun 16 15:35:55.028: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:35:56.027: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:35:56.029: INFO: Number of nodes with available pods: 1
Jun 16 15:35:56.029: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:35:57.027: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:35:57.029: INFO: Number of nodes with available pods: 1
Jun 16 15:35:57.029: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:35:58.027: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:35:58.029: INFO: Number of nodes with available pods: 1
Jun 16 15:35:58.029: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:35:59.027: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:35:59.028: INFO: Number of nodes with available pods: 3
Jun 16 15:35:59.029: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 16 15:35:59.037: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:35:59.039: INFO: Number of nodes with available pods: 2
Jun 16 15:35:59.039: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:36:00.041: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:36:00.043: INFO: Number of nodes with available pods: 2
Jun 16 15:36:00.043: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:36:01.041: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:36:01.043: INFO: Number of nodes with available pods: 2
Jun 16 15:36:01.043: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:36:02.042: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:36:02.044: INFO: Number of nodes with available pods: 2
Jun 16 15:36:02.044: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:36:03.042: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:36:03.043: INFO: Number of nodes with available pods: 2
Jun 16 15:36:03.043: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:36:04.041: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:36:04.044: INFO: Number of nodes with available pods: 2
Jun 16 15:36:04.044: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:36:05.041: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:36:05.043: INFO: Number of nodes with available pods: 2
Jun 16 15:36:05.043: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:36:06.041: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:36:06.043: INFO: Number of nodes with available pods: 2
Jun 16 15:36:06.043: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:36:07.041: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:36:07.043: INFO: Number of nodes with available pods: 2
Jun 16 15:36:07.043: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:36:08.041: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:36:08.043: INFO: Number of nodes with available pods: 2
Jun 16 15:36:08.043: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:36:09.041: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:36:09.043: INFO: Number of nodes with available pods: 2
Jun 16 15:36:09.043: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:36:10.041: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:36:10.044: INFO: Number of nodes with available pods: 2
Jun 16 15:36:10.044: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:36:11.042: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:36:11.043: INFO: Number of nodes with available pods: 2
Jun 16 15:36:11.043: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:36:12.041: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:36:12.043: INFO: Number of nodes with available pods: 2
Jun 16 15:36:12.043: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:36:13.041: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:36:13.043: INFO: Number of nodes with available pods: 2
Jun 16 15:36:13.043: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:36:14.042: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:36:14.043: INFO: Number of nodes with available pods: 2
Jun 16 15:36:14.043: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 15:36:15.041: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 15:36:15.043: INFO: Number of nodes with available pods: 3
Jun 16 15:36:15.043: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9042, will wait for the garbage collector to delete the pods
Jun 16 15:36:15.099: INFO: Deleting DaemonSet.extensions daemon-set took: 3.394698ms
Jun 16 15:36:15.200: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.179032ms
Jun 16 15:36:26.902: INFO: Number of nodes with available pods: 0
Jun 16 15:36:26.902: INFO: Number of running nodes: 0, number of available pods: 0
Jun 16 15:36:26.904: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9042/daemonsets","resourceVersion":"7211"},"items":null}

Jun 16 15:36:26.906: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9042/pods","resourceVersion":"7211"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:36:26.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9042" for this suite.
Jun 16 15:36:32.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:36:32.969: INFO: namespace daemonsets-9042 deletion completed in 6.054805262s

• [SLOW TEST:40.081 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:36:32.969: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3371
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Jun 16 15:36:33.094: INFO: Waiting up to 5m0s for pod "var-expansion-ea2d7e79-cd35-408c-80c1-4c8fdac0098b" in namespace "var-expansion-3371" to be "success or failure"
Jun 16 15:36:33.100: INFO: Pod "var-expansion-ea2d7e79-cd35-408c-80c1-4c8fdac0098b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.868096ms
Jun 16 15:36:35.102: INFO: Pod "var-expansion-ea2d7e79-cd35-408c-80c1-4c8fdac0098b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007949338s
STEP: Saw pod success
Jun 16 15:36:35.102: INFO: Pod "var-expansion-ea2d7e79-cd35-408c-80c1-4c8fdac0098b" satisfied condition "success or failure"
Jun 16 15:36:35.104: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod var-expansion-ea2d7e79-cd35-408c-80c1-4c8fdac0098b container dapi-container: <nil>
STEP: delete the pod
Jun 16 15:36:35.113: INFO: Waiting for pod var-expansion-ea2d7e79-cd35-408c-80c1-4c8fdac0098b to disappear
Jun 16 15:36:35.114: INFO: Pod var-expansion-ea2d7e79-cd35-408c-80c1-4c8fdac0098b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:36:35.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3371" for this suite.
Jun 16 15:36:41.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:36:41.175: INFO: namespace var-expansion-3371 deletion completed in 6.058958829s

• [SLOW TEST:8.206 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:36:41.175: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-873
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 15:37:01.310: INFO: Container started at 2020-06-16 15:36:43 +0000 UTC, pod became ready at 2020-06-16 15:37:00 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:37:01.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-873" for this suite.
Jun 16 15:37:29.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:37:29.366: INFO: namespace container-probe-873 deletion completed in 28.054639273s

• [SLOW TEST:48.191 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:37:29.367: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9480
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 15:37:29.494: INFO: Waiting up to 5m0s for pod "downwardapi-volume-193a05b9-a197-41ca-b429-c1b8448ba2d0" in namespace "downward-api-9480" to be "success or failure"
Jun 16 15:37:29.499: INFO: Pod "downwardapi-volume-193a05b9-a197-41ca-b429-c1b8448ba2d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.370183ms
Jun 16 15:37:31.501: INFO: Pod "downwardapi-volume-193a05b9-a197-41ca-b429-c1b8448ba2d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006545124s
STEP: Saw pod success
Jun 16 15:37:31.501: INFO: Pod "downwardapi-volume-193a05b9-a197-41ca-b429-c1b8448ba2d0" satisfied condition "success or failure"
Jun 16 15:37:31.502: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod downwardapi-volume-193a05b9-a197-41ca-b429-c1b8448ba2d0 container client-container: <nil>
STEP: delete the pod
Jun 16 15:37:31.513: INFO: Waiting for pod downwardapi-volume-193a05b9-a197-41ca-b429-c1b8448ba2d0 to disappear
Jun 16 15:37:31.514: INFO: Pod downwardapi-volume-193a05b9-a197-41ca-b429-c1b8448ba2d0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:37:31.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9480" for this suite.
Jun 16 15:37:37.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:37:37.574: INFO: namespace downward-api-9480 deletion completed in 6.058043338s

• [SLOW TEST:8.208 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:37:37.574: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2328
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2272
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3782
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:37:43.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2328" for this suite.
Jun 16 15:37:49.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:37:50.005: INFO: namespace namespaces-2328 deletion completed in 6.056638703s
STEP: Destroying namespace "nsdeletetest-2272" for this suite.
Jun 16 15:37:50.007: INFO: Namespace nsdeletetest-2272 was already deleted
STEP: Destroying namespace "nsdeletetest-3782" for this suite.
Jun 16 15:37:56.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:37:56.154: INFO: namespace nsdeletetest-3782 deletion completed in 6.147751571s

• [SLOW TEST:18.580 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:37:56.155: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6250
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 15:37:56.758: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 15:37:59.767: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 15:37:59.769: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:38:00.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6250" for this suite.
Jun 16 15:38:06.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:38:06.890: INFO: namespace webhook-6250 deletion completed in 6.05258087s
STEP: Destroying namespace "webhook-6250-markers" for this suite.
Jun 16 15:38:12.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:38:12.994: INFO: namespace webhook-6250-markers deletion completed in 6.104218208s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.995 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:38:13.150: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7038
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-a8021394-f066-4bc1-91a7-8a78b0a2ae4c
STEP: Creating secret with name secret-projected-all-test-volume-b49e3d95-e67f-4ddd-af94-1105865d5f27
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 16 15:38:13.278: INFO: Waiting up to 5m0s for pod "projected-volume-386cedf0-c092-4b8d-b2a0-a4db989ed021" in namespace "projected-7038" to be "success or failure"
Jun 16 15:38:13.280: INFO: Pod "projected-volume-386cedf0-c092-4b8d-b2a0-a4db989ed021": Phase="Pending", Reason="", readiness=false. Elapsed: 1.968208ms
Jun 16 15:38:15.282: INFO: Pod "projected-volume-386cedf0-c092-4b8d-b2a0-a4db989ed021": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004014277s
STEP: Saw pod success
Jun 16 15:38:15.282: INFO: Pod "projected-volume-386cedf0-c092-4b8d-b2a0-a4db989ed021" satisfied condition "success or failure"
Jun 16 15:38:15.283: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod projected-volume-386cedf0-c092-4b8d-b2a0-a4db989ed021 container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 16 15:38:15.292: INFO: Waiting for pod projected-volume-386cedf0-c092-4b8d-b2a0-a4db989ed021 to disappear
Jun 16 15:38:15.294: INFO: Pod projected-volume-386cedf0-c092-4b8d-b2a0-a4db989ed021 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:38:15.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7038" for this suite.
Jun 16 15:38:21.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:38:21.352: INFO: namespace projected-7038 deletion completed in 6.056082553s

• [SLOW TEST:8.202 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:38:21.352: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2067
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 16 15:38:21.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2067'
Jun 16 15:38:21.533: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 16 15:38:21.533: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Jun 16 15:38:21.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete jobs e2e-test-httpd-job --namespace=kubectl-2067'
Jun 16 15:38:21.594: INFO: stderr: ""
Jun 16 15:38:21.594: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:38:21.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2067" for this suite.
Jun 16 15:38:33.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:38:33.651: INFO: namespace kubectl-2067 deletion completed in 12.055445477s

• [SLOW TEST:12.299 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:38:33.652: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7171
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-7825d6b3-50cd-4203-ae66-4506c47fff63
STEP: Creating a pod to test consume secrets
Jun 16 15:38:33.783: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-45c4a158-ba1a-4d48-904c-9cb820da0abb" in namespace "projected-7171" to be "success or failure"
Jun 16 15:38:33.794: INFO: Pod "pod-projected-secrets-45c4a158-ba1a-4d48-904c-9cb820da0abb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.974399ms
Jun 16 15:38:35.797: INFO: Pod "pod-projected-secrets-45c4a158-ba1a-4d48-904c-9cb820da0abb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013175838s
STEP: Saw pod success
Jun 16 15:38:35.797: INFO: Pod "pod-projected-secrets-45c4a158-ba1a-4d48-904c-9cb820da0abb" satisfied condition "success or failure"
Jun 16 15:38:35.799: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-projected-secrets-45c4a158-ba1a-4d48-904c-9cb820da0abb container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 16 15:38:35.808: INFO: Waiting for pod pod-projected-secrets-45c4a158-ba1a-4d48-904c-9cb820da0abb to disappear
Jun 16 15:38:35.810: INFO: Pod pod-projected-secrets-45c4a158-ba1a-4d48-904c-9cb820da0abb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:38:35.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7171" for this suite.
Jun 16 15:38:41.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:38:41.870: INFO: namespace projected-7171 deletion completed in 6.057487105s

• [SLOW TEST:8.219 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:38:41.870: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8916
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 15:38:41.996: INFO: Waiting up to 5m0s for pod "downwardapi-volume-52c0a081-18fd-499c-9036-da761ca80fbc" in namespace "downward-api-8916" to be "success or failure"
Jun 16 15:38:42.000: INFO: Pod "downwardapi-volume-52c0a081-18fd-499c-9036-da761ca80fbc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061197ms
Jun 16 15:38:44.002: INFO: Pod "downwardapi-volume-52c0a081-18fd-499c-9036-da761ca80fbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006371173s
STEP: Saw pod success
Jun 16 15:38:44.002: INFO: Pod "downwardapi-volume-52c0a081-18fd-499c-9036-da761ca80fbc" satisfied condition "success or failure"
Jun 16 15:38:44.004: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod downwardapi-volume-52c0a081-18fd-499c-9036-da761ca80fbc container client-container: <nil>
STEP: delete the pod
Jun 16 15:38:44.014: INFO: Waiting for pod downwardapi-volume-52c0a081-18fd-499c-9036-da761ca80fbc to disappear
Jun 16 15:38:44.016: INFO: Pod downwardapi-volume-52c0a081-18fd-499c-9036-da761ca80fbc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:38:44.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8916" for this suite.
Jun 16 15:38:50.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:38:50.073: INFO: namespace downward-api-8916 deletion completed in 6.054619338s

• [SLOW TEST:8.202 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:38:50.073: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 16 15:38:50.430: INFO: Pod name wrapped-volume-race-f8b660c2-a386-4672-87ca-b580b2c4eb81: Found 4 pods out of 5
Jun 16 15:38:55.433: INFO: Pod name wrapped-volume-race-f8b660c2-a386-4672-87ca-b580b2c4eb81: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f8b660c2-a386-4672-87ca-b580b2c4eb81 in namespace emptydir-wrapper-169, will wait for the garbage collector to delete the pods
Jun 16 15:39:11.501: INFO: Deleting ReplicationController wrapped-volume-race-f8b660c2-a386-4672-87ca-b580b2c4eb81 took: 3.815258ms
Jun 16 15:39:11.601: INFO: Terminating ReplicationController wrapped-volume-race-f8b660c2-a386-4672-87ca-b580b2c4eb81 pods took: 100.178462ms
STEP: Creating RC which spawns configmap-volume pods
Jun 16 15:39:53.510: INFO: Pod name wrapped-volume-race-4b0f5647-993f-4702-be40-3b68b8bff6c5: Found 0 pods out of 5
Jun 16 15:39:58.514: INFO: Pod name wrapped-volume-race-4b0f5647-993f-4702-be40-3b68b8bff6c5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4b0f5647-993f-4702-be40-3b68b8bff6c5 in namespace emptydir-wrapper-169, will wait for the garbage collector to delete the pods
Jun 16 15:40:08.579: INFO: Deleting ReplicationController wrapped-volume-race-4b0f5647-993f-4702-be40-3b68b8bff6c5 took: 3.780886ms
Jun 16 15:40:08.680: INFO: Terminating ReplicationController wrapped-volume-race-4b0f5647-993f-4702-be40-3b68b8bff6c5 pods took: 100.165326ms
STEP: Creating RC which spawns configmap-volume pods
Jun 16 15:40:53.591: INFO: Pod name wrapped-volume-race-9c9d58b7-2d67-41dc-84fc-a2418a88d8b9: Found 0 pods out of 5
Jun 16 15:40:58.594: INFO: Pod name wrapped-volume-race-9c9d58b7-2d67-41dc-84fc-a2418a88d8b9: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-9c9d58b7-2d67-41dc-84fc-a2418a88d8b9 in namespace emptydir-wrapper-169, will wait for the garbage collector to delete the pods
Jun 16 15:41:08.660: INFO: Deleting ReplicationController wrapped-volume-race-9c9d58b7-2d67-41dc-84fc-a2418a88d8b9 took: 3.955047ms
Jun 16 15:41:08.760: INFO: Terminating ReplicationController wrapped-volume-race-9c9d58b7-2d67-41dc-84fc-a2418a88d8b9 pods took: 100.164714ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:41:53.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-169" for this suite.
Jun 16 15:41:59.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:41:59.763: INFO: namespace emptydir-wrapper-169 deletion completed in 6.075117702s

• [SLOW TEST:189.691 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:41:59.764: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-780
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 16 15:42:01.894: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:42:01.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-780" for this suite.
Jun 16 15:42:07.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:42:07.967: INFO: namespace container-runtime-780 deletion completed in 6.062486819s

• [SLOW TEST:8.203 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:42:07.967: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7534
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Jun 16 15:42:08.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 api-versions'
Jun 16 15:42:08.143: INFO: stderr: ""
Jun 16 15:42:08.143: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napplication.giantswarm.io/v1alpha1\napps/v1\napps/v1beta1\napps/v1beta2\nauditregistration.k8s.io/v1alpha1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncertmanager.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.k8s.amazonaws.com/v1alpha1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1alpha1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1alpha1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1alpha1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nsettings.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1alpha1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:42:08.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7534" for this suite.
Jun 16 15:42:14.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:42:14.209: INFO: namespace kubectl-7534 deletion completed in 6.063735322s

• [SLOW TEST:6.242 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:42:14.209: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9304
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jun 16 15:42:14.335: INFO: Waiting up to 5m0s for pod "downward-api-128580ec-448a-4171-a407-d6cfeb07fafb" in namespace "downward-api-9304" to be "success or failure"
Jun 16 15:42:14.337: INFO: Pod "downward-api-128580ec-448a-4171-a407-d6cfeb07fafb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.089513ms
Jun 16 15:42:16.339: INFO: Pod "downward-api-128580ec-448a-4171-a407-d6cfeb07fafb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003879688s
STEP: Saw pod success
Jun 16 15:42:16.339: INFO: Pod "downward-api-128580ec-448a-4171-a407-d6cfeb07fafb" satisfied condition "success or failure"
Jun 16 15:42:16.340: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod downward-api-128580ec-448a-4171-a407-d6cfeb07fafb container dapi-container: <nil>
STEP: delete the pod
Jun 16 15:42:16.351: INFO: Waiting for pod downward-api-128580ec-448a-4171-a407-d6cfeb07fafb to disappear
Jun 16 15:42:16.353: INFO: Pod downward-api-128580ec-448a-4171-a407-d6cfeb07fafb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:42:16.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9304" for this suite.
Jun 16 15:42:22.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:42:22.408: INFO: namespace downward-api-9304 deletion completed in 6.053684822s

• [SLOW TEST:8.200 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:42:22.409: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4296
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Jun 16 15:42:22.530: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-790258633 proxy --unix-socket=/tmp/kubectl-proxy-unix836680676/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:42:22.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4296" for this suite.
Jun 16 15:42:28.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:42:28.640: INFO: namespace kubectl-4296 deletion completed in 6.065453781s

• [SLOW TEST:6.231 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:42:28.640: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9914
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Jun 16 15:42:28.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 create -f - --namespace=kubectl-9914'
Jun 16 15:42:29.005: INFO: stderr: ""
Jun 16 15:42:29.005: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 16 15:42:29.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9914'
Jun 16 15:42:29.074: INFO: stderr: ""
Jun 16 15:42:29.074: INFO: stdout: "update-demo-nautilus-4m48s update-demo-nautilus-kbrd9 "
Jun 16 15:42:29.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-4m48s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9914'
Jun 16 15:42:29.129: INFO: stderr: ""
Jun 16 15:42:29.129: INFO: stdout: ""
Jun 16 15:42:29.129: INFO: update-demo-nautilus-4m48s is created but not running
Jun 16 15:42:34.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9914'
Jun 16 15:42:34.185: INFO: stderr: ""
Jun 16 15:42:34.185: INFO: stdout: "update-demo-nautilus-4m48s update-demo-nautilus-kbrd9 "
Jun 16 15:42:34.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-4m48s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9914'
Jun 16 15:42:34.238: INFO: stderr: ""
Jun 16 15:42:34.238: INFO: stdout: "true"
Jun 16 15:42:34.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-4m48s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9914'
Jun 16 15:42:34.292: INFO: stderr: ""
Jun 16 15:42:34.292: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 16 15:42:34.292: INFO: validating pod update-demo-nautilus-4m48s
Jun 16 15:42:34.294: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 16 15:42:34.294: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 16 15:42:34.294: INFO: update-demo-nautilus-4m48s is verified up and running
Jun 16 15:42:34.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-kbrd9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9914'
Jun 16 15:42:34.350: INFO: stderr: ""
Jun 16 15:42:34.350: INFO: stdout: "true"
Jun 16 15:42:34.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-kbrd9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9914'
Jun 16 15:42:34.404: INFO: stderr: ""
Jun 16 15:42:34.404: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 16 15:42:34.404: INFO: validating pod update-demo-nautilus-kbrd9
Jun 16 15:42:34.406: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 16 15:42:34.406: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 16 15:42:34.406: INFO: update-demo-nautilus-kbrd9 is verified up and running
STEP: scaling down the replication controller
Jun 16 15:42:34.408: INFO: scanned /root for discovery docs: <nil>
Jun 16 15:42:34.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9914'
Jun 16 15:42:35.476: INFO: stderr: ""
Jun 16 15:42:35.476: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 16 15:42:35.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9914'
Jun 16 15:42:35.534: INFO: stderr: ""
Jun 16 15:42:35.534: INFO: stdout: "update-demo-nautilus-4m48s update-demo-nautilus-kbrd9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 16 15:42:40.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9914'
Jun 16 15:42:40.592: INFO: stderr: ""
Jun 16 15:42:40.592: INFO: stdout: "update-demo-nautilus-4m48s update-demo-nautilus-kbrd9 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 16 15:42:45.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9914'
Jun 16 15:42:45.650: INFO: stderr: ""
Jun 16 15:42:45.650: INFO: stdout: "update-demo-nautilus-4m48s "
Jun 16 15:42:45.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-4m48s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9914'
Jun 16 15:42:45.705: INFO: stderr: ""
Jun 16 15:42:45.705: INFO: stdout: "true"
Jun 16 15:42:45.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-4m48s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9914'
Jun 16 15:42:45.761: INFO: stderr: ""
Jun 16 15:42:45.761: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 16 15:42:45.761: INFO: validating pod update-demo-nautilus-4m48s
Jun 16 15:42:45.763: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 16 15:42:45.764: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 16 15:42:45.764: INFO: update-demo-nautilus-4m48s is verified up and running
STEP: scaling up the replication controller
Jun 16 15:42:45.765: INFO: scanned /root for discovery docs: <nil>
Jun 16 15:42:45.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9914'
Jun 16 15:42:46.835: INFO: stderr: ""
Jun 16 15:42:46.835: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 16 15:42:46.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9914'
Jun 16 15:42:46.894: INFO: stderr: ""
Jun 16 15:42:46.894: INFO: stdout: "update-demo-nautilus-4m48s update-demo-nautilus-7ksmf "
Jun 16 15:42:46.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-4m48s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9914'
Jun 16 15:42:46.949: INFO: stderr: ""
Jun 16 15:42:46.949: INFO: stdout: "true"
Jun 16 15:42:46.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-4m48s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9914'
Jun 16 15:42:47.005: INFO: stderr: ""
Jun 16 15:42:47.005: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 16 15:42:47.005: INFO: validating pod update-demo-nautilus-4m48s
Jun 16 15:42:47.008: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 16 15:42:47.008: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 16 15:42:47.008: INFO: update-demo-nautilus-4m48s is verified up and running
Jun 16 15:42:47.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-7ksmf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9914'
Jun 16 15:42:47.068: INFO: stderr: ""
Jun 16 15:42:47.068: INFO: stdout: ""
Jun 16 15:42:47.068: INFO: update-demo-nautilus-7ksmf is created but not running
Jun 16 15:42:52.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9914'
Jun 16 15:42:52.145: INFO: stderr: ""
Jun 16 15:42:52.145: INFO: stdout: "update-demo-nautilus-4m48s update-demo-nautilus-7ksmf "
Jun 16 15:42:52.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-4m48s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9914'
Jun 16 15:42:52.202: INFO: stderr: ""
Jun 16 15:42:52.203: INFO: stdout: "true"
Jun 16 15:42:52.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-4m48s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9914'
Jun 16 15:42:52.257: INFO: stderr: ""
Jun 16 15:42:52.257: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 16 15:42:52.257: INFO: validating pod update-demo-nautilus-4m48s
Jun 16 15:42:52.259: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 16 15:42:52.259: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 16 15:42:52.259: INFO: update-demo-nautilus-4m48s is verified up and running
Jun 16 15:42:52.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-7ksmf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9914'
Jun 16 15:42:52.315: INFO: stderr: ""
Jun 16 15:42:52.315: INFO: stdout: "true"
Jun 16 15:42:52.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-7ksmf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9914'
Jun 16 15:42:52.380: INFO: stderr: ""
Jun 16 15:42:52.380: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 16 15:42:52.380: INFO: validating pod update-demo-nautilus-7ksmf
Jun 16 15:42:52.382: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 16 15:42:52.382: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 16 15:42:52.382: INFO: update-demo-nautilus-7ksmf is verified up and running
STEP: using delete to clean up resources
Jun 16 15:42:52.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete --grace-period=0 --force -f - --namespace=kubectl-9914'
Jun 16 15:42:52.439: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 16 15:42:52.439: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 16 15:42:52.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9914'
Jun 16 15:42:52.521: INFO: stderr: "No resources found in kubectl-9914 namespace.\n"
Jun 16 15:42:52.521: INFO: stdout: ""
Jun 16 15:42:52.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -l name=update-demo --namespace=kubectl-9914 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 16 15:42:52.595: INFO: stderr: ""
Jun 16 15:42:52.595: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:42:52.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9914" for this suite.
Jun 16 15:43:04.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:43:04.690: INFO: namespace kubectl-9914 deletion completed in 12.092219049s

• [SLOW TEST:36.050 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:43:04.690: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6035
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 16 15:43:07.348: INFO: Successfully updated pod "pod-update-activedeadlineseconds-9258f624-6acf-47b0-a16f-292f4190ffef"
Jun 16 15:43:07.348: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9258f624-6acf-47b0-a16f-292f4190ffef" in namespace "pods-6035" to be "terminated due to deadline exceeded"
Jun 16 15:43:07.350: INFO: Pod "pod-update-activedeadlineseconds-9258f624-6acf-47b0-a16f-292f4190ffef": Phase="Running", Reason="", readiness=true. Elapsed: 1.90342ms
Jun 16 15:43:09.352: INFO: Pod "pod-update-activedeadlineseconds-9258f624-6acf-47b0-a16f-292f4190ffef": Phase="Running", Reason="", readiness=true. Elapsed: 2.004075886s
Jun 16 15:43:11.354: INFO: Pod "pod-update-activedeadlineseconds-9258f624-6acf-47b0-a16f-292f4190ffef": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.006315721s
Jun 16 15:43:11.354: INFO: Pod "pod-update-activedeadlineseconds-9258f624-6acf-47b0-a16f-292f4190ffef" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:43:11.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6035" for this suite.
Jun 16 15:43:17.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:43:17.411: INFO: namespace pods-6035 deletion completed in 6.055165968s

• [SLOW TEST:12.721 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:43:17.411: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-136
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-136
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-136
STEP: Deleting pre-stop pod
Jun 16 15:43:26.553: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:43:26.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-136" for this suite.
Jun 16 15:44:10.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:44:10.612: INFO: namespace prestop-136 deletion completed in 44.053765354s

• [SLOW TEST:53.201 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:44:10.613: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1016
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:44:10.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1016" for this suite.
Jun 16 15:44:16.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:44:16.799: INFO: namespace kubelet-test-1016 deletion completed in 6.05384124s

• [SLOW TEST:6.187 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:44:16.799: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5292
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5292
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-5292
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5292
Jun 16 15:44:16.930: INFO: Found 0 stateful pods, waiting for 1
Jun 16 15:44:26.932: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 16 15:44:26.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-5292 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 16 15:44:27.069: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 16 15:44:27.069: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 16 15:44:27.069: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 16 15:44:27.071: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 16 15:44:37.073: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 16 15:44:37.073: INFO: Waiting for statefulset status.replicas updated to 0
Jun 16 15:44:37.080: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Jun 16 15:44:37.080: INFO: ss-0  ip-172-19-65-6.eu-west-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:27 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:16 +0000 UTC  }]
Jun 16 15:44:37.080: INFO: 
Jun 16 15:44:37.080: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 16 15:44:38.083: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998226849s
Jun 16 15:44:39.085: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995844423s
Jun 16 15:44:40.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.993269995s
Jun 16 15:44:41.091: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.990378629s
Jun 16 15:44:42.093: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.9879546s
Jun 16 15:44:43.095: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.985514346s
Jun 16 15:44:44.098: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.983079887s
Jun 16 15:44:45.101: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.980385089s
Jun 16 15:44:46.103: INFO: Verifying statefulset ss doesn't scale past 3 for another 977.857062ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5292
Jun 16 15:44:47.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-5292 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 16 15:44:47.245: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 16 15:44:47.245: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 16 15:44:47.245: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 16 15:44:47.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-5292 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 16 15:44:47.395: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 16 15:44:47.395: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 16 15:44:47.395: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 16 15:44:47.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-5292 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 16 15:44:47.535: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 16 15:44:47.535: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 16 15:44:47.535: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 16 15:44:47.537: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 16 15:44:47.537: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 16 15:44:47.537: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 16 15:44:47.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-5292 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 16 15:44:47.699: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 16 15:44:47.699: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 16 15:44:47.699: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 16 15:44:47.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-5292 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 16 15:44:47.837: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 16 15:44:47.837: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 16 15:44:47.837: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 16 15:44:47.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-5292 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 16 15:44:47.986: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 16 15:44:47.986: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 16 15:44:47.986: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 16 15:44:47.986: INFO: Waiting for statefulset status.replicas updated to 0
Jun 16 15:44:47.988: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 16 15:44:57.992: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 16 15:44:57.992: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 16 15:44:57.992: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 16 15:44:57.997: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jun 16 15:44:57.997: INFO: ss-0  ip-172-19-65-6.eu-west-1.compute.internal    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:16 +0000 UTC  }]
Jun 16 15:44:57.997: INFO: ss-1  ip-172-19-65-219.eu-west-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:37 +0000 UTC  }]
Jun 16 15:44:57.997: INFO: ss-2  ip-172-19-65-122.eu-west-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:37 +0000 UTC  }]
Jun 16 15:44:57.997: INFO: 
Jun 16 15:44:57.997: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 16 15:44:58.999: INFO: POD   NODE                                         PHASE    GRACE  CONDITIONS
Jun 16 15:44:58.999: INFO: ss-0  ip-172-19-65-6.eu-west-1.compute.internal    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:16 +0000 UTC  }]
Jun 16 15:44:59.000: INFO: ss-1  ip-172-19-65-219.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:47 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:37 +0000 UTC  }]
Jun 16 15:44:59.000: INFO: ss-2  ip-172-19-65-122.eu-west-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:48 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-06-16 15:44:37 +0000 UTC  }]
Jun 16 15:44:59.000: INFO: 
Jun 16 15:44:59.000: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 16 15:45:00.002: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.9955105s
Jun 16 15:45:01.004: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.993368644s
Jun 16 15:45:02.007: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.991080857s
Jun 16 15:45:03.009: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.987910283s
Jun 16 15:45:04.011: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.98573455s
Jun 16 15:45:05.014: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.983586834s
Jun 16 15:45:06.016: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.981424372s
Jun 16 15:45:07.018: INFO: Verifying statefulset ss doesn't scale past 0 for another 979.256245ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5292
Jun 16 15:45:08.020: INFO: Scaling statefulset ss to 0
Jun 16 15:45:08.025: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun 16 15:45:08.026: INFO: Deleting all statefulset in ns statefulset-5292
Jun 16 15:45:08.028: INFO: Scaling statefulset ss to 0
Jun 16 15:45:08.032: INFO: Waiting for statefulset status.replicas updated to 0
Jun 16 15:45:08.033: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:45:08.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5292" for this suite.
Jun 16 15:45:14.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:45:14.102: INFO: namespace statefulset-5292 deletion completed in 6.061026455s

• [SLOW TEST:57.302 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:45:14.102: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1277
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jun 16 15:45:14.228: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 16 15:45:14.234: INFO: Waiting for terminating namespaces to be deleted...
Jun 16 15:45:14.235: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-122.eu-west-1.compute.internal before test
Jun 16 15:45:14.239: INFO: node-exporter-5vz2p from kube-system started at 2020-06-16 15:10:31 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.239: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 15:45:14.239: INFO: kube-state-metrics-6d998ffd8b-jpbpj from kube-system started at 2020-06-16 15:10:43 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.239: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 16 15:45:14.239: INFO: calico-node-sk9rd from kube-system started at 2020-06-16 15:06:53 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.239: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 15:45:14.239: INFO: kiam-agent-x89wb from kube-system started at 2020-06-16 15:10:38 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.239: INFO: 	Container kiam-agent ready: true, restart count 3
Jun 16 15:45:14.239: INFO: external-dns-776fc66667-24mls from kube-system started at 2020-06-16 15:10:42 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.239: INFO: 	Container external-dns ready: true, restart count 0
Jun 16 15:45:14.239: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-6qlkz from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 15:45:14.239: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 15:45:14.240: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 15:45:14.240: INFO: cert-exporter-sgwbs from kube-system started at 2020-06-16 15:10:32 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.240: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 15:45:14.240: INFO: coredns-6d56c484c-wcs6v from kube-system started at 2020-06-16 15:10:34 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.240: INFO: 	Container coredns ready: true, restart count 0
Jun 16 15:45:14.240: INFO: aws-node-jbq4c from kube-system started at 2020-06-16 15:10:43 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.240: INFO: 	Container aws-node ready: true, restart count 0
Jun 16 15:45:14.240: INFO: net-exporter-ltbgf from kube-system started at 2020-06-16 15:14:31 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.240: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 15:45:14.240: INFO: kube-proxy-xdgfc from kube-system started at 2020-06-16 15:06:53 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.240: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 15:45:14.240: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-219.eu-west-1.compute.internal before test
Jun 16 15:45:14.244: INFO: tiller-deploy-684c6b545b-xfl8b from giantswarm started at 2020-06-16 15:09:33 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.244: INFO: 	Container tiller ready: true, restart count 0
Jun 16 15:45:14.244: INFO: coredns-6d56c484c-l7qk9 from kube-system started at 2020-06-16 15:10:34 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.244: INFO: 	Container coredns ready: true, restart count 0
Jun 16 15:45:14.244: INFO: sonobuoy from sonobuoy started at 2020-06-16 15:26:40 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.244: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 16 15:45:14.244: INFO: kiam-agent-6q62w from kube-system started at 2020-06-16 15:10:38 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.244: INFO: 	Container kiam-agent ready: true, restart count 3
Jun 16 15:45:14.244: INFO: cert-exporter-fthcq from kube-system started at 2020-06-16 15:10:32 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.244: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 15:45:14.244: INFO: net-exporter-p7cpr from kube-system started at 2020-06-16 15:14:31 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.244: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 15:45:14.244: INFO: sonobuoy-e2e-job-40469eeb076c4210 from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 15:45:14.244: INFO: 	Container e2e ready: true, restart count 0
Jun 16 15:45:14.244: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 15:45:14.244: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-bbzmx from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 15:45:14.244: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 15:45:14.244: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 15:45:14.244: INFO: aws-node-kmx2q from kube-system started at 2020-06-16 15:10:51 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.244: INFO: 	Container aws-node ready: true, restart count 0
Jun 16 15:45:14.244: INFO: kube-proxy-r2g9b from kube-system started at 2020-06-16 15:06:51 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.244: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 15:45:14.244: INFO: calico-node-xpkvc from kube-system started at 2020-06-16 15:06:51 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.244: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 15:45:14.244: INFO: node-exporter-pqhv4 from kube-system started at 2020-06-16 15:10:31 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.244: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 15:45:14.244: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-6.eu-west-1.compute.internal before test
Jun 16 15:45:14.248: INFO: kube-proxy-g6cmw from kube-system started at 2020-06-16 15:06:46 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.248: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 15:45:14.248: INFO: calico-node-ddwlg from kube-system started at 2020-06-16 15:06:46 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.248: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 15:45:14.248: INFO: aws-node-vjj2v from kube-system started at 2020-06-16 15:10:39 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.248: INFO: 	Container aws-node ready: true, restart count 0
Jun 16 15:45:14.248: INFO: node-exporter-z96vr from kube-system started at 2020-06-16 15:10:31 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.248: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 15:45:14.248: INFO: coredns-6d56c484c-8z6gz from kube-system started at 2020-06-16 15:10:34 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.248: INFO: 	Container coredns ready: true, restart count 0
Jun 16 15:45:14.248: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-87l2v from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 15:45:14.248: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 15:45:14.248: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 15:45:14.248: INFO: cert-exporter-2k4kc from kube-system started at 2020-06-16 15:10:32 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.248: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 15:45:14.248: INFO: cert-manager-67dfd96fcd-n8nrk from kube-system started at 2020-06-16 15:10:36 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.248: INFO: 	Container cert-manager ready: true, restart count 0
Jun 16 15:45:14.248: INFO: kiam-agent-5xldj from kube-system started at 2020-06-16 15:10:38 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.248: INFO: 	Container kiam-agent ready: true, restart count 3
Jun 16 15:45:14.248: INFO: metrics-server-66df9f5b56-mgt2q from kube-system started at 2020-06-16 15:10:44 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.248: INFO: 	Container metrics-server ready: true, restart count 0
Jun 16 15:45:14.248: INFO: net-exporter-5wtl4 from kube-system started at 2020-06-16 15:14:31 +0000 UTC (1 container statuses recorded)
Jun 16 15:45:14.248: INFO: 	Container net-exporter ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-516ab371-8297-4a0b-a135-78970b9ca1d3 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-516ab371-8297-4a0b-a135-78970b9ca1d3 off the node ip-172-19-65-6.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-516ab371-8297-4a0b-a135-78970b9ca1d3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:50:18.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
Jun 16 15:50:18.313: INFO: Condition Ready of node ip-172-19-65-87.eu-west-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2020-06-16 15:49:45 +0000 UTC}]. Failure
Jun 16 15:50:20.316: INFO: Condition Ready of node ip-172-19-65-87.eu-west-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2020-06-16 15:49:45 +0000 UTC}]. Failure
Jun 16 15:50:22.316: INFO: Condition Ready of node ip-172-19-65-87.eu-west-1.compute.internal is false, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoSchedule <nil>} {node.kubernetes.io/not-ready  NoExecute 2020-06-16 15:49:45 +0000 UTC}]. Failure
Jun 16 15:50:24.316: INFO: Condition Ready of node ip-172-19-65-87.eu-west-1.compute.internal is true, but Node is tainted by NodeController with [{node.kubernetes.io/not-ready  NoExecute 2020-06-16 15:49:45 +0000 UTC}]. Failure
STEP: Destroying namespace "sched-pred-1277" for this suite.
Jun 16 15:50:34.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:50:34.375: INFO: namespace sched-pred-1277 deletion completed in 8.058008787s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:320.273 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:50:34.375: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6130
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-4e82c8d2-ebce-4a7b-9707-cee8ad75c918 in namespace container-probe-6130
Jun 16 15:50:38.504: INFO: Started pod liveness-4e82c8d2-ebce-4a7b-9707-cee8ad75c918 in namespace container-probe-6130
STEP: checking the pod's current state and verifying that restartCount is present
Jun 16 15:50:38.506: INFO: Initial restart count of pod liveness-4e82c8d2-ebce-4a7b-9707-cee8ad75c918 is 0
Jun 16 15:51:04.534: INFO: Restart count of pod container-probe-6130/liveness-4e82c8d2-ebce-4a7b-9707-cee8ad75c918 is now 1 (26.027838683s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:51:04.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6130" for this suite.
Jun 16 15:51:10.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:51:10.596: INFO: namespace container-probe-6130 deletion completed in 6.05476542s

• [SLOW TEST:36.221 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:51:10.596: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1926
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-a9e091f3-806d-4b1c-b1f2-e049a6bc65e5
STEP: Creating a pod to test consume configMaps
Jun 16 15:51:10.720: INFO: Waiting up to 5m0s for pod "pod-configmaps-c3e19752-0ff4-4afb-bece-6b29c8e5d4fa" in namespace "configmap-1926" to be "success or failure"
Jun 16 15:51:10.724: INFO: Pod "pod-configmaps-c3e19752-0ff4-4afb-bece-6b29c8e5d4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.480523ms
Jun 16 15:51:12.726: INFO: Pod "pod-configmaps-c3e19752-0ff4-4afb-bece-6b29c8e5d4fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005548464s
Jun 16 15:51:14.728: INFO: Pod "pod-configmaps-c3e19752-0ff4-4afb-bece-6b29c8e5d4fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007601651s
STEP: Saw pod success
Jun 16 15:51:14.728: INFO: Pod "pod-configmaps-c3e19752-0ff4-4afb-bece-6b29c8e5d4fa" satisfied condition "success or failure"
Jun 16 15:51:14.729: INFO: Trying to get logs from node ip-172-19-65-87.eu-west-1.compute.internal pod pod-configmaps-c3e19752-0ff4-4afb-bece-6b29c8e5d4fa container configmap-volume-test: <nil>
STEP: delete the pod
Jun 16 15:51:14.739: INFO: Waiting for pod pod-configmaps-c3e19752-0ff4-4afb-bece-6b29c8e5d4fa to disappear
Jun 16 15:51:14.741: INFO: Pod pod-configmaps-c3e19752-0ff4-4afb-bece-6b29c8e5d4fa no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:51:14.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1926" for this suite.
Jun 16 15:51:20.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:51:20.800: INFO: namespace configmap-1926 deletion completed in 6.056302352s

• [SLOW TEST:10.204 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:51:20.800: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2758
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jun 16 15:51:23.444: INFO: Successfully updated pod "annotationupdate8ceff895-62c3-4089-b517-c1db6a88f812"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:51:25.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2758" for this suite.
Jun 16 15:51:37.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:51:37.514: INFO: namespace projected-2758 deletion completed in 12.053791553s

• [SLOW TEST:16.714 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:51:37.514: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1445
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-gnjk
STEP: Creating a pod to test atomic-volume-subpath
Jun 16 15:51:37.643: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-gnjk" in namespace "subpath-1445" to be "success or failure"
Jun 16 15:51:37.645: INFO: Pod "pod-subpath-test-configmap-gnjk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.187752ms
Jun 16 15:51:39.647: INFO: Pod "pod-subpath-test-configmap-gnjk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004279117s
Jun 16 15:51:41.684: INFO: Pod "pod-subpath-test-configmap-gnjk": Phase="Running", Reason="", readiness=true. Elapsed: 4.041253906s
Jun 16 15:51:43.686: INFO: Pod "pod-subpath-test-configmap-gnjk": Phase="Running", Reason="", readiness=true. Elapsed: 6.043086984s
Jun 16 15:51:45.688: INFO: Pod "pod-subpath-test-configmap-gnjk": Phase="Running", Reason="", readiness=true. Elapsed: 8.044716158s
Jun 16 15:51:47.689: INFO: Pod "pod-subpath-test-configmap-gnjk": Phase="Running", Reason="", readiness=true. Elapsed: 10.046279564s
Jun 16 15:51:49.691: INFO: Pod "pod-subpath-test-configmap-gnjk": Phase="Running", Reason="", readiness=true. Elapsed: 12.048316434s
Jun 16 15:51:51.693: INFO: Pod "pod-subpath-test-configmap-gnjk": Phase="Running", Reason="", readiness=true. Elapsed: 14.050244587s
Jun 16 15:51:53.695: INFO: Pod "pod-subpath-test-configmap-gnjk": Phase="Running", Reason="", readiness=true. Elapsed: 16.052355646s
Jun 16 15:51:55.697: INFO: Pod "pod-subpath-test-configmap-gnjk": Phase="Running", Reason="", readiness=true. Elapsed: 18.054412697s
Jun 16 15:51:57.700: INFO: Pod "pod-subpath-test-configmap-gnjk": Phase="Running", Reason="", readiness=true. Elapsed: 20.056670801s
Jun 16 15:51:59.702: INFO: Pod "pod-subpath-test-configmap-gnjk": Phase="Running", Reason="", readiness=true. Elapsed: 22.058731129s
Jun 16 15:52:01.704: INFO: Pod "pod-subpath-test-configmap-gnjk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.06064727s
STEP: Saw pod success
Jun 16 15:52:01.704: INFO: Pod "pod-subpath-test-configmap-gnjk" satisfied condition "success or failure"
Jun 16 15:52:01.705: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod pod-subpath-test-configmap-gnjk container test-container-subpath-configmap-gnjk: <nil>
STEP: delete the pod
Jun 16 15:52:01.716: INFO: Waiting for pod pod-subpath-test-configmap-gnjk to disappear
Jun 16 15:52:01.718: INFO: Pod pod-subpath-test-configmap-gnjk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-gnjk
Jun 16 15:52:01.718: INFO: Deleting pod "pod-subpath-test-configmap-gnjk" in namespace "subpath-1445"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:52:01.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1445" for this suite.
Jun 16 15:52:07.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:52:07.777: INFO: namespace subpath-1445 deletion completed in 6.055737329s

• [SLOW TEST:30.263 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:52:07.777: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9220
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Jun 16 15:52:07.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 create -f - --namespace=kubectl-9220'
Jun 16 15:52:08.042: INFO: stderr: ""
Jun 16 15:52:08.042: INFO: stdout: "pod/pause created\n"
Jun 16 15:52:08.042: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 16 15:52:08.042: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9220" to be "running and ready"
Jun 16 15:52:08.044: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.578095ms
Jun 16 15:52:10.046: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.003707125s
Jun 16 15:52:10.046: INFO: Pod "pause" satisfied condition "running and ready"
Jun 16 15:52:10.046: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 16 15:52:10.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 label pods pause testing-label=testing-label-value --namespace=kubectl-9220'
Jun 16 15:52:10.111: INFO: stderr: ""
Jun 16 15:52:10.111: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 16 15:52:10.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pod pause -L testing-label --namespace=kubectl-9220'
Jun 16 15:52:10.166: INFO: stderr: ""
Jun 16 15:52:10.166: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 16 15:52:10.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 label pods pause testing-label- --namespace=kubectl-9220'
Jun 16 15:52:10.224: INFO: stderr: ""
Jun 16 15:52:10.224: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 16 15:52:10.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pod pause -L testing-label --namespace=kubectl-9220'
Jun 16 15:52:10.279: INFO: stderr: ""
Jun 16 15:52:10.279: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Jun 16 15:52:10.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete --grace-period=0 --force -f - --namespace=kubectl-9220'
Jun 16 15:52:10.337: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 16 15:52:10.337: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 16 15:52:10.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get rc,svc -l name=pause --no-headers --namespace=kubectl-9220'
Jun 16 15:52:10.395: INFO: stderr: "No resources found in kubectl-9220 namespace.\n"
Jun 16 15:52:10.395: INFO: stdout: ""
Jun 16 15:52:10.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -l name=pause --namespace=kubectl-9220 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 16 15:52:10.450: INFO: stderr: ""
Jun 16 15:52:10.450: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:52:10.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9220" for this suite.
Jun 16 15:52:16.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:52:16.516: INFO: namespace kubectl-9220 deletion completed in 6.063127577s

• [SLOW TEST:8.739 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:52:16.516: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5703
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 16 15:52:19.660: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:52:19.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5703" for this suite.
Jun 16 15:52:25.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:52:25.723: INFO: namespace container-runtime-5703 deletion completed in 6.053079512s

• [SLOW TEST:9.207 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:52:25.723: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-634
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 15:52:25.851: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8bf13a16-3daf-4831-98ff-0813bd0e25cc" in namespace "projected-634" to be "success or failure"
Jun 16 15:52:25.853: INFO: Pod "downwardapi-volume-8bf13a16-3daf-4831-98ff-0813bd0e25cc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.913846ms
Jun 16 15:52:27.855: INFO: Pod "downwardapi-volume-8bf13a16-3daf-4831-98ff-0813bd0e25cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003980841s
STEP: Saw pod success
Jun 16 15:52:27.855: INFO: Pod "downwardapi-volume-8bf13a16-3daf-4831-98ff-0813bd0e25cc" satisfied condition "success or failure"
Jun 16 15:52:27.857: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod downwardapi-volume-8bf13a16-3daf-4831-98ff-0813bd0e25cc container client-container: <nil>
STEP: delete the pod
Jun 16 15:52:27.866: INFO: Waiting for pod downwardapi-volume-8bf13a16-3daf-4831-98ff-0813bd0e25cc to disappear
Jun 16 15:52:27.868: INFO: Pod downwardapi-volume-8bf13a16-3daf-4831-98ff-0813bd0e25cc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:52:27.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-634" for this suite.
Jun 16 15:52:33.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:52:33.926: INFO: namespace projected-634 deletion completed in 6.055388896s

• [SLOW TEST:8.202 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:52:33.926: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2472
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 16 15:52:34.050: INFO: Waiting up to 5m0s for pod "pod-10cce14f-d2aa-4bdd-b1bf-55d2ed28b2db" in namespace "emptydir-2472" to be "success or failure"
Jun 16 15:52:34.054: INFO: Pod "pod-10cce14f-d2aa-4bdd-b1bf-55d2ed28b2db": Phase="Pending", Reason="", readiness=false. Elapsed: 3.731492ms
Jun 16 15:52:36.056: INFO: Pod "pod-10cce14f-d2aa-4bdd-b1bf-55d2ed28b2db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006040976s
STEP: Saw pod success
Jun 16 15:52:36.056: INFO: Pod "pod-10cce14f-d2aa-4bdd-b1bf-55d2ed28b2db" satisfied condition "success or failure"
Jun 16 15:52:36.058: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod pod-10cce14f-d2aa-4bdd-b1bf-55d2ed28b2db container test-container: <nil>
STEP: delete the pod
Jun 16 15:52:36.068: INFO: Waiting for pod pod-10cce14f-d2aa-4bdd-b1bf-55d2ed28b2db to disappear
Jun 16 15:52:36.070: INFO: Pod pod-10cce14f-d2aa-4bdd-b1bf-55d2ed28b2db no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:52:36.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2472" for this suite.
Jun 16 15:52:42.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:52:42.132: INFO: namespace emptydir-2472 deletion completed in 6.060668118s

• [SLOW TEST:8.207 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:52:42.132: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3289
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:53:42.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3289" for this suite.
Jun 16 15:54:10.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:54:10.316: INFO: namespace container-probe-3289 deletion completed in 28.054746826s

• [SLOW TEST:88.183 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:54:10.316: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-441
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 15:54:10.436: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 16 15:54:13.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-441 create -f -'
Jun 16 15:54:14.437: INFO: stderr: ""
Jun 16 15:54:14.437: INFO: stdout: "e2e-test-crd-publish-openapi-6479-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 16 15:54:14.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-441 delete e2e-test-crd-publish-openapi-6479-crds test-cr'
Jun 16 15:54:14.498: INFO: stderr: ""
Jun 16 15:54:14.498: INFO: stdout: "e2e-test-crd-publish-openapi-6479-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun 16 15:54:14.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-441 apply -f -'
Jun 16 15:54:14.632: INFO: stderr: ""
Jun 16 15:54:14.632: INFO: stdout: "e2e-test-crd-publish-openapi-6479-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 16 15:54:14.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-441 delete e2e-test-crd-publish-openapi-6479-crds test-cr'
Jun 16 15:54:14.692: INFO: stderr: ""
Jun 16 15:54:14.692: INFO: stdout: "e2e-test-crd-publish-openapi-6479-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 16 15:54:14.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 explain e2e-test-crd-publish-openapi-6479-crds'
Jun 16 15:54:14.816: INFO: stderr: ""
Jun 16 15:54:14.816: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6479-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:54:18.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-441" for this suite.
Jun 16 15:54:24.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:54:24.434: INFO: namespace crd-publish-openapi-441 deletion completed in 6.05649525s

• [SLOW TEST:14.119 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:54:24.435: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-332
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 15:54:30.573: INFO: Waiting up to 5m0s for pod "client-envvars-53bea9a7-9c58-4d6c-b755-eed0a5af2f3e" in namespace "pods-332" to be "success or failure"
Jun 16 15:54:30.576: INFO: Pod "client-envvars-53bea9a7-9c58-4d6c-b755-eed0a5af2f3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.600873ms
Jun 16 15:54:32.577: INFO: Pod "client-envvars-53bea9a7-9c58-4d6c-b755-eed0a5af2f3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004504674s
STEP: Saw pod success
Jun 16 15:54:32.577: INFO: Pod "client-envvars-53bea9a7-9c58-4d6c-b755-eed0a5af2f3e" satisfied condition "success or failure"
Jun 16 15:54:32.579: INFO: Trying to get logs from node ip-172-19-65-134.eu-west-1.compute.internal pod client-envvars-53bea9a7-9c58-4d6c-b755-eed0a5af2f3e container env3cont: <nil>
STEP: delete the pod
Jun 16 15:54:32.591: INFO: Waiting for pod client-envvars-53bea9a7-9c58-4d6c-b755-eed0a5af2f3e to disappear
Jun 16 15:54:32.593: INFO: Pod client-envvars-53bea9a7-9c58-4d6c-b755-eed0a5af2f3e no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:54:32.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-332" for this suite.
Jun 16 15:55:00.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:55:00.649: INFO: namespace pods-332 deletion completed in 28.054015478s

• [SLOW TEST:36.214 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:55:00.649: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 16 15:55:00.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-5945'
Jun 16 15:55:00.832: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 16 15:55:00.832: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Jun 16 15:55:00.837: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-j2xbr]
Jun 16 15:55:00.837: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-j2xbr" in namespace "kubectl-5945" to be "running and ready"
Jun 16 15:55:00.840: INFO: Pod "e2e-test-httpd-rc-j2xbr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.748822ms
Jun 16 15:55:02.842: INFO: Pod "e2e-test-httpd-rc-j2xbr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004669024s
Jun 16 15:55:04.844: INFO: Pod "e2e-test-httpd-rc-j2xbr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006618392s
Jun 16 15:55:06.846: INFO: Pod "e2e-test-httpd-rc-j2xbr": Phase="Running", Reason="", readiness=true. Elapsed: 6.008979833s
Jun 16 15:55:06.846: INFO: Pod "e2e-test-httpd-rc-j2xbr" satisfied condition "running and ready"
Jun 16 15:55:06.846: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-j2xbr]
Jun 16 15:55:06.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 logs rc/e2e-test-httpd-rc --namespace=kubectl-5945'
Jun 16 15:55:06.916: INFO: stderr: ""
Jun 16 15:55:06.916: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.18.138.97. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.18.138.97. Set the 'ServerName' directive globally to suppress this message\n[Tue Jun 16 15:55:06.368125 2020] [mpm_event:notice] [pid 1:tid 140172770347880] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Tue Jun 16 15:55:06.368194 2020] [core:notice] [pid 1:tid 140172770347880] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Jun 16 15:55:06.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete rc e2e-test-httpd-rc --namespace=kubectl-5945'
Jun 16 15:55:06.976: INFO: stderr: ""
Jun 16 15:55:06.976: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:55:06.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5945" for this suite.
Jun 16 15:55:18.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:55:19.036: INFO: namespace kubectl-5945 deletion completed in 12.057432201s

• [SLOW TEST:18.386 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:55:19.036: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3077
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 15:55:19.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 version'
Jun 16 15:55:19.206: INFO: stderr: ""
Jun 16 15:55:19.206: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.9\", GitCommit:\"a17149e1a189050796ced469dbd78d380f2ed5ef\", GitTreeState:\"clean\", BuildDate:\"2020-04-16T11:44:51Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.9\", GitCommit:\"a17149e1a189050796ced469dbd78d380f2ed5ef\", GitTreeState:\"clean\", BuildDate:\"2020-04-16T11:36:15Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:55:19.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3077" for this suite.
Jun 16 15:55:25.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:55:25.261: INFO: namespace kubectl-3077 deletion completed in 6.052702021s

• [SLOW TEST:6.226 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:55:25.262: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8317
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:55:30.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8317" for this suite.
Jun 16 15:55:36.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:55:36.965: INFO: namespace watch-8317 deletion completed in 6.149742454s

• [SLOW TEST:11.704 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:55:36.965: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8119
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jun 16 15:55:37.086: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:55:40.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8119" for this suite.
Jun 16 15:55:46.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:55:46.725: INFO: namespace init-container-8119 deletion completed in 6.056605914s

• [SLOW TEST:9.759 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:55:46.725: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5515
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 15:55:46.845: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:55:52.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5515" for this suite.
Jun 16 15:55:58.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:55:58.530: INFO: namespace custom-resource-definition-5515 deletion completed in 6.056033831s

• [SLOW TEST:11.805 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:55:58.530: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5840
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 15:55:58.650: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 16 15:56:02.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-5840 create -f -'
Jun 16 15:56:02.556: INFO: stderr: ""
Jun 16 15:56:02.556: INFO: stdout: "e2e-test-crd-publish-openapi-3392-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 16 15:56:02.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-5840 delete e2e-test-crd-publish-openapi-3392-crds test-cr'
Jun 16 15:56:02.614: INFO: stderr: ""
Jun 16 15:56:02.614: INFO: stdout: "e2e-test-crd-publish-openapi-3392-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun 16 15:56:02.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-5840 apply -f -'
Jun 16 15:56:02.747: INFO: stderr: ""
Jun 16 15:56:02.747: INFO: stdout: "e2e-test-crd-publish-openapi-3392-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 16 15:56:02.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-5840 delete e2e-test-crd-publish-openapi-3392-crds test-cr'
Jun 16 15:56:02.806: INFO: stderr: ""
Jun 16 15:56:02.806: INFO: stdout: "e2e-test-crd-publish-openapi-3392-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jun 16 15:56:02.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 explain e2e-test-crd-publish-openapi-3392-crds'
Jun 16 15:56:02.930: INFO: stderr: ""
Jun 16 15:56:02.930: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3392-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:56:06.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5840" for this suite.
Jun 16 15:56:12.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:56:12.542: INFO: namespace crd-publish-openapi-5840 deletion completed in 6.059672578s

• [SLOW TEST:14.012 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:56:12.542: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Jun 16 15:56:12.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-4849 -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun 16 15:56:12.726: INFO: stderr: ""
Jun 16 15:56:12.726: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Jun 16 15:56:12.726: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun 16 15:56:12.726: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4849" to be "running and ready, or succeeded"
Jun 16 15:56:12.727: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.450089ms
Jun 16 15:56:14.729: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003600397s
Jun 16 15:56:16.732: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005671182s
Jun 16 15:56:18.734: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 6.007788525s
Jun 16 15:56:18.734: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun 16 15:56:18.734: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jun 16 15:56:18.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 logs logs-generator logs-generator --namespace=kubectl-4849'
Jun 16 15:56:18.803: INFO: stderr: ""
Jun 16 15:56:18.803: INFO: stdout: "I0616 15:56:16.310655       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/44bq 362\nI0616 15:56:16.510765       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/m8q 273\nI0616 15:56:16.710817       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/jlkb 441\nI0616 15:56:16.910801       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/8g6 552\nI0616 15:56:17.110812       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/vnd 220\nI0616 15:56:17.310832       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/wsj9 204\nI0616 15:56:17.510820       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2m6j 265\nI0616 15:56:17.710840       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/hgt5 520\nI0616 15:56:17.910813       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/bh5m 549\nI0616 15:56:18.110810       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/8l22 208\nI0616 15:56:18.310817       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/9vn 297\nI0616 15:56:18.510798       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/97qx 390\nI0616 15:56:18.710812       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/2llk 567\n"
STEP: limiting log lines
Jun 16 15:56:18.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 logs logs-generator logs-generator --namespace=kubectl-4849 --tail=1'
Jun 16 15:56:18.868: INFO: stderr: ""
Jun 16 15:56:18.868: INFO: stdout: "I0616 15:56:18.710812       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/2llk 567\n"
STEP: limiting log bytes
Jun 16 15:56:18.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 logs logs-generator logs-generator --namespace=kubectl-4849 --limit-bytes=1'
Jun 16 15:56:18.934: INFO: stderr: ""
Jun 16 15:56:18.934: INFO: stdout: "I"
STEP: exposing timestamps
Jun 16 15:56:18.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 logs logs-generator logs-generator --namespace=kubectl-4849 --tail=1 --timestamps'
Jun 16 15:56:18.997: INFO: stderr: ""
Jun 16 15:56:18.998: INFO: stdout: "2020-06-16T15:56:18.910928428Z I0616 15:56:18.910815       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/f5w 289\n"
STEP: restricting to a time range
Jun 16 15:56:21.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 logs logs-generator logs-generator --namespace=kubectl-4849 --since=1s'
Jun 16 15:56:21.561: INFO: stderr: ""
Jun 16 15:56:21.561: INFO: stdout: "I0616 15:56:20.710800       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/lwk 439\nI0616 15:56:20.910761       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/kk7p 512\nI0616 15:56:21.110843       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/f68f 342\nI0616 15:56:21.310813       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/pmr 304\nI0616 15:56:21.510814       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/8q6w 405\n"
Jun 16 15:56:21.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 logs logs-generator logs-generator --namespace=kubectl-4849 --since=24h'
Jun 16 15:56:21.624: INFO: stderr: ""
Jun 16 15:56:21.624: INFO: stdout: "I0616 15:56:16.310655       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/44bq 362\nI0616 15:56:16.510765       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/m8q 273\nI0616 15:56:16.710817       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/jlkb 441\nI0616 15:56:16.910801       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/8g6 552\nI0616 15:56:17.110812       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/vnd 220\nI0616 15:56:17.310832       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/wsj9 204\nI0616 15:56:17.510820       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/2m6j 265\nI0616 15:56:17.710840       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/hgt5 520\nI0616 15:56:17.910813       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/bh5m 549\nI0616 15:56:18.110810       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/8l22 208\nI0616 15:56:18.310817       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/9vn 297\nI0616 15:56:18.510798       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/97qx 390\nI0616 15:56:18.710812       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/2llk 567\nI0616 15:56:18.910815       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/f5w 289\nI0616 15:56:19.110809       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/h2l2 511\nI0616 15:56:19.310810       1 logs_generator.go:76] 15 GET /api/v1/namespaces/ns/pods/9lph 470\nI0616 15:56:19.510796       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/v8rv 580\nI0616 15:56:19.710811       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/n4b8 328\nI0616 15:56:19.910809       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/5rt2 279\nI0616 15:56:20.110816       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/fxx 256\nI0616 15:56:20.310811       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/57r6 528\nI0616 15:56:20.510816       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/hv8 474\nI0616 15:56:20.710800       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/lwk 439\nI0616 15:56:20.910761       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/kk7p 512\nI0616 15:56:21.110843       1 logs_generator.go:76] 24 GET /api/v1/namespaces/kube-system/pods/f68f 342\nI0616 15:56:21.310813       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/pmr 304\nI0616 15:56:21.510814       1 logs_generator.go:76] 26 GET /api/v1/namespaces/kube-system/pods/8q6w 405\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Jun 16 15:56:21.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete pod logs-generator --namespace=kubectl-4849'
Jun 16 15:56:27.979: INFO: stderr: ""
Jun 16 15:56:27.979: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:56:27.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4849" for this suite.
Jun 16 15:56:33.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:56:34.039: INFO: namespace kubectl-4849 deletion completed in 6.057439962s

• [SLOW TEST:21.497 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:56:34.039: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1372
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jun 16 15:56:34.165: INFO: Waiting up to 5m0s for pod "downward-api-93cffa39-e630-4689-a109-3920ee4306aa" in namespace "downward-api-1372" to be "success or failure"
Jun 16 15:56:34.167: INFO: Pod "downward-api-93cffa39-e630-4689-a109-3920ee4306aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.544661ms
Jun 16 15:56:36.169: INFO: Pod "downward-api-93cffa39-e630-4689-a109-3920ee4306aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004744046s
Jun 16 15:56:38.171: INFO: Pod "downward-api-93cffa39-e630-4689-a109-3920ee4306aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006919421s
STEP: Saw pod success
Jun 16 15:56:38.172: INFO: Pod "downward-api-93cffa39-e630-4689-a109-3920ee4306aa" satisfied condition "success or failure"
Jun 16 15:56:38.173: INFO: Trying to get logs from node ip-172-19-65-87.eu-west-1.compute.internal pod downward-api-93cffa39-e630-4689-a109-3920ee4306aa container dapi-container: <nil>
STEP: delete the pod
Jun 16 15:56:38.184: INFO: Waiting for pod downward-api-93cffa39-e630-4689-a109-3920ee4306aa to disappear
Jun 16 15:56:38.186: INFO: Pod downward-api-93cffa39-e630-4689-a109-3920ee4306aa no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:56:38.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1372" for this suite.
Jun 16 15:56:44.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:56:44.273: INFO: namespace downward-api-1372 deletion completed in 6.084739581s

• [SLOW TEST:10.234 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:56:44.273: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1724
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 15:56:44.416: INFO: Creating deployment "webserver-deployment"
Jun 16 15:56:44.418: INFO: Waiting for observed generation 1
Jun 16 15:56:46.423: INFO: Waiting for all required pods to come up
Jun 16 15:56:46.425: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 16 15:56:54.429: INFO: Waiting for deployment "webserver-deployment" to complete
Jun 16 15:56:54.432: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun 16 15:56:54.436: INFO: Updating deployment webserver-deployment
Jun 16 15:56:54.436: INFO: Waiting for observed generation 2
Jun 16 15:56:56.439: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 16 15:56:56.441: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 16 15:56:56.442: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 16 15:56:56.446: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 16 15:56:56.446: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 16 15:56:56.448: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 16 15:56:56.450: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun 16 15:56:56.450: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun 16 15:56:56.453: INFO: Updating deployment webserver-deployment
Jun 16 15:56:56.453: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun 16 15:56:56.457: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 16 15:56:56.458: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 16 15:56:56.464: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1724 /apis/apps/v1/namespaces/deployment-1724/deployments/webserver-deployment 694998ea-a755-466b-b78f-3e61f5516e9d 13403 3 2020-06-16 15:56:44 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003566288 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-06-16 15:56:51 +0000 UTC,LastTransitionTime:2020-06-16 15:56:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-06-16 15:56:54 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun 16 15:56:56.469: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-1724 /apis/apps/v1/namespaces/deployment-1724/replicasets/webserver-deployment-c7997dcc8 0bbbd130-0274-4249-9a72-8dfd41294b86 13406 3 2020-06-16 15:56:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 694998ea-a755-466b-b78f-3e61f5516e9d 0xc003566b87 0xc003566b88}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003566c38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 16 15:56:56.469: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun 16 15:56:56.469: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-1724 /apis/apps/v1/namespaces/deployment-1724/replicasets/webserver-deployment-595b5b9587 ac0424f5-b841-4984-8027-1f0e37bcdd92 13404 3 2020-06-16 15:56:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 694998ea-a755-466b-b78f-3e61f5516e9d 0xc003566a57 0xc003566a58}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003566af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun 16 15:56:56.491: INFO: Pod "webserver-deployment-595b5b9587-4x8bc" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4x8bc webserver-deployment-595b5b9587- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-595b5b9587-4x8bc 0b898511-e019-474b-9f73-c419e24f5c6d 13419 0 2020-06-16 15:56:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 ac0424f5-b841-4984-8027-1f0e37bcdd92 0xc0076af717 0xc0076af718}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-6.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.491: INFO: Pod "webserver-deployment-595b5b9587-8cnnb" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-8cnnb webserver-deployment-595b5b9587- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-595b5b9587-8cnnb 17ee590c-7de2-45a8-add9-1296d29fd325 13295 0 2020-06-16 15:56:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 ac0424f5-b841-4984-8027-1f0e37bcdd92 0xc0076af900 0xc0076af901}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-87.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.87,PodIP:172.18.129.225,StartTime:2020-06-16 15:56:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-16 15:56:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://06a9223fec19ffda143fced0e63731494af182e76383e415561267c9446b23da,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.18.129.225,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.491: INFO: Pod "webserver-deployment-595b5b9587-bwpzw" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bwpzw webserver-deployment-595b5b9587- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-595b5b9587-bwpzw 66147b8a-4b9c-4ab4-8d0c-2ccdd0be8333 13247 0 2020-06-16 15:56:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 ac0424f5-b841-4984-8027-1f0e37bcdd92 0xc0076afaf7 0xc0076afaf8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-219.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.219,PodIP:172.18.135.13,StartTime:2020-06-16 15:56:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-16 15:56:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://287e3654ba4a9a6f8c21cbe615ad1ab7900de62a859fbcaf37bac488924e1963,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.18.135.13,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.491: INFO: Pod "webserver-deployment-595b5b9587-bzk57" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bzk57 webserver-deployment-595b5b9587- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-595b5b9587-bzk57 2d8437e6-1c6b-4349-bfac-4886f2fa257c 13304 0 2020-06-16 15:56:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 ac0424f5-b841-4984-8027-1f0e37bcdd92 0xc0076afc97 0xc0076afc98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-209.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.209,PodIP:172.18.132.187,StartTime:2020-06-16 15:56:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-16 15:56:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://b07ef4728abbd040f7ee43142cc94525d4a4d532e57a74995fd4df26bf949ed4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.18.132.187,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.491: INFO: Pod "webserver-deployment-595b5b9587-kfc84" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-kfc84 webserver-deployment-595b5b9587- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-595b5b9587-kfc84 a138e15c-117b-453f-9c68-435e2da3be11 13265 0 2020-06-16 15:56:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 ac0424f5-b841-4984-8027-1f0e37bcdd92 0xc0076afe07 0xc0076afe08}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-134.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.134,PodIP:172.18.131.51,StartTime:2020-06-16 15:56:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-16 15:56:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://fd58abe908b3945c7f606509dc9935aed94376c627e4495ff8c153f7b84f300a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.18.131.51,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.491: INFO: Pod "webserver-deployment-595b5b9587-lzl8n" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lzl8n webserver-deployment-595b5b9587- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-595b5b9587-lzl8n 7134ee04-ecca-44ef-83b4-5e341278d37c 13258 0 2020-06-16 15:56:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 ac0424f5-b841-4984-8027-1f0e37bcdd92 0xc003586117 0xc003586118}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-6.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.6,PodIP:172.18.128.15,StartTime:2020-06-16 15:56:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-16 15:56:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://8309edf890dfef40bbfc8d0553d9ca2cbbf22ce0a7e78cd7b5adce07c7820516,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.18.128.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.492: INFO: Pod "webserver-deployment-595b5b9587-n7bkc" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-n7bkc webserver-deployment-595b5b9587- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-595b5b9587-n7bkc e1df28e0-3d86-4622-8277-40b128610f7c 13413 0 2020-06-16 15:56:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 ac0424f5-b841-4984-8027-1f0e37bcdd92 0xc003586317 0xc003586318}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-87.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.492: INFO: Pod "webserver-deployment-595b5b9587-r8spp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-r8spp webserver-deployment-595b5b9587- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-595b5b9587-r8spp c25b752c-6b11-4ea3-ae66-d65493987a35 13268 0 2020-06-16 15:56:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 ac0424f5-b841-4984-8027-1f0e37bcdd92 0xc003586460 0xc003586461}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-134.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.134,PodIP:172.18.131.150,StartTime:2020-06-16 15:56:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-16 15:56:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://786d570e8883d336b32e5efce6c744a20bc6b413b0b1ab5c607a9790fa8c12f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.18.131.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.492: INFO: Pod "webserver-deployment-595b5b9587-wl6kw" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wl6kw webserver-deployment-595b5b9587- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-595b5b9587-wl6kw fe28848b-56c7-45ae-9100-72494251d1e9 13291 0 2020-06-16 15:56:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 ac0424f5-b841-4984-8027-1f0e37bcdd92 0xc0035865e7 0xc0035865e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-209.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.209,PodIP:172.18.138.22,StartTime:2020-06-16 15:56:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-16 15:56:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://017bd671c1c671f4c5e8d830d33d82d6f9869802a392a662350ddbf374ab6eb5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.18.138.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.492: INFO: Pod "webserver-deployment-595b5b9587-xglxw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xglxw webserver-deployment-595b5b9587- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-595b5b9587-xglxw d807c4b4-d5cb-4898-afdb-f962797df7f0 13420 0 2020-06-16 15:56:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 ac0424f5-b841-4984-8027-1f0e37bcdd92 0xc003586777 0xc003586778}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-209.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.492: INFO: Pod "webserver-deployment-595b5b9587-zfc5g" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zfc5g webserver-deployment-595b5b9587- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-595b5b9587-zfc5g 2d496235-731c-47e7-a9ef-0497d2692fba 13262 0 2020-06-16 15:56:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 ac0424f5-b841-4984-8027-1f0e37bcdd92 0xc003586930 0xc003586931}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-134.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.134,PodIP:172.18.136.192,StartTime:2020-06-16 15:56:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-16 15:56:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://4c7315495c3004429d11e3328f526ede8fc96efc02b76e800649f254d3869290,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.18.136.192,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.492: INFO: Pod "webserver-deployment-c7997dcc8-6675h" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-6675h webserver-deployment-c7997dcc8- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-c7997dcc8-6675h 79d9aaa1-aea0-4650-98c6-7d442ea2ee5d 13392 0 2020-06-16 15:56:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 0bbbd130-0274-4249-9a72-8dfd41294b86 0xc003586ad7 0xc003586ad8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-6.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.6,PodIP:172.18.130.45,StartTime:2020-06-16 15:56:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login',},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.18.130.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.492: INFO: Pod "webserver-deployment-c7997dcc8-8xphq" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8xphq webserver-deployment-c7997dcc8- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-c7997dcc8-8xphq 6dda96c9-7c9d-436a-bcba-2199a98e44b3 13401 0 2020-06-16 15:56:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 0bbbd130-0274-4249-9a72-8dfd41294b86 0xc003586e10 0xc003586e11}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-134.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.134,PodIP:172.18.138.251,StartTime:2020-06-16 15:56:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login',},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.18.138.251,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.493: INFO: Pod "webserver-deployment-c7997dcc8-bkrgm" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-bkrgm webserver-deployment-c7997dcc8- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-c7997dcc8-bkrgm b81a7632-bb5c-4886-80d4-a6c486b35a45 13357 0 2020-06-16 15:56:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 0bbbd130-0274-4249-9a72-8dfd41294b86 0xc003587067 0xc003587068}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-87.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.87,PodIP:,StartTime:2020-06-16 15:56:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.493: INFO: Pod "webserver-deployment-c7997dcc8-gjbps" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-gjbps webserver-deployment-c7997dcc8- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-c7997dcc8-gjbps 1d47a6f9-2183-4376-a61d-df557658e812 13416 0 2020-06-16 15:56:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 0bbbd130-0274-4249-9a72-8dfd41294b86 0xc003587267 0xc003587268}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-122.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.493: INFO: Pod "webserver-deployment-c7997dcc8-rzz7f" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-rzz7f webserver-deployment-c7997dcc8- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-c7997dcc8-rzz7f b7426b1c-d10c-4743-b6f4-8259f41b63b7 13342 0 2020-06-16 15:56:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 0bbbd130-0274-4249-9a72-8dfd41294b86 0xc0035873e0 0xc0035873e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-209.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.209,PodIP:,StartTime:2020-06-16 15:56:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.493: INFO: Pod "webserver-deployment-c7997dcc8-tf9p4" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-tf9p4 webserver-deployment-c7997dcc8- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-c7997dcc8-tf9p4 5a6bf64d-e402-46d7-a07f-544cdc30b053 13418 0 2020-06-16 15:56:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 0bbbd130-0274-4249-9a72-8dfd41294b86 0xc003587677 0xc003587678}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 16 15:56:56.493: INFO: Pod "webserver-deployment-c7997dcc8-xbs7p" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xbs7p webserver-deployment-c7997dcc8- deployment-1724 /api/v1/namespaces/deployment-1724/pods/webserver-deployment-c7997dcc8-xbs7p 3cf7b0b4-510c-4d18-bc7d-cb7b733992f6 13398 0 2020-06-16 15:56:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 0bbbd130-0274-4249-9a72-8dfd41294b86 0xc0035877c7 0xc0035877c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-lz4w7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-lz4w7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-lz4w7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-219.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 15:56:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.219,PodIP:172.18.140.103,StartTime:2020-06-16 15:56:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login',},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.18.140.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:56:56.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1724" for this suite.
Jun 16 15:57:02.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:57:02.584: INFO: namespace deployment-1724 deletion completed in 6.084848117s

• [SLOW TEST:18.311 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:57:02.584: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 15:57:02.910: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 15:57:05.919: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 15:57:05.921: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-384-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:57:07.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5798" for this suite.
Jun 16 15:57:13.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:57:13.220: INFO: namespace webhook-5798 deletion completed in 6.054444524s
STEP: Destroying namespace "webhook-5798-markers" for this suite.
Jun 16 15:57:19.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:57:19.324: INFO: namespace webhook-5798-markers deletion completed in 6.103603205s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.891 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:57:19.475: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6191
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-2484eb06-1f96-47af-aca2-338c6b4200f9
STEP: Creating a pod to test consume secrets
Jun 16 15:57:19.603: INFO: Waiting up to 5m0s for pod "pod-secrets-c3ba93cc-27b5-4a14-bbab-70d90e135d81" in namespace "secrets-6191" to be "success or failure"
Jun 16 15:57:19.605: INFO: Pod "pod-secrets-c3ba93cc-27b5-4a14-bbab-70d90e135d81": Phase="Pending", Reason="", readiness=false. Elapsed: 1.737262ms
Jun 16 15:57:21.607: INFO: Pod "pod-secrets-c3ba93cc-27b5-4a14-bbab-70d90e135d81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003653509s
Jun 16 15:57:23.609: INFO: Pod "pod-secrets-c3ba93cc-27b5-4a14-bbab-70d90e135d81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005817469s
STEP: Saw pod success
Jun 16 15:57:23.609: INFO: Pod "pod-secrets-c3ba93cc-27b5-4a14-bbab-70d90e135d81" satisfied condition "success or failure"
Jun 16 15:57:23.611: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod pod-secrets-c3ba93cc-27b5-4a14-bbab-70d90e135d81 container secret-env-test: <nil>
STEP: delete the pod
Jun 16 15:57:23.622: INFO: Waiting for pod pod-secrets-c3ba93cc-27b5-4a14-bbab-70d90e135d81 to disappear
Jun 16 15:57:23.623: INFO: Pod pod-secrets-c3ba93cc-27b5-4a14-bbab-70d90e135d81 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:57:23.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6191" for this suite.
Jun 16 15:57:29.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:57:29.682: INFO: namespace secrets-6191 deletion completed in 6.055909999s

• [SLOW TEST:10.206 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:57:29.682: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1776
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 15:57:30.059: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 16 15:57:32.065: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919850, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919850, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919850, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919850, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 15:57:35.072: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:57:45.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1776" for this suite.
Jun 16 15:57:51.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:57:51.237: INFO: namespace webhook-1776 deletion completed in 6.060543266s
STEP: Destroying namespace "webhook-1776-markers" for this suite.
Jun 16 15:57:57.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:57:57.333: INFO: namespace webhook-1776-markers deletion completed in 6.096058313s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.805 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:57:57.487: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4251
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:58:03.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4251" for this suite.
Jun 16 15:58:09.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:58:09.669: INFO: namespace job-4251 deletion completed in 6.055436376s

• [SLOW TEST:12.183 seconds]
[sig-apps] Job
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:58:09.669: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-579
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Jun 16 15:58:09.794: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-579" to be "success or failure"
Jun 16 15:58:09.796: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 1.852762ms
Jun 16 15:58:11.798: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00400961s
STEP: Saw pod success
Jun 16 15:58:11.798: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jun 16 15:58:11.800: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jun 16 15:58:11.844: INFO: Waiting for pod pod-host-path-test to disappear
Jun 16 15:58:11.846: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:58:11.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-579" for this suite.
Jun 16 15:58:17.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:58:17.902: INFO: namespace hostpath-579 deletion completed in 6.053561871s

• [SLOW TEST:8.233 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:58:17.902: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2607
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 15:58:18.027: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1bf75cd0-0e48-4e82-bfcd-8e9a0f6744dc" in namespace "downward-api-2607" to be "success or failure"
Jun 16 15:58:18.028: INFO: Pod "downwardapi-volume-1bf75cd0-0e48-4e82-bfcd-8e9a0f6744dc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.637094ms
Jun 16 15:58:20.031: INFO: Pod "downwardapi-volume-1bf75cd0-0e48-4e82-bfcd-8e9a0f6744dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003732777s
STEP: Saw pod success
Jun 16 15:58:20.031: INFO: Pod "downwardapi-volume-1bf75cd0-0e48-4e82-bfcd-8e9a0f6744dc" satisfied condition "success or failure"
Jun 16 15:58:20.032: INFO: Trying to get logs from node ip-172-19-65-87.eu-west-1.compute.internal pod downwardapi-volume-1bf75cd0-0e48-4e82-bfcd-8e9a0f6744dc container client-container: <nil>
STEP: delete the pod
Jun 16 15:58:20.043: INFO: Waiting for pod downwardapi-volume-1bf75cd0-0e48-4e82-bfcd-8e9a0f6744dc to disappear
Jun 16 15:58:20.044: INFO: Pod downwardapi-volume-1bf75cd0-0e48-4e82-bfcd-8e9a0f6744dc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:58:20.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2607" for this suite.
Jun 16 15:58:26.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:58:26.098: INFO: namespace downward-api-2607 deletion completed in 6.052343407s

• [SLOW TEST:8.196 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:58:26.099: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5450
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 15:58:26.225: INFO: Waiting up to 5m0s for pod "downwardapi-volume-610747ed-e79e-42d1-83a2-d9f05a14549a" in namespace "downward-api-5450" to be "success or failure"
Jun 16 15:58:26.227: INFO: Pod "downwardapi-volume-610747ed-e79e-42d1-83a2-d9f05a14549a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02607ms
Jun 16 15:58:28.229: INFO: Pod "downwardapi-volume-610747ed-e79e-42d1-83a2-d9f05a14549a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003899449s
STEP: Saw pod success
Jun 16 15:58:28.229: INFO: Pod "downwardapi-volume-610747ed-e79e-42d1-83a2-d9f05a14549a" satisfied condition "success or failure"
Jun 16 15:58:28.230: INFO: Trying to get logs from node ip-172-19-65-134.eu-west-1.compute.internal pod downwardapi-volume-610747ed-e79e-42d1-83a2-d9f05a14549a container client-container: <nil>
STEP: delete the pod
Jun 16 15:58:28.243: INFO: Waiting for pod downwardapi-volume-610747ed-e79e-42d1-83a2-d9f05a14549a to disappear
Jun 16 15:58:28.246: INFO: Pod downwardapi-volume-610747ed-e79e-42d1-83a2-d9f05a14549a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:58:28.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5450" for this suite.
Jun 16 15:58:34.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:58:34.302: INFO: namespace downward-api-5450 deletion completed in 6.05377427s

• [SLOW TEST:8.203 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:58:34.303: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jun 16 15:58:36.936: INFO: Successfully updated pod "adopt-release-cw8kk"
STEP: Checking that the Job readopts the Pod
Jun 16 15:58:36.936: INFO: Waiting up to 15m0s for pod "adopt-release-cw8kk" in namespace "job-4974" to be "adopted"
Jun 16 15:58:36.938: INFO: Pod "adopt-release-cw8kk": Phase="Running", Reason="", readiness=true. Elapsed: 2.10513ms
Jun 16 15:58:38.940: INFO: Pod "adopt-release-cw8kk": Phase="Running", Reason="", readiness=true. Elapsed: 2.003969639s
Jun 16 15:58:38.940: INFO: Pod "adopt-release-cw8kk" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jun 16 15:58:39.445: INFO: Successfully updated pod "adopt-release-cw8kk"
STEP: Checking that the Job releases the Pod
Jun 16 15:58:39.445: INFO: Waiting up to 15m0s for pod "adopt-release-cw8kk" in namespace "job-4974" to be "released"
Jun 16 15:58:39.447: INFO: Pod "adopt-release-cw8kk": Phase="Running", Reason="", readiness=true. Elapsed: 2.267693ms
Jun 16 15:58:41.449: INFO: Pod "adopt-release-cw8kk": Phase="Running", Reason="", readiness=true. Elapsed: 2.004208431s
Jun 16 15:58:41.449: INFO: Pod "adopt-release-cw8kk" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:58:41.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4974" for this suite.
Jun 16 15:59:25.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:59:25.506: INFO: namespace job-4974 deletion completed in 44.054936294s

• [SLOW TEST:51.204 seconds]
[sig-apps] Job
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:59:25.507: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4810
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4810
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4810
I0616 15:59:25.639804      24 runners.go:184] Created replication controller with name: externalname-service, namespace: services-4810, replica count: 2
Jun 16 15:59:28.690: INFO: Creating new exec pod
I0616 15:59:28.690147      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 16 15:59:31.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=services-4810 execpodxstlv -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 16 15:59:31.842: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 16 15:59:31.842: INFO: stdout: ""
Jun 16 15:59:31.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=services-4810 execpodxstlv -- /bin/sh -x -c nc -zv -t -w 2 172.31.149.242 80'
Jun 16 15:59:31.976: INFO: stderr: "+ nc -zv -t -w 2 172.31.149.242 80\nConnection to 172.31.149.242 80 port [tcp/http] succeeded!\n"
Jun 16 15:59:31.976: INFO: stdout: ""
Jun 16 15:59:31.976: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:59:31.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4810" for this suite.
Jun 16 15:59:38.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:59:38.049: INFO: namespace services-4810 deletion completed in 6.058251893s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:12.542 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:59:38.049: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-6676
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Jun 16 15:59:38.170: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Jun 16 15:59:38.701: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 16 15:59:40.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919978, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919978, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919978, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919978, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 16 15:59:42.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919978, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919978, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919978, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919978, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 16 15:59:44.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919978, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919978, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919978, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919978, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 16 15:59:46.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919978, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919978, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919978, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727919978, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 16 15:59:50.143: INFO: Waited 1.417898598s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 15:59:51.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6676" for this suite.
Jun 16 15:59:57.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 15:59:57.484: INFO: namespace aggregator-6676 deletion completed in 6.149377315s

• [SLOW TEST:19.435 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 15:59:57.484: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1528
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-d1a74324-3eb9-4d8b-812a-0fc3f199e865
STEP: Creating secret with name s-test-opt-upd-05d4f397-48c4-4cf2-83fd-04aa6edd645c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d1a74324-3eb9-4d8b-812a-0fc3f199e865
STEP: Updating secret s-test-opt-upd-05d4f397-48c4-4cf2-83fd-04aa6edd645c
STEP: Creating secret with name s-test-opt-create-3ca31e37-b196-48c0-8857-900447b86248
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:00:01.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1528" for this suite.
Jun 16 16:00:13.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:00:13.726: INFO: namespace projected-1528 deletion completed in 12.054602045s

• [SLOW TEST:16.242 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:00:13.726: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3984
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 16 16:00:16.862: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:00:17.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3984" for this suite.
Jun 16 16:00:29.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:00:29.927: INFO: namespace replicaset-3984 deletion completed in 12.055152693s

• [SLOW TEST:16.201 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:00:29.927: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6185
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-5c0d0789-273b-4897-afef-7b880eb5a261
STEP: Creating a pod to test consume configMaps
Jun 16 16:00:30.054: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-626df359-7a03-4c80-9275-ef5e7ff71a5b" in namespace "projected-6185" to be "success or failure"
Jun 16 16:00:30.056: INFO: Pod "pod-projected-configmaps-626df359-7a03-4c80-9275-ef5e7ff71a5b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.539516ms
Jun 16 16:00:32.058: INFO: Pod "pod-projected-configmaps-626df359-7a03-4c80-9275-ef5e7ff71a5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003617351s
STEP: Saw pod success
Jun 16 16:00:32.058: INFO: Pod "pod-projected-configmaps-626df359-7a03-4c80-9275-ef5e7ff71a5b" satisfied condition "success or failure"
Jun 16 16:00:32.059: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod pod-projected-configmaps-626df359-7a03-4c80-9275-ef5e7ff71a5b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 16 16:00:32.069: INFO: Waiting for pod pod-projected-configmaps-626df359-7a03-4c80-9275-ef5e7ff71a5b to disappear
Jun 16 16:00:32.072: INFO: Pod pod-projected-configmaps-626df359-7a03-4c80-9275-ef5e7ff71a5b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:00:32.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6185" for this suite.
Jun 16 16:00:38.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:00:38.129: INFO: namespace projected-6185 deletion completed in 6.055226583s

• [SLOW TEST:8.202 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:00:38.130: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 16 16:00:42.274: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 16 16:00:42.276: INFO: Pod pod-with-poststart-http-hook still exists
Jun 16 16:00:44.276: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 16 16:00:44.278: INFO: Pod pod-with-poststart-http-hook still exists
Jun 16 16:00:46.276: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 16 16:00:46.279: INFO: Pod pod-with-poststart-http-hook still exists
Jun 16 16:00:48.276: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 16 16:00:48.278: INFO: Pod pod-with-poststart-http-hook still exists
Jun 16 16:00:50.276: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 16 16:00:50.278: INFO: Pod pod-with-poststart-http-hook still exists
Jun 16 16:00:52.276: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 16 16:00:52.278: INFO: Pod pod-with-poststart-http-hook still exists
Jun 16 16:00:54.276: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 16 16:00:54.278: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:00:54.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2331" for this suite.
Jun 16 16:01:10.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:01:10.336: INFO: namespace container-lifecycle-hook-2331 deletion completed in 16.054880516s

• [SLOW TEST:32.206 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:01:10.336: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-7067
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jun 16 16:01:10.816: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 16:01:13.827: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:01:13.829: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:01:14.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7067" for this suite.
Jun 16 16:01:20.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:01:20.960: INFO: namespace crd-webhook-7067 deletion completed in 6.054793159s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:10.777 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:01:21.113: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:01:25.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6736" for this suite.
Jun 16 16:01:31.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:01:31.302: INFO: namespace kubelet-test-6736 deletion completed in 6.05539614s

• [SLOW TEST:10.188 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:01:31.302: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9281
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-9065da52-606b-4db5-8b2a-dca75d903e30
STEP: Creating a pod to test consume configMaps
Jun 16 16:01:31.429: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ec178005-8783-44f0-88c7-3b3c61d618ff" in namespace "projected-9281" to be "success or failure"
Jun 16 16:01:31.431: INFO: Pod "pod-projected-configmaps-ec178005-8783-44f0-88c7-3b3c61d618ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.247176ms
Jun 16 16:01:33.434: INFO: Pod "pod-projected-configmaps-ec178005-8783-44f0-88c7-3b3c61d618ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004363878s
STEP: Saw pod success
Jun 16 16:01:33.434: INFO: Pod "pod-projected-configmaps-ec178005-8783-44f0-88c7-3b3c61d618ff" satisfied condition "success or failure"
Jun 16 16:01:33.435: INFO: Trying to get logs from node ip-172-19-65-134.eu-west-1.compute.internal pod pod-projected-configmaps-ec178005-8783-44f0-88c7-3b3c61d618ff container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 16 16:01:33.448: INFO: Waiting for pod pod-projected-configmaps-ec178005-8783-44f0-88c7-3b3c61d618ff to disappear
Jun 16 16:01:33.451: INFO: Pod pod-projected-configmaps-ec178005-8783-44f0-88c7-3b3c61d618ff no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:01:33.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9281" for this suite.
Jun 16 16:01:39.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:01:39.507: INFO: namespace projected-9281 deletion completed in 6.054627154s

• [SLOW TEST:8.206 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:01:39.507: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3703
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-e63fa8be-b171-445b-8a99-5862c3e59e18
STEP: Creating a pod to test consume secrets
Jun 16 16:01:39.637: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2273184d-21a9-4a85-9d55-8d98ba482300" in namespace "projected-3703" to be "success or failure"
Jun 16 16:01:39.641: INFO: Pod "pod-projected-secrets-2273184d-21a9-4a85-9d55-8d98ba482300": Phase="Pending", Reason="", readiness=false. Elapsed: 3.388545ms
Jun 16 16:01:41.643: INFO: Pod "pod-projected-secrets-2273184d-21a9-4a85-9d55-8d98ba482300": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005497765s
STEP: Saw pod success
Jun 16 16:01:41.643: INFO: Pod "pod-projected-secrets-2273184d-21a9-4a85-9d55-8d98ba482300" satisfied condition "success or failure"
Jun 16 16:01:41.644: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod pod-projected-secrets-2273184d-21a9-4a85-9d55-8d98ba482300 container secret-volume-test: <nil>
STEP: delete the pod
Jun 16 16:01:41.653: INFO: Waiting for pod pod-projected-secrets-2273184d-21a9-4a85-9d55-8d98ba482300 to disappear
Jun 16 16:01:41.656: INFO: Pod pod-projected-secrets-2273184d-21a9-4a85-9d55-8d98ba482300 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:01:41.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3703" for this suite.
Jun 16 16:01:47.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:01:47.709: INFO: namespace projected-3703 deletion completed in 6.051570281s

• [SLOW TEST:8.202 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:01:47.710: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7641
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-84b594f4-6146-47ed-99ee-644853b93db6
STEP: Creating a pod to test consume configMaps
Jun 16 16:01:47.834: INFO: Waiting up to 5m0s for pod "pod-configmaps-936e5231-dbb5-42d0-a105-3113123e625e" in namespace "configmap-7641" to be "success or failure"
Jun 16 16:01:47.835: INFO: Pod "pod-configmaps-936e5231-dbb5-42d0-a105-3113123e625e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.273305ms
Jun 16 16:01:49.837: INFO: Pod "pod-configmaps-936e5231-dbb5-42d0-a105-3113123e625e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003457641s
STEP: Saw pod success
Jun 16 16:01:49.837: INFO: Pod "pod-configmaps-936e5231-dbb5-42d0-a105-3113123e625e" satisfied condition "success or failure"
Jun 16 16:01:49.839: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod pod-configmaps-936e5231-dbb5-42d0-a105-3113123e625e container configmap-volume-test: <nil>
STEP: delete the pod
Jun 16 16:01:49.848: INFO: Waiting for pod pod-configmaps-936e5231-dbb5-42d0-a105-3113123e625e to disappear
Jun 16 16:01:49.849: INFO: Pod pod-configmaps-936e5231-dbb5-42d0-a105-3113123e625e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:01:49.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7641" for this suite.
Jun 16 16:01:55.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:01:55.905: INFO: namespace configmap-7641 deletion completed in 6.053050819s

• [SLOW TEST:8.195 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:01:55.905: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1339
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Jun 16 16:01:58.546: INFO: Successfully updated pod "labelsupdatefa1ae1ec-f30b-4bfb-8c78-c5a4d90a5358"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:02:02.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1339" for this suite.
Jun 16 16:02:14.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:02:14.622: INFO: namespace projected-1339 deletion completed in 12.053367456s

• [SLOW TEST:18.717 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:02:14.622: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2317
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 16 16:02:14.747: INFO: Waiting up to 5m0s for pod "pod-6e67aacf-c498-4694-a811-716397ae42b8" in namespace "emptydir-2317" to be "success or failure"
Jun 16 16:02:14.750: INFO: Pod "pod-6e67aacf-c498-4694-a811-716397ae42b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.793405ms
Jun 16 16:02:16.752: INFO: Pod "pod-6e67aacf-c498-4694-a811-716397ae42b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005227122s
STEP: Saw pod success
Jun 16 16:02:16.752: INFO: Pod "pod-6e67aacf-c498-4694-a811-716397ae42b8" satisfied condition "success or failure"
Jun 16 16:02:16.754: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod pod-6e67aacf-c498-4694-a811-716397ae42b8 container test-container: <nil>
STEP: delete the pod
Jun 16 16:02:16.766: INFO: Waiting for pod pod-6e67aacf-c498-4694-a811-716397ae42b8 to disappear
Jun 16 16:02:16.768: INFO: Pod pod-6e67aacf-c498-4694-a811-716397ae42b8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:02:16.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2317" for this suite.
Jun 16 16:02:22.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:02:22.823: INFO: namespace emptydir-2317 deletion completed in 6.053142466s

• [SLOW TEST:8.201 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:02:22.824: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5705
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jun 16 16:02:22.947: INFO: Waiting up to 5m0s for pod "downward-api-85a9fae5-2a33-4773-8799-f30dbaa23ec7" in namespace "downward-api-5705" to be "success or failure"
Jun 16 16:02:22.949: INFO: Pod "downward-api-85a9fae5-2a33-4773-8799-f30dbaa23ec7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057967ms
Jun 16 16:02:24.951: INFO: Pod "downward-api-85a9fae5-2a33-4773-8799-f30dbaa23ec7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00417002s
STEP: Saw pod success
Jun 16 16:02:24.951: INFO: Pod "downward-api-85a9fae5-2a33-4773-8799-f30dbaa23ec7" satisfied condition "success or failure"
Jun 16 16:02:24.953: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod downward-api-85a9fae5-2a33-4773-8799-f30dbaa23ec7 container dapi-container: <nil>
STEP: delete the pod
Jun 16 16:02:24.964: INFO: Waiting for pod downward-api-85a9fae5-2a33-4773-8799-f30dbaa23ec7 to disappear
Jun 16 16:02:24.965: INFO: Pod downward-api-85a9fae5-2a33-4773-8799-f30dbaa23ec7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:02:24.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5705" for this suite.
Jun 16 16:02:30.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:02:31.023: INFO: namespace downward-api-5705 deletion completed in 6.055505469s

• [SLOW TEST:8.199 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:02:31.023: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1750
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-jvlh
STEP: Creating a pod to test atomic-volume-subpath
Jun 16 16:02:31.155: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-jvlh" in namespace "subpath-1750" to be "success or failure"
Jun 16 16:02:31.158: INFO: Pod "pod-subpath-test-projected-jvlh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.949496ms
Jun 16 16:02:33.160: INFO: Pod "pod-subpath-test-projected-jvlh": Phase="Running", Reason="", readiness=true. Elapsed: 2.004970972s
Jun 16 16:02:35.162: INFO: Pod "pod-subpath-test-projected-jvlh": Phase="Running", Reason="", readiness=true. Elapsed: 4.006822094s
Jun 16 16:02:37.164: INFO: Pod "pod-subpath-test-projected-jvlh": Phase="Running", Reason="", readiness=true. Elapsed: 6.009026137s
Jun 16 16:02:39.166: INFO: Pod "pod-subpath-test-projected-jvlh": Phase="Running", Reason="", readiness=true. Elapsed: 8.01124776s
Jun 16 16:02:41.168: INFO: Pod "pod-subpath-test-projected-jvlh": Phase="Running", Reason="", readiness=true. Elapsed: 10.013728629s
Jun 16 16:02:43.171: INFO: Pod "pod-subpath-test-projected-jvlh": Phase="Running", Reason="", readiness=true. Elapsed: 12.015839858s
Jun 16 16:02:45.173: INFO: Pod "pod-subpath-test-projected-jvlh": Phase="Running", Reason="", readiness=true. Elapsed: 14.018071317s
Jun 16 16:02:47.175: INFO: Pod "pod-subpath-test-projected-jvlh": Phase="Running", Reason="", readiness=true. Elapsed: 16.020116316s
Jun 16 16:02:49.177: INFO: Pod "pod-subpath-test-projected-jvlh": Phase="Running", Reason="", readiness=true. Elapsed: 18.022214305s
Jun 16 16:02:51.179: INFO: Pod "pod-subpath-test-projected-jvlh": Phase="Running", Reason="", readiness=true. Elapsed: 20.02459885s
Jun 16 16:02:53.181: INFO: Pod "pod-subpath-test-projected-jvlh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.026484997s
STEP: Saw pod success
Jun 16 16:02:53.181: INFO: Pod "pod-subpath-test-projected-jvlh" satisfied condition "success or failure"
Jun 16 16:02:53.183: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod pod-subpath-test-projected-jvlh container test-container-subpath-projected-jvlh: <nil>
STEP: delete the pod
Jun 16 16:02:53.193: INFO: Waiting for pod pod-subpath-test-projected-jvlh to disappear
Jun 16 16:02:53.194: INFO: Pod pod-subpath-test-projected-jvlh no longer exists
STEP: Deleting pod pod-subpath-test-projected-jvlh
Jun 16 16:02:53.194: INFO: Deleting pod "pod-subpath-test-projected-jvlh" in namespace "subpath-1750"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:02:53.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1750" for this suite.
Jun 16 16:02:59.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:02:59.253: INFO: namespace subpath-1750 deletion completed in 6.055247197s

• [SLOW TEST:28.230 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:02:59.253: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6948
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jun 16 16:02:59.381: INFO: Waiting up to 5m0s for pod "downward-api-5a28b098-b174-4f70-b901-0d9de1bcb70e" in namespace "downward-api-6948" to be "success or failure"
Jun 16 16:02:59.384: INFO: Pod "downward-api-5a28b098-b174-4f70-b901-0d9de1bcb70e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.205939ms
Jun 16 16:03:01.386: INFO: Pod "downward-api-5a28b098-b174-4f70-b901-0d9de1bcb70e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004305075s
STEP: Saw pod success
Jun 16 16:03:01.386: INFO: Pod "downward-api-5a28b098-b174-4f70-b901-0d9de1bcb70e" satisfied condition "success or failure"
Jun 16 16:03:01.387: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod downward-api-5a28b098-b174-4f70-b901-0d9de1bcb70e container dapi-container: <nil>
STEP: delete the pod
Jun 16 16:03:01.398: INFO: Waiting for pod downward-api-5a28b098-b174-4f70-b901-0d9de1bcb70e to disappear
Jun 16 16:03:01.400: INFO: Pod downward-api-5a28b098-b174-4f70-b901-0d9de1bcb70e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:03:01.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6948" for this suite.
Jun 16 16:03:07.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:03:07.456: INFO: namespace downward-api-6948 deletion completed in 6.053977933s

• [SLOW TEST:8.203 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:03:07.456: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3153
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-ea99d329-586a-46b5-b9df-b6dcb1118b0c
STEP: Creating a pod to test consume secrets
Jun 16 16:03:07.582: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dcce13c1-cf2c-4fa1-bce4-02e5ec00d384" in namespace "projected-3153" to be "success or failure"
Jun 16 16:03:07.584: INFO: Pod "pod-projected-secrets-dcce13c1-cf2c-4fa1-bce4-02e5ec00d384": Phase="Pending", Reason="", readiness=false. Elapsed: 1.932896ms
Jun 16 16:03:09.586: INFO: Pod "pod-projected-secrets-dcce13c1-cf2c-4fa1-bce4-02e5ec00d384": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004089459s
STEP: Saw pod success
Jun 16 16:03:09.586: INFO: Pod "pod-projected-secrets-dcce13c1-cf2c-4fa1-bce4-02e5ec00d384" satisfied condition "success or failure"
Jun 16 16:03:09.588: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod pod-projected-secrets-dcce13c1-cf2c-4fa1-bce4-02e5ec00d384 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 16 16:03:09.598: INFO: Waiting for pod pod-projected-secrets-dcce13c1-cf2c-4fa1-bce4-02e5ec00d384 to disappear
Jun 16 16:03:09.600: INFO: Pod pod-projected-secrets-dcce13c1-cf2c-4fa1-bce4-02e5ec00d384 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:03:09.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3153" for this suite.
Jun 16 16:03:15.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:03:15.655: INFO: namespace projected-3153 deletion completed in 6.052958372s

• [SLOW TEST:8.199 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:03:15.655: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-788
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-e60ad0a1-0082-45f9-8d1b-9a30499eb903
STEP: Creating a pod to test consume secrets
Jun 16 16:03:15.780: INFO: Waiting up to 5m0s for pod "pod-secrets-c7159c69-79c5-4aa3-8764-67b28cf0ba32" in namespace "secrets-788" to be "success or failure"
Jun 16 16:03:15.782: INFO: Pod "pod-secrets-c7159c69-79c5-4aa3-8764-67b28cf0ba32": Phase="Pending", Reason="", readiness=false. Elapsed: 1.423595ms
Jun 16 16:03:17.784: INFO: Pod "pod-secrets-c7159c69-79c5-4aa3-8764-67b28cf0ba32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003556622s
STEP: Saw pod success
Jun 16 16:03:17.784: INFO: Pod "pod-secrets-c7159c69-79c5-4aa3-8764-67b28cf0ba32" satisfied condition "success or failure"
Jun 16 16:03:17.785: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod pod-secrets-c7159c69-79c5-4aa3-8764-67b28cf0ba32 container secret-volume-test: <nil>
STEP: delete the pod
Jun 16 16:03:17.799: INFO: Waiting for pod pod-secrets-c7159c69-79c5-4aa3-8764-67b28cf0ba32 to disappear
Jun 16 16:03:17.803: INFO: Pod pod-secrets-c7159c69-79c5-4aa3-8764-67b28cf0ba32 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:03:17.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-788" for this suite.
Jun 16 16:03:23.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:03:23.861: INFO: namespace secrets-788 deletion completed in 6.055997851s

• [SLOW TEST:8.206 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:03:23.861: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9325
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 16 16:03:23.985: INFO: Waiting up to 5m0s for pod "pod-3a80851b-636a-4208-890a-b18e49f27c34" in namespace "emptydir-9325" to be "success or failure"
Jun 16 16:03:23.987: INFO: Pod "pod-3a80851b-636a-4208-890a-b18e49f27c34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.16045ms
Jun 16 16:03:25.989: INFO: Pod "pod-3a80851b-636a-4208-890a-b18e49f27c34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00401414s
Jun 16 16:03:27.991: INFO: Pod "pod-3a80851b-636a-4208-890a-b18e49f27c34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006163178s
STEP: Saw pod success
Jun 16 16:03:27.991: INFO: Pod "pod-3a80851b-636a-4208-890a-b18e49f27c34" satisfied condition "success or failure"
Jun 16 16:03:27.993: INFO: Trying to get logs from node ip-172-19-65-134.eu-west-1.compute.internal pod pod-3a80851b-636a-4208-890a-b18e49f27c34 container test-container: <nil>
STEP: delete the pod
Jun 16 16:03:28.002: INFO: Waiting for pod pod-3a80851b-636a-4208-890a-b18e49f27c34 to disappear
Jun 16 16:03:28.003: INFO: Pod pod-3a80851b-636a-4208-890a-b18e49f27c34 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:03:28.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9325" for this suite.
Jun 16 16:03:34.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:03:34.065: INFO: namespace emptydir-9325 deletion completed in 6.059845166s

• [SLOW TEST:10.204 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:03:34.066: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6402
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:03:34.187: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:03:36.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6402" for this suite.
Jun 16 16:04:20.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:04:20.273: INFO: namespace pods-6402 deletion completed in 44.066041988s

• [SLOW TEST:46.208 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:04:20.273: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-4901
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:04:31.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4901" for this suite.
Jun 16 16:04:37.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:04:37.471: INFO: namespace resourcequota-4901 deletion completed in 6.053594621s

• [SLOW TEST:17.198 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:04:37.471: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-960
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-13e473f6-4b7d-478f-ae8a-762d9b753789
STEP: Creating configMap with name cm-test-opt-upd-fefa0fba-e9dd-437d-bc9a-166098b99c6e
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-13e473f6-4b7d-478f-ae8a-762d9b753789
STEP: Updating configmap cm-test-opt-upd-fefa0fba-e9dd-437d-bc9a-166098b99c6e
STEP: Creating configMap with name cm-test-opt-create-3ed66d16-e005-46bd-8a0d-60598f03f0c7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:04:41.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-960" for this suite.
Jun 16 16:05:09.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:05:09.706: INFO: namespace projected-960 deletion completed in 28.057692588s

• [SLOW TEST:32.235 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:05:09.707: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8701
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8701
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-8701
I0616 16:05:09.840536      24 runners.go:184] Created replication controller with name: externalname-service, namespace: services-8701, replica count: 2
Jun 16 16:05:12.891: INFO: Creating new exec pod
I0616 16:05:12.890982      24 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 16 16:05:15.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=services-8701 execpodw49mv -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Jun 16 16:05:16.058: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 16 16:05:16.058: INFO: stdout: ""
Jun 16 16:05:16.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=services-8701 execpodw49mv -- /bin/sh -x -c nc -zv -t -w 2 172.31.210.59 80'
Jun 16 16:05:16.192: INFO: stderr: "+ nc -zv -t -w 2 172.31.210.59 80\nConnection to 172.31.210.59 80 port [tcp/http] succeeded!\n"
Jun 16 16:05:16.192: INFO: stdout: ""
Jun 16 16:05:16.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=services-8701 execpodw49mv -- /bin/sh -x -c nc -zv -t -w 2 172.19.65.122 31558'
Jun 16 16:05:16.327: INFO: stderr: "+ nc -zv -t -w 2 172.19.65.122 31558\nConnection to 172.19.65.122 31558 port [tcp/31558] succeeded!\n"
Jun 16 16:05:16.327: INFO: stdout: ""
Jun 16 16:05:16.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=services-8701 execpodw49mv -- /bin/sh -x -c nc -zv -t -w 2 172.19.65.134 31558'
Jun 16 16:05:16.469: INFO: stderr: "+ nc -zv -t -w 2 172.19.65.134 31558\nConnection to 172.19.65.134 31558 port [tcp/31558] succeeded!\n"
Jun 16 16:05:16.469: INFO: stdout: ""
Jun 16 16:05:16.469: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:05:16.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8701" for this suite.
Jun 16 16:05:22.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:05:22.554: INFO: namespace services-8701 deletion completed in 6.066876261s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:12.847 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:05:22.554: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-4133
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 16 16:05:24.686: INFO: &Pod{ObjectMeta:{send-events-f577b447-89c9-4913-8055-652e084d065c  events-4133 /api/v1/namespaces/events-4133/pods/send-events-f577b447-89c9-4913-8055-652e084d065c f044a6ce-a351-4d23-840a-852751b2b3e9 16404 0 2020-06-16 16:05:22 +0000 UTC <nil> <nil> map[name:foo time:674236161] map[kubernetes.io/psp:cert-exporter-psp] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-w9f4j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-w9f4j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-w9f4j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-209.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 16:05:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 16:05:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 16:05:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 16:05:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.209,PodIP:172.18.128.137,StartTime:2020-06-16 16:05:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-16 16:05:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://6320848cc92b8651e28cc05632077b867a5d25e63f98391b6073ffb8e1a84ab7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.18.128.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Jun 16 16:05:26.689: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 16 16:05:28.691: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:05:28.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4133" for this suite.
Jun 16 16:06:10.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:06:10.750: INFO: namespace events-4133 deletion completed in 42.054481138s

• [SLOW TEST:48.197 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:06:10.750: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6667
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jun 16 16:06:14.885: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-790258633 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jun 16 16:06:19.939: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:06:19.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6667" for this suite.
Jun 16 16:06:25.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:06:25.997: INFO: namespace pods-6667 deletion completed in 6.053802626s

• [SLOW TEST:15.246 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:06:25.997: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6606
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Jun 16 16:06:26.121: INFO: Waiting up to 5m0s for pod "downward-api-af472b59-4750-4829-9a34-7971d14bb909" in namespace "downward-api-6606" to be "success or failure"
Jun 16 16:06:26.124: INFO: Pod "downward-api-af472b59-4750-4829-9a34-7971d14bb909": Phase="Pending", Reason="", readiness=false. Elapsed: 2.938999ms
Jun 16 16:06:28.126: INFO: Pod "downward-api-af472b59-4750-4829-9a34-7971d14bb909": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005120495s
STEP: Saw pod success
Jun 16 16:06:28.126: INFO: Pod "downward-api-af472b59-4750-4829-9a34-7971d14bb909" satisfied condition "success or failure"
Jun 16 16:06:28.128: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod downward-api-af472b59-4750-4829-9a34-7971d14bb909 container dapi-container: <nil>
STEP: delete the pod
Jun 16 16:06:28.138: INFO: Waiting for pod downward-api-af472b59-4750-4829-9a34-7971d14bb909 to disappear
Jun 16 16:06:28.140: INFO: Pod downward-api-af472b59-4750-4829-9a34-7971d14bb909 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:06:28.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6606" for this suite.
Jun 16 16:06:34.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:06:34.199: INFO: namespace downward-api-6606 deletion completed in 6.055208864s

• [SLOW TEST:8.202 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:06:34.199: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4166
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 16 16:06:40.341: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0616 16:06:40.341435      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:06:40.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4166" for this suite.
Jun 16 16:06:46.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:06:46.401: INFO: namespace gc-4166 deletion completed in 6.057571427s

• [SLOW TEST:12.202 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:06:46.401: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7970
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:06:57.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7970" for this suite.
Jun 16 16:07:03.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:07:03.612: INFO: namespace resourcequota-7970 deletion completed in 6.069936947s

• [SLOW TEST:17.211 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:07:03.612: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4168
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:07:03.752: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 16 16:07:03.757: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:03.759: INFO: Number of nodes with available pods: 0
Jun 16 16:07:03.759: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:07:04.761: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:04.763: INFO: Number of nodes with available pods: 1
Jun 16 16:07:04.763: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:07:05.762: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:05.764: INFO: Number of nodes with available pods: 6
Jun 16 16:07:05.764: INFO: Number of running nodes: 6, number of available pods: 6
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 16 16:07:05.779: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:05.779: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:05.779: INFO: Wrong image for pod: daemon-set-8p788. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:05.779: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:05.779: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:05.779: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:05.781: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:06.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:06.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:06.784: INFO: Wrong image for pod: daemon-set-8p788. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:06.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:06.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:06.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:06.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:07.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:07.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:07.784: INFO: Wrong image for pod: daemon-set-8p788. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:07.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:07.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:07.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:07.787: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:08.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:08.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:08.784: INFO: Wrong image for pod: daemon-set-8p788. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:08.784: INFO: Pod daemon-set-8p788 is not available
Jun 16 16:07:08.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:08.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:08.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:08.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:09.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:09.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:09.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:09.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:09.784: INFO: Pod daemon-set-mmwkr is not available
Jun 16 16:07:09.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:09.787: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:10.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:10.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:10.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:10.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:10.784: INFO: Pod daemon-set-mmwkr is not available
Jun 16 16:07:10.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:10.787: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:11.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:11.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:11.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:11.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:11.784: INFO: Pod daemon-set-mmwkr is not available
Jun 16 16:07:11.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:11.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:12.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:12.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:12.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:12.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:12.784: INFO: Pod daemon-set-mmwkr is not available
Jun 16 16:07:12.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:12.787: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:13.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:13.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:13.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:13.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:13.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:13.787: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:14.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:14.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:14.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:14.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:14.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:14.784: INFO: Pod daemon-set-wk547 is not available
Jun 16 16:07:14.787: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:15.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:15.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:15.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:15.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:15.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:15.784: INFO: Pod daemon-set-wk547 is not available
Jun 16 16:07:15.787: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:16.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:16.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:16.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:16.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:16.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:16.784: INFO: Pod daemon-set-wk547 is not available
Jun 16 16:07:16.787: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:17.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:17.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:17.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:17.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:17.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:17.784: INFO: Pod daemon-set-wk547 is not available
Jun 16 16:07:17.787: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:18.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:18.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:18.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:18.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:18.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:18.784: INFO: Pod daemon-set-wk547 is not available
Jun 16 16:07:18.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:19.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:19.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:19.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:19.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:19.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:19.784: INFO: Pod daemon-set-wk547 is not available
Jun 16 16:07:19.787: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:20.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:20.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:20.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:20.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:20.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:20.784: INFO: Pod daemon-set-wk547 is not available
Jun 16 16:07:20.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:21.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:21.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:21.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:21.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:21.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:21.784: INFO: Pod daemon-set-wk547 is not available
Jun 16 16:07:21.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:22.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:22.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:22.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:22.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:22.784: INFO: Wrong image for pod: daemon-set-wk547. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:22.784: INFO: Pod daemon-set-wk547 is not available
Jun 16 16:07:22.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:23.784: INFO: Pod daemon-set-5w955 is not available
Jun 16 16:07:23.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:23.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:23.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:23.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:23.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:24.784: INFO: Pod daemon-set-5w955 is not available
Jun 16 16:07:24.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:24.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:24.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:24.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:24.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:25.784: INFO: Pod daemon-set-5w955 is not available
Jun 16 16:07:25.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:25.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:25.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:25.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:25.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:26.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:26.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:26.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:26.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:26.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:27.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:27.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:27.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:27.784: INFO: Wrong image for pod: daemon-set-f4d8n. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:27.784: INFO: Pod daemon-set-f4d8n is not available
Jun 16 16:07:27.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:28.783: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:28.783: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:28.783: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:28.783: INFO: Pod daemon-set-kbqbc is not available
Jun 16 16:07:28.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:29.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:29.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:29.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:29.784: INFO: Pod daemon-set-kbqbc is not available
Jun 16 16:07:29.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:30.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:30.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:30.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:30.784: INFO: Pod daemon-set-kbqbc is not available
Jun 16 16:07:30.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:31.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:31.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:31.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:31.784: INFO: Pod daemon-set-kbqbc is not available
Jun 16 16:07:31.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:32.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:32.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:32.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:32.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:33.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:33.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:33.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:33.790: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:34.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:34.784: INFO: Pod daemon-set-5zf4r is not available
Jun 16 16:07:34.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:34.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:34.787: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:35.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:35.784: INFO: Pod daemon-set-5zf4r is not available
Jun 16 16:07:35.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:35.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:35.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:36.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:36.784: INFO: Pod daemon-set-5zf4r is not available
Jun 16 16:07:36.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:36.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:36.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:37.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:37.784: INFO: Pod daemon-set-5zf4r is not available
Jun 16 16:07:37.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:37.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:37.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:38.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:38.784: INFO: Pod daemon-set-5zf4r is not available
Jun 16 16:07:38.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:38.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:38.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:39.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:39.784: INFO: Pod daemon-set-5zf4r is not available
Jun 16 16:07:39.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:39.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:39.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:40.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:40.784: INFO: Pod daemon-set-5zf4r is not available
Jun 16 16:07:40.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:40.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:40.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:41.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:41.784: INFO: Pod daemon-set-5zf4r is not available
Jun 16 16:07:41.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:41.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:41.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:42.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:42.784: INFO: Pod daemon-set-5zf4r is not available
Jun 16 16:07:42.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:42.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:42.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:43.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:43.784: INFO: Pod daemon-set-5zf4r is not available
Jun 16 16:07:43.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:43.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:43.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:44.784: INFO: Wrong image for pod: daemon-set-5zf4r. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:44.784: INFO: Pod daemon-set-5zf4r is not available
Jun 16 16:07:44.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:44.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:44.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:45.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:45.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:45.784: INFO: Pod daemon-set-jxmzj is not available
Jun 16 16:07:45.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:46.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:46.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:46.784: INFO: Pod daemon-set-jxmzj is not available
Jun 16 16:07:46.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:47.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:47.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:47.784: INFO: Pod daemon-set-jxmzj is not available
Jun 16 16:07:47.787: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:48.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:48.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:48.784: INFO: Pod daemon-set-jxmzj is not available
Jun 16 16:07:48.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:49.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:49.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:49.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:50.784: INFO: Wrong image for pod: daemon-set-7cf2s. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:50.784: INFO: Pod daemon-set-7cf2s is not available
Jun 16 16:07:50.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:50.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:51.784: INFO: Pod daemon-set-bn65l is not available
Jun 16 16:07:51.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:51.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:52.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:52.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:53.784: INFO: Wrong image for pod: daemon-set-bwv9v. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Jun 16 16:07:53.784: INFO: Pod daemon-set-bwv9v is not available
Jun 16 16:07:53.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:54.784: INFO: Pod daemon-set-j9mlm is not available
Jun 16 16:07:54.786: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 16 16:07:54.789: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:54.790: INFO: Number of nodes with available pods: 5
Jun 16 16:07:54.790: INFO: Node ip-172-19-65-87.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:07:55.793: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:55.795: INFO: Number of nodes with available pods: 5
Jun 16 16:07:55.795: INFO: Node ip-172-19-65-87.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:07:56.793: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:56.795: INFO: Number of nodes with available pods: 5
Jun 16 16:07:56.795: INFO: Node ip-172-19-65-87.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:07:57.793: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:57.795: INFO: Number of nodes with available pods: 5
Jun 16 16:07:57.795: INFO: Node ip-172-19-65-87.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:07:58.793: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:07:58.795: INFO: Number of nodes with available pods: 6
Jun 16 16:07:58.795: INFO: Number of running nodes: 6, number of available pods: 6
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4168, will wait for the garbage collector to delete the pods
Jun 16 16:07:58.857: INFO: Deleting DaemonSet.extensions daemon-set took: 3.608252ms
Jun 16 16:07:59.858: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.000168072s
Jun 16 16:08:05.260: INFO: Number of nodes with available pods: 0
Jun 16 16:08:05.260: INFO: Number of running nodes: 0, number of available pods: 0
Jun 16 16:08:05.261: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4168/daemonsets","resourceVersion":"17369"},"items":null}

Jun 16 16:08:05.262: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4168/pods","resourceVersion":"17369"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:08:05.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4168" for this suite.
Jun 16 16:08:11.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:08:11.348: INFO: namespace daemonsets-4168 deletion completed in 6.05784017s

• [SLOW TEST:67.736 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:08:11.349: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1797
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 16 16:08:11.473: INFO: Waiting up to 5m0s for pod "pod-d9b51d81-30b6-49bc-8ee8-f2f2b3119bf5" in namespace "emptydir-1797" to be "success or failure"
Jun 16 16:08:11.475: INFO: Pod "pod-d9b51d81-30b6-49bc-8ee8-f2f2b3119bf5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.293787ms
Jun 16 16:08:13.477: INFO: Pod "pod-d9b51d81-30b6-49bc-8ee8-f2f2b3119bf5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004493369s
STEP: Saw pod success
Jun 16 16:08:13.477: INFO: Pod "pod-d9b51d81-30b6-49bc-8ee8-f2f2b3119bf5" satisfied condition "success or failure"
Jun 16 16:08:13.479: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod pod-d9b51d81-30b6-49bc-8ee8-f2f2b3119bf5 container test-container: <nil>
STEP: delete the pod
Jun 16 16:08:13.489: INFO: Waiting for pod pod-d9b51d81-30b6-49bc-8ee8-f2f2b3119bf5 to disappear
Jun 16 16:08:13.491: INFO: Pod pod-d9b51d81-30b6-49bc-8ee8-f2f2b3119bf5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:08:13.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1797" for this suite.
Jun 16 16:08:19.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:08:19.549: INFO: namespace emptydir-1797 deletion completed in 6.055467134s

• [SLOW TEST:8.200 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:08:19.549: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9840
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Jun 16 16:08:19.670: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:08:39.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9840" for this suite.
Jun 16 16:08:45.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:08:45.333: INFO: namespace crd-publish-openapi-9840 deletion completed in 6.054269865s

• [SLOW TEST:25.784 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:08:45.333: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3800
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jun 16 16:08:45.453: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 16 16:08:45.460: INFO: Waiting for terminating namespaces to be deleted...
Jun 16 16:08:45.461: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-122.eu-west-1.compute.internal before test
Jun 16 16:08:45.466: INFO: net-exporter-ltbgf from kube-system started at 2020-06-16 15:14:31 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.466: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 16:08:45.466: INFO: kube-proxy-xdgfc from kube-system started at 2020-06-16 15:06:53 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.466: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 16:08:45.466: INFO: cert-exporter-sgwbs from kube-system started at 2020-06-16 15:10:32 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.466: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 16:08:45.466: INFO: coredns-6d56c484c-wcs6v from kube-system started at 2020-06-16 15:10:34 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.466: INFO: 	Container coredns ready: true, restart count 0
Jun 16 16:08:45.466: INFO: aws-node-jbq4c from kube-system started at 2020-06-16 15:10:43 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.466: INFO: 	Container aws-node ready: true, restart count 0
Jun 16 16:08:45.466: INFO: calico-node-sk9rd from kube-system started at 2020-06-16 15:06:53 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.466: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 16:08:45.466: INFO: node-exporter-5vz2p from kube-system started at 2020-06-16 15:10:31 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.466: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 16:08:45.466: INFO: kube-state-metrics-6d998ffd8b-jpbpj from kube-system started at 2020-06-16 15:10:43 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.466: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 16 16:08:45.466: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-6qlkz from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 16:08:45.466: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 16:08:45.466: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 16:08:45.466: INFO: kiam-agent-x89wb from kube-system started at 2020-06-16 15:10:38 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.466: INFO: 	Container kiam-agent ready: true, restart count 3
Jun 16 16:08:45.466: INFO: external-dns-776fc66667-24mls from kube-system started at 2020-06-16 15:10:42 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.466: INFO: 	Container external-dns ready: true, restart count 0
Jun 16 16:08:45.466: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-134.eu-west-1.compute.internal before test
Jun 16 16:08:45.470: INFO: node-exporter-mw7j6 from kube-system started at 2020-06-16 15:48:15 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.470: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 16:08:45.470: INFO: net-exporter-sq7pm from kube-system started at 2020-06-16 15:48:15 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.470: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 16:08:45.470: INFO: kube-proxy-566k4 from kube-system started at 2020-06-16 15:48:15 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.470: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 16:08:45.470: INFO: aws-node-nxt5r from kube-system started at 2020-06-16 15:48:15 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.470: INFO: 	Container aws-node ready: true, restart count 0
Jun 16 16:08:45.470: INFO: cert-exporter-t962p from kube-system started at 2020-06-16 15:48:15 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.470: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 16:08:45.470: INFO: kiam-agent-8nsfm from kube-system started at 2020-06-16 15:48:45 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.470: INFO: 	Container kiam-agent ready: true, restart count 1
Jun 16 16:08:45.470: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-9457d from sonobuoy started at 2020-06-16 15:48:15 +0000 UTC (2 container statuses recorded)
Jun 16 16:08:45.470: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 16:08:45.470: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 16:08:45.470: INFO: calico-node-7gc92 from kube-system started at 2020-06-16 15:48:15 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.470: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 16:08:45.470: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-209.eu-west-1.compute.internal before test
Jun 16 16:08:45.473: INFO: cert-exporter-2hqzk from kube-system started at 2020-06-16 15:46:38 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.473: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 16:08:45.473: INFO: net-exporter-5vtvm from kube-system started at 2020-06-16 15:46:38 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.473: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 16:08:45.473: INFO: calico-node-rdggr from kube-system started at 2020-06-16 15:46:38 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.473: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 16:08:45.473: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-lbmqr from sonobuoy started at 2020-06-16 15:46:38 +0000 UTC (2 container statuses recorded)
Jun 16 16:08:45.473: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 16:08:45.473: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 16:08:45.473: INFO: kube-proxy-cq9zb from kube-system started at 2020-06-16 15:46:38 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.473: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 16:08:45.473: INFO: aws-node-64x29 from kube-system started at 2020-06-16 15:46:38 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.473: INFO: 	Container aws-node ready: true, restart count 0
Jun 16 16:08:45.473: INFO: node-exporter-ksmqb from kube-system started at 2020-06-16 15:46:38 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.473: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 16:08:45.473: INFO: kiam-agent-wggbf from kube-system started at 2020-06-16 15:46:48 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.473: INFO: 	Container kiam-agent ready: true, restart count 0
Jun 16 16:08:45.473: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-219.eu-west-1.compute.internal before test
Jun 16 16:08:45.477: INFO: sonobuoy from sonobuoy started at 2020-06-16 15:26:40 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.477: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 16 16:08:45.477: INFO: node-exporter-pqhv4 from kube-system started at 2020-06-16 15:10:31 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.477: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 16:08:45.477: INFO: sonobuoy-e2e-job-40469eeb076c4210 from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 16:08:45.477: INFO: 	Container e2e ready: true, restart count 0
Jun 16 16:08:45.477: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 16:08:45.477: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-bbzmx from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 16:08:45.477: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 16:08:45.477: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 16:08:45.477: INFO: aws-node-kmx2q from kube-system started at 2020-06-16 15:10:51 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.477: INFO: 	Container aws-node ready: true, restart count 0
Jun 16 16:08:45.477: INFO: kube-proxy-r2g9b from kube-system started at 2020-06-16 15:06:51 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.477: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 16:08:45.477: INFO: kiam-agent-6q62w from kube-system started at 2020-06-16 15:10:38 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.477: INFO: 	Container kiam-agent ready: true, restart count 3
Jun 16 16:08:45.477: INFO: tiller-deploy-684c6b545b-xfl8b from giantswarm started at 2020-06-16 15:09:33 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.477: INFO: 	Container tiller ready: true, restart count 0
Jun 16 16:08:45.477: INFO: coredns-6d56c484c-l7qk9 from kube-system started at 2020-06-16 15:10:34 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.477: INFO: 	Container coredns ready: true, restart count 0
Jun 16 16:08:45.477: INFO: net-exporter-p7cpr from kube-system started at 2020-06-16 15:14:31 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.477: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 16:08:45.477: INFO: calico-node-xpkvc from kube-system started at 2020-06-16 15:06:51 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.477: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 16:08:45.477: INFO: cert-exporter-fthcq from kube-system started at 2020-06-16 15:10:32 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.477: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 16:08:45.477: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-6.eu-west-1.compute.internal before test
Jun 16 16:08:45.482: INFO: cert-exporter-2k4kc from kube-system started at 2020-06-16 15:10:32 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.482: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 16:08:45.482: INFO: kiam-agent-5xldj from kube-system started at 2020-06-16 15:10:38 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.482: INFO: 	Container kiam-agent ready: true, restart count 3
Jun 16 16:08:45.482: INFO: metrics-server-66df9f5b56-mgt2q from kube-system started at 2020-06-16 15:10:44 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.482: INFO: 	Container metrics-server ready: true, restart count 0
Jun 16 16:08:45.482: INFO: net-exporter-5wtl4 from kube-system started at 2020-06-16 15:14:31 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.482: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 16:08:45.482: INFO: calico-node-ddwlg from kube-system started at 2020-06-16 15:06:46 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.482: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 16:08:45.482: INFO: aws-node-vjj2v from kube-system started at 2020-06-16 15:10:39 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.482: INFO: 	Container aws-node ready: true, restart count 0
Jun 16 16:08:45.482: INFO: coredns-6d56c484c-8z6gz from kube-system started at 2020-06-16 15:10:34 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.482: INFO: 	Container coredns ready: true, restart count 0
Jun 16 16:08:45.482: INFO: cert-manager-67dfd96fcd-n8nrk from kube-system started at 2020-06-16 15:10:36 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.482: INFO: 	Container cert-manager ready: true, restart count 0
Jun 16 16:08:45.482: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-87l2v from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 16:08:45.482: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 16:08:45.482: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 16:08:45.482: INFO: kube-proxy-g6cmw from kube-system started at 2020-06-16 15:06:46 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.482: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 16:08:45.482: INFO: node-exporter-z96vr from kube-system started at 2020-06-16 15:10:31 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.482: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 16:08:45.482: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-87.eu-west-1.compute.internal before test
Jun 16 16:08:45.485: INFO: kube-proxy-d8dvz from kube-system started at 2020-06-16 15:49:44 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.485: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 16:08:45.485: INFO: cert-exporter-xl9vw from kube-system started at 2020-06-16 15:49:44 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.485: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 16:08:45.485: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-zxxwj from sonobuoy started at 2020-06-16 15:49:44 +0000 UTC (2 container statuses recorded)
Jun 16 16:08:45.485: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 16:08:45.485: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 16:08:45.485: INFO: aws-node-zwmxv from kube-system started at 2020-06-16 15:49:44 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.485: INFO: 	Container aws-node ready: true, restart count 0
Jun 16 16:08:45.485: INFO: node-exporter-j2s7w from kube-system started at 2020-06-16 15:49:44 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.485: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 16:08:45.485: INFO: net-exporter-dnqbx from kube-system started at 2020-06-16 15:49:44 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.485: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 16:08:45.485: INFO: calico-node-sdxqj from kube-system started at 2020-06-16 15:49:44 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.485: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 16:08:45.485: INFO: kiam-agent-8tffg from kube-system started at 2020-06-16 15:50:24 +0000 UTC (1 container statuses recorded)
Jun 16 16:08:45.485: INFO: 	Container kiam-agent ready: true, restart count 3
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-607fb1e6-d4ba-4eba-8e30-799600c19693 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-607fb1e6-d4ba-4eba-8e30-799600c19693 off the node ip-172-19-65-209.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-607fb1e6-d4ba-4eba-8e30-799600c19693
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:08:49.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3800" for this suite.
Jun 16 16:08:57.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:08:57.582: INFO: namespace sched-pred-3800 deletion completed in 8.055771596s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:12.249 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:08:57.583: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6643
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 16:08:58.569: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 16:09:01.579: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:09:01.581: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3450-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:09:02.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6643" for this suite.
Jun 16 16:09:08.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:09:08.713: INFO: namespace webhook-6643 deletion completed in 6.057071451s
STEP: Destroying namespace "webhook-6643-markers" for this suite.
Jun 16 16:09:14.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:09:14.825: INFO: namespace webhook-6643-markers deletion completed in 6.11240584s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.385 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:09:14.967: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7187
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7187
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7187
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7187
Jun 16 16:09:15.097: INFO: Found 0 stateful pods, waiting for 1
Jun 16 16:09:25.099: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 16 16:09:25.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-7187 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 16 16:09:25.357: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 16 16:09:25.357: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 16 16:09:25.357: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 16 16:09:25.359: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 16 16:09:35.362: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 16 16:09:35.362: INFO: Waiting for statefulset status.replicas updated to 0
Jun 16 16:09:35.369: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999791s
Jun 16 16:09:36.371: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998003286s
Jun 16 16:09:37.374: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995737504s
Jun 16 16:09:38.376: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.993324338s
Jun 16 16:09:39.378: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.991148565s
Jun 16 16:09:40.380: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.988775752s
Jun 16 16:09:41.382: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.987084755s
Jun 16 16:09:42.383: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.985112795s
Jun 16 16:09:43.385: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.983426073s
Jun 16 16:09:44.387: INFO: Verifying statefulset ss doesn't scale past 1 for another 981.572028ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7187
Jun 16 16:09:45.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-7187 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 16 16:09:45.539: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 16 16:09:45.539: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 16 16:09:45.539: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 16 16:09:45.540: INFO: Found 1 stateful pods, waiting for 3
Jun 16 16:09:55.543: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 16 16:09:55.543: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 16 16:09:55.543: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 16 16:09:55.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-7187 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 16 16:09:55.698: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 16 16:09:55.698: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 16 16:09:55.698: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 16 16:09:55.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-7187 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 16 16:09:55.851: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 16 16:09:55.851: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 16 16:09:55.851: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 16 16:09:55.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-7187 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 16 16:09:55.999: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 16 16:09:55.999: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 16 16:09:55.999: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 16 16:09:55.999: INFO: Waiting for statefulset status.replicas updated to 0
Jun 16 16:09:56.001: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun 16 16:10:06.005: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 16 16:10:06.005: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 16 16:10:06.005: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 16 16:10:06.011: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999824s
Jun 16 16:10:07.013: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997745684s
Jun 16 16:10:08.016: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995385589s
Jun 16 16:10:09.018: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.99286835s
Jun 16 16:10:10.020: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.99072306s
Jun 16 16:10:11.022: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.98821669s
Jun 16 16:10:12.025: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.986146872s
Jun 16 16:10:13.027: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.983706481s
Jun 16 16:10:14.029: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.981684869s
Jun 16 16:10:15.031: INFO: Verifying statefulset ss doesn't scale past 3 for another 979.11716ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7187
Jun 16 16:10:16.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-7187 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 16 16:10:16.187: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 16 16:10:16.187: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 16 16:10:16.187: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 16 16:10:16.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-7187 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 16 16:10:16.333: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 16 16:10:16.333: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 16 16:10:16.333: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 16 16:10:16.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-7187 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 16 16:10:16.477: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 16 16:10:16.477: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 16 16:10:16.477: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 16 16:10:16.477: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun 16 16:10:36.484: INFO: Deleting all statefulset in ns statefulset-7187
Jun 16 16:10:36.486: INFO: Scaling statefulset ss to 0
Jun 16 16:10:36.490: INFO: Waiting for statefulset status.replicas updated to 0
Jun 16 16:10:36.491: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:10:36.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7187" for this suite.
Jun 16 16:10:42.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:10:42.553: INFO: namespace statefulset-7187 deletion completed in 6.054139554s

• [SLOW TEST:87.586 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:10:42.553: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4371
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-ffbt
STEP: Creating a pod to test atomic-volume-subpath
Jun 16 16:10:42.682: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ffbt" in namespace "subpath-4371" to be "success or failure"
Jun 16 16:10:42.683: INFO: Pod "pod-subpath-test-configmap-ffbt": Phase="Pending", Reason="", readiness=false. Elapsed: 1.339544ms
Jun 16 16:10:44.686: INFO: Pod "pod-subpath-test-configmap-ffbt": Phase="Running", Reason="", readiness=true. Elapsed: 2.003478586s
Jun 16 16:10:46.688: INFO: Pod "pod-subpath-test-configmap-ffbt": Phase="Running", Reason="", readiness=true. Elapsed: 4.005717038s
Jun 16 16:10:48.690: INFO: Pod "pod-subpath-test-configmap-ffbt": Phase="Running", Reason="", readiness=true. Elapsed: 6.007764074s
Jun 16 16:10:50.692: INFO: Pod "pod-subpath-test-configmap-ffbt": Phase="Running", Reason="", readiness=true. Elapsed: 8.009875532s
Jun 16 16:10:52.694: INFO: Pod "pod-subpath-test-configmap-ffbt": Phase="Running", Reason="", readiness=true. Elapsed: 10.012089769s
Jun 16 16:10:54.696: INFO: Pod "pod-subpath-test-configmap-ffbt": Phase="Running", Reason="", readiness=true. Elapsed: 12.014002701s
Jun 16 16:10:56.698: INFO: Pod "pod-subpath-test-configmap-ffbt": Phase="Running", Reason="", readiness=true. Elapsed: 14.016126558s
Jun 16 16:10:58.700: INFO: Pod "pod-subpath-test-configmap-ffbt": Phase="Running", Reason="", readiness=true. Elapsed: 16.018192131s
Jun 16 16:11:00.702: INFO: Pod "pod-subpath-test-configmap-ffbt": Phase="Running", Reason="", readiness=true. Elapsed: 18.02030669s
Jun 16 16:11:02.705: INFO: Pod "pod-subpath-test-configmap-ffbt": Phase="Running", Reason="", readiness=true. Elapsed: 20.022438601s
Jun 16 16:11:04.707: INFO: Pod "pod-subpath-test-configmap-ffbt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.024594721s
STEP: Saw pod success
Jun 16 16:11:04.707: INFO: Pod "pod-subpath-test-configmap-ffbt" satisfied condition "success or failure"
Jun 16 16:11:04.708: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod pod-subpath-test-configmap-ffbt container test-container-subpath-configmap-ffbt: <nil>
STEP: delete the pod
Jun 16 16:11:04.718: INFO: Waiting for pod pod-subpath-test-configmap-ffbt to disappear
Jun 16 16:11:04.720: INFO: Pod pod-subpath-test-configmap-ffbt no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ffbt
Jun 16 16:11:04.720: INFO: Deleting pod "pod-subpath-test-configmap-ffbt" in namespace "subpath-4371"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:11:04.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4371" for this suite.
Jun 16 16:11:10.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:11:10.785: INFO: namespace subpath-4371 deletion completed in 6.059823833s

• [SLOW TEST:28.232 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:11:10.785: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 16 16:11:12.918: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:11:12.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-169" for this suite.
Jun 16 16:11:18.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:11:18.984: INFO: namespace container-runtime-169 deletion completed in 6.055337284s

• [SLOW TEST:8.198 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:11:18.984: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4152
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 16:11:19.583: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 16:11:22.592: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:11:22.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4152" for this suite.
Jun 16 16:11:28.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:11:28.740: INFO: namespace webhook-4152 deletion completed in 6.0527745s
STEP: Destroying namespace "webhook-4152-markers" for this suite.
Jun 16 16:11:34.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:11:34.843: INFO: namespace webhook-4152-markers deletion completed in 6.103056229s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.011 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:11:34.995: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-7586/configmap-test-ba77a2ca-1eb2-40c6-a96a-73a1a91ffaac
STEP: Creating a pod to test consume configMaps
Jun 16 16:11:35.120: INFO: Waiting up to 5m0s for pod "pod-configmaps-cfc7a36a-a9b9-4b95-aa6c-289e9ad3709c" in namespace "configmap-7586" to be "success or failure"
Jun 16 16:11:35.122: INFO: Pod "pod-configmaps-cfc7a36a-a9b9-4b95-aa6c-289e9ad3709c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03618ms
Jun 16 16:11:37.124: INFO: Pod "pod-configmaps-cfc7a36a-a9b9-4b95-aa6c-289e9ad3709c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004244863s
STEP: Saw pod success
Jun 16 16:11:37.125: INFO: Pod "pod-configmaps-cfc7a36a-a9b9-4b95-aa6c-289e9ad3709c" satisfied condition "success or failure"
Jun 16 16:11:37.126: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod pod-configmaps-cfc7a36a-a9b9-4b95-aa6c-289e9ad3709c container env-test: <nil>
STEP: delete the pod
Jun 16 16:11:37.137: INFO: Waiting for pod pod-configmaps-cfc7a36a-a9b9-4b95-aa6c-289e9ad3709c to disappear
Jun 16 16:11:37.139: INFO: Pod pod-configmaps-cfc7a36a-a9b9-4b95-aa6c-289e9ad3709c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:11:37.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7586" for this suite.
Jun 16 16:11:43.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:11:43.196: INFO: namespace configmap-7586 deletion completed in 6.05539196s

• [SLOW TEST:8.201 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:11:43.197: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-8120
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:11:43.320: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-1d1ff7e8-d4c4-42fa-993a-50253d3841b5" in namespace "security-context-test-8120" to be "success or failure"
Jun 16 16:11:43.321: INFO: Pod "busybox-privileged-false-1d1ff7e8-d4c4-42fa-993a-50253d3841b5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.5131ms
Jun 16 16:11:45.323: INFO: Pod "busybox-privileged-false-1d1ff7e8-d4c4-42fa-993a-50253d3841b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003492417s
Jun 16 16:11:45.323: INFO: Pod "busybox-privileged-false-1d1ff7e8-d4c4-42fa-993a-50253d3841b5" satisfied condition "success or failure"
Jun 16 16:11:45.328: INFO: Got logs for pod "busybox-privileged-false-1d1ff7e8-d4c4-42fa-993a-50253d3841b5": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:11:45.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8120" for this suite.
Jun 16 16:11:51.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:11:51.385: INFO: namespace security-context-test-8120 deletion completed in 6.054679917s

• [SLOW TEST:8.188 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:11:51.385: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-554
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-51219ba1-6d2a-4521-a425-8660ad1a0471
STEP: Creating a pod to test consume secrets
Jun 16 16:11:51.510: INFO: Waiting up to 5m0s for pod "pod-secrets-264b3ac2-722b-4270-86f7-e9996abd9763" in namespace "secrets-554" to be "success or failure"
Jun 16 16:11:51.512: INFO: Pod "pod-secrets-264b3ac2-722b-4270-86f7-e9996abd9763": Phase="Pending", Reason="", readiness=false. Elapsed: 2.497558ms
Jun 16 16:11:53.514: INFO: Pod "pod-secrets-264b3ac2-722b-4270-86f7-e9996abd9763": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004733538s
STEP: Saw pod success
Jun 16 16:11:53.514: INFO: Pod "pod-secrets-264b3ac2-722b-4270-86f7-e9996abd9763" satisfied condition "success or failure"
Jun 16 16:11:53.516: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod pod-secrets-264b3ac2-722b-4270-86f7-e9996abd9763 container secret-volume-test: <nil>
STEP: delete the pod
Jun 16 16:11:53.528: INFO: Waiting for pod pod-secrets-264b3ac2-722b-4270-86f7-e9996abd9763 to disappear
Jun 16 16:11:53.530: INFO: Pod pod-secrets-264b3ac2-722b-4270-86f7-e9996abd9763 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:11:53.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-554" for this suite.
Jun 16 16:11:59.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:11:59.586: INFO: namespace secrets-554 deletion completed in 6.053568263s

• [SLOW TEST:8.201 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:11:59.586: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3581
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:11:59.706: INFO: Creating ReplicaSet my-hostname-basic-0644d168-8e14-4cc4-ae9f-42828fd603f4
Jun 16 16:11:59.710: INFO: Pod name my-hostname-basic-0644d168-8e14-4cc4-ae9f-42828fd603f4: Found 0 pods out of 1
Jun 16 16:12:04.712: INFO: Pod name my-hostname-basic-0644d168-8e14-4cc4-ae9f-42828fd603f4: Found 1 pods out of 1
Jun 16 16:12:04.712: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0644d168-8e14-4cc4-ae9f-42828fd603f4" is running
Jun 16 16:12:04.714: INFO: Pod "my-hostname-basic-0644d168-8e14-4cc4-ae9f-42828fd603f4-nk59z" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-16 16:11:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-16 16:12:01 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-16 16:12:01 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-16 16:11:59 +0000 UTC Reason: Message:}])
Jun 16 16:12:04.714: INFO: Trying to dial the pod
Jun 16 16:12:09.720: INFO: Controller my-hostname-basic-0644d168-8e14-4cc4-ae9f-42828fd603f4: Got expected result from replica 1 [my-hostname-basic-0644d168-8e14-4cc4-ae9f-42828fd603f4-nk59z]: "my-hostname-basic-0644d168-8e14-4cc4-ae9f-42828fd603f4-nk59z", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:12:09.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3581" for this suite.
Jun 16 16:12:15.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:12:15.775: INFO: namespace replicaset-3581 deletion completed in 6.05319907s

• [SLOW TEST:16.190 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:12:15.776: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5114
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jun 16 16:12:17.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec pod-sharedvolume-7641d3f6-c018-467b-ba27-1997689e96d3 -c busybox-main-container --namespace=emptydir-5114 -- cat /usr/share/volumeshare/shareddata.txt'
Jun 16 16:12:18.048: INFO: stderr: ""
Jun 16 16:12:18.048: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:12:18.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5114" for this suite.
Jun 16 16:12:24.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:12:24.108: INFO: namespace emptydir-5114 deletion completed in 6.057855427s

• [SLOW TEST:8.333 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:12:24.109: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5007
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 16:12:24.242: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cdfbcc14-c335-402d-865e-879f2576ffd5" in namespace "projected-5007" to be "success or failure"
Jun 16 16:12:24.246: INFO: Pod "downwardapi-volume-cdfbcc14-c335-402d-865e-879f2576ffd5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.536813ms
Jun 16 16:12:26.248: INFO: Pod "downwardapi-volume-cdfbcc14-c335-402d-865e-879f2576ffd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006397912s
STEP: Saw pod success
Jun 16 16:12:26.248: INFO: Pod "downwardapi-volume-cdfbcc14-c335-402d-865e-879f2576ffd5" satisfied condition "success or failure"
Jun 16 16:12:26.250: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod downwardapi-volume-cdfbcc14-c335-402d-865e-879f2576ffd5 container client-container: <nil>
STEP: delete the pod
Jun 16 16:12:26.263: INFO: Waiting for pod downwardapi-volume-cdfbcc14-c335-402d-865e-879f2576ffd5 to disappear
Jun 16 16:12:26.265: INFO: Pod downwardapi-volume-cdfbcc14-c335-402d-865e-879f2576ffd5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:12:26.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5007" for this suite.
Jun 16 16:12:32.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:12:32.321: INFO: namespace projected-5007 deletion completed in 6.053661772s

• [SLOW TEST:8.213 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:12:32.322: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9150
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 16:12:32.446: INFO: Waiting up to 5m0s for pod "downwardapi-volume-137e2983-01ac-4f37-a8e6-e8796dc2b916" in namespace "projected-9150" to be "success or failure"
Jun 16 16:12:32.449: INFO: Pod "downwardapi-volume-137e2983-01ac-4f37-a8e6-e8796dc2b916": Phase="Pending", Reason="", readiness=false. Elapsed: 2.175938ms
Jun 16 16:12:34.451: INFO: Pod "downwardapi-volume-137e2983-01ac-4f37-a8e6-e8796dc2b916": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004352663s
STEP: Saw pod success
Jun 16 16:12:34.451: INFO: Pod "downwardapi-volume-137e2983-01ac-4f37-a8e6-e8796dc2b916" satisfied condition "success or failure"
Jun 16 16:12:34.452: INFO: Trying to get logs from node ip-172-19-65-209.eu-west-1.compute.internal pod downwardapi-volume-137e2983-01ac-4f37-a8e6-e8796dc2b916 container client-container: <nil>
STEP: delete the pod
Jun 16 16:12:34.464: INFO: Waiting for pod downwardapi-volume-137e2983-01ac-4f37-a8e6-e8796dc2b916 to disappear
Jun 16 16:12:34.465: INFO: Pod downwardapi-volume-137e2983-01ac-4f37-a8e6-e8796dc2b916 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:12:34.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9150" for this suite.
Jun 16 16:12:40.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:12:40.522: INFO: namespace projected-9150 deletion completed in 6.054737905s

• [SLOW TEST:8.200 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:12:40.522: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6385
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6385
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Jun 16 16:12:40.649: INFO: Found 0 stateful pods, waiting for 3
Jun 16 16:12:50.652: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 16 16:12:50.652: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 16 16:12:50.652: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 16 16:12:50.670: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 16 16:13:00.691: INFO: Updating stateful set ss2
Jun 16 16:13:00.694: INFO: Waiting for Pod statefulset-6385/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 16 16:13:10.697: INFO: Waiting for Pod statefulset-6385/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Jun 16 16:13:20.721: INFO: Found 2 stateful pods, waiting for 3
Jun 16 16:13:30.724: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 16 16:13:30.724: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 16 16:13:30.724: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 16 16:13:30.741: INFO: Updating stateful set ss2
Jun 16 16:13:30.745: INFO: Waiting for Pod statefulset-6385/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 16 16:13:40.763: INFO: Updating stateful set ss2
Jun 16 16:13:40.766: INFO: Waiting for StatefulSet statefulset-6385/ss2 to complete update
Jun 16 16:13:40.766: INFO: Waiting for Pod statefulset-6385/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun 16 16:13:50.770: INFO: Deleting all statefulset in ns statefulset-6385
Jun 16 16:13:50.772: INFO: Scaling statefulset ss2 to 0
Jun 16 16:14:10.780: INFO: Waiting for statefulset status.replicas updated to 0
Jun 16 16:14:10.781: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:14:10.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6385" for this suite.
Jun 16 16:14:16.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:14:16.844: INFO: namespace statefulset-6385 deletion completed in 6.054569979s

• [SLOW TEST:96.322 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:14:16.844: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-5531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 16 16:14:20.982: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5531 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:14:20.982: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:14:21.069: INFO: Exec stderr: ""
Jun 16 16:14:21.069: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5531 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:14:21.069: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:14:21.139: INFO: Exec stderr: ""
Jun 16 16:14:21.139: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5531 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:14:21.139: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:14:21.210: INFO: Exec stderr: ""
Jun 16 16:14:21.210: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5531 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:14:21.210: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:14:21.283: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 16 16:14:21.283: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5531 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:14:21.283: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:14:21.360: INFO: Exec stderr: ""
Jun 16 16:14:21.360: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5531 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:14:21.360: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:14:21.432: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 16 16:14:21.432: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5531 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:14:21.433: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:14:21.515: INFO: Exec stderr: ""
Jun 16 16:14:21.515: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5531 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:14:21.515: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:14:21.592: INFO: Exec stderr: ""
Jun 16 16:14:21.592: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5531 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:14:21.592: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:14:21.673: INFO: Exec stderr: ""
Jun 16 16:14:21.673: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5531 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:14:21.673: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:14:21.744: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:14:21.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5531" for this suite.
Jun 16 16:15:05.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:15:05.801: INFO: namespace e2e-kubelet-etc-hosts-5531 deletion completed in 44.054590275s

• [SLOW TEST:48.957 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:15:05.801: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-529
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 16:15:05.927: INFO: Waiting up to 5m0s for pod "downwardapi-volume-05add9c3-fe23-43ab-91bb-095ca320dfaa" in namespace "downward-api-529" to be "success or failure"
Jun 16 16:15:05.928: INFO: Pod "downwardapi-volume-05add9c3-fe23-43ab-91bb-095ca320dfaa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.572482ms
Jun 16 16:15:07.930: INFO: Pod "downwardapi-volume-05add9c3-fe23-43ab-91bb-095ca320dfaa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00363381s
STEP: Saw pod success
Jun 16 16:15:07.930: INFO: Pod "downwardapi-volume-05add9c3-fe23-43ab-91bb-095ca320dfaa" satisfied condition "success or failure"
Jun 16 16:15:07.932: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod downwardapi-volume-05add9c3-fe23-43ab-91bb-095ca320dfaa container client-container: <nil>
STEP: delete the pod
Jun 16 16:15:07.942: INFO: Waiting for pod downwardapi-volume-05add9c3-fe23-43ab-91bb-095ca320dfaa to disappear
Jun 16 16:15:07.945: INFO: Pod downwardapi-volume-05add9c3-fe23-43ab-91bb-095ca320dfaa no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:15:07.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-529" for this suite.
Jun 16 16:15:13.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:15:14.002: INFO: namespace downward-api-529 deletion completed in 6.055007638s

• [SLOW TEST:8.201 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:15:14.002: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3619
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-e4b9eff0-b1a3-44a1-bcb3-4f157cb2de6a in namespace container-probe-3619
Jun 16 16:15:16.135: INFO: Started pod liveness-e4b9eff0-b1a3-44a1-bcb3-4f157cb2de6a in namespace container-probe-3619
STEP: checking the pod's current state and verifying that restartCount is present
Jun 16 16:15:16.140: INFO: Initial restart count of pod liveness-e4b9eff0-b1a3-44a1-bcb3-4f157cb2de6a is 0
Jun 16 16:15:26.153: INFO: Restart count of pod container-probe-3619/liveness-e4b9eff0-b1a3-44a1-bcb3-4f157cb2de6a is now 1 (10.012961447s elapsed)
Jun 16 16:15:48.175: INFO: Restart count of pod container-probe-3619/liveness-e4b9eff0-b1a3-44a1-bcb3-4f157cb2de6a is now 2 (32.035821136s elapsed)
Jun 16 16:16:08.196: INFO: Restart count of pod container-probe-3619/liveness-e4b9eff0-b1a3-44a1-bcb3-4f157cb2de6a is now 3 (52.055998499s elapsed)
Jun 16 16:16:26.214: INFO: Restart count of pod container-probe-3619/liveness-e4b9eff0-b1a3-44a1-bcb3-4f157cb2de6a is now 4 (1m10.074848096s elapsed)
Jun 16 16:17:40.295: INFO: Restart count of pod container-probe-3619/liveness-e4b9eff0-b1a3-44a1-bcb3-4f157cb2de6a is now 5 (2m24.155062259s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:17:40.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3619" for this suite.
Jun 16 16:17:46.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:17:46.371: INFO: namespace container-probe-3619 deletion completed in 6.068343209s

• [SLOW TEST:152.369 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:17:46.371: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-1765
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jun 16 16:17:46.495: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jun 16 16:17:59.843: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:18:03.408: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:18:17.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1765" for this suite.
Jun 16 16:18:23.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:18:23.307: INFO: namespace crd-publish-openapi-1765 deletion completed in 6.053535259s

• [SLOW TEST:36.936 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:18:23.308: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5137
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Jun 16 16:18:23.432: INFO: Waiting up to 5m0s for pod "client-containers-938b9344-e6ae-4319-9b87-674b7b0c710c" in namespace "containers-5137" to be "success or failure"
Jun 16 16:18:23.435: INFO: Pod "client-containers-938b9344-e6ae-4319-9b87-674b7b0c710c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.973532ms
Jun 16 16:18:25.437: INFO: Pod "client-containers-938b9344-e6ae-4319-9b87-674b7b0c710c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005043566s
STEP: Saw pod success
Jun 16 16:18:25.437: INFO: Pod "client-containers-938b9344-e6ae-4319-9b87-674b7b0c710c" satisfied condition "success or failure"
Jun 16 16:18:25.439: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod client-containers-938b9344-e6ae-4319-9b87-674b7b0c710c container test-container: <nil>
STEP: delete the pod
Jun 16 16:18:25.448: INFO: Waiting for pod client-containers-938b9344-e6ae-4319-9b87-674b7b0c710c to disappear
Jun 16 16:18:25.449: INFO: Pod client-containers-938b9344-e6ae-4319-9b87-674b7b0c710c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:18:25.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5137" for this suite.
Jun 16 16:18:31.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:18:31.507: INFO: namespace containers-5137 deletion completed in 6.055066356s

• [SLOW TEST:8.199 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:18:31.507: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1332
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-cr5n
STEP: Creating a pod to test atomic-volume-subpath
Jun 16 16:18:31.634: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-cr5n" in namespace "subpath-1332" to be "success or failure"
Jun 16 16:18:31.636: INFO: Pod "pod-subpath-test-downwardapi-cr5n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010917ms
Jun 16 16:18:33.638: INFO: Pod "pod-subpath-test-downwardapi-cr5n": Phase="Running", Reason="", readiness=true. Elapsed: 2.004236116s
Jun 16 16:18:35.640: INFO: Pod "pod-subpath-test-downwardapi-cr5n": Phase="Running", Reason="", readiness=true. Elapsed: 4.006329414s
Jun 16 16:18:37.642: INFO: Pod "pod-subpath-test-downwardapi-cr5n": Phase="Running", Reason="", readiness=true. Elapsed: 6.007972611s
Jun 16 16:18:39.644: INFO: Pod "pod-subpath-test-downwardapi-cr5n": Phase="Running", Reason="", readiness=true. Elapsed: 8.009978102s
Jun 16 16:18:41.646: INFO: Pod "pod-subpath-test-downwardapi-cr5n": Phase="Running", Reason="", readiness=true. Elapsed: 10.012072696s
Jun 16 16:18:43.649: INFO: Pod "pod-subpath-test-downwardapi-cr5n": Phase="Running", Reason="", readiness=true. Elapsed: 12.014552421s
Jun 16 16:18:45.651: INFO: Pod "pod-subpath-test-downwardapi-cr5n": Phase="Running", Reason="", readiness=true. Elapsed: 14.016622251s
Jun 16 16:18:47.653: INFO: Pod "pod-subpath-test-downwardapi-cr5n": Phase="Running", Reason="", readiness=true. Elapsed: 16.018858266s
Jun 16 16:18:49.655: INFO: Pod "pod-subpath-test-downwardapi-cr5n": Phase="Running", Reason="", readiness=true. Elapsed: 18.021072367s
Jun 16 16:18:51.657: INFO: Pod "pod-subpath-test-downwardapi-cr5n": Phase="Running", Reason="", readiness=true. Elapsed: 20.023154795s
Jun 16 16:18:53.659: INFO: Pod "pod-subpath-test-downwardapi-cr5n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.025309899s
STEP: Saw pod success
Jun 16 16:18:53.659: INFO: Pod "pod-subpath-test-downwardapi-cr5n" satisfied condition "success or failure"
Jun 16 16:18:53.661: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-subpath-test-downwardapi-cr5n container test-container-subpath-downwardapi-cr5n: <nil>
STEP: delete the pod
Jun 16 16:18:53.674: INFO: Waiting for pod pod-subpath-test-downwardapi-cr5n to disappear
Jun 16 16:18:53.676: INFO: Pod pod-subpath-test-downwardapi-cr5n no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-cr5n
Jun 16 16:18:53.676: INFO: Deleting pod "pod-subpath-test-downwardapi-cr5n" in namespace "subpath-1332"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:18:53.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1332" for this suite.
Jun 16 16:18:59.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:18:59.733: INFO: namespace subpath-1332 deletion completed in 6.053578862s

• [SLOW TEST:28.226 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:18:59.733: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8371
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 16 16:18:59.857: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8371 /api/v1/namespaces/watch-8371/configmaps/e2e-watch-test-configmap-a 54ebfe2d-71d2-4f65-9169-bb15029b9936 20453 0 2020-06-16 16:18:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 16 16:18:59.857: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8371 /api/v1/namespaces/watch-8371/configmaps/e2e-watch-test-configmap-a 54ebfe2d-71d2-4f65-9169-bb15029b9936 20453 0 2020-06-16 16:18:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 16 16:19:09.860: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8371 /api/v1/namespaces/watch-8371/configmaps/e2e-watch-test-configmap-a 54ebfe2d-71d2-4f65-9169-bb15029b9936 20479 0 2020-06-16 16:18:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 16 16:19:09.860: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8371 /api/v1/namespaces/watch-8371/configmaps/e2e-watch-test-configmap-a 54ebfe2d-71d2-4f65-9169-bb15029b9936 20479 0 2020-06-16 16:18:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 16 16:19:19.864: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8371 /api/v1/namespaces/watch-8371/configmaps/e2e-watch-test-configmap-a 54ebfe2d-71d2-4f65-9169-bb15029b9936 20506 0 2020-06-16 16:18:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 16 16:19:19.864: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8371 /api/v1/namespaces/watch-8371/configmaps/e2e-watch-test-configmap-a 54ebfe2d-71d2-4f65-9169-bb15029b9936 20506 0 2020-06-16 16:18:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 16 16:19:29.868: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8371 /api/v1/namespaces/watch-8371/configmaps/e2e-watch-test-configmap-a 54ebfe2d-71d2-4f65-9169-bb15029b9936 20533 0 2020-06-16 16:18:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 16 16:19:29.868: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8371 /api/v1/namespaces/watch-8371/configmaps/e2e-watch-test-configmap-a 54ebfe2d-71d2-4f65-9169-bb15029b9936 20533 0 2020-06-16 16:18:59 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 16 16:19:39.872: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8371 /api/v1/namespaces/watch-8371/configmaps/e2e-watch-test-configmap-b ef0ea1dd-219b-4e68-932e-1f33ec909e42 20558 0 2020-06-16 16:19:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 16 16:19:39.872: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8371 /api/v1/namespaces/watch-8371/configmaps/e2e-watch-test-configmap-b ef0ea1dd-219b-4e68-932e-1f33ec909e42 20558 0 2020-06-16 16:19:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 16 16:19:49.876: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8371 /api/v1/namespaces/watch-8371/configmaps/e2e-watch-test-configmap-b ef0ea1dd-219b-4e68-932e-1f33ec909e42 20611 0 2020-06-16 16:19:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 16 16:19:49.876: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8371 /api/v1/namespaces/watch-8371/configmaps/e2e-watch-test-configmap-b ef0ea1dd-219b-4e68-932e-1f33ec909e42 20611 0 2020-06-16 16:19:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:19:59.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8371" for this suite.
Jun 16 16:20:05.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:20:05.934: INFO: namespace watch-8371 deletion completed in 6.055888934s

• [SLOW TEST:66.202 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:20:05.935: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5778
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 16 16:20:06.059: INFO: Waiting up to 5m0s for pod "pod-c4ac7d42-119a-451c-a8f0-ef8974f86059" in namespace "emptydir-5778" to be "success or failure"
Jun 16 16:20:06.061: INFO: Pod "pod-c4ac7d42-119a-451c-a8f0-ef8974f86059": Phase="Pending", Reason="", readiness=false. Elapsed: 1.929132ms
Jun 16 16:20:08.064: INFO: Pod "pod-c4ac7d42-119a-451c-a8f0-ef8974f86059": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004243809s
STEP: Saw pod success
Jun 16 16:20:08.064: INFO: Pod "pod-c4ac7d42-119a-451c-a8f0-ef8974f86059" satisfied condition "success or failure"
Jun 16 16:20:08.065: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-c4ac7d42-119a-451c-a8f0-ef8974f86059 container test-container: <nil>
STEP: delete the pod
Jun 16 16:20:08.076: INFO: Waiting for pod pod-c4ac7d42-119a-451c-a8f0-ef8974f86059 to disappear
Jun 16 16:20:08.078: INFO: Pod pod-c4ac7d42-119a-451c-a8f0-ef8974f86059 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:20:08.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5778" for this suite.
Jun 16 16:20:14.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:20:14.147: INFO: namespace emptydir-5778 deletion completed in 6.067373171s

• [SLOW TEST:8.212 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:20:14.147: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5888
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:20:16.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5888" for this suite.
Jun 16 16:21:00.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:21:00.342: INFO: namespace kubelet-test-5888 deletion completed in 44.054721041s

• [SLOW TEST:46.195 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:21:00.342: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8487
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-3eb0ad1d-eb8e-448c-9108-760aaa1cc365
STEP: Creating a pod to test consume configMaps
Jun 16 16:21:00.468: INFO: Waiting up to 5m0s for pod "pod-configmaps-4c76054c-62f7-447a-8c2d-38b0275d6fdd" in namespace "configmap-8487" to be "success or failure"
Jun 16 16:21:00.470: INFO: Pod "pod-configmaps-4c76054c-62f7-447a-8c2d-38b0275d6fdd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.963167ms
Jun 16 16:21:02.472: INFO: Pod "pod-configmaps-4c76054c-62f7-447a-8c2d-38b0275d6fdd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004051725s
STEP: Saw pod success
Jun 16 16:21:02.472: INFO: Pod "pod-configmaps-4c76054c-62f7-447a-8c2d-38b0275d6fdd" satisfied condition "success or failure"
Jun 16 16:21:02.474: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-configmaps-4c76054c-62f7-447a-8c2d-38b0275d6fdd container configmap-volume-test: <nil>
STEP: delete the pod
Jun 16 16:21:02.485: INFO: Waiting for pod pod-configmaps-4c76054c-62f7-447a-8c2d-38b0275d6fdd to disappear
Jun 16 16:21:02.486: INFO: Pod pod-configmaps-4c76054c-62f7-447a-8c2d-38b0275d6fdd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:21:02.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8487" for this suite.
Jun 16 16:21:08.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:21:08.551: INFO: namespace configmap-8487 deletion completed in 6.062927101s

• [SLOW TEST:8.209 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:21:08.551: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8725
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:21:08.693: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 16 16:21:13.695: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 16 16:21:13.695: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 16 16:21:15.697: INFO: Creating deployment "test-rollover-deployment"
Jun 16 16:21:15.701: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 16 16:21:17.704: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 16 16:21:17.707: INFO: Ensure that both replica sets have 1 created replica
Jun 16 16:21:17.709: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 16 16:21:17.712: INFO: Updating deployment test-rollover-deployment
Jun 16 16:21:17.712: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 16 16:21:19.716: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 16 16:21:19.719: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 16 16:21:19.721: INFO: all replica sets need to contain the pod-template-hash label
Jun 16 16:21:19.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921275, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921275, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921279, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921275, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 16 16:21:21.725: INFO: all replica sets need to contain the pod-template-hash label
Jun 16 16:21:21.725: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921275, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921275, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921279, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921275, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 16 16:21:23.725: INFO: all replica sets need to contain the pod-template-hash label
Jun 16 16:21:23.725: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921275, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921275, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921279, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921275, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 16 16:21:25.725: INFO: all replica sets need to contain the pod-template-hash label
Jun 16 16:21:25.725: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921275, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921275, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921279, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921275, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 16 16:21:27.725: INFO: all replica sets need to contain the pod-template-hash label
Jun 16 16:21:27.725: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921275, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921275, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921279, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921275, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 16 16:21:29.725: INFO: 
Jun 16 16:21:29.725: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 16 16:21:29.729: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8725 /apis/apps/v1/namespaces/deployment-8725/deployments/test-rollover-deployment 59b84a23-ff1f-4873-b65c-a952fcbe568d 21038 2 2020-06-16 16:21:15 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006b957e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-06-16 16:21:15 +0000 UTC,LastTransitionTime:2020-06-16 16:21:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-06-16 16:21:29 +0000 UTC,LastTransitionTime:2020-06-16 16:21:15 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 16 16:21:29.731: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-8725 /apis/apps/v1/namespaces/deployment-8725/replicasets/test-rollover-deployment-7d7dc6548c 0de3dc5e-3625-4592-9868-39169c6678b0 21027 2 2020-06-16 16:21:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 59b84a23-ff1f-4873-b65c-a952fcbe568d 0xc006baedc7 0xc006baedc8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006baee28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 16 16:21:29.731: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 16 16:21:29.731: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8725 /apis/apps/v1/namespaces/deployment-8725/replicasets/test-rollover-controller 0cd55713-4277-4efd-b449-ce5612404a81 21037 2 2020-06-16 16:21:08 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 59b84a23-ff1f-4873-b65c-a952fcbe568d 0xc006baecf7 0xc006baecf8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006baed58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 16 16:21:29.731: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-8725 /apis/apps/v1/namespaces/deployment-8725/replicasets/test-rollover-deployment-f6c94f66c 2f810f2f-d6b7-4cb1-a328-10e55b8edef8 20983 2 2020-06-16 16:21:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 59b84a23-ff1f-4873-b65c-a952fcbe568d 0xc006baee90 0xc006baee91}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006baef08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 16 16:21:29.732: INFO: Pod "test-rollover-deployment-7d7dc6548c-fln5q" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-fln5q test-rollover-deployment-7d7dc6548c- deployment-8725 /api/v1/namespaces/deployment-8725/pods/test-rollover-deployment-7d7dc6548c-fln5q fe2a0ee0-1a13-426c-bab3-e483d523d7a7 21001 0 2020-06-16 16:21:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 0de3dc5e-3625-4592-9868-39169c6678b0 0xc006baf467 0xc006baf468}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sfx69,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sfx69,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sfx69,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-6.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 16:21:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 16:21:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 16:21:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 16:21:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.6,PodIP:172.18.129.55,StartTime:2020-06-16 16:21:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-16 16:21:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://7b8d3742bd7e1e9968948417e8834f7d9802803e501e1bfc75ce737b9194fb67,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.18.129.55,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:21:29.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8725" for this suite.
Jun 16 16:21:35.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:21:35.791: INFO: namespace deployment-8725 deletion completed in 6.056357869s

• [SLOW TEST:27.240 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:21:35.791: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1471
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-8976b9b6-29e0-483a-9d3d-ed647217e616 in namespace container-probe-1471
Jun 16 16:21:37.920: INFO: Started pod test-webserver-8976b9b6-29e0-483a-9d3d-ed647217e616 in namespace container-probe-1471
STEP: checking the pod's current state and verifying that restartCount is present
Jun 16 16:21:37.921: INFO: Initial restart count of pod test-webserver-8976b9b6-29e0-483a-9d3d-ed647217e616 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:25:38.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1471" for this suite.
Jun 16 16:25:44.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:25:44.282: INFO: namespace container-probe-1471 deletion completed in 6.097207073s

• [SLOW TEST:248.491 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:25:44.283: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-269
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 16 16:25:44.407: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 16 16:25:49.409: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:25:50.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-269" for this suite.
Jun 16 16:25:56.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:25:56.482: INFO: namespace replication-controller-269 deletion completed in 6.06195141s

• [SLOW TEST:12.199 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:25:56.482: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8093
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-93cb010b-afe9-4c57-839a-4f0e511c7f4c
STEP: Creating a pod to test consume configMaps
Jun 16 16:25:56.613: INFO: Waiting up to 5m0s for pod "pod-configmaps-b04bac26-61de-4133-8a2a-de46cf6d2e98" in namespace "configmap-8093" to be "success or failure"
Jun 16 16:25:56.618: INFO: Pod "pod-configmaps-b04bac26-61de-4133-8a2a-de46cf6d2e98": Phase="Pending", Reason="", readiness=false. Elapsed: 4.537946ms
Jun 16 16:25:58.620: INFO: Pod "pod-configmaps-b04bac26-61de-4133-8a2a-de46cf6d2e98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006673749s
STEP: Saw pod success
Jun 16 16:25:58.620: INFO: Pod "pod-configmaps-b04bac26-61de-4133-8a2a-de46cf6d2e98" satisfied condition "success or failure"
Jun 16 16:25:58.622: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-configmaps-b04bac26-61de-4133-8a2a-de46cf6d2e98 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 16 16:25:58.632: INFO: Waiting for pod pod-configmaps-b04bac26-61de-4133-8a2a-de46cf6d2e98 to disappear
Jun 16 16:25:58.634: INFO: Pod pod-configmaps-b04bac26-61de-4133-8a2a-de46cf6d2e98 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:25:58.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8093" for this suite.
Jun 16 16:26:04.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:26:04.690: INFO: namespace configmap-8093 deletion completed in 6.054441376s

• [SLOW TEST:8.208 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:26:04.690: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6759
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-5845
STEP: Creating secret with name secret-test-6bc15236-0d62-407b-acd4-9d9110aceb57
STEP: Creating a pod to test consume secrets
Jun 16 16:26:04.939: INFO: Waiting up to 5m0s for pod "pod-secrets-3870a20d-9349-4b34-b2ea-7727eb7cec44" in namespace "secrets-6759" to be "success or failure"
Jun 16 16:26:04.942: INFO: Pod "pod-secrets-3870a20d-9349-4b34-b2ea-7727eb7cec44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.46639ms
Jun 16 16:26:06.944: INFO: Pod "pod-secrets-3870a20d-9349-4b34-b2ea-7727eb7cec44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004600557s
STEP: Saw pod success
Jun 16 16:26:06.944: INFO: Pod "pod-secrets-3870a20d-9349-4b34-b2ea-7727eb7cec44" satisfied condition "success or failure"
Jun 16 16:26:06.945: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-secrets-3870a20d-9349-4b34-b2ea-7727eb7cec44 container secret-volume-test: <nil>
STEP: delete the pod
Jun 16 16:26:06.956: INFO: Waiting for pod pod-secrets-3870a20d-9349-4b34-b2ea-7727eb7cec44 to disappear
Jun 16 16:26:06.958: INFO: Pod pod-secrets-3870a20d-9349-4b34-b2ea-7727eb7cec44 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:26:06.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6759" for this suite.
Jun 16 16:26:12.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:26:13.013: INFO: namespace secrets-6759 deletion completed in 6.053826288s
STEP: Destroying namespace "secret-namespace-5845" for this suite.
Jun 16 16:26:19.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:26:19.115: INFO: namespace secret-namespace-5845 deletion completed in 6.101290364s

• [SLOW TEST:14.425 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:26:19.115: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3586
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:26:40.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3586" for this suite.
Jun 16 16:26:46.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:26:46.388: INFO: namespace container-runtime-3586 deletion completed in 6.057401895s

• [SLOW TEST:27.273 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:26:46.388: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-7870
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Jun 16 16:26:46.508: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 16 16:27:46.532: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:27:46.534: INFO: Starting informer...
STEP: Starting pod...
Jun 16 16:27:46.741: INFO: Pod is running on ip-172-19-65-6.eu-west-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jun 16 16:27:46.750: INFO: Pod wasn't evicted. Proceeding
Jun 16 16:27:46.750: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jun 16 16:29:01.772: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:29:01.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7870" for this suite.
Jun 16 16:29:29.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:29:29.827: INFO: namespace taint-single-pod-7870 deletion completed in 28.052214656s

• [SLOW TEST:163.439 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:29:29.827: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8376
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 16:29:30.254: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 16 16:29:32.260: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921770, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921770, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921770, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727921770, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 16:29:35.267: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jun 16 16:29:35.278: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:29:35.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8376" for this suite.
Jun 16 16:29:41.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:29:41.341: INFO: namespace webhook-8376 deletion completed in 6.053450877s
STEP: Destroying namespace "webhook-8376-markers" for this suite.
Jun 16 16:29:47.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:29:47.443: INFO: namespace webhook-8376-markers deletion completed in 6.101947163s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.769 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:29:47.596: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7864
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:30:03.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7864" for this suite.
Jun 16 16:30:09.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:30:09.818: INFO: namespace resourcequota-7864 deletion completed in 6.053327257s

• [SLOW TEST:22.222 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:30:09.818: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5240
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5240.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5240.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 28.133.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.133.28_udp@PTR;check="$$(dig +tcp +noall +answer +search 28.133.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.133.28_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5240.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5240.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5240.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5240.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5240.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5240.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 28.133.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.133.28_udp@PTR;check="$$(dig +tcp +noall +answer +search 28.133.31.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.31.133.28_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 16 16:30:21.969: INFO: Unable to read wheezy_udp@dns-test-service.dns-5240.svc.cluster.local from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.971: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5240.svc.cluster.local from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.972: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5240.svc.cluster.local from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.974: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5240.svc.cluster.local from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.976: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-5240.svc.cluster.local from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.978: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-5240.svc.cluster.local from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.979: INFO: Unable to read wheezy_udp@PodARecord from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.981: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.983: INFO: Unable to read 172.31.133.28_udp@PTR from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.984: INFO: Unable to read 172.31.133.28_tcp@PTR from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.986: INFO: Unable to read jessie_udp@dns-test-service.dns-5240.svc.cluster.local from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.987: INFO: Unable to read jessie_tcp@dns-test-service.dns-5240.svc.cluster.local from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.989: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5240.svc.cluster.local from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.991: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5240.svc.cluster.local from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.993: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-5240.svc.cluster.local from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.995: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-5240.svc.cluster.local from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.997: INFO: Unable to read jessie_udp@PodARecord from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:21.998: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:22.001: INFO: Unable to read 172.31.133.28_udp@PTR from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:22.003: INFO: Unable to read 172.31.133.28_tcp@PTR from pod dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a: the server could not find the requested resource (get pods dns-test-23b2dc60-d857-4623-b3db-64823fb4589a)
Jun 16 16:30:22.003: INFO: Lookups using dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a failed for: [wheezy_udp@dns-test-service.dns-5240.svc.cluster.local wheezy_tcp@dns-test-service.dns-5240.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5240.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5240.svc.cluster.local wheezy_udp@_http._tcp.test-service-2.dns-5240.svc.cluster.local wheezy_tcp@_http._tcp.test-service-2.dns-5240.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord 172.31.133.28_udp@PTR 172.31.133.28_tcp@PTR jessie_udp@dns-test-service.dns-5240.svc.cluster.local jessie_tcp@dns-test-service.dns-5240.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5240.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5240.svc.cluster.local jessie_udp@_http._tcp.test-service-2.dns-5240.svc.cluster.local jessie_tcp@_http._tcp.test-service-2.dns-5240.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord 172.31.133.28_udp@PTR 172.31.133.28_tcp@PTR]

Jun 16 16:30:27.037: INFO: DNS probes using dns-5240/dns-test-23b2dc60-d857-4623-b3db-64823fb4589a succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:30:27.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5240" for this suite.
Jun 16 16:30:33.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:30:33.122: INFO: namespace dns-5240 deletion completed in 6.056031106s

• [SLOW TEST:23.304 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:30:33.122: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9338
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-ff0cf05c-6fee-4970-bc13-bf49fe09f198
STEP: Creating a pod to test consume secrets
Jun 16 16:30:33.250: INFO: Waiting up to 5m0s for pod "pod-secrets-555d8995-5e44-4800-8a76-aad0596199c0" in namespace "secrets-9338" to be "success or failure"
Jun 16 16:30:33.251: INFO: Pod "pod-secrets-555d8995-5e44-4800-8a76-aad0596199c0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.328295ms
Jun 16 16:30:35.253: INFO: Pod "pod-secrets-555d8995-5e44-4800-8a76-aad0596199c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003603886s
STEP: Saw pod success
Jun 16 16:30:35.253: INFO: Pod "pod-secrets-555d8995-5e44-4800-8a76-aad0596199c0" satisfied condition "success or failure"
Jun 16 16:30:35.255: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-secrets-555d8995-5e44-4800-8a76-aad0596199c0 container secret-volume-test: <nil>
STEP: delete the pod
Jun 16 16:30:35.265: INFO: Waiting for pod pod-secrets-555d8995-5e44-4800-8a76-aad0596199c0 to disappear
Jun 16 16:30:35.266: INFO: Pod pod-secrets-555d8995-5e44-4800-8a76-aad0596199c0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:30:35.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9338" for this suite.
Jun 16 16:30:41.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:30:41.321: INFO: namespace secrets-9338 deletion completed in 6.053243229s

• [SLOW TEST:8.200 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:30:41.322: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7663
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-7663
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 16 16:30:41.442: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 16 16:31:05.487: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.18.140.183:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7663 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:31:05.487: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:31:05.578: INFO: Found all expected endpoints: [netserver-0]
Jun 16 16:31:05.579: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.18.141.143:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7663 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:31:05.579: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:31:05.658: INFO: Found all expected endpoints: [netserver-1]
Jun 16 16:31:05.660: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.18.136.192:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7663 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:31:05.660: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:31:05.800: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:31:05.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7663" for this suite.
Jun 16 16:31:17.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:31:17.858: INFO: namespace pod-network-test-7663 deletion completed in 12.055123466s

• [SLOW TEST:36.536 seconds]
[sig-network] Networking
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:31:17.858: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-319
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Jun 16 16:31:17.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 create -f - --namespace=kubectl-319'
Jun 16 16:31:18.282: INFO: stderr: ""
Jun 16 16:31:18.282: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 16 16:31:19.284: INFO: Selector matched 1 pods for map[app:redis]
Jun 16 16:31:19.284: INFO: Found 0 / 1
Jun 16 16:31:20.284: INFO: Selector matched 1 pods for map[app:redis]
Jun 16 16:31:20.284: INFO: Found 1 / 1
Jun 16 16:31:20.284: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 16 16:31:20.286: INFO: Selector matched 1 pods for map[app:redis]
Jun 16 16:31:20.286: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 16 16:31:20.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 patch pod redis-master-pdb9v --namespace=kubectl-319 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 16 16:31:20.342: INFO: stderr: ""
Jun 16 16:31:20.342: INFO: stdout: "pod/redis-master-pdb9v patched\n"
STEP: checking annotations
Jun 16 16:31:20.346: INFO: Selector matched 1 pods for map[app:redis]
Jun 16 16:31:20.346: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:31:20.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-319" for this suite.
Jun 16 16:31:32.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:31:32.404: INFO: namespace kubectl-319 deletion completed in 12.056447076s

• [SLOW TEST:14.546 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:31:32.404: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7383
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 16:31:32.771: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 16:31:35.781: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:31:35.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7383" for this suite.
Jun 16 16:31:41.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:31:41.842: INFO: namespace webhook-7383 deletion completed in 6.054559888s
STEP: Destroying namespace "webhook-7383-markers" for this suite.
Jun 16 16:31:47.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:31:47.941: INFO: namespace webhook-7383-markers deletion completed in 6.099500989s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.692 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:31:48.097: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2105
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Jun 16 16:31:48.729: INFO: created pod pod-service-account-defaultsa
Jun 16 16:31:48.729: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 16 16:31:48.732: INFO: created pod pod-service-account-mountsa
Jun 16 16:31:48.732: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 16 16:31:48.736: INFO: created pod pod-service-account-nomountsa
Jun 16 16:31:48.736: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 16 16:31:48.740: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 16 16:31:48.740: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 16 16:31:48.744: INFO: created pod pod-service-account-mountsa-mountspec
Jun 16 16:31:48.744: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 16 16:31:48.753: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 16 16:31:48.753: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 16 16:31:48.757: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 16 16:31:48.757: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 16 16:31:48.760: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 16 16:31:48.760: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 16 16:31:48.767: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 16 16:31:48.767: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:31:48.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2105" for this suite.
Jun 16 16:31:54.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:31:54.826: INFO: namespace svcaccounts-2105 deletion completed in 6.056324038s

• [SLOW TEST:6.730 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:31:54.827: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2336
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:31:54.952: INFO: (0) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 2.180582ms)
Jun 16 16:31:54.954: INFO: (1) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 2.373868ms)
Jun 16 16:31:54.956: INFO: (2) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.712983ms)
Jun 16 16:31:54.958: INFO: (3) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.692436ms)
Jun 16 16:31:54.959: INFO: (4) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.63577ms)
Jun 16 16:31:54.961: INFO: (5) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.589232ms)
Jun 16 16:31:54.963: INFO: (6) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.562914ms)
Jun 16 16:31:54.964: INFO: (7) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.660466ms)
Jun 16 16:31:54.966: INFO: (8) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.598847ms)
Jun 16 16:31:54.967: INFO: (9) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.637793ms)
Jun 16 16:31:54.969: INFO: (10) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.668744ms)
Jun 16 16:31:54.971: INFO: (11) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.56321ms)
Jun 16 16:31:54.972: INFO: (12) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.638897ms)
Jun 16 16:31:54.974: INFO: (13) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.473372ms)
Jun 16 16:31:54.976: INFO: (14) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.633868ms)
Jun 16 16:31:54.977: INFO: (15) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.65071ms)
Jun 16 16:31:54.979: INFO: (16) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.538458ms)
Jun 16 16:31:54.980: INFO: (17) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.517788ms)
Jun 16 16:31:54.982: INFO: (18) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.505047ms)
Jun 16 16:31:54.983: INFO: (19) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.545897ms)
[AfterEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:31:54.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2336" for this suite.
Jun 16 16:32:00.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:32:01.041: INFO: namespace proxy-2336 deletion completed in 6.05622254s

• [SLOW TEST:6.215 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:32:01.042: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3083
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3083.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3083.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3083.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3083.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3083.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3083.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3083.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3083.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3083.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3083.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3083.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3083.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3083.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3083.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3083.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3083.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3083.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3083.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 16 16:32:03.191: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3083.svc.cluster.local from pod dns-3083/dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685: the server could not find the requested resource (get pods dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685)
Jun 16 16:32:03.193: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3083.svc.cluster.local from pod dns-3083/dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685: the server could not find the requested resource (get pods dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685)
Jun 16 16:32:03.195: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3083.svc.cluster.local from pod dns-3083/dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685: the server could not find the requested resource (get pods dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685)
Jun 16 16:32:03.197: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3083.svc.cluster.local from pod dns-3083/dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685: the server could not find the requested resource (get pods dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685)
Jun 16 16:32:03.198: INFO: Unable to read wheezy_udp@PodARecord from pod dns-3083/dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685: the server could not find the requested resource (get pods dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685)
Jun 16 16:32:03.200: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-3083/dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685: the server could not find the requested resource (get pods dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685)
Jun 16 16:32:03.201: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3083.svc.cluster.local from pod dns-3083/dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685: the server could not find the requested resource (get pods dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685)
Jun 16 16:32:03.203: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3083.svc.cluster.local from pod dns-3083/dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685: the server could not find the requested resource (get pods dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685)
Jun 16 16:32:03.205: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3083.svc.cluster.local from pod dns-3083/dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685: the server could not find the requested resource (get pods dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685)
Jun 16 16:32:03.206: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3083.svc.cluster.local from pod dns-3083/dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685: the server could not find the requested resource (get pods dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685)
Jun 16 16:32:03.208: INFO: Unable to read jessie_udp@PodARecord from pod dns-3083/dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685: the server could not find the requested resource (get pods dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685)
Jun 16 16:32:03.209: INFO: Unable to read jessie_tcp@PodARecord from pod dns-3083/dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685: the server could not find the requested resource (get pods dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685)
Jun 16 16:32:03.209: INFO: Lookups using dns-3083/dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3083.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3083.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3083.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3083.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@dns-querier-2.dns-test-service-2.dns-3083.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3083.svc.cluster.local jessie_udp@dns-test-service-2.dns-3083.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3083.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun 16 16:32:08.239: INFO: DNS probes using dns-3083/dns-test-957c4ac3-8972-4a90-8d19-775cdbd08685 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:32:08.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3083" for this suite.
Jun 16 16:32:14.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:32:14.317: INFO: namespace dns-3083 deletion completed in 6.055175735s

• [SLOW TEST:13.275 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:32:14.317: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5687
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:32:14.442: INFO: Waiting up to 5m0s for pod "busybox-user-65534-cb013f3c-82da-4272-b838-246d06035741" in namespace "security-context-test-5687" to be "success or failure"
Jun 16 16:32:14.445: INFO: Pod "busybox-user-65534-cb013f3c-82da-4272-b838-246d06035741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.245465ms
Jun 16 16:32:16.447: INFO: Pod "busybox-user-65534-cb013f3c-82da-4272-b838-246d06035741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004152706s
Jun 16 16:32:16.447: INFO: Pod "busybox-user-65534-cb013f3c-82da-4272-b838-246d06035741" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:32:16.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5687" for this suite.
Jun 16 16:32:22.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:32:22.503: INFO: namespace security-context-test-5687 deletion completed in 6.054387376s

• [SLOW TEST:8.186 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:32:22.503: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7712
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 16 16:32:22.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-7712'
Jun 16 16:32:22.686: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 16 16:32:22.686: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Jun 16 16:32:24.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete deployment e2e-test-httpd-deployment --namespace=kubectl-7712'
Jun 16 16:32:24.750: INFO: stderr: ""
Jun 16 16:32:24.750: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:32:24.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7712" for this suite.
Jun 16 16:32:52.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:32:52.811: INFO: namespace kubectl-7712 deletion completed in 28.058720813s

• [SLOW TEST:30.308 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:32:52.811: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:33:09.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5553" for this suite.
Jun 16 16:33:15.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:33:16.007: INFO: namespace resourcequota-5553 deletion completed in 6.053807186s

• [SLOW TEST:23.196 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:33:16.007: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 16:33:16.371: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 16:33:19.381: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:33:19.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4647" for this suite.
Jun 16 16:33:25.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:33:25.465: INFO: namespace webhook-4647 deletion completed in 6.054549233s
STEP: Destroying namespace "webhook-4647-markers" for this suite.
Jun 16 16:33:31.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:33:31.566: INFO: namespace webhook-4647-markers deletion completed in 6.100965631s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.712 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:33:31.719: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6586
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 16:33:31.843: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1446cbda-0836-4e0f-8c25-9d3e53efbd5b" in namespace "projected-6586" to be "success or failure"
Jun 16 16:33:31.845: INFO: Pod "downwardapi-volume-1446cbda-0836-4e0f-8c25-9d3e53efbd5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00087ms
Jun 16 16:33:33.847: INFO: Pod "downwardapi-volume-1446cbda-0836-4e0f-8c25-9d3e53efbd5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003918351s
STEP: Saw pod success
Jun 16 16:33:33.847: INFO: Pod "downwardapi-volume-1446cbda-0836-4e0f-8c25-9d3e53efbd5b" satisfied condition "success or failure"
Jun 16 16:33:33.849: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod downwardapi-volume-1446cbda-0836-4e0f-8c25-9d3e53efbd5b container client-container: <nil>
STEP: delete the pod
Jun 16 16:33:33.859: INFO: Waiting for pod downwardapi-volume-1446cbda-0836-4e0f-8c25-9d3e53efbd5b to disappear
Jun 16 16:33:33.861: INFO: Pod downwardapi-volume-1446cbda-0836-4e0f-8c25-9d3e53efbd5b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:33:33.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6586" for this suite.
Jun 16 16:33:39.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:33:39.922: INFO: namespace projected-6586 deletion completed in 6.059240698s

• [SLOW TEST:8.203 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:33:39.923: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-1264
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 16 16:33:40.043: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 16 16:34:02.088: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.18.138.171:8080/dial?request=hostName&protocol=udp&host=172.18.128.46&port=8081&tries=1'] Namespace:pod-network-test-1264 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:34:02.088: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:34:02.184: INFO: Waiting for endpoints: map[]
Jun 16 16:34:02.185: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.18.138.171:8080/dial?request=hostName&protocol=udp&host=172.18.134.168&port=8081&tries=1'] Namespace:pod-network-test-1264 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:34:02.185: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:34:02.267: INFO: Waiting for endpoints: map[]
Jun 16 16:34:02.268: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.18.138.171:8080/dial?request=hostName&protocol=udp&host=172.18.134.66&port=8081&tries=1'] Namespace:pod-network-test-1264 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:34:02.268: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:34:02.348: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:34:02.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1264" for this suite.
Jun 16 16:34:14.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:34:14.404: INFO: namespace pod-network-test-1264 deletion completed in 12.053459581s

• [SLOW TEST:34.481 seconds]
[sig-network] Networking
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:34:14.404: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1354
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 16 16:34:14.527: INFO: Waiting up to 5m0s for pod "pod-a6e87df0-5ac5-4137-a30c-5a0f1d90b36a" in namespace "emptydir-1354" to be "success or failure"
Jun 16 16:34:14.529: INFO: Pod "pod-a6e87df0-5ac5-4137-a30c-5a0f1d90b36a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.849835ms
Jun 16 16:34:16.530: INFO: Pod "pod-a6e87df0-5ac5-4137-a30c-5a0f1d90b36a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003679669s
Jun 16 16:34:18.532: INFO: Pod "pod-a6e87df0-5ac5-4137-a30c-5a0f1d90b36a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005716242s
STEP: Saw pod success
Jun 16 16:34:18.532: INFO: Pod "pod-a6e87df0-5ac5-4137-a30c-5a0f1d90b36a" satisfied condition "success or failure"
Jun 16 16:34:18.534: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-a6e87df0-5ac5-4137-a30c-5a0f1d90b36a container test-container: <nil>
STEP: delete the pod
Jun 16 16:34:18.544: INFO: Waiting for pod pod-a6e87df0-5ac5-4137-a30c-5a0f1d90b36a to disappear
Jun 16 16:34:18.546: INFO: Pod pod-a6e87df0-5ac5-4137-a30c-5a0f1d90b36a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:34:18.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1354" for this suite.
Jun 16 16:34:24.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:34:24.605: INFO: namespace emptydir-1354 deletion completed in 6.05669802s

• [SLOW TEST:10.200 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:34:24.605: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8009
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:34:27.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8009" for this suite.
Jun 16 16:34:39.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:34:39.803: INFO: namespace replication-controller-8009 deletion completed in 12.062422462s

• [SLOW TEST:15.198 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:34:39.803: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7939
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 16:34:40.585: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 16:34:43.594: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:34:43.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7939" for this suite.
Jun 16 16:34:49.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:34:49.667: INFO: namespace webhook-7939 deletion completed in 6.053489223s
STEP: Destroying namespace "webhook-7939-markers" for this suite.
Jun 16 16:34:55.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:34:55.767: INFO: namespace webhook-7939-markers deletion completed in 6.099933256s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.119 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:34:55.922: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3352
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:34:58.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3352" for this suite.
Jun 16 16:35:42.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:35:42.118: INFO: namespace kubelet-test-3352 deletion completed in 44.054484013s

• [SLOW TEST:46.196 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:35:42.118: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-1999
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 16:35:42.769: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 16:35:45.778: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:35:45.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1999" for this suite.
Jun 16 16:35:51.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:35:51.863: INFO: namespace webhook-1999 deletion completed in 6.052576963s
STEP: Destroying namespace "webhook-1999-markers" for this suite.
Jun 16 16:35:57.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:35:57.966: INFO: namespace webhook-1999-markers deletion completed in 6.10295194s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.000 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:35:58.119: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-2000
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2000, will wait for the garbage collector to delete the pods
Jun 16 16:36:00.298: INFO: Deleting Job.batch foo took: 2.734762ms
Jun 16 16:36:01.298: INFO: Terminating Job.batch foo pods took: 1.000197343s
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:36:36.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2000" for this suite.
Jun 16 16:36:42.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:36:42.988: INFO: namespace job-2000 deletion completed in 6.086612289s

• [SLOW TEST:44.869 seconds]
[sig-apps] Job
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:36:42.988: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-982
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-982
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Jun 16 16:36:43.119: INFO: Found 0 stateful pods, waiting for 3
Jun 16 16:36:53.121: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 16 16:36:53.121: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 16 16:36:53.121: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 16 16:36:53.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-982 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 16 16:36:53.272: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 16 16:36:53.272: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 16 16:36:53.272: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Jun 16 16:37:03.293: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 16 16:37:13.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-982 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 16 16:37:13.450: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 16 16:37:13.450: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 16 16:37:13.450: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 16 16:37:13.457: INFO: Waiting for StatefulSet statefulset-982/ss2 to complete update
Jun 16 16:37:13.457: INFO: Waiting for Pod statefulset-982/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 16 16:37:13.457: INFO: Waiting for Pod statefulset-982/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 16 16:37:13.457: INFO: Waiting for Pod statefulset-982/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 16 16:37:23.460: INFO: Waiting for StatefulSet statefulset-982/ss2 to complete update
Jun 16 16:37:23.460: INFO: Waiting for Pod statefulset-982/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 16 16:37:23.460: INFO: Waiting for Pod statefulset-982/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 16 16:37:23.460: INFO: Waiting for Pod statefulset-982/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Jun 16 16:37:33.461: INFO: Waiting for StatefulSet statefulset-982/ss2 to complete update
Jun 16 16:37:33.461: INFO: Waiting for Pod statefulset-982/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Jun 16 16:37:43.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-982 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 16 16:37:43.613: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 16 16:37:43.613: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 16 16:37:43.613: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 16 16:37:53.634: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 16 16:38:03.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=statefulset-982 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 16 16:38:03.789: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 16 16:38:03.789: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 16 16:38:03.789: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 16 16:38:13.799: INFO: Waiting for StatefulSet statefulset-982/ss2 to complete update
Jun 16 16:38:13.799: INFO: Waiting for Pod statefulset-982/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun 16 16:38:23.803: INFO: Deleting all statefulset in ns statefulset-982
Jun 16 16:38:23.804: INFO: Scaling statefulset ss2 to 0
Jun 16 16:38:43.812: INFO: Waiting for statefulset status.replicas updated to 0
Jun 16 16:38:43.814: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:38:43.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-982" for this suite.
Jun 16 16:38:49.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:38:49.876: INFO: namespace statefulset-982 deletion completed in 6.054396078s

• [SLOW TEST:126.888 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:38:49.876: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 16 16:38:49.999: INFO: Waiting up to 5m0s for pod "pod-28cc1036-0483-4746-8f18-801e169b4a70" in namespace "emptydir-6333" to be "success or failure"
Jun 16 16:38:50.001: INFO: Pod "pod-28cc1036-0483-4746-8f18-801e169b4a70": Phase="Pending", Reason="", readiness=false. Elapsed: 1.777783ms
Jun 16 16:38:52.003: INFO: Pod "pod-28cc1036-0483-4746-8f18-801e169b4a70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003906152s
STEP: Saw pod success
Jun 16 16:38:52.003: INFO: Pod "pod-28cc1036-0483-4746-8f18-801e169b4a70" satisfied condition "success or failure"
Jun 16 16:38:52.004: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-28cc1036-0483-4746-8f18-801e169b4a70 container test-container: <nil>
STEP: delete the pod
Jun 16 16:38:52.018: INFO: Waiting for pod pod-28cc1036-0483-4746-8f18-801e169b4a70 to disappear
Jun 16 16:38:52.019: INFO: Pod pod-28cc1036-0483-4746-8f18-801e169b4a70 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:38:52.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6333" for this suite.
Jun 16 16:38:58.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:38:58.079: INFO: namespace emptydir-6333 deletion completed in 6.056989121s

• [SLOW TEST:8.203 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:38:58.079: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8171
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jun 16 16:38:58.198: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 16 16:38:58.204: INFO: Waiting for terminating namespaces to be deleted...
Jun 16 16:38:58.205: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-122.eu-west-1.compute.internal before test
Jun 16 16:38:58.209: INFO: kube-proxy-xdgfc from kube-system started at 2020-06-16 15:06:53 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.209: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 16:38:58.209: INFO: cert-exporter-sgwbs from kube-system started at 2020-06-16 15:10:32 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.209: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 16:38:58.209: INFO: coredns-6d56c484c-wcs6v from kube-system started at 2020-06-16 15:10:34 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.209: INFO: 	Container coredns ready: true, restart count 0
Jun 16 16:38:58.209: INFO: aws-node-jbq4c from kube-system started at 2020-06-16 15:10:43 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.209: INFO: 	Container aws-node ready: true, restart count 0
Jun 16 16:38:58.209: INFO: net-exporter-ltbgf from kube-system started at 2020-06-16 15:14:31 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.209: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 16:38:58.209: INFO: calico-node-sk9rd from kube-system started at 2020-06-16 15:06:53 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.209: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 16:38:58.209: INFO: node-exporter-5vz2p from kube-system started at 2020-06-16 15:10:31 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.209: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 16:38:58.209: INFO: kube-state-metrics-6d998ffd8b-jpbpj from kube-system started at 2020-06-16 15:10:43 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.209: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 16 16:38:58.209: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-6qlkz from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 16:38:58.209: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 16 16:38:58.209: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 16:38:58.209: INFO: kiam-agent-x89wb from kube-system started at 2020-06-16 15:10:38 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.209: INFO: 	Container kiam-agent ready: true, restart count 3
Jun 16 16:38:58.209: INFO: external-dns-776fc66667-24mls from kube-system started at 2020-06-16 15:10:42 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.209: INFO: 	Container external-dns ready: true, restart count 0
Jun 16 16:38:58.209: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-219.eu-west-1.compute.internal before test
Jun 16 16:38:58.213: INFO: aws-node-kmx2q from kube-system started at 2020-06-16 15:10:51 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.213: INFO: 	Container aws-node ready: true, restart count 0
Jun 16 16:38:58.213: INFO: kube-proxy-r2g9b from kube-system started at 2020-06-16 15:06:51 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.213: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 16:38:58.213: INFO: node-exporter-pqhv4 from kube-system started at 2020-06-16 15:10:31 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.213: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 16:38:58.213: INFO: sonobuoy-e2e-job-40469eeb076c4210 from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 16:38:58.213: INFO: 	Container e2e ready: true, restart count 0
Jun 16 16:38:58.213: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 16:38:58.213: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-bbzmx from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 16:38:58.213: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 16 16:38:58.213: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 16:38:58.213: INFO: cert-manager-67dfd96fcd-x47b4 from kube-system started at 2020-06-16 16:27:46 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.213: INFO: 	Container cert-manager ready: true, restart count 0
Jun 16 16:38:58.213: INFO: tiller-deploy-684c6b545b-xfl8b from giantswarm started at 2020-06-16 15:09:33 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.213: INFO: 	Container tiller ready: true, restart count 0
Jun 16 16:38:58.213: INFO: coredns-6d56c484c-l7qk9 from kube-system started at 2020-06-16 15:10:34 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.213: INFO: 	Container coredns ready: true, restart count 0
Jun 16 16:38:58.213: INFO: kiam-agent-6q62w from kube-system started at 2020-06-16 15:10:38 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.213: INFO: 	Container kiam-agent ready: true, restart count 3
Jun 16 16:38:58.213: INFO: calico-node-xpkvc from kube-system started at 2020-06-16 15:06:51 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.213: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 16:38:58.213: INFO: cert-exporter-fthcq from kube-system started at 2020-06-16 15:10:32 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.213: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 16:38:58.213: INFO: net-exporter-p7cpr from kube-system started at 2020-06-16 15:14:31 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.213: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 16:38:58.213: INFO: sonobuoy from sonobuoy started at 2020-06-16 15:26:40 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.213: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 16 16:38:58.213: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-6.eu-west-1.compute.internal before test
Jun 16 16:38:58.217: INFO: coredns-6d56c484c-p2v4f from kube-system started at 2020-06-16 16:27:46 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.217: INFO: 	Container coredns ready: true, restart count 0
Jun 16 16:38:58.217: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-87l2v from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 16:38:58.217: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 16 16:38:58.217: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 16:38:58.217: INFO: metrics-server-66df9f5b56-wbjz4 from kube-system started at 2020-06-16 16:27:46 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.217: INFO: 	Container metrics-server ready: true, restart count 0
Jun 16 16:38:58.217: INFO: kube-proxy-g6cmw from kube-system started at 2020-06-16 15:06:46 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.217: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 16:38:58.217: INFO: node-exporter-z96vr from kube-system started at 2020-06-16 15:10:31 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.217: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 16:38:58.217: INFO: kiam-agent-25qmf from kube-system started at 2020-06-16 16:27:52 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.217: INFO: 	Container kiam-agent ready: true, restart count 0
Jun 16 16:38:58.217: INFO: net-exporter-5wtl4 from kube-system started at 2020-06-16 15:14:31 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.217: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 16:38:58.217: INFO: cert-exporter-2k4kc from kube-system started at 2020-06-16 15:10:32 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.217: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 16:38:58.217: INFO: calico-node-ddwlg from kube-system started at 2020-06-16 15:06:46 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.217: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 16:38:58.217: INFO: aws-node-vjj2v from kube-system started at 2020-06-16 15:10:39 +0000 UTC (1 container statuses recorded)
Jun 16 16:38:58.217: INFO: 	Container aws-node ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.161913a3f9531de9], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:38:59.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8171" for this suite.
Jun 16 16:39:05.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:39:05.292: INFO: namespace sched-pred-8171 deletion completed in 6.055313498s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:7.213 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:39:05.292: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8214
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 16 16:39:09.435: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 16 16:39:09.437: INFO: Pod pod-with-prestop-http-hook still exists
Jun 16 16:39:11.438: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 16 16:39:11.439: INFO: Pod pod-with-prestop-http-hook still exists
Jun 16 16:39:13.438: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 16 16:39:13.440: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:39:13.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8214" for this suite.
Jun 16 16:39:41.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:39:41.499: INFO: namespace container-lifecycle-hook-8214 deletion completed in 28.052312574s

• [SLOW TEST:36.206 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:39:41.499: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2833
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-4d2e3afd-1604-46e0-a60e-da11f75fdc2c
STEP: Creating a pod to test consume secrets
Jun 16 16:39:41.630: INFO: Waiting up to 5m0s for pod "pod-secrets-fd360e7d-ca0c-4319-85d2-2706c582454f" in namespace "secrets-2833" to be "success or failure"
Jun 16 16:39:41.632: INFO: Pod "pod-secrets-fd360e7d-ca0c-4319-85d2-2706c582454f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133832ms
Jun 16 16:39:43.634: INFO: Pod "pod-secrets-fd360e7d-ca0c-4319-85d2-2706c582454f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004081015s
STEP: Saw pod success
Jun 16 16:39:43.634: INFO: Pod "pod-secrets-fd360e7d-ca0c-4319-85d2-2706c582454f" satisfied condition "success or failure"
Jun 16 16:39:43.636: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-secrets-fd360e7d-ca0c-4319-85d2-2706c582454f container secret-volume-test: <nil>
STEP: delete the pod
Jun 16 16:39:43.645: INFO: Waiting for pod pod-secrets-fd360e7d-ca0c-4319-85d2-2706c582454f to disappear
Jun 16 16:39:43.647: INFO: Pod pod-secrets-fd360e7d-ca0c-4319-85d2-2706c582454f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:39:43.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2833" for this suite.
Jun 16 16:39:49.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:39:49.703: INFO: namespace secrets-2833 deletion completed in 6.054273694s

• [SLOW TEST:8.204 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:39:49.703: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2615
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:39:49.838: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 16 16:39:49.841: INFO: Number of nodes with available pods: 0
Jun 16 16:39:49.841: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 16 16:39:49.853: INFO: Number of nodes with available pods: 0
Jun 16 16:39:49.853: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:39:50.854: INFO: Number of nodes with available pods: 1
Jun 16 16:39:50.854: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 16 16:39:50.863: INFO: Number of nodes with available pods: 1
Jun 16 16:39:50.863: INFO: Number of running nodes: 0, number of available pods: 1
Jun 16 16:39:51.865: INFO: Number of nodes with available pods: 0
Jun 16 16:39:51.865: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 16 16:39:51.869: INFO: Number of nodes with available pods: 0
Jun 16 16:39:51.869: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:39:52.871: INFO: Number of nodes with available pods: 0
Jun 16 16:39:52.871: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:39:53.871: INFO: Number of nodes with available pods: 0
Jun 16 16:39:53.871: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:39:54.870: INFO: Number of nodes with available pods: 0
Jun 16 16:39:54.870: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:39:55.871: INFO: Number of nodes with available pods: 0
Jun 16 16:39:55.871: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:39:56.871: INFO: Number of nodes with available pods: 0
Jun 16 16:39:56.871: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:39:57.871: INFO: Number of nodes with available pods: 0
Jun 16 16:39:57.871: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:39:58.871: INFO: Number of nodes with available pods: 0
Jun 16 16:39:58.871: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:39:59.871: INFO: Number of nodes with available pods: 0
Jun 16 16:39:59.871: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:40:00.871: INFO: Number of nodes with available pods: 0
Jun 16 16:40:00.871: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:40:01.871: INFO: Number of nodes with available pods: 0
Jun 16 16:40:01.871: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:40:02.871: INFO: Number of nodes with available pods: 0
Jun 16 16:40:02.871: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:40:03.871: INFO: Number of nodes with available pods: 0
Jun 16 16:40:03.871: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:40:04.871: INFO: Number of nodes with available pods: 0
Jun 16 16:40:04.871: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:40:05.871: INFO: Number of nodes with available pods: 1
Jun 16 16:40:05.871: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2615, will wait for the garbage collector to delete the pods
Jun 16 16:40:05.928: INFO: Deleting DaemonSet.extensions daemon-set took: 3.042948ms
Jun 16 16:40:06.128: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.207391ms
Jun 16 16:40:13.530: INFO: Number of nodes with available pods: 0
Jun 16 16:40:13.530: INFO: Number of running nodes: 0, number of available pods: 0
Jun 16 16:40:13.532: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2615/daemonsets","resourceVersion":"25811"},"items":null}

Jun 16 16:40:13.533: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2615/pods","resourceVersion":"25811"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:40:13.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2615" for this suite.
Jun 16 16:40:19.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:40:19.605: INFO: namespace daemonsets-2615 deletion completed in 6.052913844s

• [SLOW TEST:29.902 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:40:19.606: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9958
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:40:19.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 create -f - --namespace=kubectl-9958'
Jun 16 16:40:19.856: INFO: stderr: ""
Jun 16 16:40:19.856: INFO: stdout: "replicationcontroller/redis-master created\n"
Jun 16 16:40:19.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 create -f - --namespace=kubectl-9958'
Jun 16 16:40:19.992: INFO: stderr: ""
Jun 16 16:40:19.992: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 16 16:40:20.994: INFO: Selector matched 1 pods for map[app:redis]
Jun 16 16:40:20.994: INFO: Found 0 / 1
Jun 16 16:40:21.994: INFO: Selector matched 1 pods for map[app:redis]
Jun 16 16:40:21.994: INFO: Found 1 / 1
Jun 16 16:40:21.994: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 16 16:40:21.997: INFO: Selector matched 1 pods for map[app:redis]
Jun 16 16:40:21.997: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 16 16:40:21.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 describe pod redis-master-mzj5t --namespace=kubectl-9958'
Jun 16 16:40:22.064: INFO: stderr: ""
Jun 16 16:40:22.064: INFO: stdout: "Name:         redis-master-mzj5t\nNamespace:    kubectl-9958\nPriority:     0\nNode:         ip-172-19-65-6.eu-west-1.compute.internal/172.19.65.6\nStart Time:   Tue, 16 Jun 2020 16:40:19 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  kubernetes.io/psp: e2e-test-privileged-psp\nStatus:       Running\nIP:           172.18.133.203\nIPs:\n  IP:           172.18.133.203\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://fd668233ceef5f3af0111fd8055f2ab89e99784cd198b57a4e6f75b1b2dfe08d\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 16 Jun 2020 16:40:20 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-xt4qd (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-xt4qd:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-xt4qd\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                Message\n  ----    ------     ----  ----                                                -------\n  Normal  Scheduled  3s    default-scheduler                                   Successfully assigned kubectl-9958/redis-master-mzj5t to ip-172-19-65-6.eu-west-1.compute.internal\n  Normal  Pulled     2s    kubelet, ip-172-19-65-6.eu-west-1.compute.internal  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    2s    kubelet, ip-172-19-65-6.eu-west-1.compute.internal  Created container redis-master\n  Normal  Started    2s    kubelet, ip-172-19-65-6.eu-west-1.compute.internal  Started container redis-master\n"
Jun 16 16:40:22.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 describe rc redis-master --namespace=kubectl-9958'
Jun 16 16:40:22.152: INFO: stderr: ""
Jun 16 16:40:22.152: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-9958\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-mzj5t\n"
Jun 16 16:40:22.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 describe service redis-master --namespace=kubectl-9958'
Jun 16 16:40:22.218: INFO: stderr: ""
Jun 16 16:40:22.218: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-9958\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.31.132.95\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.18.133.203:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 16 16:40:22.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 describe node ip-172-19-65-122.eu-west-1.compute.internal'
Jun 16 16:40:22.296: INFO: stderr: ""
Jun 16 16:40:22.296: INFO: stdout: "Name:               ip-172-19-65-122.eu-west-1.compute.internal\nRoles:              worker\nLabels:             aws-operator.giantswarm.io/version=8.6.1\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-west-1\n                    failure-domain.beta.kubernetes.io/zone=eu-west-1a\n                    giantswarm.io/machine-deployment=i2krm\n                    giantswarm.io/provider=aws\n                    ip=172.19.65.122\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-19-65-122.eu-west-1.compute.internal\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=worker\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/worker=\n                    role=worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 16 Jun 2020 15:06:53 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 16 Jun 2020 16:39:56 +0000   Tue, 16 Jun 2020 15:06:53 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 16 Jun 2020 16:39:56 +0000   Tue, 16 Jun 2020 15:06:53 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 16 Jun 2020 16:39:56 +0000   Tue, 16 Jun 2020 15:06:53 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 16 Jun 2020 16:39:56 +0000   Tue, 16 Jun 2020 15:07:13 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   172.19.65.122\n  Hostname:     ip-172-19-65-122.eu-west-1.compute.internal\n  InternalDNS:  ip-172-19-65-122.eu-west-1.compute.internal\nCapacity:\n attachable-volumes-aws-ebs:  25\n cpu:                         4\n ephemeral-storage:           102350Mi\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      15950468Ki\n pods:                        40\nAllocatable:\n attachable-volumes-aws-ebs:  25\n cpu:                         3500m\n ephemeral-storage:           101326Mi\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      14566020Ki\n pods:                        40\nSystem Info:\n Machine ID:                 ec2a1075c163b86863566278891194cd\n System UUID:                ec2a1075-c163-b868-6356-6278891194cd\n Boot ID:                    546d0791-5f86-4b10-bd14-ee76a6dc9a37\n Kernel Version:             4.19.107-flatcar\n OS Image:                   Flatcar Container Linux by Kinvolk 2345.3.1 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.16.9\n Kube-Proxy Version:         v1.16.9\nProviderID:                  aws:///eu-west-1a/i-0e9eab97ac8d0f0bd\nNon-terminated Pods:         (11 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                aws-node-jbq4c                                             30m (0%)      0 (0%)      0 (0%)           0 (0%)         89m\n  kube-system                calico-node-sk9rd                                          250m (7%)     0 (0%)      150Mi (1%)       0 (0%)         93m\n  kube-system                cert-exporter-sgwbs                                        50m (1%)      0 (0%)      50Mi (0%)        50Mi (0%)      89m\n  kube-system                coredns-6d56c484c-wcs6v                                    250m (7%)     0 (0%)      192Mi (1%)       192Mi (1%)     89m\n  kube-system                external-dns-776fc66667-24mls                              50m (1%)      0 (0%)      50Mi (0%)        50Mi (0%)      89m\n  kube-system                kiam-agent-x89wb                                           50m (1%)      0 (0%)      50Mi (0%)        50Mi (0%)      89m\n  kube-system                kube-proxy-xdgfc                                           75m (2%)      0 (0%)      80Mi (0%)        0 (0%)         93m\n  kube-system                kube-state-metrics-6d998ffd8b-jpbpj                        500m (14%)    0 (0%)      500Mi (3%)       0 (0%)         89m\n  kube-system                net-exporter-ltbgf                                         50m (1%)      0 (0%)      75Mi (0%)        75Mi (0%)      85m\n  kube-system                node-exporter-5vz2p                                        75m (2%)      0 (0%)      50Mi (0%)        50Mi (0%)      89m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-6qlkz    0 (0%)        0 (0%)      0 (0%)           0 (0%)         73m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests     Limits\n  --------                    --------     ------\n  cpu                         1380m (39%)  0 (0%)\n  memory                      1197Mi (8%)  467Mi (3%)\n  ephemeral-storage           0 (0%)       0 (0%)\n  attachable-volumes-aws-ebs  0            0\nEvents:                       <none>\n"
Jun 16 16:40:22.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 describe namespace kubectl-9958'
Jun 16 16:40:22.355: INFO: stderr: ""
Jun 16 16:40:22.355: INFO: stdout: "Name:         kubectl-9958\nLabels:       e2e-framework=kubectl\n              e2e-run=ff4c0d7c-7e88-4536-b290-a17ec70fa0af\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:40:22.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9958" for this suite.
Jun 16 16:40:34.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:40:34.409: INFO: namespace kubectl-9958 deletion completed in 12.052521853s

• [SLOW TEST:14.804 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:40:34.410: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5473
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:40:34.530: INFO: Creating deployment "test-recreate-deployment"
Jun 16 16:40:34.533: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 16 16:40:34.537: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 16 16:40:36.540: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 16 16:40:36.542: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 16 16:40:36.545: INFO: Updating deployment test-recreate-deployment
Jun 16 16:40:36.545: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 16 16:40:36.597: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5473 /apis/apps/v1/namespaces/deployment-5473/deployments/test-recreate-deployment c3b494bd-eb9f-4b7c-8b35-17105eb6e469 25979 2 2020-06-16 16:40:34 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003ca3ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-06-16 16:40:36 +0000 UTC,LastTransitionTime:2020-06-16 16:40:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-06-16 16:40:36 +0000 UTC,LastTransitionTime:2020-06-16 16:40:34 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun 16 16:40:36.598: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-5473 /apis/apps/v1/namespaces/deployment-5473/replicasets/test-recreate-deployment-5f94c574ff 8c243c16-ec69-4f2b-8055-f35f45fa96d4 25975 1 2020-06-16 16:40:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment c3b494bd-eb9f-4b7c-8b35-17105eb6e469 0xc003cc02c7 0xc003cc02c8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003cc0328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 16 16:40:36.598: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 16 16:40:36.598: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-5473 /apis/apps/v1/namespaces/deployment-5473/replicasets/test-recreate-deployment-68fc85c7bb ecf1c2ca-324b-4de6-ba86-0fbaa39fda6d 25966 2 2020-06-16 16:40:34 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment c3b494bd-eb9f-4b7c-8b35-17105eb6e469 0xc003cc03b7 0xc003cc03b8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003cc0418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 16 16:40:36.600: INFO: Pod "test-recreate-deployment-5f94c574ff-tnrk9" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-tnrk9 test-recreate-deployment-5f94c574ff- deployment-5473 /api/v1/namespaces/deployment-5473/pods/test-recreate-deployment-5f94c574ff-tnrk9 2690b936-00f5-45d4-9dae-bb0a0b98df3b 25977 0 2020-06-16 16:40:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff 8c243c16-ec69-4f2b-8055-f35f45fa96d4 0xc003cc0897 0xc003cc0898}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tnwhb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tnwhb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tnwhb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-6.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 16:40:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 16:40:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 16:40:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 16:40:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.6,PodIP:,StartTime:2020-06-16 16:40:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:40:36.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5473" for this suite.
Jun 16 16:40:42.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:40:42.665: INFO: namespace deployment-5473 deletion completed in 6.063872507s

• [SLOW TEST:8.256 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:40:42.665: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5841
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:40:42.800: INFO: Create a RollingUpdate DaemonSet
Jun 16 16:40:42.802: INFO: Check that daemon pods launch on every node of the cluster
Jun 16 16:40:42.804: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:40:42.806: INFO: Number of nodes with available pods: 0
Jun 16 16:40:42.806: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:40:43.808: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:40:43.810: INFO: Number of nodes with available pods: 1
Jun 16 16:40:43.810: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 16:40:44.808: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:40:44.810: INFO: Number of nodes with available pods: 3
Jun 16 16:40:44.810: INFO: Number of running nodes: 3, number of available pods: 3
Jun 16 16:40:44.810: INFO: Update the DaemonSet to trigger a rollout
Jun 16 16:40:44.814: INFO: Updating DaemonSet daemon-set
Jun 16 16:40:56.829: INFO: Roll back the DaemonSet before rollout is complete
Jun 16 16:40:56.832: INFO: Updating DaemonSet daemon-set
Jun 16 16:40:56.832: INFO: Make sure DaemonSet rollback is complete
Jun 16 16:40:56.833: INFO: Wrong image for pod: daemon-set-fprc5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 16 16:40:56.833: INFO: Pod daemon-set-fprc5 is not available
Jun 16 16:40:56.835: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:40:57.838: INFO: Wrong image for pod: daemon-set-fprc5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 16 16:40:57.838: INFO: Pod daemon-set-fprc5 is not available
Jun 16 16:40:57.840: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:40:58.838: INFO: Wrong image for pod: daemon-set-fprc5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 16 16:40:58.838: INFO: Pod daemon-set-fprc5 is not available
Jun 16 16:40:58.839: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:40:59.838: INFO: Wrong image for pod: daemon-set-fprc5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 16 16:40:59.838: INFO: Pod daemon-set-fprc5 is not available
Jun 16 16:40:59.840: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:41:00.838: INFO: Wrong image for pod: daemon-set-fprc5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 16 16:41:00.838: INFO: Pod daemon-set-fprc5 is not available
Jun 16 16:41:00.840: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:41:01.838: INFO: Wrong image for pod: daemon-set-fprc5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 16 16:41:01.838: INFO: Pod daemon-set-fprc5 is not available
Jun 16 16:41:01.840: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:41:02.838: INFO: Wrong image for pod: daemon-set-fprc5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 16 16:41:02.838: INFO: Pod daemon-set-fprc5 is not available
Jun 16 16:41:02.840: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:41:03.838: INFO: Wrong image for pod: daemon-set-fprc5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 16 16:41:03.838: INFO: Pod daemon-set-fprc5 is not available
Jun 16 16:41:03.840: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:41:04.838: INFO: Wrong image for pod: daemon-set-fprc5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 16 16:41:04.838: INFO: Pod daemon-set-fprc5 is not available
Jun 16 16:41:04.840: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:41:05.838: INFO: Wrong image for pod: daemon-set-fprc5. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Jun 16 16:41:05.838: INFO: Pod daemon-set-fprc5 is not available
Jun 16 16:41:05.840: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 16:41:06.837: INFO: Pod daemon-set-bbr88 is not available
Jun 16 16:41:06.839: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5841, will wait for the garbage collector to delete the pods
Jun 16 16:41:06.896: INFO: Deleting DaemonSet.extensions daemon-set took: 3.171671ms
Jun 16 16:41:06.996: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.149498ms
Jun 16 16:41:16.898: INFO: Number of nodes with available pods: 0
Jun 16 16:41:16.898: INFO: Number of running nodes: 0, number of available pods: 0
Jun 16 16:41:16.899: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5841/daemonsets","resourceVersion":"26187"},"items":null}

Jun 16 16:41:16.900: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5841/pods","resourceVersion":"26187"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:41:16.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5841" for this suite.
Jun 16 16:41:22.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:41:22.961: INFO: namespace daemonsets-5841 deletion completed in 6.052966925s

• [SLOW TEST:40.295 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:41:22.961: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1878
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-9af9b8c9-bf6f-43a1-8468-7ab3aa02ca88
STEP: Creating a pod to test consume configMaps
Jun 16 16:41:23.085: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4d761064-cc09-4d95-92ab-98b2d18dc87f" in namespace "projected-1878" to be "success or failure"
Jun 16 16:41:23.088: INFO: Pod "pod-projected-configmaps-4d761064-cc09-4d95-92ab-98b2d18dc87f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.990449ms
Jun 16 16:41:25.090: INFO: Pod "pod-projected-configmaps-4d761064-cc09-4d95-92ab-98b2d18dc87f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005063823s
STEP: Saw pod success
Jun 16 16:41:25.090: INFO: Pod "pod-projected-configmaps-4d761064-cc09-4d95-92ab-98b2d18dc87f" satisfied condition "success or failure"
Jun 16 16:41:25.092: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-projected-configmaps-4d761064-cc09-4d95-92ab-98b2d18dc87f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 16 16:41:25.102: INFO: Waiting for pod pod-projected-configmaps-4d761064-cc09-4d95-92ab-98b2d18dc87f to disappear
Jun 16 16:41:25.104: INFO: Pod pod-projected-configmaps-4d761064-cc09-4d95-92ab-98b2d18dc87f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:41:25.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1878" for this suite.
Jun 16 16:41:31.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:41:31.164: INFO: namespace projected-1878 deletion completed in 6.058492631s

• [SLOW TEST:8.203 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:41:31.165: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7698
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-g9bcp in namespace proxy-7698
I0616 16:41:31.296357      24 runners.go:184] Created replication controller with name: proxy-service-g9bcp, namespace: proxy-7698, replica count: 1
I0616 16:41:32.346644      24 runners.go:184] proxy-service-g9bcp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0616 16:41:33.346812      24 runners.go:184] proxy-service-g9bcp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0616 16:41:34.347023      24 runners.go:184] proxy-service-g9bcp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0616 16:41:35.347172      24 runners.go:184] proxy-service-g9bcp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0616 16:41:36.347319      24 runners.go:184] proxy-service-g9bcp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0616 16:41:37.347481      24 runners.go:184] proxy-service-g9bcp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0616 16:41:38.347656      24 runners.go:184] proxy-service-g9bcp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 16 16:41:38.349: INFO: setup took 7.061755557s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 16 16:41:38.355: INFO: (0) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 5.763179ms)
Jun 16 16:41:38.355: INFO: (0) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 5.841689ms)
Jun 16 16:41:38.355: INFO: (0) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 6.23438ms)
Jun 16 16:41:38.356: INFO: (0) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 6.429009ms)
Jun 16 16:41:38.356: INFO: (0) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 6.444289ms)
Jun 16 16:41:38.357: INFO: (0) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 8.087821ms)
Jun 16 16:41:38.357: INFO: (0) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 8.235818ms)
Jun 16 16:41:38.357: INFO: (0) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 8.182313ms)
Jun 16 16:41:38.357: INFO: (0) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 8.172034ms)
Jun 16 16:41:38.357: INFO: (0) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 8.235058ms)
Jun 16 16:41:38.358: INFO: (0) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 8.931901ms)
Jun 16 16:41:38.358: INFO: (0) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 8.910695ms)
Jun 16 16:41:38.359: INFO: (0) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 9.45221ms)
Jun 16 16:41:38.359: INFO: (0) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 9.364523ms)
Jun 16 16:41:38.363: INFO: (0) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 14.087623ms)
Jun 16 16:41:38.363: INFO: (0) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 14.211562ms)
Jun 16 16:41:38.369: INFO: (1) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 5.539262ms)
Jun 16 16:41:38.369: INFO: (1) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 5.422783ms)
Jun 16 16:41:38.369: INFO: (1) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 5.739479ms)
Jun 16 16:41:38.369: INFO: (1) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 5.855724ms)
Jun 16 16:41:38.369: INFO: (1) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 5.72458ms)
Jun 16 16:41:38.369: INFO: (1) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 6.025994ms)
Jun 16 16:41:38.369: INFO: (1) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 5.641547ms)
Jun 16 16:41:38.369: INFO: (1) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 5.716519ms)
Jun 16 16:41:38.369: INFO: (1) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 5.945486ms)
Jun 16 16:41:38.370: INFO: (1) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 6.132451ms)
Jun 16 16:41:38.370: INFO: (1) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 6.120924ms)
Jun 16 16:41:38.370: INFO: (1) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 6.34101ms)
Jun 16 16:41:38.370: INFO: (1) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 6.454853ms)
Jun 16 16:41:38.370: INFO: (1) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 6.015182ms)
Jun 16 16:41:38.370: INFO: (1) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 6.24098ms)
Jun 16 16:41:38.370: INFO: (1) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 6.609841ms)
Jun 16 16:41:38.373: INFO: (2) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 2.970904ms)
Jun 16 16:41:38.376: INFO: (2) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 5.554188ms)
Jun 16 16:41:38.376: INFO: (2) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 5.769976ms)
Jun 16 16:41:38.376: INFO: (2) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 6.155693ms)
Jun 16 16:41:38.377: INFO: (2) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 6.521073ms)
Jun 16 16:41:38.377: INFO: (2) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 6.917205ms)
Jun 16 16:41:38.377: INFO: (2) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 7.258551ms)
Jun 16 16:41:38.377: INFO: (2) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 7.219817ms)
Jun 16 16:41:38.378: INFO: (2) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 7.290916ms)
Jun 16 16:41:38.378: INFO: (2) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 7.436984ms)
Jun 16 16:41:38.378: INFO: (2) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 7.656071ms)
Jun 16 16:41:38.378: INFO: (2) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 7.855491ms)
Jun 16 16:41:38.378: INFO: (2) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 7.737181ms)
Jun 16 16:41:38.378: INFO: (2) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 8.15161ms)
Jun 16 16:41:38.378: INFO: (2) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 8.104086ms)
Jun 16 16:41:38.378: INFO: (2) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 8.262286ms)
Jun 16 16:41:38.382: INFO: (3) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 3.340931ms)
Jun 16 16:41:38.383: INFO: (3) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 4.061639ms)
Jun 16 16:41:38.383: INFO: (3) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 4.275781ms)
Jun 16 16:41:38.383: INFO: (3) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 4.73376ms)
Jun 16 16:41:38.383: INFO: (3) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 4.83683ms)
Jun 16 16:41:38.383: INFO: (3) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 4.892139ms)
Jun 16 16:41:38.383: INFO: (3) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 4.837421ms)
Jun 16 16:41:38.384: INFO: (3) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 4.96938ms)
Jun 16 16:41:38.384: INFO: (3) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 5.164758ms)
Jun 16 16:41:38.384: INFO: (3) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 5.186894ms)
Jun 16 16:41:38.384: INFO: (3) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 5.431729ms)
Jun 16 16:41:38.384: INFO: (3) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 5.467962ms)
Jun 16 16:41:38.384: INFO: (3) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 5.648282ms)
Jun 16 16:41:38.384: INFO: (3) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 5.670489ms)
Jun 16 16:41:38.384: INFO: (3) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 5.647331ms)
Jun 16 16:41:38.384: INFO: (3) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 5.617547ms)
Jun 16 16:41:38.387: INFO: (4) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 2.486995ms)
Jun 16 16:41:38.388: INFO: (4) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 3.557379ms)
Jun 16 16:41:38.388: INFO: (4) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 3.732433ms)
Jun 16 16:41:38.389: INFO: (4) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 4.033808ms)
Jun 16 16:41:38.389: INFO: (4) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 4.070647ms)
Jun 16 16:41:38.389: INFO: (4) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 4.3659ms)
Jun 16 16:41:38.389: INFO: (4) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 4.373615ms)
Jun 16 16:41:38.389: INFO: (4) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 4.440364ms)
Jun 16 16:41:38.389: INFO: (4) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 4.496098ms)
Jun 16 16:41:38.389: INFO: (4) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 4.765294ms)
Jun 16 16:41:38.389: INFO: (4) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 4.717071ms)
Jun 16 16:41:38.391: INFO: (4) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 6.231586ms)
Jun 16 16:41:38.391: INFO: (4) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 6.435272ms)
Jun 16 16:41:38.391: INFO: (4) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 6.594147ms)
Jun 16 16:41:38.391: INFO: (4) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 6.573698ms)
Jun 16 16:41:38.391: INFO: (4) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 6.75376ms)
Jun 16 16:41:38.394: INFO: (5) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 2.537648ms)
Jun 16 16:41:38.396: INFO: (5) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 4.399501ms)
Jun 16 16:41:38.396: INFO: (5) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 4.651533ms)
Jun 16 16:41:38.397: INFO: (5) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 5.215303ms)
Jun 16 16:41:38.397: INFO: (5) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 5.420414ms)
Jun 16 16:41:38.397: INFO: (5) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 5.244288ms)
Jun 16 16:41:38.397: INFO: (5) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 5.389551ms)
Jun 16 16:41:38.397: INFO: (5) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 5.264634ms)
Jun 16 16:41:38.397: INFO: (5) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 5.419837ms)
Jun 16 16:41:38.397: INFO: (5) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 5.527443ms)
Jun 16 16:41:38.397: INFO: (5) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 5.586058ms)
Jun 16 16:41:38.397: INFO: (5) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 5.645444ms)
Jun 16 16:41:38.397: INFO: (5) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 5.734534ms)
Jun 16 16:41:38.397: INFO: (5) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 5.858402ms)
Jun 16 16:41:38.398: INFO: (5) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 6.679797ms)
Jun 16 16:41:38.398: INFO: (5) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 6.795392ms)
Jun 16 16:41:38.401: INFO: (6) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 2.416844ms)
Jun 16 16:41:38.401: INFO: (6) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 2.409197ms)
Jun 16 16:41:38.402: INFO: (6) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 4.133826ms)
Jun 16 16:41:38.403: INFO: (6) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 4.402586ms)
Jun 16 16:41:38.403: INFO: (6) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 4.477506ms)
Jun 16 16:41:38.403: INFO: (6) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 4.626685ms)
Jun 16 16:41:38.403: INFO: (6) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 4.734349ms)
Jun 16 16:41:38.403: INFO: (6) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 4.881003ms)
Jun 16 16:41:38.403: INFO: (6) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 4.938038ms)
Jun 16 16:41:38.404: INFO: (6) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 5.562392ms)
Jun 16 16:41:38.404: INFO: (6) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 5.425343ms)
Jun 16 16:41:38.404: INFO: (6) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 6.141448ms)
Jun 16 16:41:38.405: INFO: (6) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 6.570923ms)
Jun 16 16:41:38.405: INFO: (6) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 6.585485ms)
Jun 16 16:41:38.405: INFO: (6) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 6.601605ms)
Jun 16 16:41:38.405: INFO: (6) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 6.759767ms)
Jun 16 16:41:38.409: INFO: (7) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 3.315898ms)
Jun 16 16:41:38.409: INFO: (7) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 3.76765ms)
Jun 16 16:41:38.409: INFO: (7) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 3.765929ms)
Jun 16 16:41:38.409: INFO: (7) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 4.091284ms)
Jun 16 16:41:38.409: INFO: (7) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 4.332008ms)
Jun 16 16:41:38.410: INFO: (7) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 4.527376ms)
Jun 16 16:41:38.410: INFO: (7) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 4.477066ms)
Jun 16 16:41:38.410: INFO: (7) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 4.638536ms)
Jun 16 16:41:38.410: INFO: (7) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 4.697829ms)
Jun 16 16:41:38.410: INFO: (7) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 4.838326ms)
Jun 16 16:41:38.410: INFO: (7) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 4.841128ms)
Jun 16 16:41:38.410: INFO: (7) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 5.045608ms)
Jun 16 16:41:38.410: INFO: (7) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 5.156613ms)
Jun 16 16:41:38.410: INFO: (7) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 5.0444ms)
Jun 16 16:41:38.411: INFO: (7) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 5.458221ms)
Jun 16 16:41:38.411: INFO: (7) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 5.597978ms)
Jun 16 16:41:38.415: INFO: (8) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 4.066757ms)
Jun 16 16:41:38.415: INFO: (8) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 4.186941ms)
Jun 16 16:41:38.416: INFO: (8) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 5.192018ms)
Jun 16 16:41:38.417: INFO: (8) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 5.801286ms)
Jun 16 16:41:38.417: INFO: (8) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 5.963106ms)
Jun 16 16:41:38.417: INFO: (8) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 5.914563ms)
Jun 16 16:41:38.417: INFO: (8) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 6.050309ms)
Jun 16 16:41:38.417: INFO: (8) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 5.93386ms)
Jun 16 16:41:38.417: INFO: (8) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 6.023364ms)
Jun 16 16:41:38.417: INFO: (8) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 6.079349ms)
Jun 16 16:41:38.417: INFO: (8) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 6.171526ms)
Jun 16 16:41:38.417: INFO: (8) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 6.120875ms)
Jun 16 16:41:38.418: INFO: (8) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 6.497835ms)
Jun 16 16:41:38.418: INFO: (8) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 6.637029ms)
Jun 16 16:41:38.418: INFO: (8) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 6.703586ms)
Jun 16 16:41:38.418: INFO: (8) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 6.596157ms)
Jun 16 16:41:38.420: INFO: (9) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 2.029031ms)
Jun 16 16:41:38.420: INFO: (9) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 2.262173ms)
Jun 16 16:41:38.422: INFO: (9) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 3.818966ms)
Jun 16 16:41:38.422: INFO: (9) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 3.72612ms)
Jun 16 16:41:38.423: INFO: (9) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 4.121297ms)
Jun 16 16:41:38.423: INFO: (9) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 4.278905ms)
Jun 16 16:41:38.423: INFO: (9) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 4.235268ms)
Jun 16 16:41:38.423: INFO: (9) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 4.62507ms)
Jun 16 16:41:38.423: INFO: (9) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 4.38379ms)
Jun 16 16:41:38.423: INFO: (9) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 4.905239ms)
Jun 16 16:41:38.423: INFO: (9) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 4.845482ms)
Jun 16 16:41:38.423: INFO: (9) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 4.999448ms)
Jun 16 16:41:38.423: INFO: (9) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 4.734265ms)
Jun 16 16:41:38.424: INFO: (9) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 5.810906ms)
Jun 16 16:41:38.424: INFO: (9) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 6.224404ms)
Jun 16 16:41:38.424: INFO: (9) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 6.206152ms)
Jun 16 16:41:38.428: INFO: (10) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 3.597023ms)
Jun 16 16:41:38.428: INFO: (10) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 3.636135ms)
Jun 16 16:41:38.429: INFO: (10) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 4.370072ms)
Jun 16 16:41:38.429: INFO: (10) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 4.419381ms)
Jun 16 16:41:38.429: INFO: (10) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 4.6306ms)
Jun 16 16:41:38.429: INFO: (10) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 4.430805ms)
Jun 16 16:41:38.429: INFO: (10) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 4.651274ms)
Jun 16 16:41:38.429: INFO: (10) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 4.569265ms)
Jun 16 16:41:38.429: INFO: (10) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 4.419906ms)
Jun 16 16:41:38.429: INFO: (10) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 4.663241ms)
Jun 16 16:41:38.430: INFO: (10) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 5.206201ms)
Jun 16 16:41:38.430: INFO: (10) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 4.902295ms)
Jun 16 16:41:38.430: INFO: (10) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 5.28338ms)
Jun 16 16:41:38.430: INFO: (10) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 5.461318ms)
Jun 16 16:41:38.430: INFO: (10) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 5.385094ms)
Jun 16 16:41:38.430: INFO: (10) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 5.323946ms)
Jun 16 16:41:38.435: INFO: (11) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 4.245927ms)
Jun 16 16:41:38.435: INFO: (11) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 4.483873ms)
Jun 16 16:41:38.435: INFO: (11) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 4.340321ms)
Jun 16 16:41:38.435: INFO: (11) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 4.552446ms)
Jun 16 16:41:38.435: INFO: (11) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 4.331475ms)
Jun 16 16:41:38.435: INFO: (11) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 4.428128ms)
Jun 16 16:41:38.436: INFO: (11) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 5.338461ms)
Jun 16 16:41:38.436: INFO: (11) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 5.231223ms)
Jun 16 16:41:38.436: INFO: (11) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 5.409867ms)
Jun 16 16:41:38.436: INFO: (11) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 5.333193ms)
Jun 16 16:41:38.436: INFO: (11) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 5.373093ms)
Jun 16 16:41:38.436: INFO: (11) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 5.47618ms)
Jun 16 16:41:38.436: INFO: (11) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 5.410314ms)
Jun 16 16:41:38.436: INFO: (11) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 5.835772ms)
Jun 16 16:41:38.436: INFO: (11) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 5.69692ms)
Jun 16 16:41:38.436: INFO: (11) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 5.842525ms)
Jun 16 16:41:38.440: INFO: (12) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 4.31112ms)
Jun 16 16:41:38.441: INFO: (12) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 4.429062ms)
Jun 16 16:41:38.441: INFO: (12) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 4.743856ms)
Jun 16 16:41:38.441: INFO: (12) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 5.028532ms)
Jun 16 16:41:38.441: INFO: (12) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 5.101201ms)
Jun 16 16:41:38.441: INFO: (12) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 5.291637ms)
Jun 16 16:41:38.441: INFO: (12) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 5.361809ms)
Jun 16 16:41:38.442: INFO: (12) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 5.597611ms)
Jun 16 16:41:38.442: INFO: (12) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 5.521912ms)
Jun 16 16:41:38.442: INFO: (12) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 5.64723ms)
Jun 16 16:41:38.442: INFO: (12) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 5.903562ms)
Jun 16 16:41:38.442: INFO: (12) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 6.081892ms)
Jun 16 16:41:38.442: INFO: (12) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 6.106533ms)
Jun 16 16:41:38.442: INFO: (12) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 6.180326ms)
Jun 16 16:41:38.442: INFO: (12) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 5.987119ms)
Jun 16 16:41:38.442: INFO: (12) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 5.995783ms)
Jun 16 16:41:38.446: INFO: (13) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 3.461667ms)
Jun 16 16:41:38.446: INFO: (13) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 3.551405ms)
Jun 16 16:41:38.446: INFO: (13) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 3.724138ms)
Jun 16 16:41:38.446: INFO: (13) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 3.737057ms)
Jun 16 16:41:38.446: INFO: (13) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 3.863221ms)
Jun 16 16:41:38.446: INFO: (13) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 3.68479ms)
Jun 16 16:41:38.446: INFO: (13) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 3.907063ms)
Jun 16 16:41:38.446: INFO: (13) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 3.859473ms)
Jun 16 16:41:38.446: INFO: (13) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 3.823507ms)
Jun 16 16:41:38.447: INFO: (13) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 4.517684ms)
Jun 16 16:41:38.448: INFO: (13) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 5.322653ms)
Jun 16 16:41:38.448: INFO: (13) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 5.500956ms)
Jun 16 16:41:38.448: INFO: (13) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 5.397218ms)
Jun 16 16:41:38.448: INFO: (13) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 5.47973ms)
Jun 16 16:41:38.448: INFO: (13) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 5.491216ms)
Jun 16 16:41:38.448: INFO: (13) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 5.844186ms)
Jun 16 16:41:38.451: INFO: (14) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 2.405687ms)
Jun 16 16:41:38.452: INFO: (14) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 3.328344ms)
Jun 16 16:41:38.452: INFO: (14) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 3.336114ms)
Jun 16 16:41:38.452: INFO: (14) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 3.482976ms)
Jun 16 16:41:38.452: INFO: (14) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 3.627817ms)
Jun 16 16:41:38.452: INFO: (14) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 3.298972ms)
Jun 16 16:41:38.452: INFO: (14) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 3.527111ms)
Jun 16 16:41:38.452: INFO: (14) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 3.252917ms)
Jun 16 16:41:38.453: INFO: (14) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 4.234922ms)
Jun 16 16:41:38.453: INFO: (14) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 4.051386ms)
Jun 16 16:41:38.454: INFO: (14) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 5.211203ms)
Jun 16 16:41:38.454: INFO: (14) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 5.50218ms)
Jun 16 16:41:38.454: INFO: (14) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 5.386639ms)
Jun 16 16:41:38.454: INFO: (14) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 5.253845ms)
Jun 16 16:41:38.454: INFO: (14) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 5.599538ms)
Jun 16 16:41:38.454: INFO: (14) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 5.573357ms)
Jun 16 16:41:38.458: INFO: (15) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 3.229615ms)
Jun 16 16:41:38.458: INFO: (15) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 3.392856ms)
Jun 16 16:41:38.458: INFO: (15) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 3.76149ms)
Jun 16 16:41:38.459: INFO: (15) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 4.505904ms)
Jun 16 16:41:38.459: INFO: (15) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 4.605219ms)
Jun 16 16:41:38.459: INFO: (15) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 4.966921ms)
Jun 16 16:41:38.460: INFO: (15) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 5.084697ms)
Jun 16 16:41:38.460: INFO: (15) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 5.142447ms)
Jun 16 16:41:38.460: INFO: (15) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 5.19223ms)
Jun 16 16:41:38.460: INFO: (15) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 5.11577ms)
Jun 16 16:41:38.460: INFO: (15) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 5.22909ms)
Jun 16 16:41:38.460: INFO: (15) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 5.599028ms)
Jun 16 16:41:38.460: INFO: (15) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 5.539855ms)
Jun 16 16:41:38.460: INFO: (15) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 5.512973ms)
Jun 16 16:41:38.460: INFO: (15) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 5.655401ms)
Jun 16 16:41:38.460: INFO: (15) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 5.665455ms)
Jun 16 16:41:38.463: INFO: (16) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 2.208734ms)
Jun 16 16:41:38.463: INFO: (16) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 2.393081ms)
Jun 16 16:41:38.463: INFO: (16) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 2.641993ms)
Jun 16 16:41:38.464: INFO: (16) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 3.289484ms)
Jun 16 16:41:38.464: INFO: (16) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 3.206961ms)
Jun 16 16:41:38.464: INFO: (16) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 3.452037ms)
Jun 16 16:41:38.464: INFO: (16) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 3.516061ms)
Jun 16 16:41:38.464: INFO: (16) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 3.702934ms)
Jun 16 16:41:38.465: INFO: (16) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 4.163342ms)
Jun 16 16:41:38.466: INFO: (16) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 5.395892ms)
Jun 16 16:41:38.466: INFO: (16) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 5.582247ms)
Jun 16 16:41:38.466: INFO: (16) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 5.752841ms)
Jun 16 16:41:38.466: INFO: (16) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 5.432817ms)
Jun 16 16:41:38.466: INFO: (16) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 5.843944ms)
Jun 16 16:41:38.470: INFO: (16) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 9.269277ms)
Jun 16 16:41:38.474: INFO: (16) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 12.688722ms)
Jun 16 16:41:38.476: INFO: (17) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 2.403889ms)
Jun 16 16:41:38.481: INFO: (17) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 6.848946ms)
Jun 16 16:41:38.481: INFO: (17) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 6.953107ms)
Jun 16 16:41:38.481: INFO: (17) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 7.383983ms)
Jun 16 16:41:38.481: INFO: (17) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 7.429881ms)
Jun 16 16:41:38.481: INFO: (17) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 7.300762ms)
Jun 16 16:41:38.481: INFO: (17) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 7.560414ms)
Jun 16 16:41:38.482: INFO: (17) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 7.633238ms)
Jun 16 16:41:38.482: INFO: (17) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 7.877643ms)
Jun 16 16:41:38.482: INFO: (17) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 8.120059ms)
Jun 16 16:41:38.482: INFO: (17) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 8.38473ms)
Jun 16 16:41:38.483: INFO: (17) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 8.874336ms)
Jun 16 16:41:38.483: INFO: (17) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 8.932247ms)
Jun 16 16:41:38.483: INFO: (17) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 9.130067ms)
Jun 16 16:41:38.483: INFO: (17) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 9.032698ms)
Jun 16 16:41:38.483: INFO: (17) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 9.355652ms)
Jun 16 16:41:38.485: INFO: (18) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 2.085126ms)
Jun 16 16:41:38.487: INFO: (18) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 3.83435ms)
Jun 16 16:41:38.487: INFO: (18) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 3.962527ms)
Jun 16 16:41:38.488: INFO: (18) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 4.560104ms)
Jun 16 16:41:38.488: INFO: (18) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 4.72774ms)
Jun 16 16:41:38.488: INFO: (18) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 4.642581ms)
Jun 16 16:41:38.488: INFO: (18) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 4.705674ms)
Jun 16 16:41:38.488: INFO: (18) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 4.868563ms)
Jun 16 16:41:38.489: INFO: (18) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 5.933906ms)
Jun 16 16:41:38.489: INFO: (18) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 5.918558ms)
Jun 16 16:41:38.489: INFO: (18) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 5.887278ms)
Jun 16 16:41:38.490: INFO: (18) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 6.19498ms)
Jun 16 16:41:38.490: INFO: (18) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 6.600787ms)
Jun 16 16:41:38.490: INFO: (18) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 6.610866ms)
Jun 16 16:41:38.490: INFO: (18) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 6.367457ms)
Jun 16 16:41:38.490: INFO: (18) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 6.514378ms)
Jun 16 16:41:38.493: INFO: (19) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:462/proxy/: tls qux (200; 3.064072ms)
Jun 16 16:41:38.493: INFO: (19) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:1080/proxy/rewriteme">... (200; 3.078907ms)
Jun 16 16:41:38.494: INFO: (19) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:443/proxy/tlsrewritem... (200; 3.418926ms)
Jun 16 16:41:38.494: INFO: (19) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:160/proxy/: foo (200; 3.618433ms)
Jun 16 16:41:38.494: INFO: (19) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:162/proxy/: bar (200; 3.575915ms)
Jun 16 16:41:38.494: INFO: (19) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx/proxy/rewriteme">test</a> (200; 3.710145ms)
Jun 16 16:41:38.494: INFO: (19) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:162/proxy/: bar (200; 3.697359ms)
Jun 16 16:41:38.494: INFO: (19) /api/v1/namespaces/proxy-7698/pods/http:proxy-service-g9bcp-599gx:160/proxy/: foo (200; 3.908668ms)
Jun 16 16:41:38.495: INFO: (19) /api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/: <a href="/api/v1/namespaces/proxy-7698/pods/proxy-service-g9bcp-599gx:1080/proxy/rewriteme">test<... (200; 4.845473ms)
Jun 16 16:41:38.495: INFO: (19) /api/v1/namespaces/proxy-7698/pods/https:proxy-service-g9bcp-599gx:460/proxy/: tls baz (200; 4.813265ms)
Jun 16 16:41:38.496: INFO: (19) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname2/proxy/: bar (200; 5.966206ms)
Jun 16 16:41:38.496: INFO: (19) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname2/proxy/: tls qux (200; 6.005607ms)
Jun 16 16:41:38.496: INFO: (19) /api/v1/namespaces/proxy-7698/services/https:proxy-service-g9bcp:tlsportname1/proxy/: tls baz (200; 6.126725ms)
Jun 16 16:41:38.497: INFO: (19) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname1/proxy/: foo (200; 6.345354ms)
Jun 16 16:41:38.497: INFO: (19) /api/v1/namespaces/proxy-7698/services/http:proxy-service-g9bcp:portname1/proxy/: foo (200; 6.024547ms)
Jun 16 16:41:38.497: INFO: (19) /api/v1/namespaces/proxy-7698/services/proxy-service-g9bcp:portname2/proxy/: bar (200; 6.364732ms)
STEP: deleting ReplicationController proxy-service-g9bcp in namespace proxy-7698, will wait for the garbage collector to delete the pods
Jun 16 16:41:38.551: INFO: Deleting ReplicationController proxy-service-g9bcp took: 2.739046ms
Jun 16 16:41:38.651: INFO: Terminating ReplicationController proxy-service-g9bcp pods took: 100.159933ms
[AfterEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:41:46.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7698" for this suite.
Jun 16 16:41:52.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:41:52.907: INFO: namespace proxy-7698 deletion completed in 6.053135186s

• [SLOW TEST:21.742 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:41:52.907: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9325
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-9325
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9325 to expose endpoints map[]
Jun 16 16:41:53.033: INFO: successfully validated that service multi-endpoint-test in namespace services-9325 exposes endpoints map[] (2.3998ms elapsed)
STEP: Creating pod pod1 in namespace services-9325
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9325 to expose endpoints map[pod1:[100]]
Jun 16 16:41:55.048: INFO: successfully validated that service multi-endpoint-test in namespace services-9325 exposes endpoints map[pod1:[100]] (2.010381774s elapsed)
STEP: Creating pod pod2 in namespace services-9325
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9325 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 16 16:41:57.067: INFO: successfully validated that service multi-endpoint-test in namespace services-9325 exposes endpoints map[pod1:[100] pod2:[101]] (2.016857524s elapsed)
STEP: Deleting pod pod1 in namespace services-9325
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9325 to expose endpoints map[pod2:[101]]
Jun 16 16:41:58.077: INFO: successfully validated that service multi-endpoint-test in namespace services-9325 exposes endpoints map[pod2:[101]] (1.007256554s elapsed)
STEP: Deleting pod pod2 in namespace services-9325
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9325 to expose endpoints map[]
Jun 16 16:41:59.083: INFO: successfully validated that service multi-endpoint-test in namespace services-9325 exposes endpoints map[] (1.003387548s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:41:59.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9325" for this suite.
Jun 16 16:42:27.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:42:27.165: INFO: namespace services-9325 deletion completed in 28.059339101s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:34.258 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:42:27.165: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2344
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2344.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2344.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2344.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2344.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2344.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2344.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 16 16:42:29.303: INFO: Unable to read wheezy_udp@PodARecord from pod dns-2344/dns-test-6cce82fe-3ab9-4424-a735-b1415c1966d7: the server could not find the requested resource (get pods dns-test-6cce82fe-3ab9-4424-a735-b1415c1966d7)
Jun 16 16:42:29.305: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-2344/dns-test-6cce82fe-3ab9-4424-a735-b1415c1966d7: the server could not find the requested resource (get pods dns-test-6cce82fe-3ab9-4424-a735-b1415c1966d7)
Jun 16 16:42:29.307: INFO: Unable to read jessie_hosts@dns-querier-2.dns-test-service-2.dns-2344.svc.cluster.local from pod dns-2344/dns-test-6cce82fe-3ab9-4424-a735-b1415c1966d7: the server could not find the requested resource (get pods dns-test-6cce82fe-3ab9-4424-a735-b1415c1966d7)
Jun 16 16:42:29.308: INFO: Unable to read jessie_hosts@dns-querier-2 from pod dns-2344/dns-test-6cce82fe-3ab9-4424-a735-b1415c1966d7: the server could not find the requested resource (get pods dns-test-6cce82fe-3ab9-4424-a735-b1415c1966d7)
Jun 16 16:42:29.310: INFO: Unable to read jessie_udp@PodARecord from pod dns-2344/dns-test-6cce82fe-3ab9-4424-a735-b1415c1966d7: the server could not find the requested resource (get pods dns-test-6cce82fe-3ab9-4424-a735-b1415c1966d7)
Jun 16 16:42:29.311: INFO: Unable to read jessie_tcp@PodARecord from pod dns-2344/dns-test-6cce82fe-3ab9-4424-a735-b1415c1966d7: the server could not find the requested resource (get pods dns-test-6cce82fe-3ab9-4424-a735-b1415c1966d7)
Jun 16 16:42:29.311: INFO: Lookups using dns-2344/dns-test-6cce82fe-3ab9-4424-a735-b1415c1966d7 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_hosts@dns-querier-2.dns-test-service-2.dns-2344.svc.cluster.local jessie_hosts@dns-querier-2 jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun 16 16:42:34.324: INFO: DNS probes using dns-2344/dns-test-6cce82fe-3ab9-4424-a735-b1415c1966d7 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:42:34.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2344" for this suite.
Jun 16 16:42:40.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:42:40.397: INFO: namespace dns-2344 deletion completed in 6.053643469s

• [SLOW TEST:13.232 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:42:40.398: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4051
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:42:40.521: INFO: (0) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.937411ms)
Jun 16 16:42:40.523: INFO: (1) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.709237ms)
Jun 16 16:42:40.524: INFO: (2) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.708311ms)
Jun 16 16:42:40.526: INFO: (3) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.658248ms)
Jun 16 16:42:40.528: INFO: (4) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.638696ms)
Jun 16 16:42:40.529: INFO: (5) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.562013ms)
Jun 16 16:42:40.531: INFO: (6) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.684452ms)
Jun 16 16:42:40.533: INFO: (7) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.64794ms)
Jun 16 16:42:40.534: INFO: (8) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.592317ms)
Jun 16 16:42:40.536: INFO: (9) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.697382ms)
Jun 16 16:42:40.538: INFO: (10) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.629934ms)
Jun 16 16:42:40.539: INFO: (11) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.594334ms)
Jun 16 16:42:40.541: INFO: (12) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.447662ms)
Jun 16 16:42:40.542: INFO: (13) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.515575ms)
Jun 16 16:42:40.544: INFO: (14) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.621983ms)
Jun 16 16:42:40.546: INFO: (15) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.614682ms)
Jun 16 16:42:40.547: INFO: (16) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.659082ms)
Jun 16 16:42:40.549: INFO: (17) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.638564ms)
Jun 16 16:42:40.551: INFO: (18) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.657225ms)
Jun 16 16:42:40.552: INFO: (19) /api/v1/nodes/ip-172-19-65-122.eu-west-1.compute.internal:10250/proxy/logs/: <pre>
<a href="aws-routed-eni/">aws-routed-eni/</a>
<a href="btmp">btmp</a>
<a href="containers/"... (200; 1.543388ms)
[AfterEach] version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:42:40.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4051" for this suite.
Jun 16 16:42:46.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:42:46.607: INFO: namespace proxy-4051 deletion completed in 6.05279491s

• [SLOW TEST:6.209 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:42:46.607: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3781
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-b6eeccba-9402-4572-85f5-578838104bfe in namespace container-probe-3781
Jun 16 16:42:48.735: INFO: Started pod busybox-b6eeccba-9402-4572-85f5-578838104bfe in namespace container-probe-3781
STEP: checking the pod's current state and verifying that restartCount is present
Jun 16 16:42:48.736: INFO: Initial restart count of pod busybox-b6eeccba-9402-4572-85f5-578838104bfe is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:46:48.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3781" for this suite.
Jun 16 16:46:54.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:46:55.036: INFO: namespace container-probe-3781 deletion completed in 6.055748089s

• [SLOW TEST:248.430 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:46:55.036: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-4966
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:46:55.156: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Creating first CR 
Jun 16 16:46:55.697: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-16T16:46:55Z generation:1 name:name1 resourceVersion:27235 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:92148c8c-9438-47c4-8df3-6651d96be495] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jun 16 16:47:05.700: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-16T16:47:05Z generation:1 name:name2 resourceVersion:27259 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:eb35c12c-a390-471c-b40b-35d293412d50] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jun 16 16:47:15.703: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-16T16:46:55Z generation:2 name:name1 resourceVersion:27282 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:92148c8c-9438-47c4-8df3-6651d96be495] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jun 16 16:47:25.706: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-16T16:47:05Z generation:2 name:name2 resourceVersion:27306 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:eb35c12c-a390-471c-b40b-35d293412d50] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jun 16 16:47:35.710: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-16T16:46:55Z generation:2 name:name1 resourceVersion:27329 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:92148c8c-9438-47c4-8df3-6651d96be495] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jun 16 16:47:45.713: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-06-16T16:47:05Z generation:2 name:name2 resourceVersion:27352 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:eb35c12c-a390-471c-b40b-35d293412d50] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:47:56.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-4966" for this suite.
Jun 16 16:48:02.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:48:02.279: INFO: namespace crd-watch-4966 deletion completed in 6.055327459s

• [SLOW TEST:67.243 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:48:02.280: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7726
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Jun 16 16:48:02.400: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:48:20.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7726" for this suite.
Jun 16 16:48:26.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:48:26.368: INFO: namespace crd-publish-openapi-7726 deletion completed in 6.059212484s

• [SLOW TEST:24.088 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:48:26.368: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3994
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 16:48:26.495: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b625ae71-2967-4eba-91e6-e03cfec101e7" in namespace "projected-3994" to be "success or failure"
Jun 16 16:48:26.498: INFO: Pod "downwardapi-volume-b625ae71-2967-4eba-91e6-e03cfec101e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.877193ms
Jun 16 16:48:28.500: INFO: Pod "downwardapi-volume-b625ae71-2967-4eba-91e6-e03cfec101e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004628567s
STEP: Saw pod success
Jun 16 16:48:28.500: INFO: Pod "downwardapi-volume-b625ae71-2967-4eba-91e6-e03cfec101e7" satisfied condition "success or failure"
Jun 16 16:48:28.501: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod downwardapi-volume-b625ae71-2967-4eba-91e6-e03cfec101e7 container client-container: <nil>
STEP: delete the pod
Jun 16 16:48:28.511: INFO: Waiting for pod downwardapi-volume-b625ae71-2967-4eba-91e6-e03cfec101e7 to disappear
Jun 16 16:48:28.513: INFO: Pod downwardapi-volume-b625ae71-2967-4eba-91e6-e03cfec101e7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:48:28.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3994" for this suite.
Jun 16 16:48:34.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:48:34.572: INFO: namespace projected-3994 deletion completed in 6.057969948s

• [SLOW TEST:8.204 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:48:34.573: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8347
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:48:41.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8347" for this suite.
Jun 16 16:48:47.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:48:47.752: INFO: namespace resourcequota-8347 deletion completed in 6.053186509s

• [SLOW TEST:13.179 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:48:47.752: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1446
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:48:49.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1446" for this suite.
Jun 16 16:48:55.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:48:55.951: INFO: namespace emptydir-wrapper-1446 deletion completed in 6.054442816s

• [SLOW TEST:8.199 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:48:55.951: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5165
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 16 16:48:58.081: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:48:58.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5165" for this suite.
Jun 16 16:49:04.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:49:04.150: INFO: namespace container-runtime-5165 deletion completed in 6.059508578s

• [SLOW TEST:8.199 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:49:04.150: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7481
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 16:49:04.280: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e594c3d1-3eae-4e3f-b9e0-58856269cd48" in namespace "projected-7481" to be "success or failure"
Jun 16 16:49:04.282: INFO: Pod "downwardapi-volume-e594c3d1-3eae-4e3f-b9e0-58856269cd48": Phase="Pending", Reason="", readiness=false. Elapsed: 1.575073ms
Jun 16 16:49:06.284: INFO: Pod "downwardapi-volume-e594c3d1-3eae-4e3f-b9e0-58856269cd48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003842429s
STEP: Saw pod success
Jun 16 16:49:06.284: INFO: Pod "downwardapi-volume-e594c3d1-3eae-4e3f-b9e0-58856269cd48" satisfied condition "success or failure"
Jun 16 16:49:06.286: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod downwardapi-volume-e594c3d1-3eae-4e3f-b9e0-58856269cd48 container client-container: <nil>
STEP: delete the pod
Jun 16 16:49:06.300: INFO: Waiting for pod downwardapi-volume-e594c3d1-3eae-4e3f-b9e0-58856269cd48 to disappear
Jun 16 16:49:06.303: INFO: Pod downwardapi-volume-e594c3d1-3eae-4e3f-b9e0-58856269cd48 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:49:06.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7481" for this suite.
Jun 16 16:49:12.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:49:12.361: INFO: namespace projected-7481 deletion completed in 6.055769481s

• [SLOW TEST:8.211 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:49:12.361: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3232
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Jun 16 16:49:12.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 cluster-info'
Jun 16 16:49:12.648: INFO: stderr: ""
Jun 16 16:49:12.648: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.31.0.1:443\x1b[0m\n\x1b[0;32mcluster-autoscaler\x1b[0m is running at \x1b[0;33mhttps://172.31.0.1:443/api/v1/namespaces/kube-system/services/cluster-autoscaler:metrics/proxy\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://172.31.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:49:12.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3232" for this suite.
Jun 16 16:49:18.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:49:18.704: INFO: namespace kubectl-3232 deletion completed in 6.053367445s

• [SLOW TEST:6.343 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:49:18.704: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5172
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 16:49:18.829: INFO: Waiting up to 5m0s for pod "downwardapi-volume-faee60d6-ea07-4281-a9ee-043994b4bc51" in namespace "downward-api-5172" to be "success or failure"
Jun 16 16:49:18.830: INFO: Pod "downwardapi-volume-faee60d6-ea07-4281-a9ee-043994b4bc51": Phase="Pending", Reason="", readiness=false. Elapsed: 1.761434ms
Jun 16 16:49:20.832: INFO: Pod "downwardapi-volume-faee60d6-ea07-4281-a9ee-043994b4bc51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003700019s
STEP: Saw pod success
Jun 16 16:49:20.832: INFO: Pod "downwardapi-volume-faee60d6-ea07-4281-a9ee-043994b4bc51" satisfied condition "success or failure"
Jun 16 16:49:20.834: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod downwardapi-volume-faee60d6-ea07-4281-a9ee-043994b4bc51 container client-container: <nil>
STEP: delete the pod
Jun 16 16:49:20.843: INFO: Waiting for pod downwardapi-volume-faee60d6-ea07-4281-a9ee-043994b4bc51 to disappear
Jun 16 16:49:20.845: INFO: Pod downwardapi-volume-faee60d6-ea07-4281-a9ee-043994b4bc51 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:49:20.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5172" for this suite.
Jun 16 16:49:26.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:49:26.901: INFO: namespace downward-api-5172 deletion completed in 6.054531765s

• [SLOW TEST:8.197 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:49:26.901: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8784
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 16 16:49:27.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8784'
Jun 16 16:49:27.081: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 16 16:49:27.081: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Jun 16 16:49:29.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete deployment e2e-test-httpd-deployment --namespace=kubectl-8784'
Jun 16 16:49:29.143: INFO: stderr: ""
Jun 16 16:49:29.143: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:49:29.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8784" for this suite.
Jun 16 16:49:41.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:49:41.207: INFO: namespace kubectl-8784 deletion completed in 12.061902292s

• [SLOW TEST:14.306 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:49:41.207: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2275
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0616 16:50:11.846343      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 16 16:50:11.846: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:50:11.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2275" for this suite.
Jun 16 16:50:17.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:50:17.901: INFO: namespace gc-2275 deletion completed in 6.053579978s

• [SLOW TEST:36.694 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:50:17.902: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1688
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-3c6ff549-8e33-4e8d-bc37-d222e6a560c8
STEP: Creating secret with name s-test-opt-upd-6f3794d1-5394-46d2-8e77-8d4912c119e3
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-3c6ff549-8e33-4e8d-bc37-d222e6a560c8
STEP: Updating secret s-test-opt-upd-6f3794d1-5394-46d2-8e77-8d4912c119e3
STEP: Creating secret with name s-test-opt-create-688e23dd-4808-4310-aa75-4aa270fffcb1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:50:24.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1688" for this suite.
Jun 16 16:50:40.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:50:40.136: INFO: namespace secrets-1688 deletion completed in 16.06004563s

• [SLOW TEST:22.234 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:50:40.136: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1143
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:50:40.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1143" for this suite.
Jun 16 16:50:52.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:50:52.323: INFO: namespace pods-1143 deletion completed in 12.054677766s

• [SLOW TEST:12.187 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:50:52.323: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7457
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-07fb0308-27c7-41e6-80a4-c30bdd83af99
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-07fb0308-27c7-41e6-80a4-c30bdd83af99
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:50:56.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7457" for this suite.
Jun 16 16:51:08.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:51:08.536: INFO: namespace configmap-7457 deletion completed in 12.05314051s

• [SLOW TEST:16.213 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:51:08.537: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8288
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Jun 16 16:51:08.661: INFO: Waiting up to 5m0s for pod "client-containers-fa064275-84cb-4567-ab1d-f220ce2d9921" in namespace "containers-8288" to be "success or failure"
Jun 16 16:51:08.663: INFO: Pod "client-containers-fa064275-84cb-4567-ab1d-f220ce2d9921": Phase="Pending", Reason="", readiness=false. Elapsed: 1.763791ms
Jun 16 16:51:10.665: INFO: Pod "client-containers-fa064275-84cb-4567-ab1d-f220ce2d9921": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003558403s
STEP: Saw pod success
Jun 16 16:51:10.665: INFO: Pod "client-containers-fa064275-84cb-4567-ab1d-f220ce2d9921" satisfied condition "success or failure"
Jun 16 16:51:10.666: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod client-containers-fa064275-84cb-4567-ab1d-f220ce2d9921 container test-container: <nil>
STEP: delete the pod
Jun 16 16:51:10.676: INFO: Waiting for pod client-containers-fa064275-84cb-4567-ab1d-f220ce2d9921 to disappear
Jun 16 16:51:10.677: INFO: Pod client-containers-fa064275-84cb-4567-ab1d-f220ce2d9921 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:51:10.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8288" for this suite.
Jun 16 16:51:16.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:51:16.734: INFO: namespace containers-8288 deletion completed in 6.055149319s

• [SLOW TEST:8.197 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:51:16.734: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9164
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jun 16 16:51:56.870: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0616 16:51:56.870274      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:51:56.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9164" for this suite.
Jun 16 16:52:02.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:52:02.925: INFO: namespace gc-9164 deletion completed in 6.053731962s

• [SLOW TEST:46.191 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:52:02.926: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1251
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:52:03.049: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 16 16:52:08.051: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 16 16:52:08.051: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 16 16:52:08.059: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1251 /apis/apps/v1/namespaces/deployment-1251/deployments/test-cleanup-deployment c0d09d15-54cb-4975-be9d-fbb93468183e 28607 1 2020-06-16 16:52:08 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003c5b4a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jun 16 16:52:08.061: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jun 16 16:52:08.061: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jun 16 16:52:08.062: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1251 /apis/apps/v1/namespaces/deployment-1251/replicasets/test-cleanup-controller a8f89ba7-9e13-4e8e-9751-461b461b608c 28608 1 2020-06-16 16:52:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment c0d09d15-54cb-4975-be9d-fbb93468183e 0xc003c5b827 0xc003c5b828}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c5b888 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 16 16:52:08.063: INFO: Pod "test-cleanup-controller-45rrz" is available:
&Pod{ObjectMeta:{test-cleanup-controller-45rrz test-cleanup-controller- deployment-1251 /api/v1/namespaces/deployment-1251/pods/test-cleanup-controller-45rrz ae9b0c08-1cc5-4eb3-8b92-1cf8f1f207c7 28597 0 2020-06-16 16:52:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-controller a8f89ba7-9e13-4e8e-9751-461b461b608c 0xc003c5bb97 0xc003c5bb98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mhfbc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mhfbc,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mhfbc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-19-65-6.eu-west-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 16:52:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 16:52:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 16:52:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-06-16 16:52:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.19.65.6,PodIP:172.18.142.36,StartTime:2020-06-16 16:52:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-06-16 16:52:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://c8094902f01a222fe91b988f9e9a933067cef6186e2e6158818dc545387e4dbe,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.18.142.36,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:52:08.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1251" for this suite.
Jun 16 16:52:14.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:52:14.130: INFO: namespace deployment-1251 deletion completed in 6.064344801s

• [SLOW TEST:11.204 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:52:14.130: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1805
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-a6fc81bd-8fc5-4bac-afe1-dea9a76ddb3f in namespace container-probe-1805
Jun 16 16:52:16.271: INFO: Started pod busybox-a6fc81bd-8fc5-4bac-afe1-dea9a76ddb3f in namespace container-probe-1805
STEP: checking the pod's current state and verifying that restartCount is present
Jun 16 16:52:16.272: INFO: Initial restart count of pod busybox-a6fc81bd-8fc5-4bac-afe1-dea9a76ddb3f is 0
Jun 16 16:53:10.330: INFO: Restart count of pod container-probe-1805/busybox-a6fc81bd-8fc5-4bac-afe1-dea9a76ddb3f is now 1 (54.05767983s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:53:10.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1805" for this suite.
Jun 16 16:53:16.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:53:16.392: INFO: namespace container-probe-1805 deletion completed in 6.052438689s

• [SLOW TEST:62.262 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:53:16.392: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6194
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:53:16.530: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"402074a4-c0c2-44fe-a135-834421168148", Controller:(*bool)(0xc003bd91ae), BlockOwnerDeletion:(*bool)(0xc003bd91af)}}
Jun 16 16:53:16.533: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"fedd2d9c-765a-4169-9531-add3da8c5dde", Controller:(*bool)(0xc002d6b7b6), BlockOwnerDeletion:(*bool)(0xc002d6b7b7)}}
Jun 16 16:53:16.539: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a1f17de0-4960-4a98-bcb1-8238a56ef1cb", Controller:(*bool)(0xc0006a8d2e), BlockOwnerDeletion:(*bool)(0xc0006a8d2f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:53:21.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6194" for this suite.
Jun 16 16:53:27.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:53:27.602: INFO: namespace gc-6194 deletion completed in 6.057078836s

• [SLOW TEST:11.210 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:53:27.603: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2924
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 16 16:53:30.239: INFO: Successfully updated pod "pod-update-af610ab1-e4c5-4ad8-a55e-cac2f46bf6a8"
STEP: verifying the updated pod is in kubernetes
Jun 16 16:53:30.243: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:53:30.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2924" for this suite.
Jun 16 16:53:42.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:53:42.299: INFO: namespace pods-2924 deletion completed in 12.053874604s

• [SLOW TEST:14.696 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:53:42.299: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3644
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 16:53:42.419: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jun 16 16:53:45.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-3644 create -f -'
Jun 16 16:53:46.313: INFO: stderr: ""
Jun 16 16:53:46.313: INFO: stdout: "e2e-test-crd-publish-openapi-2472-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 16 16:53:46.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-3644 delete e2e-test-crd-publish-openapi-2472-crds test-foo'
Jun 16 16:53:46.375: INFO: stderr: ""
Jun 16 16:53:46.375: INFO: stdout: "e2e-test-crd-publish-openapi-2472-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun 16 16:53:46.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-3644 apply -f -'
Jun 16 16:53:46.508: INFO: stderr: ""
Jun 16 16:53:46.508: INFO: stdout: "e2e-test-crd-publish-openapi-2472-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 16 16:53:46.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-3644 delete e2e-test-crd-publish-openapi-2472-crds test-foo'
Jun 16 16:53:46.566: INFO: stderr: ""
Jun 16 16:53:46.566: INFO: stdout: "e2e-test-crd-publish-openapi-2472-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jun 16 16:53:46.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-3644 create -f -'
Jun 16 16:53:46.707: INFO: rc: 1
Jun 16 16:53:46.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-3644 apply -f -'
Jun 16 16:53:46.829: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jun 16 16:53:46.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-3644 create -f -'
Jun 16 16:53:46.955: INFO: rc: 1
Jun 16 16:53:46.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-3644 apply -f -'
Jun 16 16:53:47.087: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jun 16 16:53:47.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 explain e2e-test-crd-publish-openapi-2472-crds'
Jun 16 16:53:47.216: INFO: stderr: ""
Jun 16 16:53:47.216: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2472-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jun 16 16:53:47.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 explain e2e-test-crd-publish-openapi-2472-crds.metadata'
Jun 16 16:53:47.351: INFO: stderr: ""
Jun 16 16:53:47.351: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2472-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun 16 16:53:47.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 explain e2e-test-crd-publish-openapi-2472-crds.spec'
Jun 16 16:53:47.481: INFO: stderr: ""
Jun 16 16:53:47.481: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2472-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun 16 16:53:47.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 explain e2e-test-crd-publish-openapi-2472-crds.spec.bars'
Jun 16 16:53:47.613: INFO: stderr: ""
Jun 16 16:53:47.613: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2472-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jun 16 16:53:47.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 explain e2e-test-crd-publish-openapi-2472-crds.spec.bars2'
Jun 16 16:53:47.750: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:53:51.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3644" for this suite.
Jun 16 16:53:57.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:53:57.363: INFO: namespace crd-publish-openapi-3644 deletion completed in 6.052598245s

• [SLOW TEST:15.064 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:53:57.363: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4304
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Jun 16 16:53:57.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 create -f - --namespace=kubectl-4304'
Jun 16 16:53:57.681: INFO: stderr: ""
Jun 16 16:53:57.681: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 16 16:53:57.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4304'
Jun 16 16:53:57.745: INFO: stderr: ""
Jun 16 16:53:57.745: INFO: stdout: "update-demo-nautilus-gth84 update-demo-nautilus-rz24v "
Jun 16 16:53:57.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-gth84 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4304'
Jun 16 16:53:57.798: INFO: stderr: ""
Jun 16 16:53:57.798: INFO: stdout: ""
Jun 16 16:53:57.798: INFO: update-demo-nautilus-gth84 is created but not running
Jun 16 16:54:02.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4304'
Jun 16 16:54:02.868: INFO: stderr: ""
Jun 16 16:54:02.868: INFO: stdout: "update-demo-nautilus-gth84 update-demo-nautilus-rz24v "
Jun 16 16:54:02.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-gth84 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4304'
Jun 16 16:54:02.942: INFO: stderr: ""
Jun 16 16:54:02.942: INFO: stdout: "true"
Jun 16 16:54:02.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-gth84 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4304'
Jun 16 16:54:02.999: INFO: stderr: ""
Jun 16 16:54:02.999: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 16 16:54:02.999: INFO: validating pod update-demo-nautilus-gth84
Jun 16 16:54:03.002: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 16 16:54:03.002: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 16 16:54:03.002: INFO: update-demo-nautilus-gth84 is verified up and running
Jun 16 16:54:03.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-rz24v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4304'
Jun 16 16:54:03.066: INFO: stderr: ""
Jun 16 16:54:03.066: INFO: stdout: "true"
Jun 16 16:54:03.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-nautilus-rz24v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4304'
Jun 16 16:54:03.123: INFO: stderr: ""
Jun 16 16:54:03.123: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 16 16:54:03.123: INFO: validating pod update-demo-nautilus-rz24v
Jun 16 16:54:03.125: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 16 16:54:03.125: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 16 16:54:03.125: INFO: update-demo-nautilus-rz24v is verified up and running
STEP: rolling-update to new replication controller
Jun 16 16:54:03.127: INFO: scanned /root for discovery docs: <nil>
Jun 16 16:54:03.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-4304'
Jun 16 16:54:25.369: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 16 16:54:25.369: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 16 16:54:25.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4304'
Jun 16 16:54:25.424: INFO: stderr: ""
Jun 16 16:54:25.424: INFO: stdout: "update-demo-kitten-56db9 update-demo-kitten-xrm4d "
Jun 16 16:54:25.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-kitten-56db9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4304'
Jun 16 16:54:25.477: INFO: stderr: ""
Jun 16 16:54:25.477: INFO: stdout: "true"
Jun 16 16:54:25.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-kitten-56db9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4304'
Jun 16 16:54:25.529: INFO: stderr: ""
Jun 16 16:54:25.529: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 16 16:54:25.529: INFO: validating pod update-demo-kitten-56db9
Jun 16 16:54:25.531: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 16 16:54:25.531: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 16 16:54:25.531: INFO: update-demo-kitten-56db9 is verified up and running
Jun 16 16:54:25.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-kitten-xrm4d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4304'
Jun 16 16:54:25.585: INFO: stderr: ""
Jun 16 16:54:25.585: INFO: stdout: "true"
Jun 16 16:54:25.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods update-demo-kitten-xrm4d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4304'
Jun 16 16:54:25.641: INFO: stderr: ""
Jun 16 16:54:25.641: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 16 16:54:25.641: INFO: validating pod update-demo-kitten-xrm4d
Jun 16 16:54:25.643: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 16 16:54:25.643: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 16 16:54:25.643: INFO: update-demo-kitten-xrm4d is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:54:25.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4304" for this suite.
Jun 16 16:54:53.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:54:53.701: INFO: namespace kubectl-4304 deletion completed in 28.05600687s

• [SLOW TEST:56.338 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:54:53.702: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6084
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-85f21e5e-b4e5-4105-9b46-af9067370ffd
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:54:53.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6084" for this suite.
Jun 16 16:54:59.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:54:59.884: INFO: namespace secrets-6084 deletion completed in 6.059781286s

• [SLOW TEST:6.182 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:54:59.884: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3046
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jun 16 16:55:00.003: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:55:02.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3046" for this suite.
Jun 16 16:55:08.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:55:08.700: INFO: namespace init-container-3046 deletion completed in 6.053072968s

• [SLOW TEST:8.817 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:55:08.700: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-688
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 16:55:09.280: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 16 16:55:11.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727923309, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727923309, loc:(*time.Location)(0x78a2900)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63727923309, loc:(*time.Location)(0x78a2900)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63727923309, loc:(*time.Location)(0x78a2900)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 16:55:14.293: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:55:14.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-688" for this suite.
Jun 16 16:55:26.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:55:26.377: INFO: namespace webhook-688 deletion completed in 12.056308016s
STEP: Destroying namespace "webhook-688-markers" for this suite.
Jun 16 16:55:32.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:55:32.477: INFO: namespace webhook-688-markers deletion completed in 6.099698443s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.930 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:55:32.630: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 16:55:33.193: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 16:55:36.202: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:55:36.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3437" for this suite.
Jun 16 16:55:42.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:55:42.330: INFO: namespace webhook-3437 deletion completed in 6.053135295s
STEP: Destroying namespace "webhook-3437-markers" for this suite.
Jun 16 16:55:48.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:55:48.436: INFO: namespace webhook-3437-markers deletion completed in 6.105897214s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.955 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:55:48.586: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8109
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 16 16:55:48.710: INFO: Waiting up to 5m0s for pod "pod-1ef3eff4-9158-4221-bf53-d79f245b8b59" in namespace "emptydir-8109" to be "success or failure"
Jun 16 16:55:48.712: INFO: Pod "pod-1ef3eff4-9158-4221-bf53-d79f245b8b59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005979ms
Jun 16 16:55:50.714: INFO: Pod "pod-1ef3eff4-9158-4221-bf53-d79f245b8b59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004032348s
STEP: Saw pod success
Jun 16 16:55:50.714: INFO: Pod "pod-1ef3eff4-9158-4221-bf53-d79f245b8b59" satisfied condition "success or failure"
Jun 16 16:55:50.715: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-1ef3eff4-9158-4221-bf53-d79f245b8b59 container test-container: <nil>
STEP: delete the pod
Jun 16 16:55:50.724: INFO: Waiting for pod pod-1ef3eff4-9158-4221-bf53-d79f245b8b59 to disappear
Jun 16 16:55:50.726: INFO: Pod pod-1ef3eff4-9158-4221-bf53-d79f245b8b59 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:55:50.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8109" for this suite.
Jun 16 16:55:56.733: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:55:56.781: INFO: namespace emptydir-8109 deletion completed in 6.052666499s

• [SLOW TEST:8.195 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:55:56.781: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-493
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 16 16:55:56.909: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-493 /api/v1/namespaces/watch-493/configmaps/e2e-watch-test-label-changed 9a85fc09-0f0c-4d2e-a565-688335208fb8 29666 0 2020-06-16 16:55:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 16 16:55:56.909: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-493 /api/v1/namespaces/watch-493/configmaps/e2e-watch-test-label-changed 9a85fc09-0f0c-4d2e-a565-688335208fb8 29667 0 2020-06-16 16:55:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 16 16:55:56.909: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-493 /api/v1/namespaces/watch-493/configmaps/e2e-watch-test-label-changed 9a85fc09-0f0c-4d2e-a565-688335208fb8 29668 0 2020-06-16 16:55:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 16 16:56:06.920: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-493 /api/v1/namespaces/watch-493/configmaps/e2e-watch-test-label-changed 9a85fc09-0f0c-4d2e-a565-688335208fb8 29693 0 2020-06-16 16:55:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 16 16:56:06.920: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-493 /api/v1/namespaces/watch-493/configmaps/e2e-watch-test-label-changed 9a85fc09-0f0c-4d2e-a565-688335208fb8 29694 0 2020-06-16 16:55:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jun 16 16:56:06.920: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-493 /api/v1/namespaces/watch-493/configmaps/e2e-watch-test-label-changed 9a85fc09-0f0c-4d2e-a565-688335208fb8 29695 0 2020-06-16 16:55:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:56:06.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-493" for this suite.
Jun 16 16:56:12.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:56:12.973: INFO: namespace watch-493 deletion completed in 6.051462985s

• [SLOW TEST:16.192 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:56:12.973: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-988
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 16 16:56:23.132: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0616 16:56:23.132288      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:56:23.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-988" for this suite.
Jun 16 16:56:29.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:56:29.188: INFO: namespace gc-988 deletion completed in 6.054288684s

• [SLOW TEST:16.215 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:56:29.188: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-618
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 16:56:29.751: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 16:56:32.760: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jun 16 16:56:34.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 attach --namespace=webhook-618 to-be-attached-pod -i -c=container1'
Jun 16 16:56:34.839: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:56:34.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-618" for this suite.
Jun 16 16:56:46.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:56:46.899: INFO: namespace webhook-618 deletion completed in 12.054522452s
STEP: Destroying namespace "webhook-618-markers" for this suite.
Jun 16 16:56:52.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:56:53.000: INFO: namespace webhook-618-markers deletion completed in 6.101672726s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.966 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:56:53.154: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7789
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 16 16:56:53.283: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7789 /api/v1/namespaces/watch-7789/configmaps/e2e-watch-test-resource-version b62f3556-bf41-4caa-9dc5-d729bf88c178 30117 0 2020-06-16 16:56:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 16 16:56:53.283: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7789 /api/v1/namespaces/watch-7789/configmaps/e2e-watch-test-resource-version b62f3556-bf41-4caa-9dc5-d729bf88c178 30118 0 2020-06-16 16:56:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:56:53.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7789" for this suite.
Jun 16 16:56:59.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:56:59.336: INFO: namespace watch-7789 deletion completed in 6.051455089s

• [SLOW TEST:6.182 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:56:59.336: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-194
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-194
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 16 16:56:59.455: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 16 16:57:21.501: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.18.134.216 8081 | grep -v '^\s*$'] Namespace:pod-network-test-194 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:57:21.501: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:57:22.581: INFO: Found all expected endpoints: [netserver-0]
Jun 16 16:57:22.583: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.18.128.35 8081 | grep -v '^\s*$'] Namespace:pod-network-test-194 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:57:22.583: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:57:23.663: INFO: Found all expected endpoints: [netserver-1]
Jun 16 16:57:23.664: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.18.136.147 8081 | grep -v '^\s*$'] Namespace:pod-network-test-194 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:57:23.664: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:57:24.734: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:57:24.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-194" for this suite.
Jun 16 16:57:36.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:57:36.790: INFO: namespace pod-network-test-194 deletion completed in 12.053448548s

• [SLOW TEST:37.454 seconds]
[sig-network] Networking
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:57:36.790: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6069
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-6069
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 16 16:57:36.909: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 16 16:57:58.958: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.18.136.80:8080/dial?request=hostName&protocol=http&host=172.18.137.234&port=8080&tries=1'] Namespace:pod-network-test-6069 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:57:58.958: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:57:59.033: INFO: Waiting for endpoints: map[]
Jun 16 16:57:59.035: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.18.136.80:8080/dial?request=hostName&protocol=http&host=172.18.140.14&port=8080&tries=1'] Namespace:pod-network-test-6069 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:57:59.035: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:57:59.109: INFO: Waiting for endpoints: map[]
Jun 16 16:57:59.111: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.18.136.80:8080/dial?request=hostName&protocol=http&host=172.18.140.158&port=8080&tries=1'] Namespace:pod-network-test-6069 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 16 16:57:59.111: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:57:59.184: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:57:59.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6069" for this suite.
Jun 16 16:58:11.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:58:11.246: INFO: namespace pod-network-test-6069 deletion completed in 12.060625347s

• [SLOW TEST:34.457 seconds]
[sig-network] Networking
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:58:11.247: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-4113
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 16:58:11.808: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 16:58:14.818: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:58:26.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4113" for this suite.
Jun 16 16:58:32.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:58:32.929: INFO: namespace webhook-4113 deletion completed in 6.053982844s
STEP: Destroying namespace "webhook-4113-markers" for this suite.
Jun 16 16:58:38.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:58:39.034: INFO: namespace webhook-4113-markers deletion completed in 6.104518937s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.937 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:58:39.183: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4501
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Jun 16 16:58:39.303: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:58:43.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4501" for this suite.
Jun 16 16:59:11.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:59:11.275: INFO: namespace init-container-4501 deletion completed in 28.054141837s

• [SLOW TEST:32.091 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:59:11.275: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3094
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 16:59:11.397: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9aa68e97-22c0-413f-97eb-15bd57f3ba20" in namespace "downward-api-3094" to be "success or failure"
Jun 16 16:59:11.400: INFO: Pod "downwardapi-volume-9aa68e97-22c0-413f-97eb-15bd57f3ba20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.286128ms
Jun 16 16:59:13.401: INFO: Pod "downwardapi-volume-9aa68e97-22c0-413f-97eb-15bd57f3ba20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003884065s
STEP: Saw pod success
Jun 16 16:59:13.401: INFO: Pod "downwardapi-volume-9aa68e97-22c0-413f-97eb-15bd57f3ba20" satisfied condition "success or failure"
Jun 16 16:59:13.402: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod downwardapi-volume-9aa68e97-22c0-413f-97eb-15bd57f3ba20 container client-container: <nil>
STEP: delete the pod
Jun 16 16:59:13.412: INFO: Waiting for pod downwardapi-volume-9aa68e97-22c0-413f-97eb-15bd57f3ba20 to disappear
Jun 16 16:59:13.413: INFO: Pod downwardapi-volume-9aa68e97-22c0-413f-97eb-15bd57f3ba20 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:59:13.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3094" for this suite.
Jun 16 16:59:19.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:59:19.468: INFO: namespace downward-api-3094 deletion completed in 6.053284106s

• [SLOW TEST:8.194 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:59:19.469: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7518
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jun 16 16:59:19.588: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 16:59:23.151: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:59:36.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7518" for this suite.
Jun 16 16:59:42.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:59:43.031: INFO: namespace crd-publish-openapi-7518 deletion completed in 6.062769912s

• [SLOW TEST:23.563 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:59:43.031: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-7859
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:59:43.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7859" for this suite.
Jun 16 16:59:49.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 16:59:49.215: INFO: namespace tables-7859 deletion completed in 6.060101077s

• [SLOW TEST:6.184 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 16:59:49.216: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8902
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 16 16:59:53.355: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 16 16:59:53.357: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 16 16:59:55.357: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 16 16:59:55.359: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 16 16:59:57.357: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 16 16:59:57.359: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 16:59:57.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8902" for this suite.
Jun 16 17:00:09.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:00:09.421: INFO: namespace container-lifecycle-hook-8902 deletion completed in 12.054741404s

• [SLOW TEST:20.205 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:00:09.421: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5960
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 17:00:09.546: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c82dbf75-1307-4794-b662-f2f6d2730dbf" in namespace "projected-5960" to be "success or failure"
Jun 16 17:00:09.548: INFO: Pod "downwardapi-volume-c82dbf75-1307-4794-b662-f2f6d2730dbf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.799229ms
Jun 16 17:00:11.550: INFO: Pod "downwardapi-volume-c82dbf75-1307-4794-b662-f2f6d2730dbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003938149s
STEP: Saw pod success
Jun 16 17:00:11.550: INFO: Pod "downwardapi-volume-c82dbf75-1307-4794-b662-f2f6d2730dbf" satisfied condition "success or failure"
Jun 16 17:00:11.552: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod downwardapi-volume-c82dbf75-1307-4794-b662-f2f6d2730dbf container client-container: <nil>
STEP: delete the pod
Jun 16 17:00:11.561: INFO: Waiting for pod downwardapi-volume-c82dbf75-1307-4794-b662-f2f6d2730dbf to disappear
Jun 16 17:00:11.563: INFO: Pod downwardapi-volume-c82dbf75-1307-4794-b662-f2f6d2730dbf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:00:11.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5960" for this suite.
Jun 16 17:00:17.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:00:17.623: INFO: namespace projected-5960 deletion completed in 6.057536173s

• [SLOW TEST:8.202 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:00:17.623: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2727
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 16 17:00:17.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2727'
Jun 16 17:00:17.806: INFO: stderr: ""
Jun 16 17:00:17.806: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Jun 16 17:00:17.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete pods e2e-test-httpd-pod --namespace=kubectl-2727'
Jun 16 17:00:26.812: INFO: stderr: ""
Jun 16 17:00:26.812: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:00:26.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2727" for this suite.
Jun 16 17:00:32.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:00:32.868: INFO: namespace kubectl-2727 deletion completed in 6.054237191s

• [SLOW TEST:15.245 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:00:32.868: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-334
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Jun 16 17:00:34.997: INFO: Pod pod-hostip-02be9d38-1389-4256-9ed8-f529d6bf1ffa has hostIP: 172.19.65.6
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:00:34.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-334" for this suite.
Jun 16 17:00:47.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:00:47.053: INFO: namespace pods-334 deletion completed in 12.054492146s

• [SLOW TEST:14.185 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:00:47.053: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 17:00:47.173: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun 16 17:00:49.186: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:00:50.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5999" for this suite.
Jun 16 17:00:56.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:00:56.251: INFO: namespace replication-controller-5999 deletion completed in 6.057072354s

• [SLOW TEST:9.198 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:00:56.252: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6886
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-0390265b-86ce-4038-8273-b2aa7d7779a9
STEP: Creating a pod to test consume secrets
Jun 16 17:00:56.378: INFO: Waiting up to 5m0s for pod "pod-secrets-731565e5-be93-4344-a9e5-49cd86f58b6c" in namespace "secrets-6886" to be "success or failure"
Jun 16 17:00:56.380: INFO: Pod "pod-secrets-731565e5-be93-4344-a9e5-49cd86f58b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.819096ms
Jun 16 17:00:58.382: INFO: Pod "pod-secrets-731565e5-be93-4344-a9e5-49cd86f58b6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003851368s
STEP: Saw pod success
Jun 16 17:00:58.382: INFO: Pod "pod-secrets-731565e5-be93-4344-a9e5-49cd86f58b6c" satisfied condition "success or failure"
Jun 16 17:00:58.383: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-secrets-731565e5-be93-4344-a9e5-49cd86f58b6c container secret-volume-test: <nil>
STEP: delete the pod
Jun 16 17:00:58.393: INFO: Waiting for pod pod-secrets-731565e5-be93-4344-a9e5-49cd86f58b6c to disappear
Jun 16 17:00:58.394: INFO: Pod pod-secrets-731565e5-be93-4344-a9e5-49cd86f58b6c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:00:58.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6886" for this suite.
Jun 16 17:01:04.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:01:04.448: INFO: namespace secrets-6886 deletion completed in 6.052113982s

• [SLOW TEST:8.196 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:01:04.448: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3225
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 16 17:01:04.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-3225'
Jun 16 17:01:04.629: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 16 17:01:04.629: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Jun 16 17:01:04.633: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Jun 16 17:01:04.635: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jun 16 17:01:04.640: INFO: scanned /root for discovery docs: <nil>
Jun 16 17:01:04.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-3225'
Jun 16 17:01:20.346: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 16 17:01:20.346: INFO: stdout: "Created e2e-test-httpd-rc-ce16c7e1b2e7a2445f6027318dcb277e\nScaling up e2e-test-httpd-rc-ce16c7e1b2e7a2445f6027318dcb277e from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-ce16c7e1b2e7a2445f6027318dcb277e up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-ce16c7e1b2e7a2445f6027318dcb277e to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Jun 16 17:01:20.346: INFO: stdout: "Created e2e-test-httpd-rc-ce16c7e1b2e7a2445f6027318dcb277e\nScaling up e2e-test-httpd-rc-ce16c7e1b2e7a2445f6027318dcb277e from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-ce16c7e1b2e7a2445f6027318dcb277e up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-ce16c7e1b2e7a2445f6027318dcb277e to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Jun 16 17:01:20.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-3225'
Jun 16 17:01:20.402: INFO: stderr: ""
Jun 16 17:01:20.402: INFO: stdout: "e2e-test-httpd-rc-ce16c7e1b2e7a2445f6027318dcb277e-v47nw "
Jun 16 17:01:20.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods e2e-test-httpd-rc-ce16c7e1b2e7a2445f6027318dcb277e-v47nw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3225'
Jun 16 17:01:20.458: INFO: stderr: ""
Jun 16 17:01:20.458: INFO: stdout: "true"
Jun 16 17:01:20.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pods e2e-test-httpd-rc-ce16c7e1b2e7a2445f6027318dcb277e-v47nw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3225'
Jun 16 17:01:20.511: INFO: stderr: ""
Jun 16 17:01:20.511: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Jun 16 17:01:20.511: INFO: e2e-test-httpd-rc-ce16c7e1b2e7a2445f6027318dcb277e-v47nw is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Jun 16 17:01:20.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete rc e2e-test-httpd-rc --namespace=kubectl-3225'
Jun 16 17:01:20.566: INFO: stderr: ""
Jun 16 17:01:20.566: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:01:20.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3225" for this suite.
Jun 16 17:01:32.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:01:32.633: INFO: namespace kubectl-3225 deletion completed in 12.065139067s

• [SLOW TEST:28.185 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:01:32.633: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9094
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 16 17:01:32.761: INFO: Waiting up to 5m0s for pod "pod-ae9109cc-6d52-4fc4-a18e-3f1997b90d12" in namespace "emptydir-9094" to be "success or failure"
Jun 16 17:01:32.764: INFO: Pod "pod-ae9109cc-6d52-4fc4-a18e-3f1997b90d12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.900529ms
Jun 16 17:01:34.766: INFO: Pod "pod-ae9109cc-6d52-4fc4-a18e-3f1997b90d12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004768364s
STEP: Saw pod success
Jun 16 17:01:34.766: INFO: Pod "pod-ae9109cc-6d52-4fc4-a18e-3f1997b90d12" satisfied condition "success or failure"
Jun 16 17:01:34.767: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-ae9109cc-6d52-4fc4-a18e-3f1997b90d12 container test-container: <nil>
STEP: delete the pod
Jun 16 17:01:34.780: INFO: Waiting for pod pod-ae9109cc-6d52-4fc4-a18e-3f1997b90d12 to disappear
Jun 16 17:01:34.788: INFO: Pod pod-ae9109cc-6d52-4fc4-a18e-3f1997b90d12 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:01:34.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9094" for this suite.
Jun 16 17:01:40.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:01:40.844: INFO: namespace emptydir-9094 deletion completed in 6.054198404s

• [SLOW TEST:8.211 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:01:40.845: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-549
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jun 16 17:01:41.381: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jun 16 17:01:44.389: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 17:01:44.391: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2464-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:01:45.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-549" for this suite.
Jun 16 17:01:51.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:01:51.514: INFO: namespace webhook-549 deletion completed in 6.054043353s
STEP: Destroying namespace "webhook-549-markers" for this suite.
Jun 16 17:01:57.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:01:57.619: INFO: namespace webhook-549-markers deletion completed in 6.104987103s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.924 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:01:57.769: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2818
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 17:01:57.903: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:01:59.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2818" for this suite.
Jun 16 17:02:44.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:02:44.059: INFO: namespace pods-2818 deletion completed in 44.056517684s

• [SLOW TEST:46.290 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:02:44.059: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7350
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Jun 16 17:02:44.184: INFO: Waiting up to 5m0s for pod "client-containers-a8ea68c1-05fa-4c37-9efe-280da34947db" in namespace "containers-7350" to be "success or failure"
Jun 16 17:02:44.185: INFO: Pod "client-containers-a8ea68c1-05fa-4c37-9efe-280da34947db": Phase="Pending", Reason="", readiness=false. Elapsed: 1.341714ms
Jun 16 17:02:46.187: INFO: Pod "client-containers-a8ea68c1-05fa-4c37-9efe-280da34947db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.002854125s
STEP: Saw pod success
Jun 16 17:02:46.187: INFO: Pod "client-containers-a8ea68c1-05fa-4c37-9efe-280da34947db" satisfied condition "success or failure"
Jun 16 17:02:46.188: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod client-containers-a8ea68c1-05fa-4c37-9efe-280da34947db container test-container: <nil>
STEP: delete the pod
Jun 16 17:02:46.197: INFO: Waiting for pod client-containers-a8ea68c1-05fa-4c37-9efe-280da34947db to disappear
Jun 16 17:02:46.198: INFO: Pod client-containers-a8ea68c1-05fa-4c37-9efe-280da34947db no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:02:46.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7350" for this suite.
Jun 16 17:02:52.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:02:52.254: INFO: namespace containers-7350 deletion completed in 6.054073711s

• [SLOW TEST:8.195 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:02:52.254: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7608
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Jun 16 17:02:52.375: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-790258633 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:02:52.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7608" for this suite.
Jun 16 17:02:58.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:02:58.480: INFO: namespace kubectl-7608 deletion completed in 6.055336559s

• [SLOW TEST:6.226 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:02:58.481: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4787
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-4787/configmap-test-4cf329c2-ce11-4254-89ea-f259eca7254e
STEP: Creating a pod to test consume configMaps
Jun 16 17:02:58.606: INFO: Waiting up to 5m0s for pod "pod-configmaps-04995aab-4bcf-444b-9d97-c056db897b20" in namespace "configmap-4787" to be "success or failure"
Jun 16 17:02:58.612: INFO: Pod "pod-configmaps-04995aab-4bcf-444b-9d97-c056db897b20": Phase="Pending", Reason="", readiness=false. Elapsed: 6.646323ms
Jun 16 17:03:00.614: INFO: Pod "pod-configmaps-04995aab-4bcf-444b-9d97-c056db897b20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008479709s
STEP: Saw pod success
Jun 16 17:03:00.614: INFO: Pod "pod-configmaps-04995aab-4bcf-444b-9d97-c056db897b20" satisfied condition "success or failure"
Jun 16 17:03:00.615: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-configmaps-04995aab-4bcf-444b-9d97-c056db897b20 container env-test: <nil>
STEP: delete the pod
Jun 16 17:03:00.625: INFO: Waiting for pod pod-configmaps-04995aab-4bcf-444b-9d97-c056db897b20 to disappear
Jun 16 17:03:00.627: INFO: Pod pod-configmaps-04995aab-4bcf-444b-9d97-c056db897b20 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:03:00.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4787" for this suite.
Jun 16 17:03:06.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:03:06.691: INFO: namespace configmap-4787 deletion completed in 6.062937357s

• [SLOW TEST:8.210 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:03:06.692: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2580
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 16 17:03:06.834: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2580 /api/v1/namespaces/watch-2580/configmaps/e2e-watch-test-watch-closed ffec10a9-0535-42cd-b16c-7caa071df3e7 31851 0 2020-06-16 17:03:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 16 17:03:06.834: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2580 /api/v1/namespaces/watch-2580/configmaps/e2e-watch-test-watch-closed ffec10a9-0535-42cd-b16c-7caa071df3e7 31852 0 2020-06-16 17:03:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 16 17:03:06.839: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2580 /api/v1/namespaces/watch-2580/configmaps/e2e-watch-test-watch-closed ffec10a9-0535-42cd-b16c-7caa071df3e7 31853 0 2020-06-16 17:03:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 16 17:03:06.839: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2580 /api/v1/namespaces/watch-2580/configmaps/e2e-watch-test-watch-closed ffec10a9-0535-42cd-b16c-7caa071df3e7 31854 0 2020-06-16 17:03:06 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:03:06.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2580" for this suite.
Jun 16 17:03:12.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:03:12.894: INFO: namespace watch-2580 deletion completed in 6.053157872s

• [SLOW TEST:6.203 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:03:12.894: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-596
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-596
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-596
STEP: Creating statefulset with conflicting port in namespace statefulset-596
STEP: Waiting until pod test-pod will start running in namespace statefulset-596
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-596
Jun 16 17:03:17.029: INFO: Observed stateful pod in namespace: statefulset-596, name: ss-0, uid: 1f5ec923-ffad-4f09-a868-602a9ff4ae7f, status phase: Pending. Waiting for statefulset controller to delete.
Jun 16 17:03:17.229: INFO: Observed stateful pod in namespace: statefulset-596, name: ss-0, uid: 1f5ec923-ffad-4f09-a868-602a9ff4ae7f, status phase: Failed. Waiting for statefulset controller to delete.
Jun 16 17:03:17.233: INFO: Observed stateful pod in namespace: statefulset-596, name: ss-0, uid: 1f5ec923-ffad-4f09-a868-602a9ff4ae7f, status phase: Failed. Waiting for statefulset controller to delete.
Jun 16 17:03:17.236: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-596
STEP: Removing pod with conflicting port in namespace statefulset-596
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-596 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun 16 17:03:21.248: INFO: Deleting all statefulset in ns statefulset-596
Jun 16 17:03:21.249: INFO: Scaling statefulset ss to 0
Jun 16 17:03:31.257: INFO: Waiting for statefulset status.replicas updated to 0
Jun 16 17:03:31.258: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:03:31.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-596" for this suite.
Jun 16 17:03:37.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:03:37.320: INFO: namespace statefulset-596 deletion completed in 6.053903781s

• [SLOW TEST:24.426 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:03:37.320: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-449
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-449.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-449.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 16 17:03:39.451: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-449/dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e: the server could not find the requested resource (get pods dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e)
Jun 16 17:03:39.453: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-449/dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e: the server could not find the requested resource (get pods dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e)
Jun 16 17:03:39.454: INFO: Unable to read wheezy_udp@PodARecord from pod dns-449/dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e: the server could not find the requested resource (get pods dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e)
Jun 16 17:03:39.456: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-449/dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e: the server could not find the requested resource (get pods dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e)
Jun 16 17:03:39.457: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod dns-449/dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e: the server could not find the requested resource (get pods dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e)
Jun 16 17:03:39.459: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-449/dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e: the server could not find the requested resource (get pods dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e)
Jun 16 17:03:39.460: INFO: Unable to read jessie_udp@PodARecord from pod dns-449/dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e: the server could not find the requested resource (get pods dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e)
Jun 16 17:03:39.462: INFO: Unable to read jessie_tcp@PodARecord from pod dns-449/dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e: the server could not find the requested resource (get pods dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e)
Jun 16 17:03:39.462: INFO: Lookups using dns-449/dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun 16 17:03:44.475: INFO: DNS probes using dns-449/dns-test-751ec533-13fc-4d60-b689-9bbacdc4396e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:03:44.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-449" for this suite.
Jun 16 17:03:50.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:03:50.541: INFO: namespace dns-449 deletion completed in 6.051776479s

• [SLOW TEST:13.220 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:03:50.541: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-ed4ec7ae-ea3b-408d-9c06-ca9c17a009ae
STEP: Creating a pod to test consume configMaps
Jun 16 17:03:50.666: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3b026c19-a8c2-403c-b2b9-edb52b4320eb" in namespace "projected-4507" to be "success or failure"
Jun 16 17:03:50.668: INFO: Pod "pod-projected-configmaps-3b026c19-a8c2-403c-b2b9-edb52b4320eb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.999337ms
Jun 16 17:03:52.670: INFO: Pod "pod-projected-configmaps-3b026c19-a8c2-403c-b2b9-edb52b4320eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004093832s
STEP: Saw pod success
Jun 16 17:03:52.670: INFO: Pod "pod-projected-configmaps-3b026c19-a8c2-403c-b2b9-edb52b4320eb" satisfied condition "success or failure"
Jun 16 17:03:52.671: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-projected-configmaps-3b026c19-a8c2-403c-b2b9-edb52b4320eb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 16 17:03:52.682: INFO: Waiting for pod pod-projected-configmaps-3b026c19-a8c2-403c-b2b9-edb52b4320eb to disappear
Jun 16 17:03:52.683: INFO: Pod pod-projected-configmaps-3b026c19-a8c2-403c-b2b9-edb52b4320eb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:03:52.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4507" for this suite.
Jun 16 17:03:58.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:03:58.738: INFO: namespace projected-4507 deletion completed in 6.052744419s

• [SLOW TEST:8.197 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:03:58.738: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9467
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-d89f4d30-c478-4faa-bb87-ab437ff8a60d
STEP: Creating a pod to test consume configMaps
Jun 16 17:03:58.862: INFO: Waiting up to 5m0s for pod "pod-configmaps-bcd06ea2-b456-4cb0-aa4c-de6865405b14" in namespace "configmap-9467" to be "success or failure"
Jun 16 17:03:58.865: INFO: Pod "pod-configmaps-bcd06ea2-b456-4cb0-aa4c-de6865405b14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.202447ms
Jun 16 17:04:00.866: INFO: Pod "pod-configmaps-bcd06ea2-b456-4cb0-aa4c-de6865405b14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004090694s
STEP: Saw pod success
Jun 16 17:04:00.866: INFO: Pod "pod-configmaps-bcd06ea2-b456-4cb0-aa4c-de6865405b14" satisfied condition "success or failure"
Jun 16 17:04:00.868: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-configmaps-bcd06ea2-b456-4cb0-aa4c-de6865405b14 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 16 17:04:00.877: INFO: Waiting for pod pod-configmaps-bcd06ea2-b456-4cb0-aa4c-de6865405b14 to disappear
Jun 16 17:04:00.879: INFO: Pod pod-configmaps-bcd06ea2-b456-4cb0-aa4c-de6865405b14 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:04:00.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9467" for this suite.
Jun 16 17:04:06.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:04:06.969: INFO: namespace configmap-9467 deletion completed in 6.087939226s

• [SLOW TEST:8.231 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:04:06.969: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7625
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:04:23.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7625" for this suite.
Jun 16 17:04:29.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:04:29.198: INFO: namespace resourcequota-7625 deletion completed in 6.052284347s

• [SLOW TEST:22.229 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:04:29.199: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-2292
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 17:04:29.323: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-c4f31c85-a53a-4576-9ebc-453aba19e93a" in namespace "security-context-test-2292" to be "success or failure"
Jun 16 17:04:29.325: INFO: Pod "alpine-nnp-false-c4f31c85-a53a-4576-9ebc-453aba19e93a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.515696ms
Jun 16 17:04:31.327: INFO: Pod "alpine-nnp-false-c4f31c85-a53a-4576-9ebc-453aba19e93a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003164078s
Jun 16 17:04:33.329: INFO: Pod "alpine-nnp-false-c4f31c85-a53a-4576-9ebc-453aba19e93a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005203993s
Jun 16 17:04:33.329: INFO: Pod "alpine-nnp-false-c4f31c85-a53a-4576-9ebc-453aba19e93a" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:04:33.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2292" for this suite.
Jun 16 17:04:39.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:04:39.388: INFO: namespace security-context-test-2292 deletion completed in 6.05279928s

• [SLOW TEST:10.189 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:04:39.388: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1906
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 16 17:05:15.534: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 16 17:05:15.536: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 16 17:05:17.536: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 16 17:05:17.538: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 16 17:05:19.536: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 16 17:05:19.538: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:05:19.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1906" for this suite.
Jun 16 17:05:47.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:05:47.597: INFO: namespace container-lifecycle-hook-1906 deletion completed in 28.057065989s

• [SLOW TEST:68.209 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:05:47.597: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-ef562179-802d-4f5c-8e8c-6a9bc742004d
STEP: Creating a pod to test consume configMaps
Jun 16 17:05:47.727: INFO: Waiting up to 5m0s for pod "pod-configmaps-460d2da4-ff96-425a-b32b-ab0945390895" in namespace "configmap-7974" to be "success or failure"
Jun 16 17:05:47.731: INFO: Pod "pod-configmaps-460d2da4-ff96-425a-b32b-ab0945390895": Phase="Pending", Reason="", readiness=false. Elapsed: 3.788949ms
Jun 16 17:05:49.733: INFO: Pod "pod-configmaps-460d2da4-ff96-425a-b32b-ab0945390895": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005685169s
STEP: Saw pod success
Jun 16 17:05:49.733: INFO: Pod "pod-configmaps-460d2da4-ff96-425a-b32b-ab0945390895" satisfied condition "success or failure"
Jun 16 17:05:49.734: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-configmaps-460d2da4-ff96-425a-b32b-ab0945390895 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 16 17:05:49.744: INFO: Waiting for pod pod-configmaps-460d2da4-ff96-425a-b32b-ab0945390895 to disappear
Jun 16 17:05:49.745: INFO: Pod pod-configmaps-460d2da4-ff96-425a-b32b-ab0945390895 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:05:49.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7974" for this suite.
Jun 16 17:05:55.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:05:55.799: INFO: namespace configmap-7974 deletion completed in 6.051939794s

• [SLOW TEST:8.202 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:05:55.799: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4462
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 17:05:55.923: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e41d71f8-80f6-43aa-bd45-cc76673bf17f" in namespace "projected-4462" to be "success or failure"
Jun 16 17:05:55.925: INFO: Pod "downwardapi-volume-e41d71f8-80f6-43aa-bd45-cc76673bf17f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.538402ms
Jun 16 17:05:57.927: INFO: Pod "downwardapi-volume-e41d71f8-80f6-43aa-bd45-cc76673bf17f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003730127s
STEP: Saw pod success
Jun 16 17:05:57.927: INFO: Pod "downwardapi-volume-e41d71f8-80f6-43aa-bd45-cc76673bf17f" satisfied condition "success or failure"
Jun 16 17:05:57.928: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod downwardapi-volume-e41d71f8-80f6-43aa-bd45-cc76673bf17f container client-container: <nil>
STEP: delete the pod
Jun 16 17:05:57.939: INFO: Waiting for pod downwardapi-volume-e41d71f8-80f6-43aa-bd45-cc76673bf17f to disappear
Jun 16 17:05:57.941: INFO: Pod downwardapi-volume-e41d71f8-80f6-43aa-bd45-cc76673bf17f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:05:57.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4462" for this suite.
Jun 16 17:06:03.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:06:03.997: INFO: namespace projected-4462 deletion completed in 6.054287617s

• [SLOW TEST:8.198 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:06:03.997: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-501
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 16 17:06:04.123: INFO: Waiting up to 5m0s for pod "pod-72d50952-038b-44fe-b59f-e94ecbdca81a" in namespace "emptydir-501" to be "success or failure"
Jun 16 17:06:04.125: INFO: Pod "pod-72d50952-038b-44fe-b59f-e94ecbdca81a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.692817ms
Jun 16 17:06:06.127: INFO: Pod "pod-72d50952-038b-44fe-b59f-e94ecbdca81a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003634075s
STEP: Saw pod success
Jun 16 17:06:06.127: INFO: Pod "pod-72d50952-038b-44fe-b59f-e94ecbdca81a" satisfied condition "success or failure"
Jun 16 17:06:06.128: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-72d50952-038b-44fe-b59f-e94ecbdca81a container test-container: <nil>
STEP: delete the pod
Jun 16 17:06:06.138: INFO: Waiting for pod pod-72d50952-038b-44fe-b59f-e94ecbdca81a to disappear
Jun 16 17:06:06.139: INFO: Pod pod-72d50952-038b-44fe-b59f-e94ecbdca81a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:06:06.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-501" for this suite.
Jun 16 17:06:12.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:06:12.193: INFO: namespace emptydir-501 deletion completed in 6.051787181s

• [SLOW TEST:8.196 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:06:12.193: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7550
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:06:12.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7550" for this suite.
Jun 16 17:06:18.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:06:18.377: INFO: namespace resourcequota-7550 deletion completed in 6.052347158s

• [SLOW TEST:6.184 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:06:18.377: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4390
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-f02ab1e4-fd82-4d7d-b2a0-3cb7389e99bc
Jun 16 17:06:18.499: INFO: Pod name my-hostname-basic-f02ab1e4-fd82-4d7d-b2a0-3cb7389e99bc: Found 0 pods out of 1
Jun 16 17:06:23.502: INFO: Pod name my-hostname-basic-f02ab1e4-fd82-4d7d-b2a0-3cb7389e99bc: Found 1 pods out of 1
Jun 16 17:06:23.502: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f02ab1e4-fd82-4d7d-b2a0-3cb7389e99bc" are running
Jun 16 17:06:23.503: INFO: Pod "my-hostname-basic-f02ab1e4-fd82-4d7d-b2a0-3cb7389e99bc-gpz7f" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-16 17:06:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-16 17:06:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-16 17:06:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-06-16 17:06:18 +0000 UTC Reason: Message:}])
Jun 16 17:06:23.503: INFO: Trying to dial the pod
Jun 16 17:06:28.509: INFO: Controller my-hostname-basic-f02ab1e4-fd82-4d7d-b2a0-3cb7389e99bc: Got expected result from replica 1 [my-hostname-basic-f02ab1e4-fd82-4d7d-b2a0-3cb7389e99bc-gpz7f]: "my-hostname-basic-f02ab1e4-fd82-4d7d-b2a0-3cb7389e99bc-gpz7f", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:06:28.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4390" for this suite.
Jun 16 17:06:34.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:06:34.571: INFO: namespace replication-controller-4390 deletion completed in 6.058604047s

• [SLOW TEST:16.194 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:06:34.571: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6381
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 16 17:06:34.694: INFO: Waiting up to 5m0s for pod "pod-96c15c6a-9f06-4e45-98ef-1cdc42c27050" in namespace "emptydir-6381" to be "success or failure"
Jun 16 17:06:34.696: INFO: Pod "pod-96c15c6a-9f06-4e45-98ef-1cdc42c27050": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008612ms
Jun 16 17:06:36.698: INFO: Pod "pod-96c15c6a-9f06-4e45-98ef-1cdc42c27050": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00371384s
STEP: Saw pod success
Jun 16 17:06:36.698: INFO: Pod "pod-96c15c6a-9f06-4e45-98ef-1cdc42c27050" satisfied condition "success or failure"
Jun 16 17:06:36.699: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-96c15c6a-9f06-4e45-98ef-1cdc42c27050 container test-container: <nil>
STEP: delete the pod
Jun 16 17:06:36.709: INFO: Waiting for pod pod-96c15c6a-9f06-4e45-98ef-1cdc42c27050 to disappear
Jun 16 17:06:36.710: INFO: Pod pod-96c15c6a-9f06-4e45-98ef-1cdc42c27050 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:06:36.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6381" for this suite.
Jun 16 17:06:42.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:06:42.767: INFO: namespace emptydir-6381 deletion completed in 6.054748051s

• [SLOW TEST:8.195 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:06:42.767: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4212
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-3a92cd90-8794-48e9-b3f8-418e648bd2a1
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:06:42.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4212" for this suite.
Jun 16 17:06:48.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:06:48.942: INFO: namespace configmap-4212 deletion completed in 6.053593105s

• [SLOW TEST:6.176 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:06:48.943: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6861
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Jun 16 17:06:49.068: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c5a1e64e-1595-42c4-9da7-873e7e0f9373" in namespace "projected-6861" to be "success or failure"
Jun 16 17:06:49.070: INFO: Pod "downwardapi-volume-c5a1e64e-1595-42c4-9da7-873e7e0f9373": Phase="Pending", Reason="", readiness=false. Elapsed: 1.869869ms
Jun 16 17:06:51.072: INFO: Pod "downwardapi-volume-c5a1e64e-1595-42c4-9da7-873e7e0f9373": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003876694s
STEP: Saw pod success
Jun 16 17:06:51.072: INFO: Pod "downwardapi-volume-c5a1e64e-1595-42c4-9da7-873e7e0f9373" satisfied condition "success or failure"
Jun 16 17:06:51.076: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod downwardapi-volume-c5a1e64e-1595-42c4-9da7-873e7e0f9373 container client-container: <nil>
STEP: delete the pod
Jun 16 17:06:51.095: INFO: Waiting for pod downwardapi-volume-c5a1e64e-1595-42c4-9da7-873e7e0f9373 to disappear
Jun 16 17:06:51.097: INFO: Pod downwardapi-volume-c5a1e64e-1595-42c4-9da7-873e7e0f9373 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:06:51.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6861" for this suite.
Jun 16 17:06:57.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:06:57.152: INFO: namespace projected-6861 deletion completed in 6.053118259s

• [SLOW TEST:8.210 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:06:57.152: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:07:10.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8382" for this suite.
Jun 16 17:07:16.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:07:16.364: INFO: namespace resourcequota-8382 deletion completed in 6.05814437s

• [SLOW TEST:19.211 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:07:16.364: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3950
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-8d4b4ea8-72f4-4f33-8e29-bb950f1c5a50
STEP: Creating a pod to test consume secrets
Jun 16 17:07:16.493: INFO: Waiting up to 5m0s for pod "pod-secrets-62f7de23-8bcd-4d71-961a-8c1dabe5aa22" in namespace "secrets-3950" to be "success or failure"
Jun 16 17:07:16.495: INFO: Pod "pod-secrets-62f7de23-8bcd-4d71-961a-8c1dabe5aa22": Phase="Pending", Reason="", readiness=false. Elapsed: 1.883055ms
Jun 16 17:07:18.497: INFO: Pod "pod-secrets-62f7de23-8bcd-4d71-961a-8c1dabe5aa22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003976847s
STEP: Saw pod success
Jun 16 17:07:18.497: INFO: Pod "pod-secrets-62f7de23-8bcd-4d71-961a-8c1dabe5aa22" satisfied condition "success or failure"
Jun 16 17:07:18.498: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-secrets-62f7de23-8bcd-4d71-961a-8c1dabe5aa22 container secret-volume-test: <nil>
STEP: delete the pod
Jun 16 17:07:18.509: INFO: Waiting for pod pod-secrets-62f7de23-8bcd-4d71-961a-8c1dabe5aa22 to disappear
Jun 16 17:07:18.511: INFO: Pod pod-secrets-62f7de23-8bcd-4d71-961a-8c1dabe5aa22 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:07:18.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3950" for this suite.
Jun 16 17:07:24.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:07:24.567: INFO: namespace secrets-3950 deletion completed in 6.053982532s

• [SLOW TEST:8.203 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:07:24.567: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1167
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Jun 16 17:07:24.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=kubectl-1167 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jun 16 17:07:25.709: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jun 16 17:07:25.709: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:07:27.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1167" for this suite.
Jun 16 17:07:33.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:07:33.768: INFO: namespace kubectl-1167 deletion completed in 6.053953569s

• [SLOW TEST:9.201 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:07:33.768: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6522
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Jun 16 17:07:33.888: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jun 16 17:07:33.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 create -f - --namespace=kubectl-6522'
Jun 16 17:07:34.092: INFO: stderr: ""
Jun 16 17:07:34.092: INFO: stdout: "service/redis-slave created\n"
Jun 16 17:07:34.093: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jun 16 17:07:34.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 create -f - --namespace=kubectl-6522'
Jun 16 17:07:34.244: INFO: stderr: ""
Jun 16 17:07:34.244: INFO: stdout: "service/redis-master created\n"
Jun 16 17:07:34.245: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 16 17:07:34.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 create -f - --namespace=kubectl-6522'
Jun 16 17:07:34.392: INFO: stderr: ""
Jun 16 17:07:34.392: INFO: stdout: "service/frontend created\n"
Jun 16 17:07:34.392: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jun 16 17:07:34.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 create -f - --namespace=kubectl-6522'
Jun 16 17:07:34.540: INFO: stderr: ""
Jun 16 17:07:34.540: INFO: stdout: "deployment.apps/frontend created\n"
Jun 16 17:07:34.540: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 16 17:07:34.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 create -f - --namespace=kubectl-6522'
Jun 16 17:07:34.674: INFO: stderr: ""
Jun 16 17:07:34.674: INFO: stdout: "deployment.apps/redis-master created\n"
Jun 16 17:07:34.674: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jun 16 17:07:34.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 create -f - --namespace=kubectl-6522'
Jun 16 17:07:34.806: INFO: stderr: ""
Jun 16 17:07:34.806: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jun 16 17:07:34.806: INFO: Waiting for all frontend pods to be Running.
Jun 16 17:07:49.856: INFO: Waiting for frontend to serve content.
Jun 16 17:07:49.865: INFO: Trying to add a new entry to the guestbook.
Jun 16 17:07:49.874: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun 16 17:07:49.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete --grace-period=0 --force -f - --namespace=kubectl-6522'
Jun 16 17:07:49.942: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 16 17:07:49.942: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jun 16 17:07:49.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete --grace-period=0 --force -f - --namespace=kubectl-6522'
Jun 16 17:07:50.016: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 16 17:07:50.016: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 16 17:07:50.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete --grace-period=0 --force -f - --namespace=kubectl-6522'
Jun 16 17:07:50.091: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 16 17:07:50.091: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 16 17:07:50.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete --grace-period=0 --force -f - --namespace=kubectl-6522'
Jun 16 17:07:50.166: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 16 17:07:50.166: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 16 17:07:50.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete --grace-period=0 --force -f - --namespace=kubectl-6522'
Jun 16 17:07:50.238: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 16 17:07:50.238: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 16 17:07:50.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete --grace-period=0 --force -f - --namespace=kubectl-6522'
Jun 16 17:07:50.305: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 16 17:07:50.305: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:07:50.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6522" for this suite.
Jun 16 17:08:02.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:08:02.367: INFO: namespace kubectl-6522 deletion completed in 12.059361512s

• [SLOW TEST:28.598 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:08:02.367: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1229
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 16 17:08:02.490: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:08:16.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1229" for this suite.
Jun 16 17:08:22.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:08:22.870: INFO: namespace pods-1229 deletion completed in 6.053229203s

• [SLOW TEST:20.503 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:08:22.870: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6227
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Jun 16 17:08:22.989: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 16 17:08:22.995: INFO: Waiting for terminating namespaces to be deleted...
Jun 16 17:08:22.996: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-122.eu-west-1.compute.internal before test
Jun 16 17:08:23.000: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-6qlkz from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 17:08:23.000: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 16 17:08:23.000: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 17:08:23.000: INFO: kiam-agent-x89wb from kube-system started at 2020-06-16 15:10:38 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.000: INFO: 	Container kiam-agent ready: true, restart count 3
Jun 16 17:08:23.000: INFO: external-dns-776fc66667-24mls from kube-system started at 2020-06-16 15:10:42 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.000: INFO: 	Container external-dns ready: true, restart count 0
Jun 16 17:08:23.000: INFO: kube-proxy-xdgfc from kube-system started at 2020-06-16 15:06:53 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.000: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 17:08:23.000: INFO: cert-exporter-sgwbs from kube-system started at 2020-06-16 15:10:32 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.000: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 17:08:23.000: INFO: coredns-6d56c484c-wcs6v from kube-system started at 2020-06-16 15:10:34 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.000: INFO: 	Container coredns ready: true, restart count 0
Jun 16 17:08:23.000: INFO: aws-node-jbq4c from kube-system started at 2020-06-16 15:10:43 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.000: INFO: 	Container aws-node ready: true, restart count 0
Jun 16 17:08:23.000: INFO: net-exporter-ltbgf from kube-system started at 2020-06-16 15:14:31 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.000: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 17:08:23.000: INFO: calico-node-sk9rd from kube-system started at 2020-06-16 15:06:53 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.000: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 17:08:23.000: INFO: node-exporter-5vz2p from kube-system started at 2020-06-16 15:10:31 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.000: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 17:08:23.000: INFO: kube-state-metrics-6d998ffd8b-jpbpj from kube-system started at 2020-06-16 15:10:43 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.000: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 16 17:08:23.000: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-219.eu-west-1.compute.internal before test
Jun 16 17:08:23.004: INFO: sonobuoy from sonobuoy started at 2020-06-16 15:26:40 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.004: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 16 17:08:23.004: INFO: aws-node-kmx2q from kube-system started at 2020-06-16 15:10:51 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.004: INFO: 	Container aws-node ready: true, restart count 0
Jun 16 17:08:23.004: INFO: kube-proxy-r2g9b from kube-system started at 2020-06-16 15:06:51 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.004: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 17:08:23.004: INFO: node-exporter-pqhv4 from kube-system started at 2020-06-16 15:10:31 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.004: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 17:08:23.004: INFO: sonobuoy-e2e-job-40469eeb076c4210 from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 17:08:23.004: INFO: 	Container e2e ready: true, restart count 0
Jun 16 17:08:23.004: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 16 17:08:23.004: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-bbzmx from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 17:08:23.004: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 16 17:08:23.004: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 17:08:23.004: INFO: cert-manager-67dfd96fcd-x47b4 from kube-system started at 2020-06-16 16:27:46 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.004: INFO: 	Container cert-manager ready: true, restart count 0
Jun 16 17:08:23.004: INFO: tiller-deploy-684c6b545b-xfl8b from giantswarm started at 2020-06-16 15:09:33 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.004: INFO: 	Container tiller ready: true, restart count 0
Jun 16 17:08:23.004: INFO: coredns-6d56c484c-l7qk9 from kube-system started at 2020-06-16 15:10:34 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.004: INFO: 	Container coredns ready: true, restart count 0
Jun 16 17:08:23.004: INFO: kiam-agent-6q62w from kube-system started at 2020-06-16 15:10:38 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.004: INFO: 	Container kiam-agent ready: true, restart count 3
Jun 16 17:08:23.004: INFO: calico-node-xpkvc from kube-system started at 2020-06-16 15:06:51 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.004: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 17:08:23.004: INFO: cert-exporter-fthcq from kube-system started at 2020-06-16 15:10:32 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.004: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 17:08:23.004: INFO: net-exporter-p7cpr from kube-system started at 2020-06-16 15:14:31 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.004: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 17:08:23.004: INFO: 
Logging pods the kubelet thinks is on node ip-172-19-65-6.eu-west-1.compute.internal before test
Jun 16 17:08:23.009: INFO: sonobuoy-systemd-logs-daemon-set-df8ec83f67f5472b-87l2v from sonobuoy started at 2020-06-16 15:26:45 +0000 UTC (2 container statuses recorded)
Jun 16 17:08:23.009: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 16 17:08:23.009: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 16 17:08:23.009: INFO: metrics-server-66df9f5b56-wbjz4 from kube-system started at 2020-06-16 16:27:46 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.009: INFO: 	Container metrics-server ready: true, restart count 0
Jun 16 17:08:23.009: INFO: coredns-6d56c484c-p2v4f from kube-system started at 2020-06-16 16:27:46 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.009: INFO: 	Container coredns ready: true, restart count 0
Jun 16 17:08:23.009: INFO: kube-proxy-g6cmw from kube-system started at 2020-06-16 15:06:46 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.009: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 16 17:08:23.009: INFO: node-exporter-z96vr from kube-system started at 2020-06-16 15:10:31 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.009: INFO: 	Container node-exporter ready: true, restart count 0
Jun 16 17:08:23.009: INFO: kiam-agent-25qmf from kube-system started at 2020-06-16 16:27:52 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.009: INFO: 	Container kiam-agent ready: true, restart count 0
Jun 16 17:08:23.009: INFO: cert-exporter-2k4kc from kube-system started at 2020-06-16 15:10:32 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.009: INFO: 	Container cert-exporter ready: true, restart count 0
Jun 16 17:08:23.009: INFO: net-exporter-5wtl4 from kube-system started at 2020-06-16 15:14:31 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.009: INFO: 	Container net-exporter ready: true, restart count 0
Jun 16 17:08:23.009: INFO: calico-node-ddwlg from kube-system started at 2020-06-16 15:06:46 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.009: INFO: 	Container calico-node ready: true, restart count 0
Jun 16 17:08:23.009: INFO: aws-node-vjj2v from kube-system started at 2020-06-16 15:10:39 +0000 UTC (1 container statuses recorded)
Jun 16 17:08:23.009: INFO: 	Container aws-node ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b3469765-bd74-4a6e-83d7-d5d480157bcd 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-b3469765-bd74-4a6e-83d7-d5d480157bcd off the node ip-172-19-65-6.eu-west-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b3469765-bd74-4a6e-83d7-d5d480157bcd
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:08:31.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6227" for this suite.
Jun 16 17:08:49.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:08:49.115: INFO: namespace sched-pred-6227 deletion completed in 18.057744131s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:26.245 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:08:49.116: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6566
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 17:08:49.237: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:08:49.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6566" for this suite.
Jun 16 17:08:55.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:08:55.816: INFO: namespace custom-resource-definition-6566 deletion completed in 6.055433015s

• [SLOW TEST:6.701 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:08:55.816: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7959
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0616 17:09:05.950052      24 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 16 17:09:05.950: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:09:05.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7959" for this suite.
Jun 16 17:09:11.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:09:12.029: INFO: namespace gc-7959 deletion completed in 6.077674255s

• [SLOW TEST:16.213 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:09:12.030: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8179
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:09:14.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8179" for this suite.
Jun 16 17:09:22.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:09:22.228: INFO: namespace containers-8179 deletion completed in 8.055276752s

• [SLOW TEST:10.198 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:09:22.228: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6043
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:09:22.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6043" for this suite.
Jun 16 17:09:28.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:09:28.403: INFO: namespace services-6043 deletion completed in 6.052017258s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.174 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:09:28.403: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9608
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-9608/secret-test-3ecb7018-f6d4-48d0-9afd-ead93c2b5f7a
STEP: Creating a pod to test consume secrets
Jun 16 17:09:28.529: INFO: Waiting up to 5m0s for pod "pod-configmaps-3fee5610-350d-43a6-b6ac-b1c7a744c5d9" in namespace "secrets-9608" to be "success or failure"
Jun 16 17:09:28.531: INFO: Pod "pod-configmaps-3fee5610-350d-43a6-b6ac-b1c7a744c5d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.001331ms
Jun 16 17:09:30.533: INFO: Pod "pod-configmaps-3fee5610-350d-43a6-b6ac-b1c7a744c5d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004723406s
STEP: Saw pod success
Jun 16 17:09:30.533: INFO: Pod "pod-configmaps-3fee5610-350d-43a6-b6ac-b1c7a744c5d9" satisfied condition "success or failure"
Jun 16 17:09:30.535: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-configmaps-3fee5610-350d-43a6-b6ac-b1c7a744c5d9 container env-test: <nil>
STEP: delete the pod
Jun 16 17:09:30.545: INFO: Waiting for pod pod-configmaps-3fee5610-350d-43a6-b6ac-b1c7a744c5d9 to disappear
Jun 16 17:09:30.547: INFO: Pod pod-configmaps-3fee5610-350d-43a6-b6ac-b1c7a744c5d9 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:09:30.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9608" for this suite.
Jun 16 17:09:36.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:09:36.604: INFO: namespace secrets-9608 deletion completed in 6.055102707s

• [SLOW TEST:8.201 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:09:36.604: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1588
I0616 17:09:36.726876      24 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1588, replica count: 1
I0616 17:09:37.777174      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0616 17:09:38.777325      24 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 16 17:09:38.883: INFO: Created: latency-svc-xrl9p
Jun 16 17:09:38.886: INFO: Got endpoints: latency-svc-xrl9p [9.257662ms]
Jun 16 17:09:38.893: INFO: Created: latency-svc-xttg7
Jun 16 17:09:38.895: INFO: Got endpoints: latency-svc-xttg7 [8.406117ms]
Jun 16 17:09:38.898: INFO: Created: latency-svc-shx6w
Jun 16 17:09:38.903: INFO: Created: latency-svc-vtmkl
Jun 16 17:09:38.903: INFO: Got endpoints: latency-svc-shx6w [16.011882ms]
Jun 16 17:09:38.907: INFO: Created: latency-svc-mgxv4
Jun 16 17:09:38.907: INFO: Got endpoints: latency-svc-vtmkl [20.596934ms]
Jun 16 17:09:38.911: INFO: Created: latency-svc-r87hj
Jun 16 17:09:38.912: INFO: Got endpoints: latency-svc-mgxv4 [25.180983ms]
Jun 16 17:09:38.915: INFO: Got endpoints: latency-svc-r87hj [27.989666ms]
Jun 16 17:09:38.916: INFO: Created: latency-svc-v7ncg
Jun 16 17:09:38.920: INFO: Got endpoints: latency-svc-v7ncg [32.613096ms]
Jun 16 17:09:38.921: INFO: Created: latency-svc-2zpx6
Jun 16 17:09:38.923: INFO: Created: latency-svc-wnmfh
Jun 16 17:09:38.923: INFO: Got endpoints: latency-svc-2zpx6 [35.486006ms]
Jun 16 17:09:38.927: INFO: Got endpoints: latency-svc-wnmfh [38.65096ms]
Jun 16 17:09:38.927: INFO: Created: latency-svc-z5w6z
Jun 16 17:09:38.930: INFO: Got endpoints: latency-svc-z5w6z [42.618627ms]
Jun 16 17:09:38.931: INFO: Created: latency-svc-5j224
Jun 16 17:09:38.933: INFO: Created: latency-svc-lqbqb
Jun 16 17:09:38.936: INFO: Created: latency-svc-kvtrt
Jun 16 17:09:38.937: INFO: Got endpoints: latency-svc-lqbqb [48.739051ms]
Jun 16 17:09:38.937: INFO: Got endpoints: latency-svc-5j224 [49.498184ms]
Jun 16 17:09:38.940: INFO: Created: latency-svc-fnctm
Jun 16 17:09:38.940: INFO: Got endpoints: latency-svc-kvtrt [51.984478ms]
Jun 16 17:09:38.942: INFO: Created: latency-svc-kg4g8
Jun 16 17:09:38.945: INFO: Created: latency-svc-s8dpc
Jun 16 17:09:38.946: INFO: Got endpoints: latency-svc-fnctm [57.753026ms]
Jun 16 17:09:38.949: INFO: Got endpoints: latency-svc-s8dpc [61.042978ms]
Jun 16 17:09:38.949: INFO: Got endpoints: latency-svc-kg4g8 [61.369459ms]
Jun 16 17:09:38.950: INFO: Created: latency-svc-85c5s
Jun 16 17:09:38.953: INFO: Created: latency-svc-2ngvm
Jun 16 17:09:38.953: INFO: Got endpoints: latency-svc-85c5s [58.448867ms]
Jun 16 17:09:38.956: INFO: Got endpoints: latency-svc-2ngvm [52.743559ms]
Jun 16 17:09:38.958: INFO: Created: latency-svc-6rd9n
Jun 16 17:09:38.961: INFO: Created: latency-svc-t4pw7
Jun 16 17:09:38.961: INFO: Got endpoints: latency-svc-6rd9n [53.685845ms]
Jun 16 17:09:38.964: INFO: Created: latency-svc-l8rph
Jun 16 17:09:38.965: INFO: Got endpoints: latency-svc-t4pw7 [52.570043ms]
Jun 16 17:09:38.968: INFO: Created: latency-svc-6pd7v
Jun 16 17:09:38.971: INFO: Got endpoints: latency-svc-l8rph [55.5931ms]
Jun 16 17:09:38.973: INFO: Got endpoints: latency-svc-6pd7v [53.38336ms]
Jun 16 17:09:38.973: INFO: Created: latency-svc-lrtq2
Jun 16 17:09:38.975: INFO: Got endpoints: latency-svc-lrtq2 [51.605098ms]
Jun 16 17:09:38.976: INFO: Created: latency-svc-kmxvg
Jun 16 17:09:38.979: INFO: Created: latency-svc-h9mmd
Jun 16 17:09:38.982: INFO: Got endpoints: latency-svc-kmxvg [55.315036ms]
Jun 16 17:09:38.984: INFO: Got endpoints: latency-svc-h9mmd [54.360844ms]
Jun 16 17:09:38.985: INFO: Created: latency-svc-sl6zz
Jun 16 17:09:38.988: INFO: Created: latency-svc-lrp54
Jun 16 17:09:38.989: INFO: Got endpoints: latency-svc-sl6zz [52.369668ms]
Jun 16 17:09:38.994: INFO: Created: latency-svc-9b2kq
Jun 16 17:09:38.995: INFO: Got endpoints: latency-svc-lrp54 [58.075872ms]
Jun 16 17:09:38.999: INFO: Created: latency-svc-n48wv
Jun 16 17:09:38.999: INFO: Got endpoints: latency-svc-9b2kq [59.238522ms]
Jun 16 17:09:39.002: INFO: Created: latency-svc-kvkcd
Jun 16 17:09:39.008: INFO: Created: latency-svc-vwwkz
Jun 16 17:09:39.008: INFO: Got endpoints: latency-svc-kvkcd [59.09581ms]
Jun 16 17:09:39.008: INFO: Got endpoints: latency-svc-n48wv [62.396158ms]
Jun 16 17:09:39.010: INFO: Got endpoints: latency-svc-vwwkz [60.853396ms]
Jun 16 17:09:39.010: INFO: Created: latency-svc-f8ld8
Jun 16 17:09:39.014: INFO: Got endpoints: latency-svc-f8ld8 [60.308338ms]
Jun 16 17:09:39.015: INFO: Created: latency-svc-fr4zg
Jun 16 17:09:39.018: INFO: Created: latency-svc-xxw26
Jun 16 17:09:39.032: INFO: Created: latency-svc-7vff7
Jun 16 17:09:39.038: INFO: Got endpoints: latency-svc-fr4zg [81.67089ms]
Jun 16 17:09:39.040: INFO: Created: latency-svc-h4zmj
Jun 16 17:09:39.042: INFO: Created: latency-svc-8phw9
Jun 16 17:09:39.047: INFO: Created: latency-svc-jrwz7
Jun 16 17:09:39.052: INFO: Created: latency-svc-6q5ks
Jun 16 17:09:39.056: INFO: Created: latency-svc-2rx8k
Jun 16 17:09:39.059: INFO: Created: latency-svc-dg7cd
Jun 16 17:09:39.061: INFO: Created: latency-svc-srltq
Jun 16 17:09:39.066: INFO: Created: latency-svc-b5dzb
Jun 16 17:09:39.069: INFO: Created: latency-svc-dxzgm
Jun 16 17:09:39.073: INFO: Created: latency-svc-f8cvg
Jun 16 17:09:39.075: INFO: Created: latency-svc-g67br
Jun 16 17:09:39.079: INFO: Created: latency-svc-hkxph
Jun 16 17:09:39.083: INFO: Created: latency-svc-4nfrc
Jun 16 17:09:39.086: INFO: Got endpoints: latency-svc-xxw26 [124.529249ms]
Jun 16 17:09:39.092: INFO: Created: latency-svc-l97bm
Jun 16 17:09:39.136: INFO: Got endpoints: latency-svc-7vff7 [171.749886ms]
Jun 16 17:09:39.141: INFO: Created: latency-svc-482hr
Jun 16 17:09:39.186: INFO: Got endpoints: latency-svc-h4zmj [215.78551ms]
Jun 16 17:09:39.192: INFO: Created: latency-svc-swgqz
Jun 16 17:09:39.236: INFO: Got endpoints: latency-svc-8phw9 [263.090335ms]
Jun 16 17:09:39.241: INFO: Created: latency-svc-6d2vq
Jun 16 17:09:39.287: INFO: Got endpoints: latency-svc-jrwz7 [311.386233ms]
Jun 16 17:09:39.291: INFO: Created: latency-svc-z8jx6
Jun 16 17:09:39.336: INFO: Got endpoints: latency-svc-6q5ks [354.275913ms]
Jun 16 17:09:39.341: INFO: Created: latency-svc-pvwq2
Jun 16 17:09:39.386: INFO: Got endpoints: latency-svc-2rx8k [402.190425ms]
Jun 16 17:09:39.392: INFO: Created: latency-svc-gvcv8
Jun 16 17:09:39.437: INFO: Got endpoints: latency-svc-dg7cd [448.049537ms]
Jun 16 17:09:39.442: INFO: Created: latency-svc-s5srx
Jun 16 17:09:39.486: INFO: Got endpoints: latency-svc-srltq [491.456946ms]
Jun 16 17:09:39.492: INFO: Created: latency-svc-nzftc
Jun 16 17:09:39.536: INFO: Got endpoints: latency-svc-b5dzb [537.188982ms]
Jun 16 17:09:39.541: INFO: Created: latency-svc-4m8zh
Jun 16 17:09:39.587: INFO: Got endpoints: latency-svc-dxzgm [578.487357ms]
Jun 16 17:09:39.592: INFO: Created: latency-svc-jlwwq
Jun 16 17:09:39.637: INFO: Got endpoints: latency-svc-f8cvg [628.888102ms]
Jun 16 17:09:39.641: INFO: Created: latency-svc-7ff7f
Jun 16 17:09:39.686: INFO: Got endpoints: latency-svc-g67br [676.850487ms]
Jun 16 17:09:39.692: INFO: Created: latency-svc-kzr95
Jun 16 17:09:39.736: INFO: Got endpoints: latency-svc-hkxph [722.619192ms]
Jun 16 17:09:39.742: INFO: Created: latency-svc-hlbpt
Jun 16 17:09:39.786: INFO: Got endpoints: latency-svc-4nfrc [748.089622ms]
Jun 16 17:09:39.791: INFO: Created: latency-svc-bxfcl
Jun 16 17:09:39.836: INFO: Got endpoints: latency-svc-l97bm [750.705493ms]
Jun 16 17:09:39.842: INFO: Created: latency-svc-lmv9k
Jun 16 17:09:39.887: INFO: Got endpoints: latency-svc-482hr [749.976749ms]
Jun 16 17:09:39.893: INFO: Created: latency-svc-2qv48
Jun 16 17:09:39.936: INFO: Got endpoints: latency-svc-swgqz [749.730129ms]
Jun 16 17:09:39.943: INFO: Created: latency-svc-tbgw7
Jun 16 17:09:39.987: INFO: Got endpoints: latency-svc-6d2vq [750.332318ms]
Jun 16 17:09:39.992: INFO: Created: latency-svc-qcgcv
Jun 16 17:09:40.037: INFO: Got endpoints: latency-svc-z8jx6 [750.027779ms]
Jun 16 17:09:40.049: INFO: Created: latency-svc-b8mh4
Jun 16 17:09:40.087: INFO: Got endpoints: latency-svc-pvwq2 [750.866933ms]
Jun 16 17:09:40.093: INFO: Created: latency-svc-95jns
Jun 16 17:09:40.137: INFO: Got endpoints: latency-svc-gvcv8 [750.162362ms]
Jun 16 17:09:40.143: INFO: Created: latency-svc-zd6v2
Jun 16 17:09:40.187: INFO: Got endpoints: latency-svc-s5srx [749.977411ms]
Jun 16 17:09:40.193: INFO: Created: latency-svc-7rczz
Jun 16 17:09:40.240: INFO: Got endpoints: latency-svc-nzftc [753.683424ms]
Jun 16 17:09:40.246: INFO: Created: latency-svc-xz4ng
Jun 16 17:09:40.286: INFO: Got endpoints: latency-svc-4m8zh [750.174821ms]
Jun 16 17:09:40.293: INFO: Created: latency-svc-5cwzv
Jun 16 17:09:40.337: INFO: Got endpoints: latency-svc-jlwwq [749.927448ms]
Jun 16 17:09:40.346: INFO: Created: latency-svc-4l672
Jun 16 17:09:40.387: INFO: Got endpoints: latency-svc-7ff7f [749.95864ms]
Jun 16 17:09:40.393: INFO: Created: latency-svc-p7nfv
Jun 16 17:09:40.436: INFO: Got endpoints: latency-svc-kzr95 [749.842487ms]
Jun 16 17:09:40.445: INFO: Created: latency-svc-mz7cv
Jun 16 17:09:40.489: INFO: Got endpoints: latency-svc-hlbpt [752.522683ms]
Jun 16 17:09:40.494: INFO: Created: latency-svc-7tkvz
Jun 16 17:09:40.537: INFO: Got endpoints: latency-svc-bxfcl [750.892039ms]
Jun 16 17:09:40.547: INFO: Created: latency-svc-85qxx
Jun 16 17:09:40.586: INFO: Got endpoints: latency-svc-lmv9k [749.215457ms]
Jun 16 17:09:40.591: INFO: Created: latency-svc-9hbr9
Jun 16 17:09:40.637: INFO: Got endpoints: latency-svc-2qv48 [749.968696ms]
Jun 16 17:09:40.642: INFO: Created: latency-svc-rg9qj
Jun 16 17:09:40.687: INFO: Got endpoints: latency-svc-tbgw7 [750.467448ms]
Jun 16 17:09:40.694: INFO: Created: latency-svc-dznrm
Jun 16 17:09:40.736: INFO: Got endpoints: latency-svc-qcgcv [749.522881ms]
Jun 16 17:09:40.741: INFO: Created: latency-svc-cfl8s
Jun 16 17:09:40.786: INFO: Got endpoints: latency-svc-b8mh4 [743.977475ms]
Jun 16 17:09:40.791: INFO: Created: latency-svc-4ndxn
Jun 16 17:09:40.836: INFO: Got endpoints: latency-svc-95jns [748.716178ms]
Jun 16 17:09:40.843: INFO: Created: latency-svc-gqmdh
Jun 16 17:09:40.887: INFO: Got endpoints: latency-svc-zd6v2 [750.171622ms]
Jun 16 17:09:40.895: INFO: Created: latency-svc-g4n9x
Jun 16 17:09:40.937: INFO: Got endpoints: latency-svc-7rczz [749.759852ms]
Jun 16 17:09:40.944: INFO: Created: latency-svc-wtxzc
Jun 16 17:09:40.989: INFO: Got endpoints: latency-svc-xz4ng [749.320688ms]
Jun 16 17:09:40.995: INFO: Created: latency-svc-4xtsc
Jun 16 17:09:41.037: INFO: Got endpoints: latency-svc-5cwzv [750.139619ms]
Jun 16 17:09:41.042: INFO: Created: latency-svc-wtdwh
Jun 16 17:09:41.087: INFO: Got endpoints: latency-svc-4l672 [750.131883ms]
Jun 16 17:09:41.093: INFO: Created: latency-svc-psqr4
Jun 16 17:09:41.137: INFO: Got endpoints: latency-svc-p7nfv [749.865925ms]
Jun 16 17:09:41.142: INFO: Created: latency-svc-rvzjh
Jun 16 17:09:41.187: INFO: Got endpoints: latency-svc-mz7cv [750.965379ms]
Jun 16 17:09:41.192: INFO: Created: latency-svc-ddz7q
Jun 16 17:09:41.236: INFO: Got endpoints: latency-svc-7tkvz [746.79367ms]
Jun 16 17:09:41.241: INFO: Created: latency-svc-4c22j
Jun 16 17:09:41.286: INFO: Got endpoints: latency-svc-85qxx [748.963076ms]
Jun 16 17:09:41.291: INFO: Created: latency-svc-wlr94
Jun 16 17:09:41.336: INFO: Got endpoints: latency-svc-9hbr9 [750.100346ms]
Jun 16 17:09:41.344: INFO: Created: latency-svc-95psx
Jun 16 17:09:41.386: INFO: Got endpoints: latency-svc-rg9qj [749.483463ms]
Jun 16 17:09:41.391: INFO: Created: latency-svc-blqzl
Jun 16 17:09:41.436: INFO: Got endpoints: latency-svc-dznrm [749.170379ms]
Jun 16 17:09:41.443: INFO: Created: latency-svc-mks68
Jun 16 17:09:41.486: INFO: Got endpoints: latency-svc-cfl8s [750.161675ms]
Jun 16 17:09:41.491: INFO: Created: latency-svc-vjdh6
Jun 16 17:09:41.536: INFO: Got endpoints: latency-svc-4ndxn [750.23088ms]
Jun 16 17:09:41.541: INFO: Created: latency-svc-qth99
Jun 16 17:09:41.586: INFO: Got endpoints: latency-svc-gqmdh [750.463818ms]
Jun 16 17:09:41.592: INFO: Created: latency-svc-2wzrs
Jun 16 17:09:41.637: INFO: Got endpoints: latency-svc-g4n9x [749.928827ms]
Jun 16 17:09:41.642: INFO: Created: latency-svc-mmbzp
Jun 16 17:09:41.686: INFO: Got endpoints: latency-svc-wtxzc [749.343234ms]
Jun 16 17:09:41.691: INFO: Created: latency-svc-h5h9d
Jun 16 17:09:41.737: INFO: Got endpoints: latency-svc-4xtsc [747.085716ms]
Jun 16 17:09:41.741: INFO: Created: latency-svc-m4znm
Jun 16 17:09:41.819: INFO: Got endpoints: latency-svc-wtdwh [782.637184ms]
Jun 16 17:09:41.824: INFO: Created: latency-svc-qv57s
Jun 16 17:09:41.836: INFO: Got endpoints: latency-svc-psqr4 [749.492325ms]
Jun 16 17:09:41.841: INFO: Created: latency-svc-zpq87
Jun 16 17:09:41.887: INFO: Got endpoints: latency-svc-rvzjh [749.899208ms]
Jun 16 17:09:41.892: INFO: Created: latency-svc-lvn7g
Jun 16 17:09:41.937: INFO: Got endpoints: latency-svc-ddz7q [749.196325ms]
Jun 16 17:09:41.941: INFO: Created: latency-svc-98tjx
Jun 16 17:09:41.987: INFO: Got endpoints: latency-svc-4c22j [750.512941ms]
Jun 16 17:09:41.993: INFO: Created: latency-svc-jmxz7
Jun 16 17:09:42.038: INFO: Got endpoints: latency-svc-wlr94 [751.807913ms]
Jun 16 17:09:42.043: INFO: Created: latency-svc-7zmzn
Jun 16 17:09:42.087: INFO: Got endpoints: latency-svc-95psx [750.502478ms]
Jun 16 17:09:42.093: INFO: Created: latency-svc-8427j
Jun 16 17:09:42.139: INFO: Got endpoints: latency-svc-blqzl [752.800817ms]
Jun 16 17:09:42.148: INFO: Created: latency-svc-xctsf
Jun 16 17:09:42.187: INFO: Got endpoints: latency-svc-mks68 [750.254825ms]
Jun 16 17:09:42.192: INFO: Created: latency-svc-vj9vq
Jun 16 17:09:42.236: INFO: Got endpoints: latency-svc-vjdh6 [750.179114ms]
Jun 16 17:09:42.242: INFO: Created: latency-svc-b2nnb
Jun 16 17:09:42.287: INFO: Got endpoints: latency-svc-qth99 [750.173589ms]
Jun 16 17:09:42.292: INFO: Created: latency-svc-8jfdq
Jun 16 17:09:42.336: INFO: Got endpoints: latency-svc-2wzrs [749.706848ms]
Jun 16 17:09:42.343: INFO: Created: latency-svc-xxjrb
Jun 16 17:09:42.387: INFO: Got endpoints: latency-svc-mmbzp [749.686294ms]
Jun 16 17:09:42.392: INFO: Created: latency-svc-9b6nb
Jun 16 17:09:42.437: INFO: Got endpoints: latency-svc-h5h9d [750.240456ms]
Jun 16 17:09:42.441: INFO: Created: latency-svc-9h5cp
Jun 16 17:09:42.487: INFO: Got endpoints: latency-svc-m4znm [750.001315ms]
Jun 16 17:09:42.492: INFO: Created: latency-svc-9vtqn
Jun 16 17:09:42.536: INFO: Got endpoints: latency-svc-qv57s [717.038442ms]
Jun 16 17:09:42.541: INFO: Created: latency-svc-hzttg
Jun 16 17:09:42.587: INFO: Got endpoints: latency-svc-zpq87 [750.366089ms]
Jun 16 17:09:42.592: INFO: Created: latency-svc-92658
Jun 16 17:09:42.637: INFO: Got endpoints: latency-svc-lvn7g [750.646716ms]
Jun 16 17:09:42.643: INFO: Created: latency-svc-gffnb
Jun 16 17:09:42.687: INFO: Got endpoints: latency-svc-98tjx [749.919601ms]
Jun 16 17:09:42.694: INFO: Created: latency-svc-b4p9s
Jun 16 17:09:42.736: INFO: Got endpoints: latency-svc-jmxz7 [749.519569ms]
Jun 16 17:09:42.742: INFO: Created: latency-svc-fxn24
Jun 16 17:09:42.787: INFO: Got endpoints: latency-svc-7zmzn [748.853244ms]
Jun 16 17:09:42.792: INFO: Created: latency-svc-ghpxn
Jun 16 17:09:42.837: INFO: Got endpoints: latency-svc-8427j [749.988347ms]
Jun 16 17:09:42.841: INFO: Created: latency-svc-l7mpf
Jun 16 17:09:42.887: INFO: Got endpoints: latency-svc-xctsf [747.570343ms]
Jun 16 17:09:42.897: INFO: Created: latency-svc-ghwhv
Jun 16 17:09:42.937: INFO: Got endpoints: latency-svc-vj9vq [749.983619ms]
Jun 16 17:09:42.943: INFO: Created: latency-svc-qmdjh
Jun 16 17:09:42.986: INFO: Got endpoints: latency-svc-b2nnb [749.472366ms]
Jun 16 17:09:42.992: INFO: Created: latency-svc-s2kqs
Jun 16 17:09:43.037: INFO: Got endpoints: latency-svc-8jfdq [749.587358ms]
Jun 16 17:09:43.043: INFO: Created: latency-svc-fdtxw
Jun 16 17:09:43.087: INFO: Got endpoints: latency-svc-xxjrb [750.643348ms]
Jun 16 17:09:43.092: INFO: Created: latency-svc-rdsvh
Jun 16 17:09:43.137: INFO: Got endpoints: latency-svc-9b6nb [749.913993ms]
Jun 16 17:09:43.142: INFO: Created: latency-svc-8tldl
Jun 16 17:09:43.188: INFO: Got endpoints: latency-svc-9h5cp [751.790719ms]
Jun 16 17:09:43.193: INFO: Created: latency-svc-ghdkw
Jun 16 17:09:43.236: INFO: Got endpoints: latency-svc-9vtqn [749.794504ms]
Jun 16 17:09:43.243: INFO: Created: latency-svc-d5tqp
Jun 16 17:09:43.286: INFO: Got endpoints: latency-svc-hzttg [749.775345ms]
Jun 16 17:09:43.291: INFO: Created: latency-svc-qlggx
Jun 16 17:09:43.336: INFO: Got endpoints: latency-svc-92658 [749.57731ms]
Jun 16 17:09:43.341: INFO: Created: latency-svc-76b82
Jun 16 17:09:43.386: INFO: Got endpoints: latency-svc-gffnb [748.934041ms]
Jun 16 17:09:43.392: INFO: Created: latency-svc-544tf
Jun 16 17:09:43.436: INFO: Got endpoints: latency-svc-b4p9s [749.424099ms]
Jun 16 17:09:43.441: INFO: Created: latency-svc-lk5g5
Jun 16 17:09:43.486: INFO: Got endpoints: latency-svc-fxn24 [749.328001ms]
Jun 16 17:09:43.491: INFO: Created: latency-svc-fq8sq
Jun 16 17:09:43.536: INFO: Got endpoints: latency-svc-ghpxn [749.632473ms]
Jun 16 17:09:43.542: INFO: Created: latency-svc-gbrqh
Jun 16 17:09:43.588: INFO: Got endpoints: latency-svc-l7mpf [751.785509ms]
Jun 16 17:09:43.595: INFO: Created: latency-svc-xmckk
Jun 16 17:09:43.637: INFO: Got endpoints: latency-svc-ghwhv [749.400031ms]
Jun 16 17:09:43.642: INFO: Created: latency-svc-lhdll
Jun 16 17:09:43.687: INFO: Got endpoints: latency-svc-qmdjh [749.874247ms]
Jun 16 17:09:43.692: INFO: Created: latency-svc-td9p5
Jun 16 17:09:43.736: INFO: Got endpoints: latency-svc-s2kqs [750.107268ms]
Jun 16 17:09:43.741: INFO: Created: latency-svc-nld7h
Jun 16 17:09:43.787: INFO: Got endpoints: latency-svc-fdtxw [749.918443ms]
Jun 16 17:09:43.796: INFO: Created: latency-svc-frdnn
Jun 16 17:09:43.836: INFO: Got endpoints: latency-svc-rdsvh [749.377008ms]
Jun 16 17:09:43.842: INFO: Created: latency-svc-4h7d5
Jun 16 17:09:43.886: INFO: Got endpoints: latency-svc-8tldl [749.316469ms]
Jun 16 17:09:43.908: INFO: Created: latency-svc-7zvfl
Jun 16 17:09:43.937: INFO: Got endpoints: latency-svc-ghdkw [748.641108ms]
Jun 16 17:09:43.949: INFO: Created: latency-svc-m4w75
Jun 16 17:09:43.986: INFO: Got endpoints: latency-svc-d5tqp [749.608913ms]
Jun 16 17:09:43.992: INFO: Created: latency-svc-pzs88
Jun 16 17:09:44.037: INFO: Got endpoints: latency-svc-qlggx [750.240233ms]
Jun 16 17:09:44.044: INFO: Created: latency-svc-ls7gj
Jun 16 17:09:44.088: INFO: Got endpoints: latency-svc-76b82 [751.385636ms]
Jun 16 17:09:44.094: INFO: Created: latency-svc-npbpr
Jun 16 17:09:44.138: INFO: Got endpoints: latency-svc-544tf [751.051082ms]
Jun 16 17:09:44.148: INFO: Created: latency-svc-rm5wh
Jun 16 17:09:44.187: INFO: Got endpoints: latency-svc-lk5g5 [751.145462ms]
Jun 16 17:09:44.193: INFO: Created: latency-svc-tqrvx
Jun 16 17:09:44.237: INFO: Got endpoints: latency-svc-fq8sq [750.388151ms]
Jun 16 17:09:44.243: INFO: Created: latency-svc-qpwb6
Jun 16 17:09:44.287: INFO: Got endpoints: latency-svc-gbrqh [750.065841ms]
Jun 16 17:09:44.294: INFO: Created: latency-svc-7v8zt
Jun 16 17:09:44.337: INFO: Got endpoints: latency-svc-xmckk [748.088994ms]
Jun 16 17:09:44.342: INFO: Created: latency-svc-8z4fv
Jun 16 17:09:44.386: INFO: Got endpoints: latency-svc-lhdll [749.351185ms]
Jun 16 17:09:44.391: INFO: Created: latency-svc-p557t
Jun 16 17:09:44.436: INFO: Got endpoints: latency-svc-td9p5 [749.456388ms]
Jun 16 17:09:44.441: INFO: Created: latency-svc-dzzft
Jun 16 17:09:44.486: INFO: Got endpoints: latency-svc-nld7h [750.132328ms]
Jun 16 17:09:44.492: INFO: Created: latency-svc-sxm7c
Jun 16 17:09:44.537: INFO: Got endpoints: latency-svc-frdnn [745.855511ms]
Jun 16 17:09:44.542: INFO: Created: latency-svc-4xc67
Jun 16 17:09:44.586: INFO: Got endpoints: latency-svc-4h7d5 [750.012557ms]
Jun 16 17:09:44.592: INFO: Created: latency-svc-djcj2
Jun 16 17:09:44.636: INFO: Got endpoints: latency-svc-7zvfl [750.281577ms]
Jun 16 17:09:44.641: INFO: Created: latency-svc-dz562
Jun 16 17:09:44.687: INFO: Got endpoints: latency-svc-m4w75 [749.649553ms]
Jun 16 17:09:44.693: INFO: Created: latency-svc-ltl8n
Jun 16 17:09:44.737: INFO: Got endpoints: latency-svc-pzs88 [750.349559ms]
Jun 16 17:09:44.742: INFO: Created: latency-svc-6fvxx
Jun 16 17:09:44.786: INFO: Got endpoints: latency-svc-ls7gj [749.538256ms]
Jun 16 17:09:44.792: INFO: Created: latency-svc-fgmkd
Jun 16 17:09:44.837: INFO: Got endpoints: latency-svc-npbpr [748.886934ms]
Jun 16 17:09:44.842: INFO: Created: latency-svc-bpxqp
Jun 16 17:09:44.886: INFO: Got endpoints: latency-svc-rm5wh [748.865204ms]
Jun 16 17:09:44.892: INFO: Created: latency-svc-m4t22
Jun 16 17:09:44.936: INFO: Got endpoints: latency-svc-tqrvx [749.237405ms]
Jun 16 17:09:44.942: INFO: Created: latency-svc-xv4vl
Jun 16 17:09:44.986: INFO: Got endpoints: latency-svc-qpwb6 [749.762008ms]
Jun 16 17:09:44.991: INFO: Created: latency-svc-b4dcf
Jun 16 17:09:45.036: INFO: Got endpoints: latency-svc-7v8zt [749.382928ms]
Jun 16 17:09:45.042: INFO: Created: latency-svc-zk256
Jun 16 17:09:45.086: INFO: Got endpoints: latency-svc-8z4fv [748.968768ms]
Jun 16 17:09:45.091: INFO: Created: latency-svc-rfhjm
Jun 16 17:09:45.136: INFO: Got endpoints: latency-svc-p557t [750.108687ms]
Jun 16 17:09:45.141: INFO: Created: latency-svc-gdjq8
Jun 16 17:09:45.188: INFO: Got endpoints: latency-svc-dzzft [751.416862ms]
Jun 16 17:09:45.193: INFO: Created: latency-svc-222jq
Jun 16 17:09:45.239: INFO: Got endpoints: latency-svc-sxm7c [752.792106ms]
Jun 16 17:09:45.246: INFO: Created: latency-svc-5dcjk
Jun 16 17:09:45.287: INFO: Got endpoints: latency-svc-4xc67 [749.810344ms]
Jun 16 17:09:45.293: INFO: Created: latency-svc-dcwqw
Jun 16 17:09:45.337: INFO: Got endpoints: latency-svc-djcj2 [750.346298ms]
Jun 16 17:09:45.345: INFO: Created: latency-svc-w4nns
Jun 16 17:09:45.386: INFO: Got endpoints: latency-svc-dz562 [749.848546ms]
Jun 16 17:09:45.392: INFO: Created: latency-svc-r2nrs
Jun 16 17:09:45.437: INFO: Got endpoints: latency-svc-ltl8n [749.473235ms]
Jun 16 17:09:45.442: INFO: Created: latency-svc-t5n2k
Jun 16 17:09:45.486: INFO: Got endpoints: latency-svc-6fvxx [749.705293ms]
Jun 16 17:09:45.492: INFO: Created: latency-svc-2frws
Jun 16 17:09:45.536: INFO: Got endpoints: latency-svc-fgmkd [749.904601ms]
Jun 16 17:09:45.542: INFO: Created: latency-svc-msxs4
Jun 16 17:09:45.587: INFO: Got endpoints: latency-svc-bpxqp [749.798428ms]
Jun 16 17:09:45.591: INFO: Created: latency-svc-mn585
Jun 16 17:09:45.636: INFO: Got endpoints: latency-svc-m4t22 [749.905273ms]
Jun 16 17:09:45.642: INFO: Created: latency-svc-pw6gb
Jun 16 17:09:45.695: INFO: Got endpoints: latency-svc-xv4vl [758.362232ms]
Jun 16 17:09:45.705: INFO: Created: latency-svc-j4vjt
Jun 16 17:09:45.740: INFO: Got endpoints: latency-svc-b4dcf [753.61469ms]
Jun 16 17:09:45.746: INFO: Created: latency-svc-7879m
Jun 16 17:09:45.787: INFO: Got endpoints: latency-svc-zk256 [750.439996ms]
Jun 16 17:09:45.793: INFO: Created: latency-svc-bz2j7
Jun 16 17:09:45.836: INFO: Got endpoints: latency-svc-rfhjm [750.325172ms]
Jun 16 17:09:45.843: INFO: Created: latency-svc-9j2b2
Jun 16 17:09:45.887: INFO: Got endpoints: latency-svc-gdjq8 [750.43557ms]
Jun 16 17:09:45.892: INFO: Created: latency-svc-jd89d
Jun 16 17:09:45.937: INFO: Got endpoints: latency-svc-222jq [748.805354ms]
Jun 16 17:09:45.941: INFO: Created: latency-svc-qnvhv
Jun 16 17:09:45.987: INFO: Got endpoints: latency-svc-5dcjk [747.1522ms]
Jun 16 17:09:45.991: INFO: Created: latency-svc-vxjl2
Jun 16 17:09:46.037: INFO: Got endpoints: latency-svc-dcwqw [749.740115ms]
Jun 16 17:09:46.042: INFO: Created: latency-svc-vcchw
Jun 16 17:09:46.087: INFO: Got endpoints: latency-svc-w4nns [749.60126ms]
Jun 16 17:09:46.091: INFO: Created: latency-svc-kn96t
Jun 16 17:09:46.146: INFO: Got endpoints: latency-svc-r2nrs [759.636873ms]
Jun 16 17:09:46.152: INFO: Created: latency-svc-dkd7m
Jun 16 17:09:46.190: INFO: Got endpoints: latency-svc-t5n2k [753.142074ms]
Jun 16 17:09:46.198: INFO: Created: latency-svc-z9hkr
Jun 16 17:09:46.245: INFO: Got endpoints: latency-svc-2frws [758.363879ms]
Jun 16 17:09:46.254: INFO: Created: latency-svc-4vd8m
Jun 16 17:09:46.286: INFO: Got endpoints: latency-svc-msxs4 [750.254414ms]
Jun 16 17:09:46.293: INFO: Created: latency-svc-9nmk9
Jun 16 17:09:46.336: INFO: Got endpoints: latency-svc-mn585 [749.590693ms]
Jun 16 17:09:46.342: INFO: Created: latency-svc-7pdd5
Jun 16 17:09:46.387: INFO: Got endpoints: latency-svc-pw6gb [749.933192ms]
Jun 16 17:09:46.392: INFO: Created: latency-svc-4s2pl
Jun 16 17:09:46.437: INFO: Got endpoints: latency-svc-j4vjt [741.801361ms]
Jun 16 17:09:46.441: INFO: Created: latency-svc-pww69
Jun 16 17:09:46.487: INFO: Got endpoints: latency-svc-7879m [746.210653ms]
Jun 16 17:09:46.491: INFO: Created: latency-svc-xtkvx
Jun 16 17:09:46.536: INFO: Got endpoints: latency-svc-bz2j7 [749.711942ms]
Jun 16 17:09:46.543: INFO: Created: latency-svc-t74nz
Jun 16 17:09:46.586: INFO: Got endpoints: latency-svc-9j2b2 [749.972157ms]
Jun 16 17:09:46.592: INFO: Created: latency-svc-c7b4t
Jun 16 17:09:46.636: INFO: Got endpoints: latency-svc-jd89d [749.480562ms]
Jun 16 17:09:46.641: INFO: Created: latency-svc-8ns9w
Jun 16 17:09:46.687: INFO: Got endpoints: latency-svc-qnvhv [750.381042ms]
Jun 16 17:09:46.694: INFO: Created: latency-svc-67bl7
Jun 16 17:09:46.736: INFO: Got endpoints: latency-svc-vxjl2 [749.62705ms]
Jun 16 17:09:46.790: INFO: Got endpoints: latency-svc-vcchw [752.749351ms]
Jun 16 17:09:46.836: INFO: Got endpoints: latency-svc-kn96t [749.62505ms]
Jun 16 17:09:46.886: INFO: Got endpoints: latency-svc-dkd7m [739.840056ms]
Jun 16 17:09:46.936: INFO: Got endpoints: latency-svc-z9hkr [746.619904ms]
Jun 16 17:09:46.986: INFO: Got endpoints: latency-svc-4vd8m [741.491639ms]
Jun 16 17:09:47.036: INFO: Got endpoints: latency-svc-9nmk9 [749.853242ms]
Jun 16 17:09:47.086: INFO: Got endpoints: latency-svc-7pdd5 [750.218187ms]
Jun 16 17:09:47.137: INFO: Got endpoints: latency-svc-4s2pl [749.902328ms]
Jun 16 17:09:47.187: INFO: Got endpoints: latency-svc-pww69 [750.798975ms]
Jun 16 17:09:47.237: INFO: Got endpoints: latency-svc-xtkvx [749.881874ms]
Jun 16 17:09:47.286: INFO: Got endpoints: latency-svc-t74nz [749.891386ms]
Jun 16 17:09:47.337: INFO: Got endpoints: latency-svc-c7b4t [750.175643ms]
Jun 16 17:09:47.387: INFO: Got endpoints: latency-svc-8ns9w [750.183009ms]
Jun 16 17:09:47.436: INFO: Got endpoints: latency-svc-67bl7 [748.967589ms]
Jun 16 17:09:47.436: INFO: Latencies: [8.406117ms 16.011882ms 20.596934ms 25.180983ms 27.989666ms 32.613096ms 35.486006ms 38.65096ms 42.618627ms 48.739051ms 49.498184ms 51.605098ms 51.984478ms 52.369668ms 52.570043ms 52.743559ms 53.38336ms 53.685845ms 54.360844ms 55.315036ms 55.5931ms 57.753026ms 58.075872ms 58.448867ms 59.09581ms 59.238522ms 60.308338ms 60.853396ms 61.042978ms 61.369459ms 62.396158ms 81.67089ms 124.529249ms 171.749886ms 215.78551ms 263.090335ms 311.386233ms 354.275913ms 402.190425ms 448.049537ms 491.456946ms 537.188982ms 578.487357ms 628.888102ms 676.850487ms 717.038442ms 722.619192ms 739.840056ms 741.491639ms 741.801361ms 743.977475ms 745.855511ms 746.210653ms 746.619904ms 746.79367ms 747.085716ms 747.1522ms 747.570343ms 748.088994ms 748.089622ms 748.641108ms 748.716178ms 748.805354ms 748.853244ms 748.865204ms 748.886934ms 748.934041ms 748.963076ms 748.967589ms 748.968768ms 749.170379ms 749.196325ms 749.215457ms 749.237405ms 749.316469ms 749.320688ms 749.328001ms 749.343234ms 749.351185ms 749.377008ms 749.382928ms 749.400031ms 749.424099ms 749.456388ms 749.472366ms 749.473235ms 749.480562ms 749.483463ms 749.492325ms 749.519569ms 749.522881ms 749.538256ms 749.57731ms 749.587358ms 749.590693ms 749.60126ms 749.608913ms 749.62505ms 749.62705ms 749.632473ms 749.649553ms 749.686294ms 749.705293ms 749.706848ms 749.711942ms 749.730129ms 749.740115ms 749.759852ms 749.762008ms 749.775345ms 749.794504ms 749.798428ms 749.810344ms 749.842487ms 749.848546ms 749.853242ms 749.865925ms 749.874247ms 749.881874ms 749.891386ms 749.899208ms 749.902328ms 749.904601ms 749.905273ms 749.913993ms 749.918443ms 749.919601ms 749.927448ms 749.928827ms 749.933192ms 749.95864ms 749.968696ms 749.972157ms 749.976749ms 749.977411ms 749.983619ms 749.988347ms 750.001315ms 750.012557ms 750.027779ms 750.065841ms 750.100346ms 750.107268ms 750.108687ms 750.131883ms 750.132328ms 750.139619ms 750.161675ms 750.162362ms 750.171622ms 750.173589ms 750.174821ms 750.175643ms 750.179114ms 750.183009ms 750.218187ms 750.23088ms 750.240233ms 750.240456ms 750.254414ms 750.254825ms 750.281577ms 750.325172ms 750.332318ms 750.346298ms 750.349559ms 750.366089ms 750.381042ms 750.388151ms 750.43557ms 750.439996ms 750.463818ms 750.467448ms 750.502478ms 750.512941ms 750.643348ms 750.646716ms 750.705493ms 750.798975ms 750.866933ms 750.892039ms 750.965379ms 751.051082ms 751.145462ms 751.385636ms 751.416862ms 751.785509ms 751.790719ms 751.807913ms 752.522683ms 752.749351ms 752.792106ms 752.800817ms 753.142074ms 753.61469ms 753.683424ms 758.362232ms 758.363879ms 759.636873ms 782.637184ms]
Jun 16 17:09:47.436: INFO: 50 %ile: 749.649553ms
Jun 16 17:09:47.436: INFO: 90 %ile: 750.892039ms
Jun 16 17:09:47.436: INFO: 99 %ile: 759.636873ms
Jun 16 17:09:47.436: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:09:47.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1588" for this suite.
Jun 16 17:09:57.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:09:57.500: INFO: namespace svc-latency-1588 deletion completed in 10.061580492s

• [SLOW TEST:20.897 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:09:57.501: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4416
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jun 16 17:09:57.621: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
Jun 16 17:10:01.187: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:10:15.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4416" for this suite.
Jun 16 17:10:21.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:10:21.122: INFO: namespace crd-publish-openapi-4416 deletion completed in 6.054499695s

• [SLOW TEST:23.622 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:10:21.123: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-766
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:10:32.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-766" for this suite.
Jun 16 17:10:38.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:10:38.319: INFO: namespace resourcequota-766 deletion completed in 6.055813277s

• [SLOW TEST:17.196 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:10:38.319: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-6570
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 17:10:38.441: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jun 16 17:10:41.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-6570 create -f -'
Jun 16 17:10:42.317: INFO: stderr: ""
Jun 16 17:10:42.317: INFO: stdout: "e2e-test-crd-publish-openapi-8187-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 16 17:10:42.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-6570 delete e2e-test-crd-publish-openapi-8187-crds test-cr'
Jun 16 17:10:42.376: INFO: stderr: ""
Jun 16 17:10:42.376: INFO: stdout: "e2e-test-crd-publish-openapi-8187-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun 16 17:10:42.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-6570 apply -f -'
Jun 16 17:10:42.514: INFO: stderr: ""
Jun 16 17:10:42.514: INFO: stdout: "e2e-test-crd-publish-openapi-8187-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 16 17:10:42.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 --namespace=crd-publish-openapi-6570 delete e2e-test-crd-publish-openapi-8187-crds test-cr'
Jun 16 17:10:42.572: INFO: stderr: ""
Jun 16 17:10:42.572: INFO: stdout: "e2e-test-crd-publish-openapi-8187-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jun 16 17:10:42.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 explain e2e-test-crd-publish-openapi-8187-crds'
Jun 16 17:10:42.703: INFO: stderr: ""
Jun 16 17:10:42.703: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8187-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:10:46.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6570" for this suite.
Jun 16 17:10:52.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:10:52.307: INFO: namespace crd-publish-openapi-6570 deletion completed in 6.0550598s

• [SLOW TEST:13.988 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:10:52.307: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8311
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Jun 16 17:10:52.430: INFO: namespace kubectl-8311
Jun 16 17:10:52.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 create -f - --namespace=kubectl-8311'
Jun 16 17:10:52.619: INFO: stderr: ""
Jun 16 17:10:52.619: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 16 17:10:53.621: INFO: Selector matched 1 pods for map[app:redis]
Jun 16 17:10:53.621: INFO: Found 0 / 1
Jun 16 17:10:54.621: INFO: Selector matched 1 pods for map[app:redis]
Jun 16 17:10:54.621: INFO: Found 1 / 1
Jun 16 17:10:54.621: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 16 17:10:54.622: INFO: Selector matched 1 pods for map[app:redis]
Jun 16 17:10:54.622: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 16 17:10:54.622: INFO: wait on redis-master startup in kubectl-8311 
Jun 16 17:10:54.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 logs redis-master-fmtcn redis-master --namespace=kubectl-8311'
Jun 16 17:10:54.698: INFO: stderr: ""
Jun 16 17:10:54.698: INFO: stdout: "1:C 16 Jun 2020 17:10:53.317 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 16 Jun 2020 17:10:53.317 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 16 Jun 2020 17:10:53.317 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 16 Jun 2020 17:10:53.319 * Running mode=standalone, port=6379.\n1:M 16 Jun 2020 17:10:53.319 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 16 Jun 2020 17:10:53.319 # Server initialized\n1:M 16 Jun 2020 17:10:53.319 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 16 Jun 2020 17:10:53.319 * Ready to accept connections\n"
STEP: exposing RC
Jun 16 17:10:54.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8311'
Jun 16 17:10:54.767: INFO: stderr: ""
Jun 16 17:10:54.767: INFO: stdout: "service/rm2 exposed\n"
Jun 16 17:10:54.770: INFO: Service rm2 in namespace kubectl-8311 found.
STEP: exposing service
Jun 16 17:10:56.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8311'
Jun 16 17:10:56.838: INFO: stderr: ""
Jun 16 17:10:56.838: INFO: stdout: "service/rm3 exposed\n"
Jun 16 17:10:56.840: INFO: Service rm3 in namespace kubectl-8311 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:10:58.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8311" for this suite.
Jun 16 17:11:26.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:11:26.901: INFO: namespace kubectl-8311 deletion completed in 28.056004474s

• [SLOW TEST:34.594 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:11:26.901: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8185
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:11:27.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8185" for this suite.
Jun 16 17:11:33.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:11:33.081: INFO: namespace custom-resource-definition-8185 deletion completed in 6.053925226s

• [SLOW TEST:6.179 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:11:33.081: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3302
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-3302
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-3302
Jun 16 17:11:33.209: INFO: Found 0 stateful pods, waiting for 1
Jun 16 17:11:43.211: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Jun 16 17:11:43.220: INFO: Deleting all statefulset in ns statefulset-3302
Jun 16 17:11:43.221: INFO: Scaling statefulset ss to 0
Jun 16 17:12:03.239: INFO: Waiting for statefulset status.replicas updated to 0
Jun 16 17:12:03.241: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:12:03.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3302" for this suite.
Jun 16 17:12:09.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:12:09.307: INFO: namespace statefulset-3302 deletion completed in 6.058614494s

• [SLOW TEST:36.226 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:12:09.307: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5583
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-69a1a7c6-67b5-402f-b80b-bf0d6780f441
STEP: Creating a pod to test consume secrets
Jun 16 17:12:09.435: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c24d5ef7-98e0-4afd-99fc-861b42391248" in namespace "projected-5583" to be "success or failure"
Jun 16 17:12:09.438: INFO: Pod "pod-projected-secrets-c24d5ef7-98e0-4afd-99fc-861b42391248": Phase="Pending", Reason="", readiness=false. Elapsed: 2.887519ms
Jun 16 17:12:11.440: INFO: Pod "pod-projected-secrets-c24d5ef7-98e0-4afd-99fc-861b42391248": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005029739s
STEP: Saw pod success
Jun 16 17:12:11.440: INFO: Pod "pod-projected-secrets-c24d5ef7-98e0-4afd-99fc-861b42391248" satisfied condition "success or failure"
Jun 16 17:12:11.442: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-projected-secrets-c24d5ef7-98e0-4afd-99fc-861b42391248 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 16 17:12:11.452: INFO: Waiting for pod pod-projected-secrets-c24d5ef7-98e0-4afd-99fc-861b42391248 to disappear
Jun 16 17:12:11.453: INFO: Pod pod-projected-secrets-c24d5ef7-98e0-4afd-99fc-861b42391248 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:12:11.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5583" for this suite.
Jun 16 17:12:17.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:12:17.509: INFO: namespace projected-5583 deletion completed in 6.054361671s

• [SLOW TEST:8.202 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:12:17.510: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7232
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:12:33.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7232" for this suite.
Jun 16 17:12:39.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:12:39.708: INFO: namespace resourcequota-7232 deletion completed in 6.057177505s

• [SLOW TEST:22.198 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:12:39.708: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5098
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 16 17:12:39.833: INFO: Waiting up to 5m0s for pod "pod-128b23c5-9dd7-42cc-a857-751ad135fb70" in namespace "emptydir-5098" to be "success or failure"
Jun 16 17:12:39.835: INFO: Pod "pod-128b23c5-9dd7-42cc-a857-751ad135fb70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020371ms
Jun 16 17:12:41.837: INFO: Pod "pod-128b23c5-9dd7-42cc-a857-751ad135fb70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004121674s
STEP: Saw pod success
Jun 16 17:12:41.837: INFO: Pod "pod-128b23c5-9dd7-42cc-a857-751ad135fb70" satisfied condition "success or failure"
Jun 16 17:12:41.838: INFO: Trying to get logs from node ip-172-19-65-6.eu-west-1.compute.internal pod pod-128b23c5-9dd7-42cc-a857-751ad135fb70 container test-container: <nil>
STEP: delete the pod
Jun 16 17:12:41.848: INFO: Waiting for pod pod-128b23c5-9dd7-42cc-a857-751ad135fb70 to disappear
Jun 16 17:12:41.850: INFO: Pod pod-128b23c5-9dd7-42cc-a857-751ad135fb70 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:12:41.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5098" for this suite.
Jun 16 17:12:47.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:12:47.906: INFO: namespace emptydir-5098 deletion completed in 6.053153698s

• [SLOW TEST:8.198 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:12:47.906: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2532
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 16 17:12:48.039: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 17:12:48.040: INFO: Number of nodes with available pods: 0
Jun 16 17:12:48.040: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 17:12:49.043: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 17:12:49.045: INFO: Number of nodes with available pods: 0
Jun 16 17:12:49.045: INFO: Node ip-172-19-65-122.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 17:12:50.043: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 17:12:50.045: INFO: Number of nodes with available pods: 3
Jun 16 17:12:50.045: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 16 17:12:50.056: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 17:12:50.062: INFO: Number of nodes with available pods: 2
Jun 16 17:12:50.062: INFO: Node ip-172-19-65-219.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 17:12:51.065: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 17:12:51.066: INFO: Number of nodes with available pods: 2
Jun 16 17:12:51.066: INFO: Node ip-172-19-65-219.eu-west-1.compute.internal is running more than one daemon pod
Jun 16 17:12:52.065: INFO: DaemonSet pods can't tolerate node ip-172-19-67-38.eu-west-1.compute.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 16 17:12:52.067: INFO: Number of nodes with available pods: 3
Jun 16 17:12:52.067: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2532, will wait for the garbage collector to delete the pods
Jun 16 17:12:52.124: INFO: Deleting DaemonSet.extensions daemon-set took: 3.181603ms
Jun 16 17:12:52.224: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.161557ms
Jun 16 17:12:56.826: INFO: Number of nodes with available pods: 0
Jun 16 17:12:56.826: INFO: Number of running nodes: 0, number of available pods: 0
Jun 16 17:12:56.827: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2532/daemonsets","resourceVersion":"36012"},"items":null}

Jun 16 17:12:56.828: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2532/pods","resourceVersion":"36012"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:12:56.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2532" for this suite.
Jun 16 17:13:02.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:13:02.891: INFO: namespace daemonsets-2532 deletion completed in 6.054898882s

• [SLOW TEST:14.985 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:13:02.891: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-4084
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Jun 16 17:13:03.014: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 16 17:14:03.038: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Jun 16 17:14:03.039: INFO: Starting informer...
STEP: Starting pods...
Jun 16 17:14:03.250: INFO: Pod1 is running on ip-172-19-65-6.eu-west-1.compute.internal. Tainting Node
Jun 16 17:14:05.463: INFO: Pod2 is running on ip-172-19-65-6.eu-west-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jun 16 17:14:16.814: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun 16 17:14:36.814: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:14:36.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-4084" for this suite.
Jun 16 17:14:42.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:14:42.893: INFO: namespace taint-multiple-pods-4084 deletion completed in 6.068594735s

• [SLOW TEST:100.002 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:14:42.893: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3792
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3792.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3792.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3792.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3792.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 16 17:14:45.048: INFO: File wheezy_udp@dns-test-service-3.dns-3792.svc.cluster.local from pod  dns-3792/dns-test-20317e2f-c426-4117-8fff-078f1a958206 contains '' instead of 'foo.example.com.'
Jun 16 17:14:45.050: INFO: File jessie_udp@dns-test-service-3.dns-3792.svc.cluster.local from pod  dns-3792/dns-test-20317e2f-c426-4117-8fff-078f1a958206 contains '' instead of 'foo.example.com.'
Jun 16 17:14:45.050: INFO: Lookups using dns-3792/dns-test-20317e2f-c426-4117-8fff-078f1a958206 failed for: [wheezy_udp@dns-test-service-3.dns-3792.svc.cluster.local jessie_udp@dns-test-service-3.dns-3792.svc.cluster.local]

Jun 16 17:14:50.054: INFO: DNS probes using dns-test-20317e2f-c426-4117-8fff-078f1a958206 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3792.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3792.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3792.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3792.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 16 17:14:52.080: INFO: File wheezy_udp@dns-test-service-3.dns-3792.svc.cluster.local from pod  dns-3792/dns-test-e83f34b8-4024-4ee8-994e-f5e8b380e655 contains '' instead of 'bar.example.com.'
Jun 16 17:14:52.082: INFO: File jessie_udp@dns-test-service-3.dns-3792.svc.cluster.local from pod  dns-3792/dns-test-e83f34b8-4024-4ee8-994e-f5e8b380e655 contains '' instead of 'bar.example.com.'
Jun 16 17:14:52.082: INFO: Lookups using dns-3792/dns-test-e83f34b8-4024-4ee8-994e-f5e8b380e655 failed for: [wheezy_udp@dns-test-service-3.dns-3792.svc.cluster.local jessie_udp@dns-test-service-3.dns-3792.svc.cluster.local]

Jun 16 17:14:57.087: INFO: DNS probes using dns-test-e83f34b8-4024-4ee8-994e-f5e8b380e655 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3792.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3792.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3792.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3792.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 16 17:14:59.119: INFO: File wheezy_udp@dns-test-service-3.dns-3792.svc.cluster.local from pod  dns-3792/dns-test-a63d3919-467e-4491-9d24-99172252bfec contains '' instead of '172.31.104.26'
Jun 16 17:14:59.121: INFO: File jessie_udp@dns-test-service-3.dns-3792.svc.cluster.local from pod  dns-3792/dns-test-a63d3919-467e-4491-9d24-99172252bfec contains '' instead of '172.31.104.26'
Jun 16 17:14:59.121: INFO: Lookups using dns-3792/dns-test-a63d3919-467e-4491-9d24-99172252bfec failed for: [wheezy_udp@dns-test-service-3.dns-3792.svc.cluster.local jessie_udp@dns-test-service-3.dns-3792.svc.cluster.local]

Jun 16 17:15:04.126: INFO: DNS probes using dns-test-a63d3919-467e-4491-9d24-99172252bfec succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:15:04.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3792" for this suite.
Jun 16 17:15:10.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:15:10.204: INFO: namespace dns-3792 deletion completed in 6.059117903s

• [SLOW TEST:27.311 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:15:10.204: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9090
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9090.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9090.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9090.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9090.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9090.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9090.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 16 17:15:12.346: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9090/dns-test-73eabd12-62a5-4bd9-9ba7-9030e582c99a: the server could not find the requested resource (get pods dns-test-73eabd12-62a5-4bd9-9ba7-9030e582c99a)
Jun 16 17:15:12.347: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9090/dns-test-73eabd12-62a5-4bd9-9ba7-9030e582c99a: the server could not find the requested resource (get pods dns-test-73eabd12-62a5-4bd9-9ba7-9030e582c99a)
Jun 16 17:15:12.349: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.dns-9090.svc.cluster.local from pod dns-9090/dns-test-73eabd12-62a5-4bd9-9ba7-9030e582c99a: the server could not find the requested resource (get pods dns-test-73eabd12-62a5-4bd9-9ba7-9030e582c99a)
Jun 16 17:15:12.351: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-9090/dns-test-73eabd12-62a5-4bd9-9ba7-9030e582c99a: the server could not find the requested resource (get pods dns-test-73eabd12-62a5-4bd9-9ba7-9030e582c99a)
Jun 16 17:15:12.352: INFO: Unable to read jessie_udp@PodARecord from pod dns-9090/dns-test-73eabd12-62a5-4bd9-9ba7-9030e582c99a: the server could not find the requested resource (get pods dns-test-73eabd12-62a5-4bd9-9ba7-9030e582c99a)
Jun 16 17:15:12.354: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9090/dns-test-73eabd12-62a5-4bd9-9ba7-9030e582c99a: the server could not find the requested resource (get pods dns-test-73eabd12-62a5-4bd9-9ba7-9030e582c99a)
Jun 16 17:15:12.354: INFO: Lookups using dns-9090/dns-test-73eabd12-62a5-4bd9-9ba7-9030e582c99a failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_hosts@dns-querier-1.dns-test-service.dns-9090.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Jun 16 17:15:17.367: INFO: DNS probes using dns-9090/dns-test-73eabd12-62a5-4bd9-9ba7-9030e582c99a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:15:17.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9090" for this suite.
Jun 16 17:15:23.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:15:23.431: INFO: namespace dns-9090 deletion completed in 6.054964952s

• [SLOW TEST:13.227 seconds]
[sig-network] DNS
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:15:23.431: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4669
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Jun 16 17:15:23.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-4669'
Jun 16 17:15:23.616: INFO: stderr: ""
Jun 16 17:15:23.616: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jun 16 17:15:28.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 get pod e2e-test-httpd-pod --namespace=kubectl-4669 -o json'
Jun 16 17:15:28.722: INFO: stderr: ""
Jun 16 17:15:28.722: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"cert-exporter-psp\"\n        },\n        \"creationTimestamp\": \"2020-06-16T17:15:23Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4669\",\n        \"resourceVersion\": \"36718\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-4669/pods/e2e-test-httpd-pod\",\n        \"uid\": \"f8ab6c05-9a94-4c33-aedd-7c748732abb7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-bdhsd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-19-65-6.eu-west-1.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-bdhsd\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-bdhsd\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-16T17:15:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-16T17:15:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-16T17:15:25Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-06-16T17:15:23Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://296aa0a8a3b116848f753eb7ef39a464d70e99261cc8152aa99297aa014fceac\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-06-16T17:15:24Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.19.65.6\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.18.141.209\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.18.141.209\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-06-16T17:15:23Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 16 17:15:28.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 replace -f - --namespace=kubectl-4669'
Jun 16 17:15:28.858: INFO: stderr: ""
Jun 16 17:15:28.858: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Jun 16 17:15:28.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 delete pods e2e-test-httpd-pod --namespace=kubectl-4669'
Jun 16 17:15:36.815: INFO: stderr: ""
Jun 16 17:15:36.815: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:15:36.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4669" for this suite.
Jun 16 17:15:42.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:15:42.894: INFO: namespace kubectl-4669 deletion completed in 6.0763365s

• [SLOW TEST:19.462 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:15:42.894: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5404
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5404
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-5404
STEP: creating replication controller externalsvc in namespace services-5404
I0616 17:15:43.034448      24 runners.go:184] Created replication controller with name: externalsvc, namespace: services-5404, replica count: 2
I0616 17:15:46.084698      24 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jun 16 17:15:46.093: INFO: Creating new exec pod
Jun 16 17:15:48.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-790258633 exec --namespace=services-5404 execpodjs2wq -- /bin/sh -x -c nslookup nodeport-service'
Jun 16 17:15:48.256: INFO: stderr: "+ nslookup nodeport-service\n"
Jun 16 17:15:48.256: INFO: stdout: "Server:\t\t172.31.0.10\nAddress:\t172.31.0.10#53\n\nnodeport-service.services-5404.svc.cluster.local\tcanonical name = externalsvc.services-5404.svc.cluster.local.\nName:\texternalsvc.services-5404.svc.cluster.local\nAddress: 172.31.220.144\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5404, will wait for the garbage collector to delete the pods
Jun 16 17:15:48.311: INFO: Deleting ReplicationController externalsvc took: 2.996254ms
Jun 16 17:15:48.412: INFO: Terminating ReplicationController externalsvc pods took: 100.173115ms
Jun 16 17:16:03.521: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:16:03.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5404" for this suite.
Jun 16 17:16:09.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:16:09.593: INFO: namespace services-5404 deletion completed in 6.057801564s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:26.699 seconds]
[sig-network] Services
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Jun 16 17:16:09.593: INFO: >>> kubeConfig: /tmp/kubeconfig-790258633
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8618
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-93ed5ee7-7930-41fc-bb6e-25c9fae708e7
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-93ed5ee7-7930-41fc-bb6e-25c9fae708e7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Jun 16 17:17:15.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8618" for this suite.
Jun 16 17:17:27.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 16 17:17:27.960: INFO: namespace projected-8618 deletion completed in 12.057537565s

• [SLOW TEST:78.366 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.9-beta.0.49+25599b5adea930/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSJun 16 17:17:27.960: INFO: Running AfterSuite actions on all nodes
Jun 16 17:17:27.960: INFO: Running AfterSuite actions on node 1
Jun 16 17:17:27.960: INFO: Skipping dumping logs from cluster

Ran 276 of 4732 Specs in 6625.887 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4456 Skipped
PASS

Ginkgo ran 1 suite in 1h50m27.055518957s
Test Suite Passed
