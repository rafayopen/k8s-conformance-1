I0323 14:36:22.895815      22 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-012311840
I0323 14:36:22.896239      22 e2e.go:92] Starting e2e run "571c74f6-e0ed-4913-8ac9-e1f6a3e1baf7" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1584974180 - Will randomize all specs
Will run 276 of 4897 specs

Mar 23 14:36:22.920: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 14:36:22.924: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 23 14:36:22.967: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 23 14:36:23.129: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 23 14:36:23.129: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Mar 23 14:36:23.129: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 23 14:36:23.155: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Mar 23 14:36:23.155: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Mar 23 14:36:23.155: INFO: e2e test version: v1.16.2
Mar 23 14:36:23.157: INFO: kube-apiserver version: v1.16.2
Mar 23 14:36:23.158: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 14:36:23.179: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:36:23.180: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
Mar 23 14:36:23.568: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-41164924-181f-409e-8399-6ca2cff73cc1
STEP: Creating a pod to test consume configMaps
Mar 23 14:36:23.714: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7b26ec73-06e1-44ed-9e09-0718a404a71c" in namespace "projected-3781" to be "success or failure"
Mar 23 14:36:23.738: INFO: Pod "pod-projected-configmaps-7b26ec73-06e1-44ed-9e09-0718a404a71c": Phase="Pending", Reason="", readiness=false. Elapsed: 23.872077ms
Mar 23 14:36:25.765: INFO: Pod "pod-projected-configmaps-7b26ec73-06e1-44ed-9e09-0718a404a71c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050630871s
Mar 23 14:36:27.787: INFO: Pod "pod-projected-configmaps-7b26ec73-06e1-44ed-9e09-0718a404a71c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072339816s
STEP: Saw pod success
Mar 23 14:36:27.787: INFO: Pod "pod-projected-configmaps-7b26ec73-06e1-44ed-9e09-0718a404a71c" satisfied condition "success or failure"
Mar 23 14:36:27.820: INFO: Trying to get logs from node 10.241.69.181 pod pod-projected-configmaps-7b26ec73-06e1-44ed-9e09-0718a404a71c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 14:36:28.017: INFO: Waiting for pod pod-projected-configmaps-7b26ec73-06e1-44ed-9e09-0718a404a71c to disappear
Mar 23 14:36:28.035: INFO: Pod pod-projected-configmaps-7b26ec73-06e1-44ed-9e09-0718a404a71c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:36:28.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3781" for this suite.
Mar 23 14:36:40.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:36:43.052: INFO: namespace projected-3781 deletion completed in 14.993202458s

• [SLOW TEST:19.873 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:36:43.052: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 14:36:43.480: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6cd5374-e2e2-4503-99b2-54c40986a2ad" in namespace "downward-api-6252" to be "success or failure"
Mar 23 14:36:43.522: INFO: Pod "downwardapi-volume-e6cd5374-e2e2-4503-99b2-54c40986a2ad": Phase="Pending", Reason="", readiness=false. Elapsed: 42.195808ms
Mar 23 14:36:45.561: INFO: Pod "downwardapi-volume-e6cd5374-e2e2-4503-99b2-54c40986a2ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081581552s
Mar 23 14:36:47.590: INFO: Pod "downwardapi-volume-e6cd5374-e2e2-4503-99b2-54c40986a2ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.110088318s
STEP: Saw pod success
Mar 23 14:36:47.590: INFO: Pod "downwardapi-volume-e6cd5374-e2e2-4503-99b2-54c40986a2ad" satisfied condition "success or failure"
Mar 23 14:36:47.611: INFO: Trying to get logs from node 10.241.69.184 pod downwardapi-volume-e6cd5374-e2e2-4503-99b2-54c40986a2ad container client-container: <nil>
STEP: delete the pod
Mar 23 14:36:47.796: INFO: Waiting for pod downwardapi-volume-e6cd5374-e2e2-4503-99b2-54c40986a2ad to disappear
Mar 23 14:36:47.831: INFO: Pod downwardapi-volume-e6cd5374-e2e2-4503-99b2-54c40986a2ad no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:36:47.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6252" for this suite.
Mar 23 14:37:00.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:37:03.294: INFO: namespace downward-api-6252 deletion completed in 15.437372358s

• [SLOW TEST:20.241 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:37:03.294: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Mar 23 14:37:04.679: INFO: mount service account has no secret references
STEP: getting the auto-created API token
Mar 23 14:37:05.679: INFO: created pod pod-service-account-defaultsa
Mar 23 14:37:05.679: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 23 14:37:05.994: INFO: created pod pod-service-account-mountsa
Mar 23 14:37:05.994: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 23 14:37:06.075: INFO: created pod pod-service-account-nomountsa
Mar 23 14:37:06.081: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 23 14:37:06.208: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 23 14:37:06.208: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 23 14:37:06.301: INFO: created pod pod-service-account-mountsa-mountspec
Mar 23 14:37:06.301: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 23 14:37:06.472: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 23 14:37:06.472: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 23 14:37:06.673: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 23 14:37:06.673: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 23 14:37:06.839: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 23 14:37:06.839: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 23 14:37:06.961: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 23 14:37:06.961: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:37:06.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9834" for this suite.
Mar 23 14:37:21.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:37:25.071: INFO: namespace svcaccounts-9834 deletion completed in 18.089412799s

• [SLOW TEST:21.777 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:37:25.071: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-ae853420-5083-4946-8083-675d9c264c27 in namespace container-probe-9273
Mar 23 14:37:33.820: INFO: Started pod liveness-ae853420-5083-4946-8083-675d9c264c27 in namespace container-probe-9273
STEP: checking the pod's current state and verifying that restartCount is present
Mar 23 14:37:33.858: INFO: Initial restart count of pod liveness-ae853420-5083-4946-8083-675d9c264c27 is 0
Mar 23 14:37:54.156: INFO: Restart count of pod container-probe-9273/liveness-ae853420-5083-4946-8083-675d9c264c27 is now 1 (20.297431179s elapsed)
Mar 23 14:38:12.523: INFO: Restart count of pod container-probe-9273/liveness-ae853420-5083-4946-8083-675d9c264c27 is now 2 (38.665004185s elapsed)
Mar 23 14:38:32.903: INFO: Restart count of pod container-probe-9273/liveness-ae853420-5083-4946-8083-675d9c264c27 is now 3 (59.044873939s elapsed)
Mar 23 14:38:53.263: INFO: Restart count of pod container-probe-9273/liveness-ae853420-5083-4946-8083-675d9c264c27 is now 4 (1m19.404923076s elapsed)
Mar 23 14:39:54.237: INFO: Restart count of pod container-probe-9273/liveness-ae853420-5083-4946-8083-675d9c264c27 is now 5 (2m20.378988424s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:39:54.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9273" for this suite.
Mar 23 14:40:10.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:40:14.583: INFO: namespace container-probe-9273 deletion completed in 20.211588886s

• [SLOW TEST:169.512 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:40:14.587: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 23 14:40:26.161: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4570 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 14:40:26.161: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 14:40:27.013: INFO: Exec stderr: ""
Mar 23 14:40:27.013: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4570 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 14:40:27.013: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 14:40:27.547: INFO: Exec stderr: ""
Mar 23 14:40:27.548: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4570 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 14:40:27.548: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 14:40:27.907: INFO: Exec stderr: ""
Mar 23 14:40:27.907: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4570 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 14:40:27.907: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 14:40:28.570: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 23 14:40:28.571: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4570 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 14:40:28.571: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 14:40:28.966: INFO: Exec stderr: ""
Mar 23 14:40:28.967: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4570 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 14:40:28.967: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 14:40:29.283: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 23 14:40:29.284: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4570 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 14:40:29.284: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 14:40:29.741: INFO: Exec stderr: ""
Mar 23 14:40:29.741: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4570 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 14:40:29.741: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 14:40:30.102: INFO: Exec stderr: ""
Mar 23 14:40:30.102: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4570 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 14:40:30.102: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 14:40:32.506: INFO: Exec stderr: ""
Mar 23 14:40:32.506: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4570 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 14:40:32.506: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 14:40:33.035: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:40:33.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4570" for this suite.
Mar 23 14:41:23.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:41:26.441: INFO: namespace e2e-kubelet-etc-hosts-4570 deletion completed in 53.372388161s

• [SLOW TEST:71.855 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:41:26.441: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar 23 14:41:28.026: INFO: Waiting up to 5m0s for pod "downward-api-283b901b-03b9-4eb0-bab5-6aa5397e5d78" in namespace "downward-api-6688" to be "success or failure"
Mar 23 14:41:28.042: INFO: Pod "downward-api-283b901b-03b9-4eb0-bab5-6aa5397e5d78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.12155ms
Mar 23 14:41:30.071: INFO: Pod "downward-api-283b901b-03b9-4eb0-bab5-6aa5397e5d78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045663176s
Mar 23 14:41:32.093: INFO: Pod "downward-api-283b901b-03b9-4eb0-bab5-6aa5397e5d78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067568512s
Mar 23 14:41:34.157: INFO: Pod "downward-api-283b901b-03b9-4eb0-bab5-6aa5397e5d78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.131291498s
STEP: Saw pod success
Mar 23 14:41:34.157: INFO: Pod "downward-api-283b901b-03b9-4eb0-bab5-6aa5397e5d78" satisfied condition "success or failure"
Mar 23 14:41:34.191: INFO: Trying to get logs from node 10.241.69.184 pod downward-api-283b901b-03b9-4eb0-bab5-6aa5397e5d78 container dapi-container: <nil>
STEP: delete the pod
Mar 23 14:41:34.450: INFO: Waiting for pod downward-api-283b901b-03b9-4eb0-bab5-6aa5397e5d78 to disappear
Mar 23 14:41:34.478: INFO: Pod downward-api-283b901b-03b9-4eb0-bab5-6aa5397e5d78 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:41:34.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6688" for this suite.
Mar 23 14:41:46.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:41:50.454: INFO: namespace downward-api-6688 deletion completed in 15.945900504s

• [SLOW TEST:24.013 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:41:50.457: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 14:41:51.093: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-32ba0fc6-0391-4091-89fb-062b91c018d1
STEP: Creating secret with name s-test-opt-upd-b4c00539-c28b-4fdd-a6b2-371f50edf550
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-32ba0fc6-0391-4091-89fb-062b91c018d1
STEP: Updating secret s-test-opt-upd-b4c00539-c28b-4fdd-a6b2-371f50edf550
STEP: Creating secret with name s-test-opt-create-1630b376-205b-45e3-8c0b-f9d6f4a218f0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:43:11.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6572" for this suite.
Mar 23 14:43:33.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:43:36.563: INFO: namespace secrets-6572 deletion completed in 25.206415185s

• [SLOW TEST:106.106 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:43:36.563: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 14:43:38.207: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 14:43:40.320: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720571418, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720571418, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720571418, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720571418, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 14:43:43.567: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Mar 23 14:43:47.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 attach --namespace=webhook-5617 to-be-attached-pod -i -c=container1'
Mar 23 14:43:48.290: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:43:48.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5617" for this suite.
Mar 23 14:44:30.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:44:33.607: INFO: namespace webhook-5617 deletion completed in 45.245006781s
STEP: Destroying namespace "webhook-5617-markers" for this suite.
Mar 23 14:44:43.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:44:46.339: INFO: namespace webhook-5617-markers deletion completed in 12.732065352s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:69.891 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:44:46.454: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 14:44:46.774: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Mar 23 14:44:47.926: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:44:48.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3911" for this suite.
Mar 23 14:45:03.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:45:07.406: INFO: namespace replication-controller-3911 deletion completed in 18.407228474s

• [SLOW TEST:20.953 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:45:07.407: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 14:45:08.117: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c56450a-d1bf-453c-ac8f-57893f66469c" in namespace "projected-294" to be "success or failure"
Mar 23 14:45:08.149: INFO: Pod "downwardapi-volume-0c56450a-d1bf-453c-ac8f-57893f66469c": Phase="Pending", Reason="", readiness=false. Elapsed: 31.48179ms
Mar 23 14:45:10.178: INFO: Pod "downwardapi-volume-0c56450a-d1bf-453c-ac8f-57893f66469c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060784635s
Mar 23 14:45:12.201: INFO: Pod "downwardapi-volume-0c56450a-d1bf-453c-ac8f-57893f66469c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083084821s
STEP: Saw pod success
Mar 23 14:45:12.201: INFO: Pod "downwardapi-volume-0c56450a-d1bf-453c-ac8f-57893f66469c" satisfied condition "success or failure"
Mar 23 14:45:12.224: INFO: Trying to get logs from node 10.241.69.184 pod downwardapi-volume-0c56450a-d1bf-453c-ac8f-57893f66469c container client-container: <nil>
STEP: delete the pod
Mar 23 14:45:12.662: INFO: Waiting for pod downwardapi-volume-0c56450a-d1bf-453c-ac8f-57893f66469c to disappear
Mar 23 14:45:12.686: INFO: Pod downwardapi-volume-0c56450a-d1bf-453c-ac8f-57893f66469c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:45:12.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-294" for this suite.
Mar 23 14:45:28.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:45:32.092: INFO: namespace projected-294 deletion completed in 19.378939585s

• [SLOW TEST:24.685 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:45:32.092: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:45:44.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4977" for this suite.
Mar 23 14:45:58.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:46:01.644: INFO: namespace resourcequota-4977 deletion completed in 17.338786382s

• [SLOW TEST:29.552 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:46:01.644: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Mar 23 14:46:02.310: INFO: Waiting up to 5m0s for pod "var-expansion-735143cb-9712-4e1f-bad1-a08aae5a6579" in namespace "var-expansion-985" to be "success or failure"
Mar 23 14:46:02.336: INFO: Pod "var-expansion-735143cb-9712-4e1f-bad1-a08aae5a6579": Phase="Pending", Reason="", readiness=false. Elapsed: 25.178078ms
Mar 23 14:46:04.356: INFO: Pod "var-expansion-735143cb-9712-4e1f-bad1-a08aae5a6579": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04517721s
Mar 23 14:46:06.380: INFO: Pod "var-expansion-735143cb-9712-4e1f-bad1-a08aae5a6579": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069857023s
Mar 23 14:46:08.396: INFO: Pod "var-expansion-735143cb-9712-4e1f-bad1-a08aae5a6579": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.085819248s
STEP: Saw pod success
Mar 23 14:46:08.396: INFO: Pod "var-expansion-735143cb-9712-4e1f-bad1-a08aae5a6579" satisfied condition "success or failure"
Mar 23 14:46:08.410: INFO: Trying to get logs from node 10.241.69.181 pod var-expansion-735143cb-9712-4e1f-bad1-a08aae5a6579 container dapi-container: <nil>
STEP: delete the pod
Mar 23 14:46:08.566: INFO: Waiting for pod var-expansion-735143cb-9712-4e1f-bad1-a08aae5a6579 to disappear
Mar 23 14:46:08.582: INFO: Pod var-expansion-735143cb-9712-4e1f-bad1-a08aae5a6579 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:46:08.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-985" for this suite.
Mar 23 14:46:18.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:46:21.684: INFO: namespace var-expansion-985 deletion completed in 13.077000573s

• [SLOW TEST:20.039 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:46:21.684: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 23 14:46:22.336: INFO: Waiting up to 5m0s for pod "pod-ccbe34b9-b35f-4213-81db-05d20d1a46d5" in namespace "emptydir-4124" to be "success or failure"
Mar 23 14:46:22.368: INFO: Pod "pod-ccbe34b9-b35f-4213-81db-05d20d1a46d5": Phase="Pending", Reason="", readiness=false. Elapsed: 31.494843ms
Mar 23 14:46:24.401: INFO: Pod "pod-ccbe34b9-b35f-4213-81db-05d20d1a46d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064990206s
Mar 23 14:46:26.477: INFO: Pod "pod-ccbe34b9-b35f-4213-81db-05d20d1a46d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.141092262s
STEP: Saw pod success
Mar 23 14:46:26.477: INFO: Pod "pod-ccbe34b9-b35f-4213-81db-05d20d1a46d5" satisfied condition "success or failure"
Mar 23 14:46:26.504: INFO: Trying to get logs from node 10.241.69.181 pod pod-ccbe34b9-b35f-4213-81db-05d20d1a46d5 container test-container: <nil>
STEP: delete the pod
Mar 23 14:46:26.780: INFO: Waiting for pod pod-ccbe34b9-b35f-4213-81db-05d20d1a46d5 to disappear
Mar 23 14:46:26.826: INFO: Pod pod-ccbe34b9-b35f-4213-81db-05d20d1a46d5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:46:26.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4124" for this suite.
Mar 23 14:46:41.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:46:44.054: INFO: namespace emptydir-4124 deletion completed in 17.180181884s

• [SLOW TEST:22.371 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:46:44.056: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 14:46:44.843: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0045f797-b79f-4ace-a313-e7f16dbab738" in namespace "projected-2479" to be "success or failure"
Mar 23 14:46:44.891: INFO: Pod "downwardapi-volume-0045f797-b79f-4ace-a313-e7f16dbab738": Phase="Pending", Reason="", readiness=false. Elapsed: 47.957102ms
Mar 23 14:46:46.915: INFO: Pod "downwardapi-volume-0045f797-b79f-4ace-a313-e7f16dbab738": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.071609825s
STEP: Saw pod success
Mar 23 14:46:46.915: INFO: Pod "downwardapi-volume-0045f797-b79f-4ace-a313-e7f16dbab738" satisfied condition "success or failure"
Mar 23 14:46:46.944: INFO: Trying to get logs from node 10.241.69.181 pod downwardapi-volume-0045f797-b79f-4ace-a313-e7f16dbab738 container client-container: <nil>
STEP: delete the pod
Mar 23 14:46:47.096: INFO: Waiting for pod downwardapi-volume-0045f797-b79f-4ace-a313-e7f16dbab738 to disappear
Mar 23 14:46:47.118: INFO: Pod downwardapi-volume-0045f797-b79f-4ace-a313-e7f16dbab738 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:46:47.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2479" for this suite.
Mar 23 14:47:03.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:47:07.037: INFO: namespace projected-2479 deletion completed in 19.894617074s

• [SLOW TEST:22.981 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:47:07.037: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 14:47:08.644: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 14:47:11.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720571628, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720571628, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720571628, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720571628, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 14:47:14.342: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 14:47:14.364: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3866-crds.webhook.example.com via the AdmissionRegistration API
Mar 23 14:47:14.587: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:47:15.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7773" for this suite.
Mar 23 14:47:27.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:47:30.941: INFO: namespace webhook-7773 deletion completed in 15.062806735s
STEP: Destroying namespace "webhook-7773-markers" for this suite.
Mar 23 14:47:43.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:47:45.965: INFO: namespace webhook-7773-markers deletion completed in 15.02425968s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:39.059 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:47:46.097: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-t2p2
STEP: Creating a pod to test atomic-volume-subpath
Mar 23 14:47:46.705: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-t2p2" in namespace "subpath-130" to be "success or failure"
Mar 23 14:47:46.733: INFO: Pod "pod-subpath-test-secret-t2p2": Phase="Pending", Reason="", readiness=false. Elapsed: 28.26573ms
Mar 23 14:47:48.804: INFO: Pod "pod-subpath-test-secret-t2p2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.098777284s
Mar 23 14:47:50.836: INFO: Pod "pod-subpath-test-secret-t2p2": Phase="Running", Reason="", readiness=true. Elapsed: 4.130859384s
Mar 23 14:47:52.857: INFO: Pod "pod-subpath-test-secret-t2p2": Phase="Running", Reason="", readiness=true. Elapsed: 6.151731966s
Mar 23 14:47:54.890: INFO: Pod "pod-subpath-test-secret-t2p2": Phase="Running", Reason="", readiness=true. Elapsed: 8.185034386s
Mar 23 14:47:56.912: INFO: Pod "pod-subpath-test-secret-t2p2": Phase="Running", Reason="", readiness=true. Elapsed: 10.207339974s
Mar 23 14:47:58.932: INFO: Pod "pod-subpath-test-secret-t2p2": Phase="Running", Reason="", readiness=true. Elapsed: 12.226511447s
Mar 23 14:48:00.949: INFO: Pod "pod-subpath-test-secret-t2p2": Phase="Running", Reason="", readiness=true. Elapsed: 14.243797885s
Mar 23 14:48:02.971: INFO: Pod "pod-subpath-test-secret-t2p2": Phase="Running", Reason="", readiness=true. Elapsed: 16.265982913s
Mar 23 14:48:04.991: INFO: Pod "pod-subpath-test-secret-t2p2": Phase="Running", Reason="", readiness=true. Elapsed: 18.286157043s
Mar 23 14:48:07.013: INFO: Pod "pod-subpath-test-secret-t2p2": Phase="Running", Reason="", readiness=true. Elapsed: 20.307526086s
Mar 23 14:48:09.037: INFO: Pod "pod-subpath-test-secret-t2p2": Phase="Running", Reason="", readiness=true. Elapsed: 22.332163519s
Mar 23 14:48:11.068: INFO: Pod "pod-subpath-test-secret-t2p2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.362701199s
STEP: Saw pod success
Mar 23 14:48:11.068: INFO: Pod "pod-subpath-test-secret-t2p2" satisfied condition "success or failure"
Mar 23 14:48:11.095: INFO: Trying to get logs from node 10.241.69.181 pod pod-subpath-test-secret-t2p2 container test-container-subpath-secret-t2p2: <nil>
STEP: delete the pod
Mar 23 14:48:11.306: INFO: Waiting for pod pod-subpath-test-secret-t2p2 to disappear
Mar 23 14:48:11.341: INFO: Pod pod-subpath-test-secret-t2p2 no longer exists
STEP: Deleting pod pod-subpath-test-secret-t2p2
Mar 23 14:48:11.341: INFO: Deleting pod "pod-subpath-test-secret-t2p2" in namespace "subpath-130"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:48:11.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-130" for this suite.
Mar 23 14:48:23.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:48:26.399: INFO: namespace subpath-130 deletion completed in 15.012959157s

• [SLOW TEST:40.302 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:48:26.399: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Mar 23 14:48:27.225: INFO: Waiting up to 5m0s for pod "pod-bef8ee0c-ef81-4581-8e97-ec780e2e37a5" in namespace "emptydir-3802" to be "success or failure"
Mar 23 14:48:27.252: INFO: Pod "pod-bef8ee0c-ef81-4581-8e97-ec780e2e37a5": Phase="Pending", Reason="", readiness=false. Elapsed: 27.320317ms
Mar 23 14:48:29.274: INFO: Pod "pod-bef8ee0c-ef81-4581-8e97-ec780e2e37a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.048579063s
STEP: Saw pod success
Mar 23 14:48:29.274: INFO: Pod "pod-bef8ee0c-ef81-4581-8e97-ec780e2e37a5" satisfied condition "success or failure"
Mar 23 14:48:29.293: INFO: Trying to get logs from node 10.241.69.181 pod pod-bef8ee0c-ef81-4581-8e97-ec780e2e37a5 container test-container: <nil>
STEP: delete the pod
Mar 23 14:48:29.446: INFO: Waiting for pod pod-bef8ee0c-ef81-4581-8e97-ec780e2e37a5 to disappear
Mar 23 14:48:29.464: INFO: Pod pod-bef8ee0c-ef81-4581-8e97-ec780e2e37a5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:48:29.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3802" for this suite.
Mar 23 14:48:41.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:48:44.923: INFO: namespace emptydir-3802 deletion completed in 15.430969195s

• [SLOW TEST:18.524 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:48:44.923: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 14:48:45.563: INFO: Creating deployment "webserver-deployment"
Mar 23 14:48:45.609: INFO: Waiting for observed generation 1
Mar 23 14:48:47.775: INFO: Waiting for all required pods to come up
Mar 23 14:48:47.865: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Mar 23 14:48:58.235: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 23 14:48:58.449: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 23 14:48:58.623: INFO: Updating deployment webserver-deployment
Mar 23 14:48:58.623: INFO: Waiting for observed generation 2
Mar 23 14:49:00.674: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 23 14:49:00.705: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 23 14:49:00.725: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 23 14:49:00.816: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 23 14:49:00.816: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 23 14:49:00.835: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 23 14:49:00.915: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 23 14:49:00.915: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 23 14:49:00.974: INFO: Updating deployment webserver-deployment
Mar 23 14:49:00.974: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 23 14:49:01.018: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 23 14:49:01.105: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar 23 14:49:01.234: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8452 /apis/apps/v1/namespaces/deployment-8452/deployments/webserver-deployment 97a1f6e1-8399-45b0-8d8d-63e28885dcbb 46114 3 2020-03-23 14:48:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003297918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-03-23 14:49:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:45 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-23 14:49:01 +0000 UTC,LastTransitionTime:2020-03-23 14:49:01 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 23 14:49:01.268: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-8452 /apis/apps/v1/namespaces/deployment-8452/replicasets/webserver-deployment-c7997dcc8 a0d06b57-8f6d-4ad9-81b5-7c7fa8844cb7 46107 3 2020-03-23 14:48:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 97a1f6e1-8399-45b0-8d8d-63e28885dcbb 0xc003739fd7 0xc003739fd8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc000e94048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 14:49:01.268: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 23 14:49:01.268: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-8452 /apis/apps/v1/namespaces/deployment-8452/replicasets/webserver-deployment-595b5b9587 1a8e15f5-a530-4cf1-8298-55a718f69aaa 46104 3 2020-03-23 14:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 97a1f6e1-8399-45b0-8d8d-63e28885dcbb 0xc003739f17 0xc003739f18}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003739f78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 23 14:49:01.313: INFO: Pod "webserver-deployment-595b5b9587-4kxs4" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4kxs4 webserver-deployment-595b5b9587- deployment-8452 /api/v1/namespaces/deployment-8452/pods/webserver-deployment-595b5b9587-4kxs4 2a68bb6a-bce1-4125-b534-273664882a11 46013 0 2020-03-23 14:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.64.219/32 cni.projectcalico.org/podIPs:172.30.64.219/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.64.219"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1a8e15f5-a530-4cf1-8298-55a718f69aaa 0xc000e94577 0xc000e94578}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqnm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqnm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqnm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vcvzt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.172,PodIP:172.30.64.219,StartTime:2020-03-23 14:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 14:48:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://f06e409811d9167b4303f80b98b877aff483824d9891db2b7361b309fc6f66c9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.64.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 14:49:01.313: INFO: Pod "webserver-deployment-595b5b9587-7d7hw" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7d7hw webserver-deployment-595b5b9587- deployment-8452 /api/v1/namespaces/deployment-8452/pods/webserver-deployment-595b5b9587-7d7hw e9718ecd-3a5c-44f1-92ee-897433a01537 45936 0 2020-03-23 14:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.123.249/32 cni.projectcalico.org/podIPs:172.30.123.249/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.123.249"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1a8e15f5-a530-4cf1-8298-55a718f69aaa 0xc000e94717 0xc000e94718}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqnm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqnm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqnm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.184,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vcvzt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.184,PodIP:172.30.123.249,StartTime:2020-03-23 14:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 14:48:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://ea1f1371925253d047f52b1bb4e1831ced4d35fb812d4385edbbae0eedf166b6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.123.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 14:49:01.313: INFO: Pod "webserver-deployment-595b5b9587-dnn75" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-dnn75 webserver-deployment-595b5b9587- deployment-8452 /api/v1/namespaces/deployment-8452/pods/webserver-deployment-595b5b9587-dnn75 9ee4d9a4-02fc-44f7-83c7-f1541bc13d69 45935 0 2020-03-23 14:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.197.133/32 cni.projectcalico.org/podIPs:172.30.197.133/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.197.133"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1a8e15f5-a530-4cf1-8298-55a718f69aaa 0xc000e948b7 0xc000e948b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqnm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqnm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqnm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.181,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vcvzt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.181,PodIP:172.30.197.133,StartTime:2020-03-23 14:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 14:48:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://fb5abc1c90928b159488ac4af5a354cba7227f4acbda891723009625fce97017,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.197.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 14:49:01.313: INFO: Pod "webserver-deployment-595b5b9587-gnzqt" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-gnzqt webserver-deployment-595b5b9587- deployment-8452 /api/v1/namespaces/deployment-8452/pods/webserver-deployment-595b5b9587-gnzqt 0378b309-ad54-4ce4-b6ad-a3f00d46afa4 45941 0 2020-03-23 14:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.123.248/32 cni.projectcalico.org/podIPs:172.30.123.248/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.123.248"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1a8e15f5-a530-4cf1-8298-55a718f69aaa 0xc000e94a57 0xc000e94a58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqnm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqnm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqnm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.184,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vcvzt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.184,PodIP:172.30.123.248,StartTime:2020-03-23 14:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 14:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://dff66f349ad69f58f3963e54c02a8c2956ff1bf03d024b275d7f302eff823737,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.123.248,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 14:49:01.313: INFO: Pod "webserver-deployment-595b5b9587-h7b92" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-h7b92 webserver-deployment-595b5b9587- deployment-8452 /api/v1/namespaces/deployment-8452/pods/webserver-deployment-595b5b9587-h7b92 4893ec3a-cea3-4400-9486-bbb2db0fc8ed 46001 0 2020-03-23 14:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.64.215/32 cni.projectcalico.org/podIPs:172.30.64.215/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.64.215"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1a8e15f5-a530-4cf1-8298-55a718f69aaa 0xc000e94c17 0xc000e94c18}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqnm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqnm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqnm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vcvzt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.172,PodIP:172.30.64.215,StartTime:2020-03-23 14:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 14:48:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://a85f8b10f234a80c3bcdc48583ae9d357941503659acf024c83e5702b556c6aa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.64.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 14:49:01.313: INFO: Pod "webserver-deployment-595b5b9587-lfj2j" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-lfj2j webserver-deployment-595b5b9587- deployment-8452 /api/v1/namespaces/deployment-8452/pods/webserver-deployment-595b5b9587-lfj2j ed2b5203-73b7-40eb-9af0-36ba11f891fd 45965 0 2020-03-23 14:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.123.250/32 cni.projectcalico.org/podIPs:172.30.123.250/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.123.250"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1a8e15f5-a530-4cf1-8298-55a718f69aaa 0xc000e94db7 0xc000e94db8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqnm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqnm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqnm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.184,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vcvzt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.184,PodIP:172.30.123.250,StartTime:2020-03-23 14:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 14:48:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://61caccd9592d1acb5ecee23595719610b500d006e91d8f3f9ec205a2025d09c9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.123.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 14:49:01.314: INFO: Pod "webserver-deployment-595b5b9587-njb84" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-njb84 webserver-deployment-595b5b9587- deployment-8452 /api/v1/namespaces/deployment-8452/pods/webserver-deployment-595b5b9587-njb84 776be960-1cb2-4309-82b2-b4ef5800e19a 45943 0 2020-03-23 14:48:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.197.135/32 cni.projectcalico.org/podIPs:172.30.197.135/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.197.135"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1a8e15f5-a530-4cf1-8298-55a718f69aaa 0xc000e94f57 0xc000e94f58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqnm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqnm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqnm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.181,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.181,PodIP:172.30.197.135,StartTime:2020-03-23 14:48:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 14:48:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://564a20951363500f2e69629ee95eb186e657172e49f8460581358714157ba7ff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.197.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 14:49:01.314: INFO: Pod "webserver-deployment-595b5b9587-pg4xg" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pg4xg webserver-deployment-595b5b9587- deployment-8452 /api/v1/namespaces/deployment-8452/pods/webserver-deployment-595b5b9587-pg4xg e8703b9b-2a56-4168-b068-0753ab4a2005 46017 0 2020-03-23 14:48:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.64.217/32 cni.projectcalico.org/podIPs:172.30.64.217/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.64.217"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1a8e15f5-a530-4cf1-8298-55a718f69aaa 0xc000e95117 0xc000e95118}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqnm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqnm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqnm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vcvzt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.172,PodIP:172.30.64.217,StartTime:2020-03-23 14:48:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 14:48:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://6cc5a23b0488726a2df4a91475b68715d21e44ea2c1d01af7bf3e26f91138306,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.64.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 14:49:01.314: INFO: Pod "webserver-deployment-595b5b9587-s7hjm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-s7hjm webserver-deployment-595b5b9587- deployment-8452 /api/v1/namespaces/deployment-8452/pods/webserver-deployment-595b5b9587-s7hjm 0af4248d-6475-4aed-b2f5-383a9afda447 46121 0 2020-03-23 14:49:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1a8e15f5-a530-4cf1-8298-55a718f69aaa 0xc000e952b7 0xc000e952b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqnm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqnm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqnm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vcvzt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 14:49:01.314: INFO: Pod "webserver-deployment-595b5b9587-wrtrp" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wrtrp webserver-deployment-595b5b9587- deployment-8452 /api/v1/namespaces/deployment-8452/pods/webserver-deployment-595b5b9587-wrtrp fde1c1e5-7c06-46ef-b5f1-f4b408398568 46123 0 2020-03-23 14:49:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1a8e15f5-a530-4cf1-8298-55a718f69aaa 0xc000e953c7 0xc000e953c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqnm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqnm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqnm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.184,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vcvzt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:49:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:49:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:49:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:49:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.184,PodIP:,StartTime:2020-03-23 14:49:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 14:49:01.314: INFO: Pod "webserver-deployment-c7997dcc8-4b8p9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-4b8p9 webserver-deployment-c7997dcc8- deployment-8452 /api/v1/namespaces/deployment-8452/pods/webserver-deployment-c7997dcc8-4b8p9 a2b750fd-f0dd-4b0b-a528-6262b754baa9 46081 0 2020-03-23 14:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a0d06b57-8f6d-4ad9-81b5-7c7fa8844cb7 0xc000e95547 0xc000e95548}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqnm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqnm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqnm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.181,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vcvzt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:49:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:49:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:49:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:49:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.181,PodIP:,StartTime:2020-03-23 14:49:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 14:49:01.314: INFO: Pod "webserver-deployment-c7997dcc8-c56lq" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-c56lq webserver-deployment-c7997dcc8- deployment-8452 /api/v1/namespaces/deployment-8452/pods/webserver-deployment-c7997dcc8-c56lq 9cfa246b-35d9-4755-8528-a808dafbdfb9 46105 0 2020-03-23 14:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.123.252/32 cni.projectcalico.org/podIPs:172.30.123.252/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.123.252"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a0d06b57-8f6d-4ad9-81b5-7c7fa8844cb7 0xc000e956e7 0xc000e956e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqnm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqnm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqnm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.184,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vcvzt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.184,PodIP:,StartTime:2020-03-23 14:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 14:49:01.315: INFO: Pod "webserver-deployment-c7997dcc8-mfdjd" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-mfdjd webserver-deployment-c7997dcc8- deployment-8452 /api/v1/namespaces/deployment-8452/pods/webserver-deployment-c7997dcc8-mfdjd 18837b7d-cba3-4825-b028-01b7d280defa 46088 0 2020-03-23 14:49:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a0d06b57-8f6d-4ad9-81b5-7c7fa8844cb7 0xc000e95887 0xc000e95888}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqnm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqnm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqnm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.184,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vcvzt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:49:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:49:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:49:00 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:49:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.184,PodIP:,StartTime:2020-03-23 14:49:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 14:49:01.315: INFO: Pod "webserver-deployment-c7997dcc8-nz8k2" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-nz8k2 webserver-deployment-c7997dcc8- deployment-8452 /api/v1/namespaces/deployment-8452/pods/webserver-deployment-c7997dcc8-nz8k2 0f2977e3-0c31-4a9d-bb1a-cbedfb250d52 46122 0 2020-03-23 14:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.64.218/32 cni.projectcalico.org/podIPs:172.30.64.218/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.64.218"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a0d06b57-8f6d-4ad9-81b5-7c7fa8844cb7 0xc000e95a47 0xc000e95a48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqnm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqnm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqnm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.172,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vcvzt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.172,PodIP:,StartTime:2020-03-23 14:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 14:49:01.315: INFO: Pod "webserver-deployment-c7997dcc8-vzfw9" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-vzfw9 webserver-deployment-c7997dcc8- deployment-8452 /api/v1/namespaces/deployment-8452/pods/webserver-deployment-c7997dcc8-vzfw9 aa9c0974-3c7f-40ad-8885-40b2bea0e485 46100 0 2020-03-23 14:48:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.197.147/32 cni.projectcalico.org/podIPs:172.30.197.147/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.197.147"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 a0d06b57-8f6d-4ad9-81b5-7c7fa8844cb7 0xc000e95be7 0xc000e95be8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-dqnm7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-dqnm7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-dqnm7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.181,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vcvzt,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 14:48:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.181,PodIP:,StartTime:2020-03-23 14:48:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:49:01.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8452" for this suite.
Mar 23 14:49:19.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:49:22.147: INFO: namespace deployment-8452 deletion completed in 20.801223578s

• [SLOW TEST:37.224 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:49:22.147: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-fe307847-c960-47dd-a175-437ff73ea542 in namespace container-probe-8830
Mar 23 14:49:26.814: INFO: Started pod busybox-fe307847-c960-47dd-a175-437ff73ea542 in namespace container-probe-8830
STEP: checking the pod's current state and verifying that restartCount is present
Mar 23 14:49:26.836: INFO: Initial restart count of pod busybox-fe307847-c960-47dd-a175-437ff73ea542 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:53:27.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8830" for this suite.
Mar 23 14:53:39.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:53:43.306: INFO: namespace container-probe-8830 deletion completed in 15.600968905s

• [SLOW TEST:261.159 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:53:43.307: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-ea9d8649-337f-4258-9a99-047eba053998
STEP: Creating a pod to test consume secrets
Mar 23 14:53:43.870: INFO: Waiting up to 5m0s for pod "pod-secrets-6c8a968c-df61-4601-927f-5dc56d5e4013" in namespace "secrets-5472" to be "success or failure"
Mar 23 14:53:43.888: INFO: Pod "pod-secrets-6c8a968c-df61-4601-927f-5dc56d5e4013": Phase="Pending", Reason="", readiness=false. Elapsed: 17.724288ms
Mar 23 14:53:45.907: INFO: Pod "pod-secrets-6c8a968c-df61-4601-927f-5dc56d5e4013": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036586382s
Mar 23 14:53:47.929: INFO: Pod "pod-secrets-6c8a968c-df61-4601-927f-5dc56d5e4013": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058348044s
STEP: Saw pod success
Mar 23 14:53:47.929: INFO: Pod "pod-secrets-6c8a968c-df61-4601-927f-5dc56d5e4013" satisfied condition "success or failure"
Mar 23 14:53:47.952: INFO: Trying to get logs from node 10.241.69.181 pod pod-secrets-6c8a968c-df61-4601-927f-5dc56d5e4013 container secret-volume-test: <nil>
STEP: delete the pod
Mar 23 14:53:48.193: INFO: Waiting for pod pod-secrets-6c8a968c-df61-4601-927f-5dc56d5e4013 to disappear
Mar 23 14:53:48.214: INFO: Pod pod-secrets-6c8a968c-df61-4601-927f-5dc56d5e4013 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:53:48.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5472" for this suite.
Mar 23 14:54:00.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:54:04.195: INFO: namespace secrets-5472 deletion completed in 15.951293603s

• [SLOW TEST:20.888 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:54:04.196: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-23f17be3-1656-4246-833f-5adcc993f8c2
STEP: Creating a pod to test consume secrets
Mar 23 14:54:05.925: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-99acc266-929b-4e90-8917-708897641308" in namespace "projected-9156" to be "success or failure"
Mar 23 14:54:05.946: INFO: Pod "pod-projected-secrets-99acc266-929b-4e90-8917-708897641308": Phase="Pending", Reason="", readiness=false. Elapsed: 21.056538ms
Mar 23 14:54:07.973: INFO: Pod "pod-projected-secrets-99acc266-929b-4e90-8917-708897641308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047935003s
Mar 23 14:54:09.992: INFO: Pod "pod-projected-secrets-99acc266-929b-4e90-8917-708897641308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066587766s
STEP: Saw pod success
Mar 23 14:54:09.992: INFO: Pod "pod-projected-secrets-99acc266-929b-4e90-8917-708897641308" satisfied condition "success or failure"
Mar 23 14:54:10.015: INFO: Trying to get logs from node 10.241.69.181 pod pod-projected-secrets-99acc266-929b-4e90-8917-708897641308 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 23 14:54:10.251: INFO: Waiting for pod pod-projected-secrets-99acc266-929b-4e90-8917-708897641308 to disappear
Mar 23 14:54:10.379: INFO: Pod pod-projected-secrets-99acc266-929b-4e90-8917-708897641308 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:54:10.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9156" for this suite.
Mar 23 14:54:24.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:54:28.455: INFO: namespace projected-9156 deletion completed in 18.049528711s

• [SLOW TEST:24.259 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:54:28.455: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Mar 23 14:54:31.418: INFO: Pod pod-hostip-4c7dace0-5f23-4abc-8950-2e6b79527d46 has hostIP: 10.241.69.181
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:54:31.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-655" for this suite.
Mar 23 14:54:53.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:54:56.096: INFO: namespace pods-655 deletion completed in 24.649567093s

• [SLOW TEST:27.641 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:54:56.096: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 14:54:56.906: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dbcf2b6f-dfc4-43c6-822b-046e1d8179bd" in namespace "downward-api-6641" to be "success or failure"
Mar 23 14:54:56.954: INFO: Pod "downwardapi-volume-dbcf2b6f-dfc4-43c6-822b-046e1d8179bd": Phase="Pending", Reason="", readiness=false. Elapsed: 47.894776ms
Mar 23 14:54:58.971: INFO: Pod "downwardapi-volume-dbcf2b6f-dfc4-43c6-822b-046e1d8179bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065105911s
Mar 23 14:55:00.998: INFO: Pod "downwardapi-volume-dbcf2b6f-dfc4-43c6-822b-046e1d8179bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.091487986s
STEP: Saw pod success
Mar 23 14:55:00.998: INFO: Pod "downwardapi-volume-dbcf2b6f-dfc4-43c6-822b-046e1d8179bd" satisfied condition "success or failure"
Mar 23 14:55:01.025: INFO: Trying to get logs from node 10.241.69.184 pod downwardapi-volume-dbcf2b6f-dfc4-43c6-822b-046e1d8179bd container client-container: <nil>
STEP: delete the pod
Mar 23 14:55:01.221: INFO: Waiting for pod downwardapi-volume-dbcf2b6f-dfc4-43c6-822b-046e1d8179bd to disappear
Mar 23 14:55:01.238: INFO: Pod downwardapi-volume-dbcf2b6f-dfc4-43c6-822b-046e1d8179bd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:55:01.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6641" for this suite.
Mar 23 14:55:15.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:55:18.437: INFO: namespace downward-api-6641 deletion completed in 17.176682177s

• [SLOW TEST:22.341 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:55:18.443: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 14:55:23.275: INFO: Waiting up to 5m0s for pod "client-envvars-22a57e0e-7d09-4c0b-8bcd-6b71461aaf10" in namespace "pods-7941" to be "success or failure"
Mar 23 14:55:23.299: INFO: Pod "client-envvars-22a57e0e-7d09-4c0b-8bcd-6b71461aaf10": Phase="Pending", Reason="", readiness=false. Elapsed: 23.506233ms
Mar 23 14:55:25.320: INFO: Pod "client-envvars-22a57e0e-7d09-4c0b-8bcd-6b71461aaf10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044403184s
Mar 23 14:55:27.345: INFO: Pod "client-envvars-22a57e0e-7d09-4c0b-8bcd-6b71461aaf10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.070260686s
STEP: Saw pod success
Mar 23 14:55:27.345: INFO: Pod "client-envvars-22a57e0e-7d09-4c0b-8bcd-6b71461aaf10" satisfied condition "success or failure"
Mar 23 14:55:27.369: INFO: Trying to get logs from node 10.241.69.181 pod client-envvars-22a57e0e-7d09-4c0b-8bcd-6b71461aaf10 container env3cont: <nil>
STEP: delete the pod
Mar 23 14:55:27.512: INFO: Waiting for pod client-envvars-22a57e0e-7d09-4c0b-8bcd-6b71461aaf10 to disappear
Mar 23 14:55:27.551: INFO: Pod client-envvars-22a57e0e-7d09-4c0b-8bcd-6b71461aaf10 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:55:27.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7941" for this suite.
Mar 23 14:56:11.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:56:14.624: INFO: namespace pods-7941 deletion completed in 47.04723238s

• [SLOW TEST:56.181 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:56:14.624: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar 23 14:56:19.792: INFO: Successfully updated pod "annotationupdatead9ccbc6-1b6f-464a-89f2-c11163d9b62b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:56:21.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3206" for this suite.
Mar 23 14:56:44.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:56:46.453: INFO: namespace projected-3206 deletion completed in 24.509046197s

• [SLOW TEST:31.829 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:56:46.454: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Mar 23 14:56:46.877: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 23 14:57:47.015: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 14:57:47.042: INFO: Starting informer...
STEP: Starting pods...
Mar 23 14:57:47.373: INFO: Pod1 is running on 10.241.69.181. Tainting Node
Mar 23 14:57:51.831: INFO: Pod2 is running on 10.241.69.181. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Mar 23 14:58:00.101: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 23 14:58:27.523: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:58:27.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-475" for this suite.
Mar 23 14:58:39.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:58:43.123: INFO: namespace taint-multiple-pods-475 deletion completed in 15.520215324s

• [SLOW TEST:116.670 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:58:43.123: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 14:58:43.606: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:58:43.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3331" for this suite.
Mar 23 14:58:58.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:59:01.555: INFO: namespace custom-resource-definition-3331 deletion completed in 17.5222903s

• [SLOW TEST:18.431 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:59:01.555: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-fabea0fb-5ea1-44e9-ae47-6c0b7d4a4ff5
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:59:02.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1497" for this suite.
Mar 23 14:59:16.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 14:59:19.292: INFO: namespace secrets-1497 deletion completed in 17.129926848s

• [SLOW TEST:17.737 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 14:59:19.293: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 14:59:45.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3259" for this suite.
Mar 23 14:59:57.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:00:00.319: INFO: namespace namespaces-3259 deletion completed in 15.2688171s
STEP: Destroying namespace "nsdeletetest-7328" for this suite.
Mar 23 15:00:00.344: INFO: Namespace nsdeletetest-7328 was already deleted
STEP: Destroying namespace "nsdeletetest-7789" for this suite.
Mar 23 15:00:14.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:00:17.949: INFO: namespace nsdeletetest-7789 deletion completed in 17.605342977s

• [SLOW TEST:58.656 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:00:17.949: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 15:00:19.093: INFO: (0) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 144.057995ms)
Mar 23 15:00:19.132: INFO: (1) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 38.531227ms)
Mar 23 15:00:19.171: INFO: (2) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 39.868214ms)
Mar 23 15:00:19.214: INFO: (3) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 42.470171ms)
Mar 23 15:00:19.252: INFO: (4) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 38.175237ms)
Mar 23 15:00:19.290: INFO: (5) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 38.168292ms)
Mar 23 15:00:19.332: INFO: (6) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 41.863887ms)
Mar 23 15:00:19.367: INFO: (7) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 35.10574ms)
Mar 23 15:00:19.407: INFO: (8) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 39.390846ms)
Mar 23 15:00:19.434: INFO: (9) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 27.41939ms)
Mar 23 15:00:19.468: INFO: (10) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 34.083621ms)
Mar 23 15:00:19.511: INFO: (11) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 43.01818ms)
Mar 23 15:00:19.550: INFO: (12) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 38.094907ms)
Mar 23 15:00:19.585: INFO: (13) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 34.949212ms)
Mar 23 15:00:19.620: INFO: (14) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 34.994705ms)
Mar 23 15:00:19.676: INFO: (15) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 56.222442ms)
Mar 23 15:00:19.717: INFO: (16) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 41.331428ms)
Mar 23 15:00:19.753: INFO: (17) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 35.868123ms)
Mar 23 15:00:19.793: INFO: (18) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 39.964204ms)
Mar 23 15:00:19.837: INFO: (19) /api/v1/nodes/10.241.69.172/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 43.479463ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:00:19.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1345" for this suite.
Mar 23 15:00:33.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:00:37.062: INFO: namespace proxy-1345 deletion completed in 17.194017809s

• [SLOW TEST:19.113 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:00:37.065: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 15:00:38.699: INFO: Waiting up to 5m0s for pod "downwardapi-volume-887dda18-251b-49dd-9760-a9fded516b4f" in namespace "downward-api-8332" to be "success or failure"
Mar 23 15:00:38.739: INFO: Pod "downwardapi-volume-887dda18-251b-49dd-9760-a9fded516b4f": Phase="Pending", Reason="", readiness=false. Elapsed: 39.347011ms
Mar 23 15:00:40.790: INFO: Pod "downwardapi-volume-887dda18-251b-49dd-9760-a9fded516b4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.090413477s
STEP: Saw pod success
Mar 23 15:00:40.790: INFO: Pod "downwardapi-volume-887dda18-251b-49dd-9760-a9fded516b4f" satisfied condition "success or failure"
Mar 23 15:00:40.817: INFO: Trying to get logs from node 10.241.69.181 pod downwardapi-volume-887dda18-251b-49dd-9760-a9fded516b4f container client-container: <nil>
STEP: delete the pod
Mar 23 15:00:41.179: INFO: Waiting for pod downwardapi-volume-887dda18-251b-49dd-9760-a9fded516b4f to disappear
Mar 23 15:00:41.244: INFO: Pod downwardapi-volume-887dda18-251b-49dd-9760-a9fded516b4f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:00:41.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8332" for this suite.
Mar 23 15:00:57.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:01:00.312: INFO: namespace downward-api-8332 deletion completed in 19.034791346s

• [SLOW TEST:23.248 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:01:00.316: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-3e1a5b4f-8e2a-401a-98b0-315ebcf648cf
STEP: Creating a pod to test consume configMaps
Mar 23 15:01:01.120: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a59ca204-76fd-4d08-9428-65a002339257" in namespace "projected-4419" to be "success or failure"
Mar 23 15:01:01.379: INFO: Pod "pod-projected-configmaps-a59ca204-76fd-4d08-9428-65a002339257": Phase="Pending", Reason="", readiness=false. Elapsed: 259.275846ms
Mar 23 15:01:05.540: INFO: Pod "pod-projected-configmaps-a59ca204-76fd-4d08-9428-65a002339257": Phase="Pending", Reason="", readiness=false. Elapsed: 4.420293553s
Mar 23 15:01:07.584: INFO: Pod "pod-projected-configmaps-a59ca204-76fd-4d08-9428-65a002339257": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.46437906s
STEP: Saw pod success
Mar 23 15:01:07.585: INFO: Pod "pod-projected-configmaps-a59ca204-76fd-4d08-9428-65a002339257" satisfied condition "success or failure"
Mar 23 15:01:07.610: INFO: Trying to get logs from node 10.241.69.181 pod pod-projected-configmaps-a59ca204-76fd-4d08-9428-65a002339257 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 15:01:08.107: INFO: Waiting for pod pod-projected-configmaps-a59ca204-76fd-4d08-9428-65a002339257 to disappear
Mar 23 15:01:08.145: INFO: Pod pod-projected-configmaps-a59ca204-76fd-4d08-9428-65a002339257 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:01:08.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4419" for this suite.
Mar 23 15:01:22.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:01:25.404: INFO: namespace projected-4419 deletion completed in 17.228256803s

• [SLOW TEST:25.089 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:01:25.408: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:01:26.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3080" for this suite.
Mar 23 15:02:04.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:02:08.864: INFO: namespace pods-3080 deletion completed in 42.42810741s

• [SLOW TEST:43.457 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:02:08.866: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-5325
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 23 15:02:09.535: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 23 15:02:33.083: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.197.167:8080/dial?request=hostName&protocol=udp&host=172.30.197.166&port=8081&tries=1'] Namespace:pod-network-test-5325 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 15:02:33.083: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 15:02:33.414: INFO: Waiting for endpoints: map[]
Mar 23 15:02:33.429: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.197.167:8080/dial?request=hostName&protocol=udp&host=172.30.64.223&port=8081&tries=1'] Namespace:pod-network-test-5325 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 15:02:33.429: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 15:02:33.740: INFO: Waiting for endpoints: map[]
Mar 23 15:02:33.760: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.197.167:8080/dial?request=hostName&protocol=udp&host=172.30.123.231&port=8081&tries=1'] Namespace:pod-network-test-5325 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 15:02:33.760: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 15:02:34.094: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:02:34.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5325" for this suite.
Mar 23 15:02:48.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:02:52.319: INFO: namespace pod-network-test-5325 deletion completed in 18.195799718s

• [SLOW TEST:43.453 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:02:52.319: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar 23 15:02:52.834: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:02:56.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8008" for this suite.
Mar 23 15:03:13.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:03:17.061: INFO: namespace init-container-8008 deletion completed in 20.143146156s

• [SLOW TEST:24.742 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:03:17.061: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-4bb85439-5527-467b-8e19-3584287ff9b0
STEP: Creating a pod to test consume configMaps
Mar 23 15:03:19.240: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-29601977-9244-45f3-9488-df61d53ba089" in namespace "projected-6204" to be "success or failure"
Mar 23 15:03:19.291: INFO: Pod "pod-projected-configmaps-29601977-9244-45f3-9488-df61d53ba089": Phase="Pending", Reason="", readiness=false. Elapsed: 50.545266ms
Mar 23 15:03:21.308: INFO: Pod "pod-projected-configmaps-29601977-9244-45f3-9488-df61d53ba089": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.067955436s
STEP: Saw pod success
Mar 23 15:03:21.308: INFO: Pod "pod-projected-configmaps-29601977-9244-45f3-9488-df61d53ba089" satisfied condition "success or failure"
Mar 23 15:03:21.337: INFO: Trying to get logs from node 10.241.69.181 pod pod-projected-configmaps-29601977-9244-45f3-9488-df61d53ba089 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 15:03:21.572: INFO: Waiting for pod pod-projected-configmaps-29601977-9244-45f3-9488-df61d53ba089 to disappear
Mar 23 15:03:21.604: INFO: Pod pod-projected-configmaps-29601977-9244-45f3-9488-df61d53ba089 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:03:21.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6204" for this suite.
Mar 23 15:03:37.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:03:42.177: INFO: namespace projected-6204 deletion completed in 20.54226778s

• [SLOW TEST:25.116 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:03:42.178: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 23 15:03:42.957: INFO: Waiting up to 5m0s for pod "pod-89249248-fa8d-4bd1-9a74-e322b56e005b" in namespace "emptydir-1859" to be "success or failure"
Mar 23 15:03:43.003: INFO: Pod "pod-89249248-fa8d-4bd1-9a74-e322b56e005b": Phase="Pending", Reason="", readiness=false. Elapsed: 45.292673ms
Mar 23 15:03:45.034: INFO: Pod "pod-89249248-fa8d-4bd1-9a74-e322b56e005b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.076773497s
STEP: Saw pod success
Mar 23 15:03:45.034: INFO: Pod "pod-89249248-fa8d-4bd1-9a74-e322b56e005b" satisfied condition "success or failure"
Mar 23 15:03:45.052: INFO: Trying to get logs from node 10.241.69.181 pod pod-89249248-fa8d-4bd1-9a74-e322b56e005b container test-container: <nil>
STEP: delete the pod
Mar 23 15:03:45.277: INFO: Waiting for pod pod-89249248-fa8d-4bd1-9a74-e322b56e005b to disappear
Mar 23 15:03:45.303: INFO: Pod pod-89249248-fa8d-4bd1-9a74-e322b56e005b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:03:45.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1859" for this suite.
Mar 23 15:03:59.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:04:02.970: INFO: namespace emptydir-1859 deletion completed in 17.639969443s

• [SLOW TEST:20.792 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:04:02.970: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-6858
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6858 to expose endpoints map[]
Mar 23 15:04:03.795: INFO: Get endpoints failed (14.496347ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Mar 23 15:04:04.817: INFO: successfully validated that service multi-endpoint-test in namespace services-6858 exposes endpoints map[] (1.036760984s elapsed)
STEP: Creating pod pod1 in namespace services-6858
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6858 to expose endpoints map[pod1:[100]]
Mar 23 15:04:08.111: INFO: successfully validated that service multi-endpoint-test in namespace services-6858 exposes endpoints map[pod1:[100]] (3.179716936s elapsed)
STEP: Creating pod pod2 in namespace services-6858
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6858 to expose endpoints map[pod1:[100] pod2:[101]]
Mar 23 15:04:11.521: INFO: successfully validated that service multi-endpoint-test in namespace services-6858 exposes endpoints map[pod1:[100] pod2:[101]] (3.342070115s elapsed)
STEP: Deleting pod pod1 in namespace services-6858
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6858 to expose endpoints map[pod2:[101]]
Mar 23 15:04:12.820: INFO: successfully validated that service multi-endpoint-test in namespace services-6858 exposes endpoints map[pod2:[101]] (1.248202874s elapsed)
STEP: Deleting pod pod2 in namespace services-6858
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6858 to expose endpoints map[]
Mar 23 15:04:14.126: INFO: successfully validated that service multi-endpoint-test in namespace services-6858 exposes endpoints map[] (1.039076839s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:04:14.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6858" for this suite.
Mar 23 15:04:24.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:04:26.999: INFO: namespace services-6858 deletion completed in 12.689751522s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:24.029 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:04:27.003: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Mar 23 15:04:27.516: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-9312" to be "success or failure"
Mar 23 15:04:27.540: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 23.653108ms
Mar 23 15:04:29.565: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048521273s
Mar 23 15:04:31.589: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.072357136s
STEP: Saw pod success
Mar 23 15:04:31.589: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar 23 15:04:31.610: INFO: Trying to get logs from node 10.241.69.181 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Mar 23 15:04:31.756: INFO: Waiting for pod pod-host-path-test to disappear
Mar 23 15:04:31.780: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:04:31.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-9312" for this suite.
Mar 23 15:04:43.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:04:46.302: INFO: namespace hostpath-9312 deletion completed in 14.494144438s

• [SLOW TEST:19.299 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:04:46.302: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar 23 15:04:46.715: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 23 15:04:46.888: INFO: Waiting for terminating namespaces to be deleted...
Mar 23 15:04:46.913: INFO: 
Logging pods the kubelet thinks is on node 10.241.69.172 before test
Mar 23 15:04:47.097: INFO: vpn-774dcbf8fc-mqtz2 from kube-system started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.097: INFO: 	Container vpn ready: true, restart count 0
Mar 23 15:04:47.097: INFO: prometheus-adapter-6445ff7666-kwdxl from openshift-monitoring started at 2020-03-23 13:28:03 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.097: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar 23 15:04:47.097: INFO: ibm-master-proxy-static-10.241.69.172 from kube-system started at 2020-03-23 13:20:33 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.097: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 23 15:04:47.097: INFO: 	Container pause ready: true, restart count 0
Mar 23 15:04:47.097: INFO: ibm-keepalived-watcher-zqvkl from kube-system started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.097: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 23 15:04:47.097: INFO: redhat-operators-5d5b56f44f-9c86l from openshift-marketplace started at 2020-03-23 14:22:14 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.097: INFO: 	Container redhat-operators ready: true, restart count 0
Mar 23 15:04:47.097: INFO: tuned-zh9qp from openshift-cluster-node-tuning-operator started at 2020-03-23 13:21:11 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.097: INFO: 	Container tuned ready: true, restart count 0
Mar 23 15:04:47.097: INFO: dns-default-fmdtj from openshift-dns started at 2020-03-23 13:21:48 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.097: INFO: 	Container dns ready: true, restart count 0
Mar 23 15:04:47.097: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar 23 15:04:47.097: INFO: ibm-cloud-provider-ip-169-47-106-243-9577f85bb-d7n68 from ibm-system started at 2020-03-23 13:22:40 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.097: INFO: 	Container ibm-cloud-provider-ip-169-47-106-243 ready: true, restart count 0
Mar 23 15:04:47.097: INFO: calico-node-6f276 from calico-system started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.097: INFO: 	Container calico-node ready: true, restart count 0
Mar 23 15:04:47.097: INFO: ibmcloud-block-storage-driver-rxcc5 from kube-system started at 2020-03-23 13:20:43 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.098: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 23 15:04:47.098: INFO: cluster-samples-operator-6b5459b7b9-fwhtx from openshift-cluster-samples-operator started at 2020-03-23 14:57:52 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.098: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar 23 15:04:47.098: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar 23 15:04:47.098: INFO: thanos-querier-866887d744-whk26 from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (4 container statuses recorded)
Mar 23 15:04:47.098: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 15:04:47.098: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar 23 15:04:47.098: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 15:04:47.098: INFO: 	Container thanos-querier ready: true, restart count 0
Mar 23 15:04:47.098: INFO: calico-typha-7d46744f5d-5mvhh from calico-system started at 2020-03-23 13:21:09 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.098: INFO: 	Container calico-typha ready: true, restart count 0
Mar 23 15:04:47.098: INFO: certified-operators-6c5fb44f9-j2x6t from openshift-marketplace started at 2020-03-23 13:22:22 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.098: INFO: 	Container certified-operators ready: true, restart count 0
Mar 23 15:04:47.098: INFO: grafana-5bdf758c94-j7bvk from openshift-monitoring started at 2020-03-23 13:28:09 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.098: INFO: 	Container grafana ready: true, restart count 0
Mar 23 15:04:47.098: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar 23 15:04:47.098: INFO: sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-9fzqx from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.098: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 15:04:47.098: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 15:04:47.098: INFO: console-5b755cbdcd-7sqjq from openshift-console started at 2020-03-23 13:22:24 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.098: INFO: 	Container console ready: true, restart count 0
Mar 23 15:04:47.098: INFO: packageserver-5c7d978cbb-llzxd from openshift-operator-lifecycle-manager started at 2020-03-23 13:31:00 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.098: INFO: 	Container packageserver ready: true, restart count 0
Mar 23 15:04:47.098: INFO: openshift-kube-proxy-lv75h from openshift-kube-proxy started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.098: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 15:04:47.098: INFO: node-exporter-2c5jl from openshift-monitoring started at 2020-03-23 13:20:40 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.098: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 15:04:47.098: INFO: 	Container node-exporter ready: true, restart count 0
Mar 23 15:04:47.098: INFO: multus-admission-controller-shsp6 from openshift-multus started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.098: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar 23 15:04:47.098: INFO: image-registry-684f4746d9-ptcpt from openshift-image-registry started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.098: INFO: 	Container registry ready: true, restart count 0
Mar 23 15:04:47.098: INFO: multus-b8pkh from openshift-multus started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.098: INFO: 	Container kube-multus ready: true, restart count 0
Mar 23 15:04:47.098: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-03-23 13:28:54 +0000 UTC (3 container statuses recorded)
Mar 23 15:04:47.098: INFO: 	Container alertmanager ready: true, restart count 0
Mar 23 15:04:47.099: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar 23 15:04:47.099: INFO: 	Container config-reloader ready: true, restart count 0
Mar 23 15:04:47.099: INFO: community-operators-f588d85c4-gvd24 from openshift-marketplace started at 2020-03-23 13:22:21 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.099: INFO: 	Container community-operators ready: true, restart count 0
Mar 23 15:04:47.099: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-03-23 13:29:34 +0000 UTC (7 container statuses recorded)
Mar 23 15:04:47.099: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 15:04:47.099: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 15:04:47.099: INFO: 	Container prometheus ready: true, restart count 1
Mar 23 15:04:47.099: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 23 15:04:47.099: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar 23 15:04:47.099: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 23 15:04:47.099: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar 23 15:04:47.099: INFO: sonobuoy-e2e-job-b545dbbb0c064966 from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.099: INFO: 	Container e2e ready: true, restart count 0
Mar 23 15:04:47.099: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 15:04:47.099: INFO: node-ca-c6cdr from openshift-image-registry started at 2020-03-23 13:21:48 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.099: INFO: 	Container node-ca ready: true, restart count 0
Mar 23 15:04:47.099: INFO: router-default-58f5c9568-cqkqx from openshift-ingress started at 2020-03-23 13:22:00 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.099: INFO: 	Container router ready: true, restart count 0
Mar 23 15:04:47.099: INFO: 
Logging pods the kubelet thinks is on node 10.241.69.181 before test
Mar 23 15:04:47.164: INFO: tuned-n9xl7 from openshift-cluster-node-tuning-operator started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.164: INFO: 	Container tuned ready: true, restart count 0
Mar 23 15:04:47.164: INFO: multus-596gk from openshift-multus started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.164: INFO: 	Container kube-multus ready: true, restart count 0
Mar 23 15:04:47.164: INFO: ibm-keepalived-watcher-2klqj from kube-system started at 2020-03-23 13:21:48 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.164: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 23 15:04:47.164: INFO: node-exporter-k4sbd from openshift-monitoring started at 2020-03-23 13:21:48 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.165: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 15:04:47.165: INFO: 	Container node-exporter ready: true, restart count 0
Mar 23 15:04:47.165: INFO: calico-typha-7d46744f5d-qnz4h from calico-system started at 2020-03-23 13:23:09 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.165: INFO: 	Container calico-typha ready: true, restart count 0
Mar 23 15:04:47.165: INFO: multus-admission-controller-dztdz from openshift-multus started at 2020-03-23 14:58:27 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.165: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar 23 15:04:47.165: INFO: node-ca-b7fwn from openshift-image-registry started at 2020-03-23 14:58:27 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.165: INFO: 	Container node-ca ready: true, restart count 0
Mar 23 15:04:47.165: INFO: registry-pvc-permissions-gptrd from openshift-image-registry started at 2020-03-23 13:24:16 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.165: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar 23 15:04:47.165: INFO: ibmcloud-block-storage-driver-rwjct from kube-system started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.165: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 23 15:04:47.165: INFO: openshift-kube-proxy-hgttr from openshift-kube-proxy started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.165: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 15:04:47.165: INFO: calico-node-skv44 from calico-system started at 2020-03-23 13:21:48 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.165: INFO: 	Container calico-node ready: true, restart count 0
Mar 23 15:04:47.165: INFO: dns-default-gq5w7 from openshift-dns started at 2020-03-23 14:58:27 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.165: INFO: 	Container dns ready: true, restart count 0
Mar 23 15:04:47.165: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar 23 15:04:47.165: INFO: ibm-master-proxy-static-10.241.69.181 from kube-system started at 2020-03-23 13:21:17 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.165: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 23 15:04:47.165: INFO: 	Container pause ready: true, restart count 0
Mar 23 15:04:47.165: INFO: sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-m6fcz from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.165: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 15:04:47.165: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 15:04:47.165: INFO: 
Logging pods the kubelet thinks is on node 10.241.69.184 before test
Mar 23 15:04:47.345: INFO: ibm-storage-watcher-cdf8f7d4c-pnk9m from kube-system started at 2020-03-23 13:20:08 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar 23 15:04:47.345: INFO: multus-admission-controller-ckp4z from openshift-multus started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar 23 15:04:47.345: INFO: olm-operator-9d666b5b9-s6wnm from openshift-operator-lifecycle-manager started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container olm-operator ready: true, restart count 0
Mar 23 15:04:47.345: INFO: cluster-image-registry-operator-5d7d64d769-n6l5f from openshift-image-registry started at 2020-03-23 13:20:07 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar 23 15:04:47.345: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Mar 23 15:04:47.345: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-03-23 14:58:09 +0000 UTC (7 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 15:04:47.345: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 15:04:47.345: INFO: 	Container prometheus ready: true, restart count 1
Mar 23 15:04:47.345: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 23 15:04:47.345: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar 23 15:04:47.345: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 23 15:04:47.345: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar 23 15:04:47.345: INFO: ibm-cloud-provider-ip-169-47-106-243-9577f85bb-ldvxd from ibm-system started at 2020-03-23 13:22:01 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container ibm-cloud-provider-ip-169-47-106-243 ready: true, restart count 0
Mar 23 15:04:47.345: INFO: tigera-operator-c7555ddb8-fggcz from tigera-operator started at 2020-03-23 13:18:58 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container tigera-operator ready: true, restart count 0
Mar 23 15:04:47.345: INFO: ibmcloud-block-storage-plugin-5c7dcf4bdb-jm274 from kube-system started at 2020-03-23 13:19:01 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar 23 15:04:47.345: INFO: tuned-fjdv5 from openshift-cluster-node-tuning-operator started at 2020-03-23 13:21:11 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container tuned ready: true, restart count 0
Mar 23 15:04:47.345: INFO: router-default-58f5c9568-vkljz from openshift-ingress started at 2020-03-23 13:22:01 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container router ready: true, restart count 0
Mar 23 15:04:47.345: INFO: packageserver-5c7d978cbb-s4z4h from openshift-operator-lifecycle-manager started at 2020-03-23 13:30:54 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container packageserver ready: true, restart count 0
Mar 23 15:04:47.345: INFO: console-operator-597c74c496-7g9c9 from openshift-console-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container console-operator ready: true, restart count 0
Mar 23 15:04:47.345: INFO: node-exporter-xwc46 from openshift-monitoring started at 2020-03-23 13:20:33 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 15:04:47.345: INFO: 	Container node-exporter ready: true, restart count 0
Mar 23 15:04:47.345: INFO: cluster-monitoring-operator-5b6ff66676-fjscj from openshift-monitoring started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Mar 23 15:04:47.345: INFO: console-5b755cbdcd-pllqp from openshift-console started at 2020-03-23 13:22:44 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container console ready: true, restart count 0
Mar 23 15:04:47.345: INFO: telemeter-client-79bc5978cc-hnfmp from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (3 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 15:04:47.345: INFO: 	Container reload ready: true, restart count 0
Mar 23 15:04:47.345: INFO: 	Container telemeter-client ready: true, restart count 0
Mar 23 15:04:47.345: INFO: marketplace-operator-554cffcfd-d5zjt from openshift-marketplace started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar 23 15:04:47.345: INFO: ingress-operator-754bbd5fbf-q5d8c from openshift-ingress-operator started at 2020-03-23 13:20:07 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container ingress-operator ready: true, restart count 0
Mar 23 15:04:47.345: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 15:04:47.345: INFO: ibmcloud-block-storage-driver-9xhmg from kube-system started at 2020-03-23 13:19:01 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 23 15:04:47.345: INFO: cloud-credential-operator-65466dfbf8-nvzxn from openshift-cloud-credential-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container manager ready: true, restart count 0
Mar 23 15:04:47.345: INFO: downloads-7fdfb77b95-56xsf from openshift-console started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container download-server ready: true, restart count 0
Mar 23 15:04:47.345: INFO: dns-operator-6cc86f84cd-qd8jq from openshift-dns-operator started at 2020-03-23 13:20:08 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container dns-operator ready: true, restart count 0
Mar 23 15:04:47.345: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 15:04:47.345: INFO: network-operator-6f9f45bfbb-2gt5t from openshift-network-operator started at 2020-03-23 13:19:00 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container network-operator ready: true, restart count 0
Mar 23 15:04:47.345: INFO: multus-g6kbd from openshift-multus started at 2020-03-23 13:19:31 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container kube-multus ready: true, restart count 0
Mar 23 15:04:47.345: INFO: configmap-cabundle-injector-78bbf44c6b-rnztt from openshift-service-ca started at 2020-03-23 13:20:35 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.345: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Mar 23 15:04:47.346: INFO: sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-tk2rn from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 15:04:47.346: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 15:04:47.346: INFO: node-ca-mvtq9 from openshift-image-registry started at 2020-03-23 13:21:49 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container node-ca ready: true, restart count 0
Mar 23 15:04:47.346: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-03-23 14:58:08 +0000 UTC (3 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container alertmanager ready: true, restart count 0
Mar 23 15:04:47.346: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar 23 15:04:47.346: INFO: 	Container config-reloader ready: true, restart count 0
Mar 23 15:04:47.346: INFO: calico-node-jxjt7 from calico-system started at 2020-03-23 13:19:14 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container calico-node ready: true, restart count 0
Mar 23 15:04:47.346: INFO: downloads-7fdfb77b95-668gj from openshift-console started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container download-server ready: true, restart count 0
Mar 23 15:04:47.346: INFO: ibm-file-plugin-bd78b44b5-7xzx5 from kube-system started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar 23 15:04:47.346: INFO: service-serving-cert-signer-598cdff956-pxfdm from openshift-service-ca started at 2020-03-23 13:20:35 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Mar 23 15:04:47.346: INFO: prometheus-operator-54f44d89d8-xfwmx from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar 23 15:04:47.346: INFO: prometheus-adapter-6445ff7666-7lcjj from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar 23 15:04:47.346: INFO: openshift-service-catalog-controller-manager-operator-5487z68jd from openshift-service-catalog-controller-manager-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container operator ready: true, restart count 1
Mar 23 15:04:47.346: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-03-23 13:28:39 +0000 UTC (3 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container alertmanager ready: true, restart count 0
Mar 23 15:04:47.346: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar 23 15:04:47.346: INFO: 	Container config-reloader ready: true, restart count 0
Mar 23 15:04:47.346: INFO: apiservice-cabundle-injector-6b4f956cf4-rtrzx from openshift-service-ca started at 2020-03-23 13:20:35 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Mar 23 15:04:47.346: INFO: sonobuoy from sonobuoy started at 2020-03-23 14:35:40 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 23 15:04:47.346: INFO: calico-typha-7d46744f5d-c2gxq from calico-system started at 2020-03-23 13:19:14 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container calico-typha ready: true, restart count 1
Mar 23 15:04:47.346: INFO: openshift-kube-proxy-9lb45 from openshift-kube-proxy started at 2020-03-23 13:19:38 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 15:04:47.346: INFO: openshift-state-metrics-7479448b69-fsvs4 from openshift-monitoring started at 2020-03-23 13:20:33 +0000 UTC (3 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar 23 15:04:47.346: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar 23 15:04:47.346: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar 23 15:04:47.346: INFO: cluster-storage-operator-7f448d8d78-tb4bw from openshift-cluster-storage-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Mar 23 15:04:47.346: INFO: cluster-node-tuning-operator-878d4d68-48859 from openshift-cluster-node-tuning-operator started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar 23 15:04:47.346: INFO: service-ca-operator-69984d88bd-kcxc5 from openshift-service-ca-operator started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container operator ready: true, restart count 0
Mar 23 15:04:47.346: INFO: calico-kube-controllers-85c89dd878-ddpfj from calico-system started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 23 15:04:47.346: INFO: kube-state-metrics-6bcc97c9d6-vbfvx from openshift-monitoring started at 2020-03-23 13:20:32 +0000 UTC (3 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar 23 15:04:47.346: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar 23 15:04:47.346: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 23 15:04:47.346: INFO: dns-default-pvrkk from openshift-dns started at 2020-03-23 13:21:47 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container dns ready: true, restart count 0
Mar 23 15:04:47.346: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar 23 15:04:47.346: INFO: thanos-querier-866887d744-7rdmw from openshift-monitoring started at 2020-03-23 13:29:10 +0000 UTC (4 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 15:04:47.346: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar 23 15:04:47.346: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 15:04:47.346: INFO: 	Container thanos-querier ready: true, restart count 0
Mar 23 15:04:47.346: INFO: ibm-master-proxy-static-10.241.69.184 from kube-system started at 2020-03-23 13:18:49 +0000 UTC (2 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 23 15:04:47.346: INFO: 	Container pause ready: true, restart count 0
Mar 23 15:04:47.346: INFO: ibm-keepalived-watcher-xc448 from kube-system started at 2020-03-23 13:18:55 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 23 15:04:47.346: INFO: catalog-operator-bb7df57d7-xkrxt from openshift-operator-lifecycle-manager started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container catalog-operator ready: true, restart count 0
Mar 23 15:04:47.346: INFO: openshift-service-catalog-apiserver-operator-5845fbf887-jlsvg from openshift-service-catalog-apiserver-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 15:04:47.346: INFO: 	Container operator ready: true, restart count 1
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-f475b01b-b207-41d0-b4c9-ca2135efd188 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-f475b01b-b207-41d0-b4c9-ca2135efd188 off the node 10.241.69.181
STEP: verifying the node doesn't have the label kubernetes.io/e2e-f475b01b-b207-41d0-b4c9-ca2135efd188
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:09:54.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6264" for this suite.
Mar 23 15:10:16.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:10:19.669: INFO: namespace sched-pred-6264 deletion completed in 25.469324734s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:333.367 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:10:19.671: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4908
I0323 15:10:20.694809      22 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4908, replica count: 1
I0323 15:10:21.745397      22 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 15:10:22.745722      22 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 15:10:23.746000      22 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 15:10:24.746314      22 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 15:10:24.966: INFO: Created: latency-svc-g69t6
Mar 23 15:10:24.992: INFO: Got endpoints: latency-svc-g69t6 [146.081226ms]
Mar 23 15:10:25.101: INFO: Created: latency-svc-d574x
Mar 23 15:10:25.164: INFO: Got endpoints: latency-svc-d574x [171.138474ms]
Mar 23 15:10:25.248: INFO: Created: latency-svc-5gxq2
Mar 23 15:10:25.267: INFO: Got endpoints: latency-svc-5gxq2 [273.718478ms]
Mar 23 15:10:25.355: INFO: Created: latency-svc-zp26g
Mar 23 15:10:25.501: INFO: Created: latency-svc-9q67s
Mar 23 15:10:25.592: INFO: Created: latency-svc-g5k8t
Mar 23 15:10:25.601: INFO: Got endpoints: latency-svc-zp26g [607.918623ms]
Mar 23 15:10:25.610: INFO: Got endpoints: latency-svc-9q67s [617.446298ms]
Mar 23 15:10:25.622: INFO: Got endpoints: latency-svc-g5k8t [629.189717ms]
Mar 23 15:10:25.712: INFO: Created: latency-svc-6pqgx
Mar 23 15:10:25.724: INFO: Got endpoints: latency-svc-6pqgx [730.366065ms]
Mar 23 15:10:25.783: INFO: Created: latency-svc-bgcjq
Mar 23 15:10:25.818: INFO: Got endpoints: latency-svc-bgcjq [823.728399ms]
Mar 23 15:10:25.852: INFO: Created: latency-svc-hc6hv
Mar 23 15:10:25.873: INFO: Got endpoints: latency-svc-hc6hv [879.4859ms]
Mar 23 15:10:25.956: INFO: Created: latency-svc-758j5
Mar 23 15:10:25.975: INFO: Got endpoints: latency-svc-758j5 [981.861751ms]
Mar 23 15:10:26.022: INFO: Created: latency-svc-t2bs5
Mar 23 15:10:26.047: INFO: Got endpoints: latency-svc-t2bs5 [1.052648534s]
Mar 23 15:10:26.114: INFO: Created: latency-svc-prf6v
Mar 23 15:10:26.136: INFO: Got endpoints: latency-svc-prf6v [1.143066044s]
Mar 23 15:10:26.205: INFO: Created: latency-svc-5c9t4
Mar 23 15:10:26.237: INFO: Got endpoints: latency-svc-5c9t4 [1.244006241s]
Mar 23 15:10:26.283: INFO: Created: latency-svc-64r6q
Mar 23 15:10:26.308: INFO: Got endpoints: latency-svc-64r6q [1.314605171s]
Mar 23 15:10:26.511: INFO: Created: latency-svc-xw9pc
Mar 23 15:10:26.518: INFO: Created: latency-svc-4ndfj
Mar 23 15:10:26.566: INFO: Got endpoints: latency-svc-4ndfj [1.573237424s]
Mar 23 15:10:26.567: INFO: Got endpoints: latency-svc-xw9pc [1.573095185s]
Mar 23 15:10:26.615: INFO: Created: latency-svc-24wjp
Mar 23 15:10:26.636: INFO: Got endpoints: latency-svc-24wjp [1.471909809s]
Mar 23 15:10:27.089: INFO: Created: latency-svc-2rfsb
Mar 23 15:10:27.112: INFO: Got endpoints: latency-svc-2rfsb [1.844531449s]
Mar 23 15:10:27.156: INFO: Created: latency-svc-wq7n6
Mar 23 15:10:27.188: INFO: Got endpoints: latency-svc-wq7n6 [1.586057474s]
Mar 23 15:10:27.233: INFO: Created: latency-svc-drnfr
Mar 23 15:10:27.258: INFO: Got endpoints: latency-svc-drnfr [1.648460726s]
Mar 23 15:10:27.328: INFO: Created: latency-svc-k259h
Mar 23 15:10:27.353: INFO: Got endpoints: latency-svc-k259h [1.730488627s]
Mar 23 15:10:27.423: INFO: Created: latency-svc-8ks9v
Mar 23 15:10:27.455: INFO: Got endpoints: latency-svc-8ks9v [1.731783293s]
Mar 23 15:10:27.503: INFO: Created: latency-svc-2kbkd
Mar 23 15:10:27.533: INFO: Got endpoints: latency-svc-2kbkd [1.712866369s]
Mar 23 15:10:27.602: INFO: Created: latency-svc-gg6hr
Mar 23 15:10:27.635: INFO: Got endpoints: latency-svc-gg6hr [1.762401559s]
Mar 23 15:10:27.687: INFO: Created: latency-svc-6nmvr
Mar 23 15:10:27.766: INFO: Created: latency-svc-bkvvk
Mar 23 15:10:27.818: INFO: Got endpoints: latency-svc-6nmvr [1.842156515s]
Mar 23 15:10:27.827: INFO: Got endpoints: latency-svc-bkvvk [1.780327815s]
Mar 23 15:10:27.834: INFO: Created: latency-svc-rhpvp
Mar 23 15:10:27.871: INFO: Got endpoints: latency-svc-rhpvp [1.735304605s]
Mar 23 15:10:27.902: INFO: Created: latency-svc-944r8
Mar 23 15:10:27.921: INFO: Got endpoints: latency-svc-944r8 [1.684433534s]
Mar 23 15:10:27.987: INFO: Created: latency-svc-cr542
Mar 23 15:10:28.006: INFO: Got endpoints: latency-svc-cr542 [1.697544106s]
Mar 23 15:10:28.061: INFO: Created: latency-svc-tgk6g
Mar 23 15:10:28.087: INFO: Got endpoints: latency-svc-tgk6g [1.52050552s]
Mar 23 15:10:28.153: INFO: Created: latency-svc-77v97
Mar 23 15:10:28.180: INFO: Got endpoints: latency-svc-77v97 [1.613172313s]
Mar 23 15:10:28.219: INFO: Created: latency-svc-q84j9
Mar 23 15:10:28.245: INFO: Got endpoints: latency-svc-q84j9 [1.609034776s]
Mar 23 15:10:28.502: INFO: Created: latency-svc-dt4lp
Mar 23 15:10:28.502: INFO: Created: latency-svc-7zv4s
Mar 23 15:10:28.503: INFO: Created: latency-svc-g446k
Mar 23 15:10:28.503: INFO: Created: latency-svc-6tnfj
Mar 23 15:10:28.524: INFO: Got endpoints: latency-svc-g446k [1.412118578s]
Mar 23 15:10:28.532: INFO: Got endpoints: latency-svc-7zv4s [1.273682094s]
Mar 23 15:10:28.533: INFO: Got endpoints: latency-svc-dt4lp [1.179750345s]
Mar 23 15:10:28.533: INFO: Got endpoints: latency-svc-6tnfj [1.345575511s]
Mar 23 15:10:28.565: INFO: Created: latency-svc-5fsl8
Mar 23 15:10:28.590: INFO: Got endpoints: latency-svc-5fsl8 [1.134915051s]
Mar 23 15:10:28.626: INFO: Created: latency-svc-qlqw4
Mar 23 15:10:28.646: INFO: Got endpoints: latency-svc-qlqw4 [1.113207031s]
Mar 23 15:10:28.695: INFO: Created: latency-svc-5hmrb
Mar 23 15:10:28.716: INFO: Got endpoints: latency-svc-5hmrb [1.080353248s]
Mar 23 15:10:28.757: INFO: Created: latency-svc-vtb95
Mar 23 15:10:28.784: INFO: Got endpoints: latency-svc-vtb95 [966.291937ms]
Mar 23 15:10:28.905: INFO: Created: latency-svc-rr8cq
Mar 23 15:10:28.937: INFO: Got endpoints: latency-svc-rr8cq [1.110111581s]
Mar 23 15:10:28.995: INFO: Created: latency-svc-2lsn5
Mar 23 15:10:29.018: INFO: Got endpoints: latency-svc-2lsn5 [1.146524908s]
Mar 23 15:10:29.090: INFO: Created: latency-svc-ch55n
Mar 23 15:10:29.113: INFO: Got endpoints: latency-svc-ch55n [1.191298466s]
Mar 23 15:10:29.178: INFO: Created: latency-svc-9x8qc
Mar 23 15:10:29.201: INFO: Got endpoints: latency-svc-9x8qc [1.195322356s]
Mar 23 15:10:29.249: INFO: Created: latency-svc-hcmcx
Mar 23 15:10:29.272: INFO: Got endpoints: latency-svc-hcmcx [1.184769828s]
Mar 23 15:10:29.323: INFO: Created: latency-svc-kwbl7
Mar 23 15:10:29.344: INFO: Got endpoints: latency-svc-kwbl7 [1.163728367s]
Mar 23 15:10:29.400: INFO: Created: latency-svc-6bjv4
Mar 23 15:10:29.425: INFO: Got endpoints: latency-svc-6bjv4 [1.179640648s]
Mar 23 15:10:29.510: INFO: Created: latency-svc-dgqxs
Mar 23 15:10:29.581: INFO: Got endpoints: latency-svc-dgqxs [1.057422358s]
Mar 23 15:10:29.572: INFO: Created: latency-svc-l5dt4
Mar 23 15:10:29.596: INFO: Got endpoints: latency-svc-l5dt4 [1.063517284s]
Mar 23 15:10:29.635: INFO: Created: latency-svc-nv748
Mar 23 15:10:29.660: INFO: Got endpoints: latency-svc-nv748 [1.126569931s]
Mar 23 15:10:29.697: INFO: Created: latency-svc-8lf52
Mar 23 15:10:29.758: INFO: Created: latency-svc-dnvbp
Mar 23 15:10:29.826: INFO: Created: latency-svc-jnrph
Mar 23 15:10:29.856: INFO: Got endpoints: latency-svc-dnvbp [1.265480392s]
Mar 23 15:10:29.856: INFO: Got endpoints: latency-svc-jnrph [1.209861776s]
Mar 23 15:10:29.857: INFO: Got endpoints: latency-svc-8lf52 [1.324069815s]
Mar 23 15:10:29.883: INFO: Created: latency-svc-zprb8
Mar 23 15:10:29.927: INFO: Got endpoints: latency-svc-zprb8 [1.210706351s]
Mar 23 15:10:29.960: INFO: Created: latency-svc-7dcsm
Mar 23 15:10:30.014: INFO: Got endpoints: latency-svc-7dcsm [1.230068852s]
Mar 23 15:10:30.044: INFO: Created: latency-svc-5vcvg
Mar 23 15:10:30.065: INFO: Got endpoints: latency-svc-5vcvg [1.127665276s]
Mar 23 15:10:30.114: INFO: Created: latency-svc-9wbkk
Mar 23 15:10:30.134: INFO: Got endpoints: latency-svc-9wbkk [1.116325021s]
Mar 23 15:10:30.205: INFO: Created: latency-svc-v6qg5
Mar 23 15:10:30.234: INFO: Got endpoints: latency-svc-v6qg5 [1.120834814s]
Mar 23 15:10:30.349: INFO: Created: latency-svc-4vvk5
Mar 23 15:10:30.364: INFO: Got endpoints: latency-svc-4vvk5 [1.162522385s]
Mar 23 15:10:30.449: INFO: Created: latency-svc-995kq
Mar 23 15:10:30.498: INFO: Got endpoints: latency-svc-995kq [1.225969401s]
Mar 23 15:10:30.526: INFO: Created: latency-svc-xzs86
Mar 23 15:10:30.551: INFO: Got endpoints: latency-svc-xzs86 [1.206444547s]
Mar 23 15:10:30.610: INFO: Created: latency-svc-swfmb
Mar 23 15:10:30.633: INFO: Got endpoints: latency-svc-swfmb [1.107800192s]
Mar 23 15:10:30.832: INFO: Created: latency-svc-khrd9
Mar 23 15:10:30.842: INFO: Created: latency-svc-ms4tt
Mar 23 15:10:30.868: INFO: Got endpoints: latency-svc-khrd9 [1.286641231s]
Mar 23 15:10:30.878: INFO: Got endpoints: latency-svc-ms4tt [1.281847711s]
Mar 23 15:10:30.900: INFO: Created: latency-svc-fkmb2
Mar 23 15:10:30.923: INFO: Got endpoints: latency-svc-fkmb2 [1.262591191s]
Mar 23 15:10:30.990: INFO: Created: latency-svc-9f2cb
Mar 23 15:10:31.017: INFO: Got endpoints: latency-svc-9f2cb [1.16105795s]
Mar 23 15:10:31.257: INFO: Created: latency-svc-gxlrg
Mar 23 15:10:31.257: INFO: Created: latency-svc-xd8cr
Mar 23 15:10:31.257: INFO: Created: latency-svc-wgf45
Mar 23 15:10:31.257: INFO: Created: latency-svc-5m8x5
Mar 23 15:10:31.290: INFO: Got endpoints: latency-svc-5m8x5 [1.363248615s]
Mar 23 15:10:31.291: INFO: Got endpoints: latency-svc-wgf45 [1.43441659s]
Mar 23 15:10:31.291: INFO: Got endpoints: latency-svc-xd8cr [1.276339732s]
Mar 23 15:10:31.299: INFO: Got endpoints: latency-svc-gxlrg [1.441700193s]
Mar 23 15:10:31.328: INFO: Created: latency-svc-lnm44
Mar 23 15:10:31.360: INFO: Got endpoints: latency-svc-lnm44 [1.295229632s]
Mar 23 15:10:31.565: INFO: Created: latency-svc-75qhx
Mar 23 15:10:31.590: INFO: Got endpoints: latency-svc-75qhx [1.455627057s]
Mar 23 15:10:31.631: INFO: Created: latency-svc-drt7q
Mar 23 15:10:31.650: INFO: Got endpoints: latency-svc-drt7q [1.415960278s]
Mar 23 15:10:31.726: INFO: Created: latency-svc-24mmr
Mar 23 15:10:31.771: INFO: Got endpoints: latency-svc-24mmr [1.406972849s]
Mar 23 15:10:31.849: INFO: Created: latency-svc-6d659
Mar 23 15:10:31.875: INFO: Got endpoints: latency-svc-6d659 [1.377448012s]
Mar 23 15:10:32.189: INFO: Created: latency-svc-2ffq9
Mar 23 15:10:32.273: INFO: Created: latency-svc-qvvsg
Mar 23 15:10:32.305: INFO: Got endpoints: latency-svc-2ffq9 [1.754374649s]
Mar 23 15:10:32.312: INFO: Got endpoints: latency-svc-qvvsg [1.679773205s]
Mar 23 15:10:32.385: INFO: Created: latency-svc-2s5ls
Mar 23 15:10:32.414: INFO: Got endpoints: latency-svc-2s5ls [1.546133147s]
Mar 23 15:10:32.561: INFO: Created: latency-svc-zmkq5
Mar 23 15:10:32.594: INFO: Got endpoints: latency-svc-zmkq5 [1.716402179s]
Mar 23 15:10:32.952: INFO: Created: latency-svc-fgcjz
Mar 23 15:10:32.987: INFO: Got endpoints: latency-svc-fgcjz [2.063933859s]
Mar 23 15:10:33.070: INFO: Created: latency-svc-4kqlh
Mar 23 15:10:33.100: INFO: Got endpoints: latency-svc-4kqlh [2.081953002s]
Mar 23 15:10:33.184: INFO: Created: latency-svc-m5zlr
Mar 23 15:10:33.231: INFO: Got endpoints: latency-svc-m5zlr [1.940467212s]
Mar 23 15:10:33.299: INFO: Created: latency-svc-z7v75
Mar 23 15:10:33.475: INFO: Created: latency-svc-bxvfw
Mar 23 15:10:33.537: INFO: Got endpoints: latency-svc-z7v75 [2.246508725s]
Mar 23 15:10:33.544: INFO: Got endpoints: latency-svc-bxvfw [2.25344892s]
Mar 23 15:10:33.589: INFO: Created: latency-svc-vwmll
Mar 23 15:10:33.612: INFO: Got endpoints: latency-svc-vwmll [2.31305553s]
Mar 23 15:10:33.654: INFO: Created: latency-svc-lbg8s
Mar 23 15:10:33.675: INFO: Got endpoints: latency-svc-lbg8s [2.315045113s]
Mar 23 15:10:33.723: INFO: Created: latency-svc-zkz2j
Mar 23 15:10:33.754: INFO: Got endpoints: latency-svc-zkz2j [2.164188175s]
Mar 23 15:10:33.803: INFO: Created: latency-svc-8qxmd
Mar 23 15:10:33.939: INFO: Got endpoints: latency-svc-8qxmd [2.288804986s]
Mar 23 15:10:34.031: INFO: Created: latency-svc-s556f
Mar 23 15:10:34.038: INFO: Created: latency-svc-ttrpk
Mar 23 15:10:34.047: INFO: Created: latency-svc-6rwrp
Mar 23 15:10:34.063: INFO: Got endpoints: latency-svc-s556f [2.292076876s]
Mar 23 15:10:34.077: INFO: Got endpoints: latency-svc-6rwrp [1.772264984s]
Mar 23 15:10:34.084: INFO: Got endpoints: latency-svc-ttrpk [2.208607364s]
Mar 23 15:10:34.130: INFO: Created: latency-svc-fqqs9
Mar 23 15:10:34.169: INFO: Got endpoints: latency-svc-fqqs9 [1.856044681s]
Mar 23 15:10:34.219: INFO: Created: latency-svc-wkshb
Mar 23 15:10:34.242: INFO: Got endpoints: latency-svc-wkshb [1.827058618s]
Mar 23 15:10:34.301: INFO: Created: latency-svc-gr8hh
Mar 23 15:10:34.324: INFO: Got endpoints: latency-svc-gr8hh [1.730035393s]
Mar 23 15:10:34.390: INFO: Created: latency-svc-h6fgv
Mar 23 15:10:34.412: INFO: Got endpoints: latency-svc-h6fgv [1.425150577s]
Mar 23 15:10:34.539: INFO: Created: latency-svc-v5pcd
Mar 23 15:10:34.571: INFO: Got endpoints: latency-svc-v5pcd [1.471458289s]
Mar 23 15:10:34.598: INFO: Created: latency-svc-sr5nv
Mar 23 15:10:34.624: INFO: Got endpoints: latency-svc-sr5nv [1.392600749s]
Mar 23 15:10:34.663: INFO: Created: latency-svc-5ghkb
Mar 23 15:10:34.684: INFO: Got endpoints: latency-svc-5ghkb [1.147321836s]
Mar 23 15:10:34.705: INFO: Created: latency-svc-hr9bj
Mar 23 15:10:34.724: INFO: Got endpoints: latency-svc-hr9bj [1.179740658s]
Mar 23 15:10:34.753: INFO: Created: latency-svc-48lhx
Mar 23 15:10:34.782: INFO: Got endpoints: latency-svc-48lhx [1.170044075s]
Mar 23 15:10:34.803: INFO: Created: latency-svc-zsg46
Mar 23 15:10:34.823: INFO: Got endpoints: latency-svc-zsg46 [1.147517289s]
Mar 23 15:10:34.843: INFO: Created: latency-svc-b5xl7
Mar 23 15:10:34.875: INFO: Got endpoints: latency-svc-b5xl7 [1.120766857s]
Mar 23 15:10:34.888: INFO: Created: latency-svc-96b7h
Mar 23 15:10:34.910: INFO: Got endpoints: latency-svc-96b7h [970.919218ms]
Mar 23 15:10:34.940: INFO: Created: latency-svc-q6zjw
Mar 23 15:10:34.972: INFO: Got endpoints: latency-svc-q6zjw [906.806629ms]
Mar 23 15:10:35.002: INFO: Created: latency-svc-jpz62
Mar 23 15:10:35.015: INFO: Got endpoints: latency-svc-jpz62 [937.595168ms]
Mar 23 15:10:35.113: INFO: Created: latency-svc-n6sr7
Mar 23 15:10:35.132: INFO: Got endpoints: latency-svc-n6sr7 [1.047358355s]
Mar 23 15:10:35.168: INFO: Created: latency-svc-njlln
Mar 23 15:10:35.202: INFO: Got endpoints: latency-svc-njlln [1.032977485s]
Mar 23 15:10:35.224: INFO: Created: latency-svc-d2cqw
Mar 23 15:10:35.248: INFO: Got endpoints: latency-svc-d2cqw [1.006390987s]
Mar 23 15:10:35.280: INFO: Created: latency-svc-glbzt
Mar 23 15:10:35.309: INFO: Got endpoints: latency-svc-glbzt [984.700084ms]
Mar 23 15:10:35.330: INFO: Created: latency-svc-8x2ng
Mar 23 15:10:35.354: INFO: Got endpoints: latency-svc-8x2ng [941.63417ms]
Mar 23 15:10:35.394: INFO: Created: latency-svc-krgbq
Mar 23 15:10:35.425: INFO: Got endpoints: latency-svc-krgbq [853.751657ms]
Mar 23 15:10:35.457: INFO: Created: latency-svc-k89xf
Mar 23 15:10:35.495: INFO: Got endpoints: latency-svc-k89xf [870.844367ms]
Mar 23 15:10:35.511: INFO: Created: latency-svc-frg2v
Mar 23 15:10:35.529: INFO: Got endpoints: latency-svc-frg2v [844.52622ms]
Mar 23 15:10:35.562: INFO: Created: latency-svc-mljhc
Mar 23 15:10:35.584: INFO: Got endpoints: latency-svc-mljhc [859.718268ms]
Mar 23 15:10:35.607: INFO: Created: latency-svc-zgdlx
Mar 23 15:10:35.639: INFO: Got endpoints: latency-svc-zgdlx [856.392974ms]
Mar 23 15:10:35.656: INFO: Created: latency-svc-9q4fd
Mar 23 15:10:35.705: INFO: Got endpoints: latency-svc-9q4fd [881.799526ms]
Mar 23 15:10:35.723: INFO: Created: latency-svc-ffzct
Mar 23 15:10:35.774: INFO: Got endpoints: latency-svc-ffzct [898.565633ms]
Mar 23 15:10:35.779: INFO: Created: latency-svc-jnn25
Mar 23 15:10:35.820: INFO: Got endpoints: latency-svc-jnn25 [910.12983ms]
Mar 23 15:10:35.843: INFO: Created: latency-svc-kq796
Mar 23 15:10:35.874: INFO: Got endpoints: latency-svc-kq796 [902.224201ms]
Mar 23 15:10:35.887: INFO: Created: latency-svc-bhpk9
Mar 23 15:10:35.917: INFO: Got endpoints: latency-svc-bhpk9 [901.500712ms]
Mar 23 15:10:35.933: INFO: Created: latency-svc-84sdf
Mar 23 15:10:35.965: INFO: Got endpoints: latency-svc-84sdf [833.387761ms]
Mar 23 15:10:35.997: INFO: Created: latency-svc-6nw4m
Mar 23 15:10:36.029: INFO: Got endpoints: latency-svc-6nw4m [827.090812ms]
Mar 23 15:10:36.047: INFO: Created: latency-svc-dxm7t
Mar 23 15:10:36.070: INFO: Got endpoints: latency-svc-dxm7t [821.971214ms]
Mar 23 15:10:36.091: INFO: Created: latency-svc-8hdxv
Mar 23 15:10:36.111: INFO: Got endpoints: latency-svc-8hdxv [802.123879ms]
Mar 23 15:10:36.136: INFO: Created: latency-svc-c74lq
Mar 23 15:10:36.166: INFO: Got endpoints: latency-svc-c74lq [812.634848ms]
Mar 23 15:10:36.197: INFO: Created: latency-svc-bfx79
Mar 23 15:10:36.229: INFO: Got endpoints: latency-svc-bfx79 [804.220348ms]
Mar 23 15:10:36.255: INFO: Created: latency-svc-p4dm9
Mar 23 15:10:36.284: INFO: Got endpoints: latency-svc-p4dm9 [788.572168ms]
Mar 23 15:10:36.327: INFO: Created: latency-svc-rh8km
Mar 23 15:10:36.347: INFO: Got endpoints: latency-svc-rh8km [818.653072ms]
Mar 23 15:10:36.431: INFO: Created: latency-svc-49qqk
Mar 23 15:10:36.454: INFO: Got endpoints: latency-svc-49qqk [869.962391ms]
Mar 23 15:10:36.522: INFO: Created: latency-svc-dr5zw
Mar 23 15:10:36.542: INFO: Got endpoints: latency-svc-dr5zw [903.374196ms]
Mar 23 15:10:36.597: INFO: Created: latency-svc-k6grq
Mar 23 15:10:36.621: INFO: Got endpoints: latency-svc-k6grq [915.924722ms]
Mar 23 15:10:36.670: INFO: Created: latency-svc-rwf9f
Mar 23 15:10:36.686: INFO: Got endpoints: latency-svc-rwf9f [911.835988ms]
Mar 23 15:10:36.708: INFO: Created: latency-svc-vwgdz
Mar 23 15:10:36.728: INFO: Got endpoints: latency-svc-vwgdz [907.914521ms]
Mar 23 15:10:36.769: INFO: Created: latency-svc-wxlv2
Mar 23 15:10:36.789: INFO: Got endpoints: latency-svc-wxlv2 [102.635436ms]
Mar 23 15:10:36.804: INFO: Created: latency-svc-hgv66
Mar 23 15:10:36.829: INFO: Got endpoints: latency-svc-hgv66 [955.076666ms]
Mar 23 15:10:36.989: INFO: Created: latency-svc-w789h
Mar 23 15:10:36.994: INFO: Created: latency-svc-d8b4j
Mar 23 15:10:36.994: INFO: Created: latency-svc-6dbks
Mar 23 15:10:36.995: INFO: Created: latency-svc-pjmcn
Mar 23 15:10:37.014: INFO: Got endpoints: latency-svc-w789h [1.048307854s]
Mar 23 15:10:37.022: INFO: Got endpoints: latency-svc-d8b4j [951.462253ms]
Mar 23 15:10:37.043: INFO: Created: latency-svc-cbsh2
Mar 23 15:10:37.043: INFO: Got endpoints: latency-svc-pjmcn [1.126493277s]
Mar 23 15:10:37.043: INFO: Got endpoints: latency-svc-6dbks [1.014428078s]
Mar 23 15:10:37.057: INFO: Got endpoints: latency-svc-cbsh2 [945.77571ms]
Mar 23 15:10:37.075: INFO: Created: latency-svc-hm95z
Mar 23 15:10:37.103: INFO: Got endpoints: latency-svc-hm95z [935.974485ms]
Mar 23 15:10:37.121: INFO: Created: latency-svc-wz6jd
Mar 23 15:10:37.145: INFO: Got endpoints: latency-svc-wz6jd [915.398216ms]
Mar 23 15:10:37.151: INFO: Created: latency-svc-w5xbm
Mar 23 15:10:37.175: INFO: Got endpoints: latency-svc-w5xbm [891.221063ms]
Mar 23 15:10:37.201: INFO: Created: latency-svc-48lb5
Mar 23 15:10:37.215: INFO: Got endpoints: latency-svc-48lb5 [867.826119ms]
Mar 23 15:10:37.237: INFO: Created: latency-svc-5tmz2
Mar 23 15:10:37.302: INFO: Got endpoints: latency-svc-5tmz2 [848.162339ms]
Mar 23 15:10:37.303: INFO: Created: latency-svc-s8ds9
Mar 23 15:10:37.331: INFO: Got endpoints: latency-svc-s8ds9 [788.095046ms]
Mar 23 15:10:37.351: INFO: Created: latency-svc-ntbw6
Mar 23 15:10:37.397: INFO: Created: latency-svc-95s8k
Mar 23 15:10:37.404: INFO: Got endpoints: latency-svc-ntbw6 [782.712854ms]
Mar 23 15:10:37.426: INFO: Got endpoints: latency-svc-95s8k [697.659247ms]
Mar 23 15:10:37.437: INFO: Created: latency-svc-95scp
Mar 23 15:10:37.461: INFO: Got endpoints: latency-svc-95scp [671.947222ms]
Mar 23 15:10:37.492: INFO: Created: latency-svc-7kd6p
Mar 23 15:10:37.524: INFO: Got endpoints: latency-svc-7kd6p [694.607139ms]
Mar 23 15:10:37.537: INFO: Created: latency-svc-jzwh6
Mar 23 15:10:37.578: INFO: Got endpoints: latency-svc-jzwh6 [563.78183ms]
Mar 23 15:10:37.650: INFO: Created: latency-svc-5h25b
Mar 23 15:10:37.677: INFO: Got endpoints: latency-svc-5h25b [655.522226ms]
Mar 23 15:10:37.718: INFO: Created: latency-svc-r54jp
Mar 23 15:10:37.741: INFO: Got endpoints: latency-svc-r54jp [697.366452ms]
Mar 23 15:10:37.804: INFO: Created: latency-svc-xkrjt
Mar 23 15:10:37.828: INFO: Got endpoints: latency-svc-xkrjt [784.302149ms]
Mar 23 15:10:37.857: INFO: Created: latency-svc-v7gk4
Mar 23 15:10:37.872: INFO: Got endpoints: latency-svc-v7gk4 [814.694845ms]
Mar 23 15:10:37.902: INFO: Created: latency-svc-rt97j
Mar 23 15:10:37.927: INFO: Got endpoints: latency-svc-rt97j [824.956893ms]
Mar 23 15:10:37.959: INFO: Created: latency-svc-kgpfr
Mar 23 15:10:37.985: INFO: Got endpoints: latency-svc-kgpfr [839.814363ms]
Mar 23 15:10:38.046: INFO: Created: latency-svc-2pqzk
Mar 23 15:10:38.071: INFO: Got endpoints: latency-svc-2pqzk [895.916706ms]
Mar 23 15:10:38.099: INFO: Created: latency-svc-lhrs8
Mar 23 15:10:38.126: INFO: Got endpoints: latency-svc-lhrs8 [910.957791ms]
Mar 23 15:10:38.195: INFO: Created: latency-svc-ns45w
Mar 23 15:10:38.237: INFO: Got endpoints: latency-svc-ns45w [934.431ms]
Mar 23 15:10:38.333: INFO: Created: latency-svc-99x8p
Mar 23 15:10:38.410: INFO: Created: latency-svc-sdf5w
Mar 23 15:10:38.441: INFO: Got endpoints: latency-svc-sdf5w [1.036765981s]
Mar 23 15:10:38.441: INFO: Got endpoints: latency-svc-99x8p [1.110248375s]
Mar 23 15:10:38.499: INFO: Created: latency-svc-sg8t5
Mar 23 15:10:38.537: INFO: Got endpoints: latency-svc-sg8t5 [1.111194422s]
Mar 23 15:10:38.576: INFO: Created: latency-svc-rbwk5
Mar 23 15:10:38.644: INFO: Created: latency-svc-p28s7
Mar 23 15:10:38.711: INFO: Created: latency-svc-98pqp
Mar 23 15:10:38.782: INFO: Got endpoints: latency-svc-98pqp [1.204290022s]
Mar 23 15:10:38.782: INFO: Got endpoints: latency-svc-rbwk5 [1.321614697s]
Mar 23 15:10:38.789: INFO: Created: latency-svc-lp7dh
Mar 23 15:10:38.791: INFO: Got endpoints: latency-svc-p28s7 [1.267264573s]
Mar 23 15:10:38.823: INFO: Got endpoints: latency-svc-lp7dh [1.145364678s]
Mar 23 15:10:38.859: INFO: Created: latency-svc-c76sp
Mar 23 15:10:38.883: INFO: Got endpoints: latency-svc-c76sp [1.142102418s]
Mar 23 15:10:39.094: INFO: Created: latency-svc-bch82
Mar 23 15:10:39.158: INFO: Got endpoints: latency-svc-bch82 [1.328519868s]
Mar 23 15:10:39.241: INFO: Created: latency-svc-mf9c8
Mar 23 15:10:39.267: INFO: Got endpoints: latency-svc-mf9c8 [1.394916397s]
Mar 23 15:10:39.572: INFO: Created: latency-svc-vqshb
Mar 23 15:10:39.595: INFO: Got endpoints: latency-svc-vqshb [1.667108537s]
Mar 23 15:10:39.720: INFO: Created: latency-svc-hmjj2
Mar 23 15:10:39.743: INFO: Got endpoints: latency-svc-hmjj2 [1.7579317s]
Mar 23 15:10:39.835: INFO: Created: latency-svc-b4z6f
Mar 23 15:10:39.868: INFO: Got endpoints: latency-svc-b4z6f [1.796141645s]
Mar 23 15:10:39.932: INFO: Created: latency-svc-gjdtj
Mar 23 15:10:39.973: INFO: Got endpoints: latency-svc-gjdtj [1.846377415s]
Mar 23 15:10:40.023: INFO: Created: latency-svc-wrmm4
Mar 23 15:10:40.034: INFO: Got endpoints: latency-svc-wrmm4 [1.797575415s]
Mar 23 15:10:40.096: INFO: Created: latency-svc-7l28t
Mar 23 15:10:40.121: INFO: Got endpoints: latency-svc-7l28t [1.680014666s]
Mar 23 15:10:40.175: INFO: Created: latency-svc-h8kst
Mar 23 15:10:40.201: INFO: Got endpoints: latency-svc-h8kst [1.760046105s]
Mar 23 15:10:40.306: INFO: Created: latency-svc-wc6j4
Mar 23 15:10:40.332: INFO: Got endpoints: latency-svc-wc6j4 [1.794192993s]
Mar 23 15:10:40.417: INFO: Created: latency-svc-49hkv
Mar 23 15:10:40.449: INFO: Got endpoints: latency-svc-49hkv [1.666285049s]
Mar 23 15:10:40.491: INFO: Created: latency-svc-7m8kc
Mar 23 15:10:40.507: INFO: Got endpoints: latency-svc-7m8kc [1.7235768s]
Mar 23 15:10:40.545: INFO: Created: latency-svc-tpjq7
Mar 23 15:10:40.565: INFO: Got endpoints: latency-svc-tpjq7 [1.773690289s]
Mar 23 15:10:40.627: INFO: Created: latency-svc-tgvqk
Mar 23 15:10:40.650: INFO: Got endpoints: latency-svc-tgvqk [1.826749274s]
Mar 23 15:10:40.701: INFO: Created: latency-svc-955t9
Mar 23 15:10:40.721: INFO: Got endpoints: latency-svc-955t9 [1.837551615s]
Mar 23 15:10:40.786: INFO: Created: latency-svc-tlmv4
Mar 23 15:10:40.813: INFO: Got endpoints: latency-svc-tlmv4 [1.654696775s]
Mar 23 15:10:40.859: INFO: Created: latency-svc-9dm7d
Mar 23 15:10:40.887: INFO: Got endpoints: latency-svc-9dm7d [1.619423894s]
Mar 23 15:10:40.933: INFO: Created: latency-svc-tzj5g
Mar 23 15:10:40.956: INFO: Got endpoints: latency-svc-tzj5g [1.360957719s]
Mar 23 15:10:41.019: INFO: Created: latency-svc-tftb2
Mar 23 15:10:41.040: INFO: Got endpoints: latency-svc-tftb2 [1.297417368s]
Mar 23 15:10:41.118: INFO: Created: latency-svc-kl4qx
Mar 23 15:10:41.145: INFO: Got endpoints: latency-svc-kl4qx [1.277164392s]
Mar 23 15:10:41.173: INFO: Created: latency-svc-x6l8w
Mar 23 15:10:41.200: INFO: Got endpoints: latency-svc-x6l8w [1.226997851s]
Mar 23 15:10:41.244: INFO: Created: latency-svc-ptf9q
Mar 23 15:10:41.279: INFO: Got endpoints: latency-svc-ptf9q [1.244848704s]
Mar 23 15:10:41.310: INFO: Created: latency-svc-m5xwg
Mar 23 15:10:41.334: INFO: Got endpoints: latency-svc-m5xwg [1.213031417s]
Mar 23 15:10:41.400: INFO: Created: latency-svc-4k2qd
Mar 23 15:10:41.417: INFO: Got endpoints: latency-svc-4k2qd [1.216185037s]
Mar 23 15:10:41.463: INFO: Created: latency-svc-6fvbf
Mar 23 15:10:41.480: INFO: Got endpoints: latency-svc-6fvbf [1.148028277s]
Mar 23 15:10:41.529: INFO: Created: latency-svc-s7n4n
Mar 23 15:10:41.550: INFO: Got endpoints: latency-svc-s7n4n [1.101240429s]
Mar 23 15:10:41.586: INFO: Created: latency-svc-rwb7q
Mar 23 15:10:41.606: INFO: Got endpoints: latency-svc-rwb7q [1.099170821s]
Mar 23 15:10:41.656: INFO: Created: latency-svc-zd2ls
Mar 23 15:10:41.671: INFO: Got endpoints: latency-svc-zd2ls [1.106385926s]
Mar 23 15:10:41.711: INFO: Created: latency-svc-7sp7c
Mar 23 15:10:41.730: INFO: Got endpoints: latency-svc-7sp7c [1.080777055s]
Mar 23 15:10:41.803: INFO: Created: latency-svc-xrs55
Mar 23 15:10:41.816: INFO: Got endpoints: latency-svc-xrs55 [1.095043267s]
Mar 23 15:10:41.871: INFO: Created: latency-svc-v74ds
Mar 23 15:10:41.893: INFO: Got endpoints: latency-svc-v74ds [1.079405793s]
Mar 23 15:10:41.942: INFO: Created: latency-svc-trvss
Mar 23 15:10:41.960: INFO: Got endpoints: latency-svc-trvss [1.073392459s]
Mar 23 15:10:42.008: INFO: Created: latency-svc-6kqsm
Mar 23 15:10:42.024: INFO: Got endpoints: latency-svc-6kqsm [1.067586325s]
Mar 23 15:10:42.024: INFO: Latencies: [102.635436ms 171.138474ms 273.718478ms 563.78183ms 607.918623ms 617.446298ms 629.189717ms 655.522226ms 671.947222ms 694.607139ms 697.366452ms 697.659247ms 730.366065ms 782.712854ms 784.302149ms 788.095046ms 788.572168ms 802.123879ms 804.220348ms 812.634848ms 814.694845ms 818.653072ms 821.971214ms 823.728399ms 824.956893ms 827.090812ms 833.387761ms 839.814363ms 844.52622ms 848.162339ms 853.751657ms 856.392974ms 859.718268ms 867.826119ms 869.962391ms 870.844367ms 879.4859ms 881.799526ms 891.221063ms 895.916706ms 898.565633ms 901.500712ms 902.224201ms 903.374196ms 906.806629ms 907.914521ms 910.12983ms 910.957791ms 911.835988ms 915.398216ms 915.924722ms 934.431ms 935.974485ms 937.595168ms 941.63417ms 945.77571ms 951.462253ms 955.076666ms 966.291937ms 970.919218ms 981.861751ms 984.700084ms 1.006390987s 1.014428078s 1.032977485s 1.036765981s 1.047358355s 1.048307854s 1.052648534s 1.057422358s 1.063517284s 1.067586325s 1.073392459s 1.079405793s 1.080353248s 1.080777055s 1.095043267s 1.099170821s 1.101240429s 1.106385926s 1.107800192s 1.110111581s 1.110248375s 1.111194422s 1.113207031s 1.116325021s 1.120766857s 1.120834814s 1.126493277s 1.126569931s 1.127665276s 1.134915051s 1.142102418s 1.143066044s 1.145364678s 1.146524908s 1.147321836s 1.147517289s 1.148028277s 1.16105795s 1.162522385s 1.163728367s 1.170044075s 1.179640648s 1.179740658s 1.179750345s 1.184769828s 1.191298466s 1.195322356s 1.204290022s 1.206444547s 1.209861776s 1.210706351s 1.213031417s 1.216185037s 1.225969401s 1.226997851s 1.230068852s 1.244006241s 1.244848704s 1.262591191s 1.265480392s 1.267264573s 1.273682094s 1.276339732s 1.277164392s 1.281847711s 1.286641231s 1.295229632s 1.297417368s 1.314605171s 1.321614697s 1.324069815s 1.328519868s 1.345575511s 1.360957719s 1.363248615s 1.377448012s 1.392600749s 1.394916397s 1.406972849s 1.412118578s 1.415960278s 1.425150577s 1.43441659s 1.441700193s 1.455627057s 1.471458289s 1.471909809s 1.52050552s 1.546133147s 1.573095185s 1.573237424s 1.586057474s 1.609034776s 1.613172313s 1.619423894s 1.648460726s 1.654696775s 1.666285049s 1.667108537s 1.679773205s 1.680014666s 1.684433534s 1.697544106s 1.712866369s 1.716402179s 1.7235768s 1.730035393s 1.730488627s 1.731783293s 1.735304605s 1.754374649s 1.7579317s 1.760046105s 1.762401559s 1.772264984s 1.773690289s 1.780327815s 1.794192993s 1.796141645s 1.797575415s 1.826749274s 1.827058618s 1.837551615s 1.842156515s 1.844531449s 1.846377415s 1.856044681s 1.940467212s 2.063933859s 2.081953002s 2.164188175s 2.208607364s 2.246508725s 2.25344892s 2.288804986s 2.292076876s 2.31305553s 2.315045113s]
Mar 23 15:10:42.024: INFO: 50 %ile: 1.162522385s
Mar 23 15:10:42.024: INFO: 90 %ile: 1.796141645s
Mar 23 15:10:42.024: INFO: 99 %ile: 2.31305553s
Mar 23 15:10:42.024: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:10:42.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4908" for this suite.
Mar 23 15:11:32.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:11:36.119: INFO: namespace svc-latency-4908 deletion completed in 54.075796941s

• [SLOW TEST:76.448 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:11:36.121: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 23 15:11:37.108: INFO: Number of nodes with available pods: 0
Mar 23 15:11:37.108: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 15:11:38.201: INFO: Number of nodes with available pods: 0
Mar 23 15:11:38.201: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 15:11:39.170: INFO: Number of nodes with available pods: 0
Mar 23 15:11:39.170: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 15:11:40.166: INFO: Number of nodes with available pods: 1
Mar 23 15:11:40.166: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 15:11:41.161: INFO: Number of nodes with available pods: 3
Mar 23 15:11:41.161: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Mar 23 15:11:41.391: INFO: Number of nodes with available pods: 2
Mar 23 15:11:41.391: INFO: Node 10.241.69.181 is running more than one daemon pod
Mar 23 15:11:42.440: INFO: Number of nodes with available pods: 2
Mar 23 15:11:42.440: INFO: Node 10.241.69.181 is running more than one daemon pod
Mar 23 15:11:43.433: INFO: Number of nodes with available pods: 2
Mar 23 15:11:43.433: INFO: Node 10.241.69.181 is running more than one daemon pod
Mar 23 15:11:44.438: INFO: Number of nodes with available pods: 2
Mar 23 15:11:44.439: INFO: Node 10.241.69.181 is running more than one daemon pod
Mar 23 15:11:45.428: INFO: Number of nodes with available pods: 2
Mar 23 15:11:45.428: INFO: Node 10.241.69.181 is running more than one daemon pod
Mar 23 15:11:46.430: INFO: Number of nodes with available pods: 2
Mar 23 15:11:46.430: INFO: Node 10.241.69.181 is running more than one daemon pod
Mar 23 15:11:47.430: INFO: Number of nodes with available pods: 2
Mar 23 15:11:47.430: INFO: Node 10.241.69.181 is running more than one daemon pod
Mar 23 15:11:48.755: INFO: Number of nodes with available pods: 2
Mar 23 15:11:48.755: INFO: Node 10.241.69.181 is running more than one daemon pod
Mar 23 15:11:49.422: INFO: Number of nodes with available pods: 2
Mar 23 15:11:49.422: INFO: Node 10.241.69.181 is running more than one daemon pod
Mar 23 15:11:50.455: INFO: Number of nodes with available pods: 3
Mar 23 15:11:50.455: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4314, will wait for the garbage collector to delete the pods
Mar 23 15:11:50.637: INFO: Deleting DaemonSet.extensions daemon-set took: 72.514624ms
Mar 23 15:11:51.238: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.326974ms
Mar 23 15:11:57.610: INFO: Number of nodes with available pods: 0
Mar 23 15:11:57.610: INFO: Number of running nodes: 0, number of available pods: 0
Mar 23 15:11:57.630: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4314/daemonsets","resourceVersion":"56238"},"items":null}

Mar 23 15:11:57.649: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4314/pods","resourceVersion":"56239"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:11:57.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4314" for this suite.
Mar 23 15:12:13.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:12:17.184: INFO: namespace daemonsets-4314 deletion completed in 19.436539541s

• [SLOW TEST:41.065 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:12:17.192: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8393
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-8393
Mar 23 15:12:17.820: INFO: Found 0 stateful pods, waiting for 1
Mar 23 15:12:27.840: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 23 15:12:27.952: INFO: Deleting all statefulset in ns statefulset-8393
Mar 23 15:12:27.982: INFO: Scaling statefulset ss to 0
Mar 23 15:12:48.120: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 15:12:48.140: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:12:48.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8393" for this suite.
Mar 23 15:13:02.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:13:05.671: INFO: namespace statefulset-8393 deletion completed in 17.416128814s

• [SLOW TEST:48.479 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:13:05.671: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 15:13:06.350: INFO: Waiting up to 5m0s for pod "busybox-user-65534-5aac23c0-a752-48d6-9256-9db6414ae7d0" in namespace "security-context-test-6174" to be "success or failure"
Mar 23 15:13:06.402: INFO: Pod "busybox-user-65534-5aac23c0-a752-48d6-9256-9db6414ae7d0": Phase="Pending", Reason="", readiness=false. Elapsed: 52.221634ms
Mar 23 15:13:08.444: INFO: Pod "busybox-user-65534-5aac23c0-a752-48d6-9256-9db6414ae7d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.094405065s
Mar 23 15:13:08.444: INFO: Pod "busybox-user-65534-5aac23c0-a752-48d6-9256-9db6414ae7d0" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:13:08.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6174" for this suite.
Mar 23 15:13:20.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:13:23.971: INFO: namespace security-context-test-6174 deletion completed in 15.507642252s

• [SLOW TEST:18.300 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:13:23.971: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 15:13:25.456: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 15:13:27.561: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720573205, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720573205, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720573205, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720573205, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 15:13:30.739: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
Mar 23 15:13:30.898: INFO: Waiting for webhook configuration to be ready...
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:13:31.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2116" for this suite.
Mar 23 15:13:43.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:13:46.628: INFO: namespace webhook-2116 deletion completed in 15.451216832s
STEP: Destroying namespace "webhook-2116-markers" for this suite.
Mar 23 15:13:58.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:14:01.672: INFO: namespace webhook-2116-markers deletion completed in 15.043922335s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:37.943 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:14:01.916: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar 23 15:14:07.456: INFO: Successfully updated pod "labelsupdatec7d54fcd-e547-4367-8343-3d36e6d411e1"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:14:09.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7885" for this suite.
Mar 23 15:14:29.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:14:32.489: INFO: namespace downward-api-7885 deletion completed in 22.909828413s

• [SLOW TEST:30.573 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:14:32.490: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-b4e27790-c226-40e1-9635-68e95b34f9a7 in namespace container-probe-2595
Mar 23 15:14:37.193: INFO: Started pod busybox-b4e27790-c226-40e1-9635-68e95b34f9a7 in namespace container-probe-2595
STEP: checking the pod's current state and verifying that restartCount is present
Mar 23 15:14:37.211: INFO: Initial restart count of pod busybox-b4e27790-c226-40e1-9635-68e95b34f9a7 is 0
Mar 23 15:15:21.726: INFO: Restart count of pod container-probe-2595/busybox-b4e27790-c226-40e1-9635-68e95b34f9a7 is now 1 (44.51436578s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:15:21.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2595" for this suite.
Mar 23 15:15:37.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:15:41.070: INFO: namespace container-probe-2595 deletion completed in 19.217539694s

• [SLOW TEST:68.581 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:15:41.071: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-kstk
STEP: Creating a pod to test atomic-volume-subpath
Mar 23 15:15:42.049: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kstk" in namespace "subpath-4023" to be "success or failure"
Mar 23 15:15:42.080: INFO: Pod "pod-subpath-test-configmap-kstk": Phase="Pending", Reason="", readiness=false. Elapsed: 30.986445ms
Mar 23 15:15:44.100: INFO: Pod "pod-subpath-test-configmap-kstk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051419129s
Mar 23 15:15:46.114: INFO: Pod "pod-subpath-test-configmap-kstk": Phase="Running", Reason="", readiness=true. Elapsed: 4.065332089s
Mar 23 15:15:48.136: INFO: Pod "pod-subpath-test-configmap-kstk": Phase="Running", Reason="", readiness=true. Elapsed: 6.086498124s
Mar 23 15:15:50.155: INFO: Pod "pod-subpath-test-configmap-kstk": Phase="Running", Reason="", readiness=true. Elapsed: 8.10630881s
Mar 23 15:15:52.217: INFO: Pod "pod-subpath-test-configmap-kstk": Phase="Running", Reason="", readiness=true. Elapsed: 10.168291974s
Mar 23 15:15:54.239: INFO: Pod "pod-subpath-test-configmap-kstk": Phase="Running", Reason="", readiness=true. Elapsed: 12.189730455s
Mar 23 15:15:56.265: INFO: Pod "pod-subpath-test-configmap-kstk": Phase="Running", Reason="", readiness=true. Elapsed: 14.215879586s
Mar 23 15:15:58.290: INFO: Pod "pod-subpath-test-configmap-kstk": Phase="Running", Reason="", readiness=true. Elapsed: 16.2410499s
Mar 23 15:16:00.311: INFO: Pod "pod-subpath-test-configmap-kstk": Phase="Running", Reason="", readiness=true. Elapsed: 18.261707392s
Mar 23 15:16:02.346: INFO: Pod "pod-subpath-test-configmap-kstk": Phase="Running", Reason="", readiness=true. Elapsed: 20.296871593s
Mar 23 15:16:04.455: INFO: Pod "pod-subpath-test-configmap-kstk": Phase="Running", Reason="", readiness=true. Elapsed: 22.40617814s
Mar 23 15:16:06.475: INFO: Pod "pod-subpath-test-configmap-kstk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.425477982s
STEP: Saw pod success
Mar 23 15:16:06.475: INFO: Pod "pod-subpath-test-configmap-kstk" satisfied condition "success or failure"
Mar 23 15:16:06.595: INFO: Trying to get logs from node 10.241.69.181 pod pod-subpath-test-configmap-kstk container test-container-subpath-configmap-kstk: <nil>
STEP: delete the pod
Mar 23 15:16:06.774: INFO: Waiting for pod pod-subpath-test-configmap-kstk to disappear
Mar 23 15:16:06.822: INFO: Pod pod-subpath-test-configmap-kstk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-kstk
Mar 23 15:16:06.822: INFO: Deleting pod "pod-subpath-test-configmap-kstk" in namespace "subpath-4023"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:16:06.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4023" for this suite.
Mar 23 15:16:18.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:16:21.937: INFO: namespace subpath-4023 deletion completed in 15.064519969s

• [SLOW TEST:40.867 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:16:21.937: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 15:16:23.553: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f30d72ad-dca3-4080-8d66-3010ad0761d2" in namespace "projected-7862" to be "success or failure"
Mar 23 15:16:23.570: INFO: Pod "downwardapi-volume-f30d72ad-dca3-4080-8d66-3010ad0761d2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.980365ms
Mar 23 15:16:25.592: INFO: Pod "downwardapi-volume-f30d72ad-dca3-4080-8d66-3010ad0761d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038877879s
Mar 23 15:16:27.607: INFO: Pod "downwardapi-volume-f30d72ad-dca3-4080-8d66-3010ad0761d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053901093s
STEP: Saw pod success
Mar 23 15:16:27.607: INFO: Pod "downwardapi-volume-f30d72ad-dca3-4080-8d66-3010ad0761d2" satisfied condition "success or failure"
Mar 23 15:16:27.626: INFO: Trying to get logs from node 10.241.69.181 pod downwardapi-volume-f30d72ad-dca3-4080-8d66-3010ad0761d2 container client-container: <nil>
STEP: delete the pod
Mar 23 15:16:27.775: INFO: Waiting for pod downwardapi-volume-f30d72ad-dca3-4080-8d66-3010ad0761d2 to disappear
Mar 23 15:16:27.804: INFO: Pod downwardapi-volume-f30d72ad-dca3-4080-8d66-3010ad0761d2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:16:27.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7862" for this suite.
Mar 23 15:16:39.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:16:44.104: INFO: namespace projected-7862 deletion completed in 16.265302737s

• [SLOW TEST:22.167 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:16:44.104: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Mar 23 15:16:44.718: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 15:17:01.599: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:17:38.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9723" for this suite.
Mar 23 15:17:50.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:17:53.957: INFO: namespace crd-publish-openapi-9723 deletion completed in 15.544603404s

• [SLOW TEST:69.853 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:17:53.958: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Mar 23 15:17:54.949: INFO: Waiting up to 5m0s for pod "pod-bc4b81c7-edab-4b5f-86f2-22ee6927183a" in namespace "emptydir-239" to be "success or failure"
Mar 23 15:17:55.112: INFO: Pod "pod-bc4b81c7-edab-4b5f-86f2-22ee6927183a": Phase="Pending", Reason="", readiness=false. Elapsed: 162.291731ms
Mar 23 15:17:57.134: INFO: Pod "pod-bc4b81c7-edab-4b5f-86f2-22ee6927183a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.184806167s
STEP: Saw pod success
Mar 23 15:17:57.134: INFO: Pod "pod-bc4b81c7-edab-4b5f-86f2-22ee6927183a" satisfied condition "success or failure"
Mar 23 15:17:57.161: INFO: Trying to get logs from node 10.241.69.181 pod pod-bc4b81c7-edab-4b5f-86f2-22ee6927183a container test-container: <nil>
STEP: delete the pod
Mar 23 15:17:57.355: INFO: Waiting for pod pod-bc4b81c7-edab-4b5f-86f2-22ee6927183a to disappear
Mar 23 15:17:57.377: INFO: Pod pod-bc4b81c7-edab-4b5f-86f2-22ee6927183a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:17:57.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-239" for this suite.
Mar 23 15:18:09.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:18:13.036: INFO: namespace emptydir-239 deletion completed in 15.62986793s

• [SLOW TEST:19.079 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:18:13.036: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 15:18:15.218: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 15:18:17.284: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720573495, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720573495, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720573495, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720573495, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 15:18:20.638: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:18:33.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2179" for this suite.
Mar 23 15:18:45.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:18:47.861: INFO: namespace webhook-2179 deletion completed in 14.4770768s
STEP: Destroying namespace "webhook-2179-markers" for this suite.
Mar 23 15:18:58.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:19:00.487: INFO: namespace webhook-2179-markers deletion completed in 12.626184274s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:47.565 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:19:00.605: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Mar 23 15:19:05.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec pod-sharedvolume-cbaedde5-a71f-455e-ae02-2b942cf1aac0 -c busybox-main-container --namespace=emptydir-9050 -- cat /usr/share/volumeshare/shareddata.txt'
Mar 23 15:19:06.338: INFO: stderr: ""
Mar 23 15:19:06.338: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:19:06.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9050" for this suite.
Mar 23 15:19:18.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:19:21.873: INFO: namespace emptydir-9050 deletion completed in 15.500960212s

• [SLOW TEST:21.268 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:19:21.873: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Mar 23 15:19:22.487: INFO: Waiting up to 5m0s for pod "client-containers-fa7044d8-f242-4ab3-a998-68cde990f558" in namespace "containers-7743" to be "success or failure"
Mar 23 15:19:22.545: INFO: Pod "client-containers-fa7044d8-f242-4ab3-a998-68cde990f558": Phase="Pending", Reason="", readiness=false. Elapsed: 57.68959ms
Mar 23 15:19:24.567: INFO: Pod "client-containers-fa7044d8-f242-4ab3-a998-68cde990f558": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079321231s
Mar 23 15:19:26.586: INFO: Pod "client-containers-fa7044d8-f242-4ab3-a998-68cde990f558": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.098612962s
STEP: Saw pod success
Mar 23 15:19:26.586: INFO: Pod "client-containers-fa7044d8-f242-4ab3-a998-68cde990f558" satisfied condition "success or failure"
Mar 23 15:19:26.605: INFO: Trying to get logs from node 10.241.69.181 pod client-containers-fa7044d8-f242-4ab3-a998-68cde990f558 container test-container: <nil>
STEP: delete the pod
Mar 23 15:19:26.753: INFO: Waiting for pod client-containers-fa7044d8-f242-4ab3-a998-68cde990f558 to disappear
Mar 23 15:19:26.778: INFO: Pod client-containers-fa7044d8-f242-4ab3-a998-68cde990f558 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:19:26.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7743" for this suite.
Mar 23 15:19:38.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:19:41.545: INFO: namespace containers-7743 deletion completed in 14.748259488s

• [SLOW TEST:19.672 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:19:41.546: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 15:19:42.151: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 23 15:19:51.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-5782 create -f -'
Mar 23 15:19:52.948: INFO: stderr: ""
Mar 23 15:19:52.948: INFO: stdout: "e2e-test-crd-publish-openapi-3749-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 23 15:19:52.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-5782 delete e2e-test-crd-publish-openapi-3749-crds test-cr'
Mar 23 15:19:53.123: INFO: stderr: ""
Mar 23 15:19:53.123: INFO: stdout: "e2e-test-crd-publish-openapi-3749-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 23 15:19:53.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-5782 apply -f -'
Mar 23 15:19:53.914: INFO: stderr: ""
Mar 23 15:19:53.914: INFO: stdout: "e2e-test-crd-publish-openapi-3749-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 23 15:19:53.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-5782 delete e2e-test-crd-publish-openapi-3749-crds test-cr'
Mar 23 15:19:54.492: INFO: stderr: ""
Mar 23 15:19:54.492: INFO: stdout: "e2e-test-crd-publish-openapi-3749-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Mar 23 15:19:54.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 explain e2e-test-crd-publish-openapi-3749-crds'
Mar 23 15:19:56.292: INFO: stderr: ""
Mar 23 15:19:56.292: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3749-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:20:10.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5782" for this suite.
Mar 23 15:20:22.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:20:24.993: INFO: namespace crd-publish-openapi-5782 deletion completed in 14.82746858s

• [SLOW TEST:43.448 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:20:24.994: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Mar 23 15:20:25.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 create -f - --namespace=kubectl-39'
Mar 23 15:20:26.240: INFO: stderr: ""
Mar 23 15:20:26.240: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 23 15:20:26.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-39'
Mar 23 15:20:26.397: INFO: stderr: ""
Mar 23 15:20:26.397: INFO: stdout: "update-demo-nautilus-8hrgt "
STEP: Replicas for name=update-demo: expected=2 actual=1
Mar 23 15:20:31.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-39'
Mar 23 15:20:31.549: INFO: stderr: ""
Mar 23 15:20:31.549: INFO: stdout: "update-demo-nautilus-8hrgt update-demo-nautilus-gtfcw "
Mar 23 15:20:31.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-8hrgt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-39'
Mar 23 15:20:31.700: INFO: stderr: ""
Mar 23 15:20:31.700: INFO: stdout: "true"
Mar 23 15:20:31.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-8hrgt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-39'
Mar 23 15:20:31.870: INFO: stderr: ""
Mar 23 15:20:31.870: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 15:20:31.870: INFO: validating pod update-demo-nautilus-8hrgt
Mar 23 15:20:31.925: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 15:20:31.925: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 15:20:31.925: INFO: update-demo-nautilus-8hrgt is verified up and running
Mar 23 15:20:31.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-gtfcw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-39'
Mar 23 15:20:32.088: INFO: stderr: ""
Mar 23 15:20:32.088: INFO: stdout: "true"
Mar 23 15:20:32.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-gtfcw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-39'
Mar 23 15:20:32.234: INFO: stderr: ""
Mar 23 15:20:32.234: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 15:20:32.234: INFO: validating pod update-demo-nautilus-gtfcw
Mar 23 15:20:32.292: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 15:20:32.292: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 15:20:32.292: INFO: update-demo-nautilus-gtfcw is verified up and running
STEP: scaling down the replication controller
Mar 23 15:20:32.297: INFO: scanned /root for discovery docs: <nil>
Mar 23 15:20:32.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-39'
Mar 23 15:20:33.677: INFO: stderr: ""
Mar 23 15:20:33.677: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 23 15:20:33.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-39'
Mar 23 15:20:33.869: INFO: stderr: ""
Mar 23 15:20:33.869: INFO: stdout: "update-demo-nautilus-8hrgt update-demo-nautilus-gtfcw "
STEP: Replicas for name=update-demo: expected=1 actual=2
Mar 23 15:20:38.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-39'
Mar 23 15:20:39.037: INFO: stderr: ""
Mar 23 15:20:39.037: INFO: stdout: "update-demo-nautilus-8hrgt "
Mar 23 15:20:39.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-8hrgt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-39'
Mar 23 15:20:39.183: INFO: stderr: ""
Mar 23 15:20:39.183: INFO: stdout: "true"
Mar 23 15:20:39.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-8hrgt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-39'
Mar 23 15:20:39.334: INFO: stderr: ""
Mar 23 15:20:39.334: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 15:20:39.334: INFO: validating pod update-demo-nautilus-8hrgt
Mar 23 15:20:39.366: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 15:20:39.366: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 15:20:39.366: INFO: update-demo-nautilus-8hrgt is verified up and running
STEP: scaling up the replication controller
Mar 23 15:20:39.371: INFO: scanned /root for discovery docs: <nil>
Mar 23 15:20:39.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-39'
Mar 23 15:20:40.654: INFO: stderr: ""
Mar 23 15:20:40.654: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 23 15:20:40.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-39'
Mar 23 15:20:40.858: INFO: stderr: ""
Mar 23 15:20:40.858: INFO: stdout: "update-demo-nautilus-8hrgt update-demo-nautilus-wpb9l "
Mar 23 15:20:40.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-8hrgt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-39'
Mar 23 15:20:41.049: INFO: stderr: ""
Mar 23 15:20:41.049: INFO: stdout: "true"
Mar 23 15:20:41.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-8hrgt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-39'
Mar 23 15:20:41.186: INFO: stderr: ""
Mar 23 15:20:41.186: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 15:20:41.186: INFO: validating pod update-demo-nautilus-8hrgt
Mar 23 15:20:41.220: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 15:20:41.220: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 15:20:41.220: INFO: update-demo-nautilus-8hrgt is verified up and running
Mar 23 15:20:41.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-wpb9l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-39'
Mar 23 15:20:41.365: INFO: stderr: ""
Mar 23 15:20:41.365: INFO: stdout: ""
Mar 23 15:20:41.365: INFO: update-demo-nautilus-wpb9l is created but not running
Mar 23 15:20:46.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-39'
Mar 23 15:20:46.514: INFO: stderr: ""
Mar 23 15:20:46.514: INFO: stdout: "update-demo-nautilus-8hrgt update-demo-nautilus-wpb9l "
Mar 23 15:20:46.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-8hrgt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-39'
Mar 23 15:20:46.692: INFO: stderr: ""
Mar 23 15:20:46.692: INFO: stdout: "true"
Mar 23 15:20:46.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-8hrgt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-39'
Mar 23 15:20:46.850: INFO: stderr: ""
Mar 23 15:20:46.850: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 15:20:46.850: INFO: validating pod update-demo-nautilus-8hrgt
Mar 23 15:20:46.886: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 15:20:46.886: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 15:20:46.886: INFO: update-demo-nautilus-8hrgt is verified up and running
Mar 23 15:20:46.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-wpb9l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-39'
Mar 23 15:20:47.048: INFO: stderr: ""
Mar 23 15:20:47.048: INFO: stdout: "true"
Mar 23 15:20:47.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-wpb9l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-39'
Mar 23 15:20:47.200: INFO: stderr: ""
Mar 23 15:20:47.200: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 15:20:47.200: INFO: validating pod update-demo-nautilus-wpb9l
Mar 23 15:20:47.242: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 15:20:47.242: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 15:20:47.242: INFO: update-demo-nautilus-wpb9l is verified up and running
STEP: using delete to clean up resources
Mar 23 15:20:47.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete --grace-period=0 --force -f - --namespace=kubectl-39'
Mar 23 15:20:47.467: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 15:20:47.468: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 23 15:20:47.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-39'
Mar 23 15:20:47.694: INFO: stderr: "No resources found in kubectl-39 namespace.\n"
Mar 23 15:20:47.694: INFO: stdout: ""
Mar 23 15:20:47.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -l name=update-demo --namespace=kubectl-39 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 23 15:20:47.898: INFO: stderr: ""
Mar 23 15:20:47.898: INFO: stdout: "update-demo-nautilus-8hrgt\nupdate-demo-nautilus-wpb9l\n"
Mar 23 15:20:48.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-39'
Mar 23 15:20:48.785: INFO: stderr: "No resources found in kubectl-39 namespace.\n"
Mar 23 15:20:48.785: INFO: stdout: ""
Mar 23 15:20:48.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -l name=update-demo --namespace=kubectl-39 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 23 15:20:48.974: INFO: stderr: ""
Mar 23 15:20:48.974: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:20:48.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-39" for this suite.
Mar 23 15:21:03.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:21:06.546: INFO: namespace kubectl-39 deletion completed in 17.549172019s

• [SLOW TEST:41.552 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:21:06.546: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:21:23.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1708" for this suite.
Mar 23 15:21:35.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:21:38.638: INFO: namespace resourcequota-1708 deletion completed in 15.027430126s

• [SLOW TEST:32.092 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:21:38.639: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-bdb95615-c7d3-4d28-aa99-9a1b11ccca77
STEP: Creating a pod to test consume configMaps
Mar 23 15:21:39.316: INFO: Waiting up to 5m0s for pod "pod-configmaps-26df710f-d147-477f-a826-0bfaee9887e7" in namespace "configmap-145" to be "success or failure"
Mar 23 15:21:39.343: INFO: Pod "pod-configmaps-26df710f-d147-477f-a826-0bfaee9887e7": Phase="Pending", Reason="", readiness=false. Elapsed: 26.275462ms
Mar 23 15:21:41.446: INFO: Pod "pod-configmaps-26df710f-d147-477f-a826-0bfaee9887e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.129806368s
Mar 23 15:21:43.466: INFO: Pod "pod-configmaps-26df710f-d147-477f-a826-0bfaee9887e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.149957602s
STEP: Saw pod success
Mar 23 15:21:43.466: INFO: Pod "pod-configmaps-26df710f-d147-477f-a826-0bfaee9887e7" satisfied condition "success or failure"
Mar 23 15:21:43.489: INFO: Trying to get logs from node 10.241.69.181 pod pod-configmaps-26df710f-d147-477f-a826-0bfaee9887e7 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 15:21:43.710: INFO: Waiting for pod pod-configmaps-26df710f-d147-477f-a826-0bfaee9887e7 to disappear
Mar 23 15:21:43.761: INFO: Pod pod-configmaps-26df710f-d147-477f-a826-0bfaee9887e7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:21:43.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-145" for this suite.
Mar 23 15:21:55.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:21:58.996: INFO: namespace configmap-145 deletion completed in 15.211229008s

• [SLOW TEST:20.357 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:21:58.996: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-c116aa89-6af5-4c74-bdb8-9c9f4962f649
STEP: Creating a pod to test consume configMaps
Mar 23 15:22:00.820: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e205a28a-11de-4d0e-8ddd-68d2ccc13c48" in namespace "projected-4478" to be "success or failure"
Mar 23 15:22:00.877: INFO: Pod "pod-projected-configmaps-e205a28a-11de-4d0e-8ddd-68d2ccc13c48": Phase="Pending", Reason="", readiness=false. Elapsed: 57.107046ms
Mar 23 15:22:02.895: INFO: Pod "pod-projected-configmaps-e205a28a-11de-4d0e-8ddd-68d2ccc13c48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.074656446s
Mar 23 15:22:04.937: INFO: Pod "pod-projected-configmaps-e205a28a-11de-4d0e-8ddd-68d2ccc13c48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.116448886s
STEP: Saw pod success
Mar 23 15:22:04.937: INFO: Pod "pod-projected-configmaps-e205a28a-11de-4d0e-8ddd-68d2ccc13c48" satisfied condition "success or failure"
Mar 23 15:22:04.958: INFO: Trying to get logs from node 10.241.69.181 pod pod-projected-configmaps-e205a28a-11de-4d0e-8ddd-68d2ccc13c48 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 15:22:05.103: INFO: Waiting for pod pod-projected-configmaps-e205a28a-11de-4d0e-8ddd-68d2ccc13c48 to disappear
Mar 23 15:22:05.124: INFO: Pod pod-projected-configmaps-e205a28a-11de-4d0e-8ddd-68d2ccc13c48 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:22:05.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4478" for this suite.
Mar 23 15:22:17.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:22:21.146: INFO: namespace projected-4478 deletion completed in 15.989705943s

• [SLOW TEST:22.150 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:22:21.149: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:22:33.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8650" for this suite.
Mar 23 15:22:47.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:22:50.798: INFO: namespace job-8650 deletion completed in 17.168637076s

• [SLOW TEST:29.649 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:22:50.798: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Mar 23 15:22:51.804: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4010 /api/v1/namespaces/watch-4010/configmaps/e2e-watch-test-resource-version 73309d85-3df4-4c2f-aedc-1d2447fe6682 61105 0 2020-03-23 15:22:51 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 23 15:22:51.804: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4010 /api/v1/namespaces/watch-4010/configmaps/e2e-watch-test-resource-version 73309d85-3df4-4c2f-aedc-1d2447fe6682 61106 0 2020-03-23 15:22:51 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:22:51.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4010" for this suite.
Mar 23 15:23:04.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:23:07.213: INFO: namespace watch-4010 deletion completed in 15.345819649s

• [SLOW TEST:16.415 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:23:07.215: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:23:13.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5591" for this suite.
Mar 23 15:23:35.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:23:38.020: INFO: namespace replication-controller-5591 deletion completed in 24.672973435s

• [SLOW TEST:30.805 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:23:38.022: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Mar 23 15:23:38.548: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar 23 15:23:38.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 create -f - --namespace=kubectl-8370'
Mar 23 15:23:39.153: INFO: stderr: ""
Mar 23 15:23:39.153: INFO: stdout: "service/redis-slave created\n"
Mar 23 15:23:39.153: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar 23 15:23:39.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 create -f - --namespace=kubectl-8370'
Mar 23 15:23:39.984: INFO: stderr: ""
Mar 23 15:23:39.984: INFO: stdout: "service/redis-master created\n"
Mar 23 15:23:39.984: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 23 15:23:39.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 create -f - --namespace=kubectl-8370'
Mar 23 15:23:40.660: INFO: stderr: ""
Mar 23 15:23:40.660: INFO: stdout: "service/frontend created\n"
Mar 23 15:23:40.660: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar 23 15:23:40.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 create -f - --namespace=kubectl-8370'
Mar 23 15:23:41.316: INFO: stderr: ""
Mar 23 15:23:41.316: INFO: stdout: "deployment.apps/frontend created\n"
Mar 23 15:23:41.317: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 23 15:23:41.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 create -f - --namespace=kubectl-8370'
Mar 23 15:23:42.302: INFO: stderr: ""
Mar 23 15:23:42.302: INFO: stdout: "deployment.apps/redis-master created\n"
Mar 23 15:23:42.302: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar 23 15:23:42.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 create -f - --namespace=kubectl-8370'
Mar 23 15:23:42.946: INFO: stderr: ""
Mar 23 15:23:42.946: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Mar 23 15:23:42.946: INFO: Waiting for all frontend pods to be Running.
Mar 23 15:24:02.997: INFO: Waiting for frontend to serve content.
Mar 23 15:24:08.066: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Mar 23 15:24:13.172: INFO: Trying to add a new entry to the guestbook.
Mar 23 15:24:13.233: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Mar 23 15:24:13.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete --grace-period=0 --force -f - --namespace=kubectl-8370'
Mar 23 15:24:13.601: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 15:24:13.601: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Mar 23 15:24:13.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete --grace-period=0 --force -f - --namespace=kubectl-8370'
Mar 23 15:24:13.839: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 15:24:13.839: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 23 15:24:13.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete --grace-period=0 --force -f - --namespace=kubectl-8370'
Mar 23 15:24:14.162: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 15:24:14.162: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 23 15:24:14.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete --grace-period=0 --force -f - --namespace=kubectl-8370'
Mar 23 15:24:14.394: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 15:24:14.394: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Mar 23 15:24:14.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete --grace-period=0 --force -f - --namespace=kubectl-8370'
Mar 23 15:24:14.644: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 15:24:14.644: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Mar 23 15:24:14.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete --grace-period=0 --force -f - --namespace=kubectl-8370'
Mar 23 15:24:14.829: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 15:24:14.829: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:24:14.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8370" for this suite.
Mar 23 15:24:37.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:24:39.667: INFO: namespace kubectl-8370 deletion completed in 24.776088177s

• [SLOW TEST:61.645 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:24:39.668: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 15:24:40.828: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 15:24:42.889: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720573880, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720573880, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720573880, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720573880, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 15:24:45.961: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:24:46.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3251" for this suite.
Mar 23 15:24:58.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:25:01.410: INFO: namespace webhook-3251 deletion completed in 15.068221422s
STEP: Destroying namespace "webhook-3251-markers" for this suite.
Mar 23 15:25:19.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:25:22.331: INFO: namespace webhook-3251-markers deletion completed in 20.920205751s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:43.020 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:25:22.692: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Mar 23 15:25:24.747: INFO: Waiting up to 5m0s for pod "pod-d7c94e25-dd81-46a0-99fd-9ac151f55179" in namespace "emptydir-8653" to be "success or failure"
Mar 23 15:25:24.776: INFO: Pod "pod-d7c94e25-dd81-46a0-99fd-9ac151f55179": Phase="Pending", Reason="", readiness=false. Elapsed: 28.810985ms
Mar 23 15:25:26.803: INFO: Pod "pod-d7c94e25-dd81-46a0-99fd-9ac151f55179": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05557123s
Mar 23 15:25:28.864: INFO: Pod "pod-d7c94e25-dd81-46a0-99fd-9ac151f55179": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.116916614s
STEP: Saw pod success
Mar 23 15:25:28.864: INFO: Pod "pod-d7c94e25-dd81-46a0-99fd-9ac151f55179" satisfied condition "success or failure"
Mar 23 15:25:28.896: INFO: Trying to get logs from node 10.241.69.181 pod pod-d7c94e25-dd81-46a0-99fd-9ac151f55179 container test-container: <nil>
STEP: delete the pod
Mar 23 15:25:29.240: INFO: Waiting for pod pod-d7c94e25-dd81-46a0-99fd-9ac151f55179 to disappear
Mar 23 15:25:29.388: INFO: Pod pod-d7c94e25-dd81-46a0-99fd-9ac151f55179 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:25:29.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8653" for this suite.
Mar 23 15:25:41.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:25:44.152: INFO: namespace emptydir-8653 deletion completed in 14.69878133s

• [SLOW TEST:21.461 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:25:44.152: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 15:25:45.089: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Mar 23 15:25:45.174: INFO: Number of nodes with available pods: 0
Mar 23 15:25:45.174: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 15:25:46.241: INFO: Number of nodes with available pods: 0
Mar 23 15:25:46.241: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 15:25:47.234: INFO: Number of nodes with available pods: 0
Mar 23 15:25:47.234: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 15:25:48.240: INFO: Number of nodes with available pods: 1
Mar 23 15:25:48.240: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 15:25:49.243: INFO: Number of nodes with available pods: 3
Mar 23 15:25:49.243: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Mar 23 15:25:49.483: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:49.483: INFO: Wrong image for pod: daemon-set-99wnr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:49.483: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:50.566: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:50.566: INFO: Wrong image for pod: daemon-set-99wnr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:50.566: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:51.534: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:51.534: INFO: Wrong image for pod: daemon-set-99wnr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:51.534: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:52.538: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:52.538: INFO: Wrong image for pod: daemon-set-99wnr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:52.538: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:53.543: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:53.543: INFO: Wrong image for pod: daemon-set-99wnr. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:53.543: INFO: Pod daemon-set-99wnr is not available
Mar 23 15:25:53.543: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:54.542: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:54.542: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:54.542: INFO: Pod daemon-set-l6xvl is not available
Mar 23 15:25:55.560: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:55.560: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:55.560: INFO: Pod daemon-set-l6xvl is not available
Mar 23 15:25:56.536: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:56.536: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:56.537: INFO: Pod daemon-set-l6xvl is not available
Mar 23 15:25:57.537: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:57.537: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:57.537: INFO: Pod daemon-set-l6xvl is not available
Mar 23 15:25:58.540: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:58.541: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:58.541: INFO: Pod daemon-set-l6xvl is not available
Mar 23 15:25:59.545: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:59.545: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:25:59.545: INFO: Pod daemon-set-l6xvl is not available
Mar 23 15:26:00.557: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:00.557: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:00.557: INFO: Pod daemon-set-l6xvl is not available
Mar 23 15:26:01.539: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:01.540: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:02.553: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:02.553: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:03.545: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:03.545: INFO: Pod daemon-set-6qhdc is not available
Mar 23 15:26:03.545: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:04.549: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:04.549: INFO: Pod daemon-set-6qhdc is not available
Mar 23 15:26:04.549: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:05.543: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:05.543: INFO: Pod daemon-set-6qhdc is not available
Mar 23 15:26:05.543: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:06.562: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:06.562: INFO: Pod daemon-set-6qhdc is not available
Mar 23 15:26:06.562: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:07.847: INFO: Wrong image for pod: daemon-set-6qhdc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:07.847: INFO: Pod daemon-set-6qhdc is not available
Mar 23 15:26:07.847: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:08.552: INFO: Pod daemon-set-g2f56 is not available
Mar 23 15:26:08.552: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:09.550: INFO: Pod daemon-set-g2f56 is not available
Mar 23 15:26:09.550: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:10.548: INFO: Pod daemon-set-g2f56 is not available
Mar 23 15:26:10.548: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:11.539: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:12.536: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:12.536: INFO: Pod daemon-set-hfgmc is not available
Mar 23 15:26:13.567: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:13.567: INFO: Pod daemon-set-hfgmc is not available
Mar 23 15:26:14.570: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:14.570: INFO: Pod daemon-set-hfgmc is not available
Mar 23 15:26:15.557: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:15.557: INFO: Pod daemon-set-hfgmc is not available
Mar 23 15:26:16.537: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:16.537: INFO: Pod daemon-set-hfgmc is not available
Mar 23 15:26:17.542: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:17.542: INFO: Pod daemon-set-hfgmc is not available
Mar 23 15:26:18.545: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:18.545: INFO: Pod daemon-set-hfgmc is not available
Mar 23 15:26:19.549: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:19.549: INFO: Pod daemon-set-hfgmc is not available
Mar 23 15:26:20.550: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:20.550: INFO: Pod daemon-set-hfgmc is not available
Mar 23 15:26:21.548: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:21.548: INFO: Pod daemon-set-hfgmc is not available
Mar 23 15:26:22.530: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:22.531: INFO: Pod daemon-set-hfgmc is not available
Mar 23 15:26:23.542: INFO: Wrong image for pod: daemon-set-hfgmc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Mar 23 15:26:23.542: INFO: Pod daemon-set-hfgmc is not available
Mar 23 15:26:24.542: INFO: Pod daemon-set-jc2rt is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Mar 23 15:26:24.656: INFO: Number of nodes with available pods: 2
Mar 23 15:26:24.656: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 15:26:25.722: INFO: Number of nodes with available pods: 2
Mar 23 15:26:25.722: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 15:26:26.719: INFO: Number of nodes with available pods: 2
Mar 23 15:26:26.719: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 15:26:27.732: INFO: Number of nodes with available pods: 2
Mar 23 15:26:27.732: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 15:26:28.722: INFO: Number of nodes with available pods: 2
Mar 23 15:26:28.722: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 15:26:29.719: INFO: Number of nodes with available pods: 2
Mar 23 15:26:29.719: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 15:26:30.727: INFO: Number of nodes with available pods: 3
Mar 23 15:26:30.727: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4142, will wait for the garbage collector to delete the pods
Mar 23 15:26:30.925: INFO: Deleting DaemonSet.extensions daemon-set took: 36.317373ms
Mar 23 15:26:31.626: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.44722ms
Mar 23 15:26:43.949: INFO: Number of nodes with available pods: 0
Mar 23 15:26:43.949: INFO: Number of running nodes: 0, number of available pods: 0
Mar 23 15:26:43.967: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4142/daemonsets","resourceVersion":"62978"},"items":null}

Mar 23 15:26:43.995: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4142/pods","resourceVersion":"62978"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:26:44.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4142" for this suite.
Mar 23 15:26:58.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:27:00.809: INFO: namespace daemonsets-4142 deletion completed in 16.684366718s

• [SLOW TEST:76.656 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:27:00.809: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar 23 15:27:06.412: INFO: Successfully updated pod "labelsupdate8af7783c-21db-406d-9193-e37eb796cd1a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:27:08.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4144" for this suite.
Mar 23 15:27:30.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:27:33.518: INFO: namespace projected-4144 deletion completed in 24.821736185s

• [SLOW TEST:32.709 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:27:33.518: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-591
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-591
STEP: creating replication controller externalsvc in namespace services-591
I0323 15:27:34.191970      22 runners.go:184] Created replication controller with name: externalsvc, namespace: services-591, replica count: 2
I0323 15:27:37.242507      22 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Mar 23 15:27:37.318: INFO: Creating new exec pod
Mar 23 15:27:41.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=services-591 execpod772mf -- /bin/sh -x -c nslookup clusterip-service'
Mar 23 15:27:42.300: INFO: stderr: "+ nslookup clusterip-service\n"
Mar 23 15:27:42.300: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-591.svc.cluster.local\tcanonical name = externalsvc.services-591.svc.cluster.local.\nName:\texternalsvc.services-591.svc.cluster.local\nAddress: 172.21.171.113\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-591, will wait for the garbage collector to delete the pods
Mar 23 15:27:42.549: INFO: Deleting ReplicationController externalsvc took: 89.626946ms
Mar 23 15:27:43.250: INFO: Terminating ReplicationController externalsvc pods took: 700.640622ms
Mar 23 15:27:54.343: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:27:54.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-591" for this suite.
Mar 23 15:28:08.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:28:11.268: INFO: namespace services-591 deletion completed in 16.827876214s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:37.751 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:28:11.269: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Mar 23 15:28:12.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 create -f - --namespace=kubectl-3925'
Mar 23 15:28:12.746: INFO: stderr: ""
Mar 23 15:28:12.746: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 23 15:28:13.782: INFO: Selector matched 1 pods for map[app:redis]
Mar 23 15:28:13.782: INFO: Found 0 / 1
Mar 23 15:28:14.806: INFO: Selector matched 1 pods for map[app:redis]
Mar 23 15:28:14.806: INFO: Found 0 / 1
Mar 23 15:28:15.777: INFO: Selector matched 1 pods for map[app:redis]
Mar 23 15:28:15.777: INFO: Found 0 / 1
Mar 23 15:28:16.771: INFO: Selector matched 1 pods for map[app:redis]
Mar 23 15:28:16.771: INFO: Found 1 / 1
Mar 23 15:28:16.771: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Mar 23 15:28:16.794: INFO: Selector matched 1 pods for map[app:redis]
Mar 23 15:28:16.794: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 23 15:28:16.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 patch pod redis-master-j5ww7 --namespace=kubectl-3925 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 23 15:28:17.050: INFO: stderr: ""
Mar 23 15:28:17.050: INFO: stdout: "pod/redis-master-j5ww7 patched\n"
STEP: checking annotations
Mar 23 15:28:17.075: INFO: Selector matched 1 pods for map[app:redis]
Mar 23 15:28:17.076: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:28:17.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3925" for this suite.
Mar 23 15:29:01.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:29:04.517: INFO: namespace kubectl-3925 deletion completed in 47.408358508s

• [SLOW TEST:53.248 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:29:04.518: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Mar 23 15:29:36.072: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0323 15:29:36.072600      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:29:36.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9663" for this suite.
Mar 23 15:29:48.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:29:50.858: INFO: namespace gc-9663 deletion completed in 14.749860283s

• [SLOW TEST:46.340 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:29:50.859: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Mar 23 15:29:51.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 create -f - --namespace=kubectl-5772'
Mar 23 15:29:52.003: INFO: stderr: ""
Mar 23 15:29:52.003: INFO: stdout: "pod/pause created\n"
Mar 23 15:29:52.003: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 23 15:29:52.003: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5772" to be "running and ready"
Mar 23 15:29:52.026: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 22.725771ms
Mar 23 15:29:54.096: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0920355s
Mar 23 15:29:56.127: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.123798025s
Mar 23 15:29:56.127: INFO: Pod "pause" satisfied condition "running and ready"
Mar 23 15:29:56.127: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Mar 23 15:29:56.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 label pods pause testing-label=testing-label-value --namespace=kubectl-5772'
Mar 23 15:29:56.750: INFO: stderr: ""
Mar 23 15:29:56.750: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Mar 23 15:29:56.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pod pause -L testing-label --namespace=kubectl-5772'
Mar 23 15:29:56.940: INFO: stderr: ""
Mar 23 15:29:56.940: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Mar 23 15:29:56.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 label pods pause testing-label- --namespace=kubectl-5772'
Mar 23 15:29:57.244: INFO: stderr: ""
Mar 23 15:29:57.244: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Mar 23 15:29:57.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pod pause -L testing-label --namespace=kubectl-5772'
Mar 23 15:29:57.396: INFO: stderr: ""
Mar 23 15:29:57.396: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Mar 23 15:29:57.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete --grace-period=0 --force -f - --namespace=kubectl-5772'
Mar 23 15:29:57.599: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 15:29:57.599: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 23 15:29:57.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get rc,svc -l name=pause --no-headers --namespace=kubectl-5772'
Mar 23 15:29:57.800: INFO: stderr: "No resources found in kubectl-5772 namespace.\n"
Mar 23 15:29:57.800: INFO: stdout: ""
Mar 23 15:29:57.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -l name=pause --namespace=kubectl-5772 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 23 15:29:57.955: INFO: stderr: ""
Mar 23 15:29:57.955: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:29:57.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5772" for this suite.
Mar 23 15:30:12.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:30:15.141: INFO: namespace kubectl-5772 deletion completed in 17.122324173s

• [SLOW TEST:24.282 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:30:15.141: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Mar 23 15:30:15.703: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-012311840 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:30:15.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8944" for this suite.
Mar 23 15:30:30.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:30:33.697: INFO: namespace kubectl-8944 deletion completed in 17.825168831s

• [SLOW TEST:18.556 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:30:33.698: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Mar 23 15:30:34.391: INFO: namespace kubectl-2647
Mar 23 15:30:34.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 create -f - --namespace=kubectl-2647'
Mar 23 15:30:35.095: INFO: stderr: ""
Mar 23 15:30:35.095: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 23 15:30:36.149: INFO: Selector matched 1 pods for map[app:redis]
Mar 23 15:30:36.149: INFO: Found 0 / 1
Mar 23 15:30:37.155: INFO: Selector matched 1 pods for map[app:redis]
Mar 23 15:30:37.155: INFO: Found 0 / 1
Mar 23 15:30:38.126: INFO: Selector matched 1 pods for map[app:redis]
Mar 23 15:30:38.126: INFO: Found 0 / 1
Mar 23 15:30:39.144: INFO: Selector matched 1 pods for map[app:redis]
Mar 23 15:30:39.144: INFO: Found 1 / 1
Mar 23 15:30:39.144: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 23 15:30:39.260: INFO: Selector matched 1 pods for map[app:redis]
Mar 23 15:30:39.260: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 23 15:30:39.260: INFO: wait on redis-master startup in kubectl-2647 
Mar 23 15:30:39.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 logs redis-master-4rzwf redis-master --namespace=kubectl-2647'
Mar 23 15:30:39.631: INFO: stderr: ""
Mar 23 15:30:39.631: INFO: stdout: "1:C 23 Mar 2020 15:30:37.616 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 23 Mar 2020 15:30:37.616 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 23 Mar 2020 15:30:37.616 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 23 Mar 2020 15:30:37.619 * Running mode=standalone, port=6379.\n1:M 23 Mar 2020 15:30:37.619 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Mar 2020 15:30:37.619 # Server initialized\n1:M 23 Mar 2020 15:30:37.619 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Mar 2020 15:30:37.619 * Ready to accept connections\n"
STEP: exposing RC
Mar 23 15:30:39.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-2647'
Mar 23 15:30:39.889: INFO: stderr: ""
Mar 23 15:30:39.889: INFO: stdout: "service/rm2 exposed\n"
Mar 23 15:30:39.903: INFO: Service rm2 in namespace kubectl-2647 found.
STEP: exposing service
Mar 23 15:30:41.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-2647'
Mar 23 15:30:42.178: INFO: stderr: ""
Mar 23 15:30:42.178: INFO: stdout: "service/rm3 exposed\n"
Mar 23 15:30:42.193: INFO: Service rm3 in namespace kubectl-2647 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:30:44.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2647" for this suite.
Mar 23 15:31:08.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:31:11.619: INFO: namespace kubectl-2647 deletion completed in 27.219002441s

• [SLOW TEST:37.921 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:31:11.619: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:31:17.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7663" for this suite.
Mar 23 15:31:29.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:31:32.020: INFO: namespace emptydir-wrapper-7663 deletion completed in 14.750477703s

• [SLOW TEST:20.401 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:31:32.022: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Mar 23 15:31:32.715: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9274 /api/v1/namespaces/watch-9274/configmaps/e2e-watch-test-watch-closed b3315044-0b6e-4efd-94a5-b489346b3943 65081 0 2020-03-23 15:31:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 23 15:31:32.715: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9274 /api/v1/namespaces/watch-9274/configmaps/e2e-watch-test-watch-closed b3315044-0b6e-4efd-94a5-b489346b3943 65088 0 2020-03-23 15:31:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 23 15:31:33.061: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9274 /api/v1/namespaces/watch-9274/configmaps/e2e-watch-test-watch-closed b3315044-0b6e-4efd-94a5-b489346b3943 65090 0 2020-03-23 15:31:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 23 15:31:33.062: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9274 /api/v1/namespaces/watch-9274/configmaps/e2e-watch-test-watch-closed b3315044-0b6e-4efd-94a5-b489346b3943 65093 0 2020-03-23 15:31:32 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:31:33.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9274" for this suite.
Mar 23 15:31:45.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:31:48.547: INFO: namespace watch-9274 deletion completed in 15.41628629s

• [SLOW TEST:16.526 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:31:48.548: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 23 15:31:49.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2740'
Mar 23 15:31:49.355: INFO: stderr: ""
Mar 23 15:31:49.355: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Mar 23 15:31:49.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete pods e2e-test-httpd-pod --namespace=kubectl-2740'
Mar 23 15:31:57.412: INFO: stderr: ""
Mar 23 15:31:57.412: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:31:57.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2740" for this suite.
Mar 23 15:32:11.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:32:14.480: INFO: namespace kubectl-2740 deletion completed in 17.007181314s

• [SLOW TEST:25.932 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:32:14.480: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 15:32:16.015: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 15:32:18.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574336, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574336, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574336, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574335, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 15:32:21.196: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:32:32.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7977" for this suite.
Mar 23 15:32:46.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:32:48.579: INFO: namespace webhook-7977 deletion completed in 15.98456598s
STEP: Destroying namespace "webhook-7977-markers" for this suite.
Mar 23 15:33:00.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:33:03.711: INFO: namespace webhook-7977-markers deletion completed in 15.131718865s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:49.444 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:33:03.925: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 23 15:33:14.816: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 23 15:33:14.915: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 23 15:33:16.916: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 23 15:33:16.947: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 23 15:33:18.916: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 23 15:33:18.937: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:33:18.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2141" for this suite.
Mar 23 15:34:03.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:34:06.243: INFO: namespace container-lifecycle-hook-2141 deletion completed in 47.249447109s

• [SLOW TEST:62.318 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:34:06.243: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar 23 15:34:07.505: INFO: Waiting up to 5m0s for pod "downward-api-c4a846c9-262e-4d62-bcf1-dcfcc7a9c656" in namespace "downward-api-5965" to be "success or failure"
Mar 23 15:34:07.545: INFO: Pod "downward-api-c4a846c9-262e-4d62-bcf1-dcfcc7a9c656": Phase="Pending", Reason="", readiness=false. Elapsed: 39.072325ms
Mar 23 15:34:09.574: INFO: Pod "downward-api-c4a846c9-262e-4d62-bcf1-dcfcc7a9c656": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068236048s
Mar 23 15:34:11.602: INFO: Pod "downward-api-c4a846c9-262e-4d62-bcf1-dcfcc7a9c656": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.095992698s
STEP: Saw pod success
Mar 23 15:34:11.602: INFO: Pod "downward-api-c4a846c9-262e-4d62-bcf1-dcfcc7a9c656" satisfied condition "success or failure"
Mar 23 15:34:11.626: INFO: Trying to get logs from node 10.241.69.181 pod downward-api-c4a846c9-262e-4d62-bcf1-dcfcc7a9c656 container dapi-container: <nil>
STEP: delete the pod
Mar 23 15:34:11.916: INFO: Waiting for pod downward-api-c4a846c9-262e-4d62-bcf1-dcfcc7a9c656 to disappear
Mar 23 15:34:11.962: INFO: Pod downward-api-c4a846c9-262e-4d62-bcf1-dcfcc7a9c656 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:34:11.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5965" for this suite.
Mar 23 15:34:24.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:34:26.923: INFO: namespace downward-api-5965 deletion completed in 14.912085164s

• [SLOW TEST:20.680 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:34:26.923: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-22ea4093-5aec-4439-a824-8631e39ecacf
STEP: Creating a pod to test consume configMaps
Mar 23 15:34:27.541: INFO: Waiting up to 5m0s for pod "pod-configmaps-c03fbc33-c287-4fa3-b078-26bda8b4397a" in namespace "configmap-6389" to be "success or failure"
Mar 23 15:34:27.570: INFO: Pod "pod-configmaps-c03fbc33-c287-4fa3-b078-26bda8b4397a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.577267ms
Mar 23 15:34:29.592: INFO: Pod "pod-configmaps-c03fbc33-c287-4fa3-b078-26bda8b4397a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050830412s
Mar 23 15:34:31.629: INFO: Pod "pod-configmaps-c03fbc33-c287-4fa3-b078-26bda8b4397a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.087512918s
STEP: Saw pod success
Mar 23 15:34:31.629: INFO: Pod "pod-configmaps-c03fbc33-c287-4fa3-b078-26bda8b4397a" satisfied condition "success or failure"
Mar 23 15:34:31.668: INFO: Trying to get logs from node 10.241.69.181 pod pod-configmaps-c03fbc33-c287-4fa3-b078-26bda8b4397a container configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 15:34:32.005: INFO: Waiting for pod pod-configmaps-c03fbc33-c287-4fa3-b078-26bda8b4397a to disappear
Mar 23 15:34:32.045: INFO: Pod pod-configmaps-c03fbc33-c287-4fa3-b078-26bda8b4397a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:34:32.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6389" for this suite.
Mar 23 15:34:42.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:34:45.543: INFO: namespace configmap-6389 deletion completed in 13.345607644s

• [SLOW TEST:18.621 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:34:45.546: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-132/configmap-test-92a91197-23f5-45ca-8894-5868af87ddb4
STEP: Creating a pod to test consume configMaps
Mar 23 15:34:46.159: INFO: Waiting up to 5m0s for pod "pod-configmaps-a5b8a663-f28e-4483-a80f-e59c415b8bc3" in namespace "configmap-132" to be "success or failure"
Mar 23 15:34:46.179: INFO: Pod "pod-configmaps-a5b8a663-f28e-4483-a80f-e59c415b8bc3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.438592ms
Mar 23 15:34:48.202: INFO: Pod "pod-configmaps-a5b8a663-f28e-4483-a80f-e59c415b8bc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.043286107s
STEP: Saw pod success
Mar 23 15:34:48.202: INFO: Pod "pod-configmaps-a5b8a663-f28e-4483-a80f-e59c415b8bc3" satisfied condition "success or failure"
Mar 23 15:34:48.228: INFO: Trying to get logs from node 10.241.69.181 pod pod-configmaps-a5b8a663-f28e-4483-a80f-e59c415b8bc3 container env-test: <nil>
STEP: delete the pod
Mar 23 15:34:48.360: INFO: Waiting for pod pod-configmaps-a5b8a663-f28e-4483-a80f-e59c415b8bc3 to disappear
Mar 23 15:34:48.384: INFO: Pod pod-configmaps-a5b8a663-f28e-4483-a80f-e59c415b8bc3 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:34:48.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-132" for this suite.
Mar 23 15:35:00.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:35:03.564: INFO: namespace configmap-132 deletion completed in 15.140207497s

• [SLOW TEST:18.019 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:35:03.566: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-60512c26-ae06-4fd5-9026-70bbcbb3754b
STEP: Creating a pod to test consume configMaps
Mar 23 15:35:04.444: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a64c9958-da5e-4b57-8885-741bc46e8908" in namespace "projected-2672" to be "success or failure"
Mar 23 15:35:04.472: INFO: Pod "pod-projected-configmaps-a64c9958-da5e-4b57-8885-741bc46e8908": Phase="Pending", Reason="", readiness=false. Elapsed: 27.29782ms
Mar 23 15:35:06.507: INFO: Pod "pod-projected-configmaps-a64c9958-da5e-4b57-8885-741bc46e8908": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062195743s
Mar 23 15:35:08.534: INFO: Pod "pod-projected-configmaps-a64c9958-da5e-4b57-8885-741bc46e8908": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.089715813s
STEP: Saw pod success
Mar 23 15:35:08.535: INFO: Pod "pod-projected-configmaps-a64c9958-da5e-4b57-8885-741bc46e8908" satisfied condition "success or failure"
Mar 23 15:35:08.567: INFO: Trying to get logs from node 10.241.69.181 pod pod-projected-configmaps-a64c9958-da5e-4b57-8885-741bc46e8908 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 15:35:08.731: INFO: Waiting for pod pod-projected-configmaps-a64c9958-da5e-4b57-8885-741bc46e8908 to disappear
Mar 23 15:35:08.754: INFO: Pod pod-projected-configmaps-a64c9958-da5e-4b57-8885-741bc46e8908 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:35:08.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2672" for this suite.
Mar 23 15:35:22.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:35:25.587: INFO: namespace projected-2672 deletion completed in 16.765965467s

• [SLOW TEST:22.021 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:35:25.587: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-6c07fc09-f9b1-400e-92e6-02015f10cdf5
STEP: Creating a pod to test consume secrets
Mar 23 15:35:26.232: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-450da4b9-fec7-4b12-86a4-dae7ef98c039" in namespace "projected-2350" to be "success or failure"
Mar 23 15:35:26.263: INFO: Pod "pod-projected-secrets-450da4b9-fec7-4b12-86a4-dae7ef98c039": Phase="Pending", Reason="", readiness=false. Elapsed: 30.954893ms
Mar 23 15:35:28.499: INFO: Pod "pod-projected-secrets-450da4b9-fec7-4b12-86a4-dae7ef98c039": Phase="Pending", Reason="", readiness=false. Elapsed: 2.267413403s
Mar 23 15:35:30.525: INFO: Pod "pod-projected-secrets-450da4b9-fec7-4b12-86a4-dae7ef98c039": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.293023588s
STEP: Saw pod success
Mar 23 15:35:30.525: INFO: Pod "pod-projected-secrets-450da4b9-fec7-4b12-86a4-dae7ef98c039" satisfied condition "success or failure"
Mar 23 15:35:30.549: INFO: Trying to get logs from node 10.241.69.181 pod pod-projected-secrets-450da4b9-fec7-4b12-86a4-dae7ef98c039 container secret-volume-test: <nil>
STEP: delete the pod
Mar 23 15:35:30.787: INFO: Waiting for pod pod-projected-secrets-450da4b9-fec7-4b12-86a4-dae7ef98c039 to disappear
Mar 23 15:35:30.827: INFO: Pod pod-projected-secrets-450da4b9-fec7-4b12-86a4-dae7ef98c039 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:35:30.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2350" for this suite.
Mar 23 15:35:42.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:35:46.166: INFO: namespace projected-2350 deletion completed in 15.273255265s

• [SLOW TEST:20.578 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:35:46.166: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 23 15:35:50.024: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:35:50.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3523" for this suite.
Mar 23 15:36:04.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:36:07.733: INFO: namespace container-runtime-3523 deletion completed in 17.505845941s

• [SLOW TEST:21.567 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:36:07.735: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-15d9aa25-88e6-45ce-b7ec-6f2013e60217 in namespace container-probe-9280
Mar 23 15:36:13.608: INFO: Started pod test-webserver-15d9aa25-88e6-45ce-b7ec-6f2013e60217 in namespace container-probe-9280
STEP: checking the pod's current state and verifying that restartCount is present
Mar 23 15:36:13.631: INFO: Initial restart count of pod test-webserver-15d9aa25-88e6-45ce-b7ec-6f2013e60217 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:40:15.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9280" for this suite.
Mar 23 15:40:25.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:40:28.232: INFO: namespace container-probe-9280 deletion completed in 13.066474981s

• [SLOW TEST:260.497 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:40:28.232: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Mar 23 15:40:28.678: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar 23 15:40:36.142: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:40:36.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6194" for this suite.
Mar 23 15:40:50.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:40:53.379: INFO: namespace pods-6194 deletion completed in 17.169562341s

• [SLOW TEST:25.147 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:40:53.379: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Mar 23 15:40:58.284: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-012311840 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Mar 23 15:41:08.690: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:41:08.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7121" for this suite.
Mar 23 15:41:20.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:41:23.872: INFO: namespace pods-7121 deletion completed in 15.031528407s

• [SLOW TEST:30.493 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:41:23.872: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-7296
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7296
STEP: Deleting pre-stop pod
Mar 23 15:41:35.764: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:41:35.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7296" for this suite.
Mar 23 15:42:08.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:42:11.144: INFO: namespace prestop-7296 deletion completed in 35.165947898s

• [SLOW TEST:47.272 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:42:11.144: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Mar 23 15:42:11.649: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Mar 23 15:42:12.563: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 23 15:42:14.995: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 15:42:17.060: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 15:42:19.045: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 15:42:21.021: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 15:42:23.089: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 15:42:25.028: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574932, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 15:42:27.788: INFO: Waited 725.313032ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:42:30.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7664" for this suite.
Mar 23 15:42:42.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:42:45.080: INFO: namespace aggregator-7664 deletion completed in 14.634322504s

• [SLOW TEST:33.935 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:42:45.080: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:42:49.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-449" for this suite.
Mar 23 15:43:02.170: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:43:05.454: INFO: namespace kubelet-test-449 deletion completed in 15.396557223s

• [SLOW TEST:20.374 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:43:05.454: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 15:43:07.338: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 15:43:09.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574987, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574987, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574987, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720574987, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 15:43:12.507: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:43:12.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4790" for this suite.
Mar 23 15:43:24.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:43:27.658: INFO: namespace webhook-4790 deletion completed in 15.063127573s
STEP: Destroying namespace "webhook-4790-markers" for this suite.
Mar 23 15:43:37.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:43:40.497: INFO: namespace webhook-4790-markers deletion completed in 12.839066363s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:35.188 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:43:40.644: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 15:43:41.806: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1839606c-4cb6-4850-860f-658d2819a23d" in namespace "downward-api-3325" to be "success or failure"
Mar 23 15:43:41.842: INFO: Pod "downwardapi-volume-1839606c-4cb6-4850-860f-658d2819a23d": Phase="Pending", Reason="", readiness=false. Elapsed: 35.874103ms
Mar 23 15:43:43.868: INFO: Pod "downwardapi-volume-1839606c-4cb6-4850-860f-658d2819a23d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061957432s
Mar 23 15:43:45.914: INFO: Pod "downwardapi-volume-1839606c-4cb6-4850-860f-658d2819a23d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.107399278s
STEP: Saw pod success
Mar 23 15:43:45.914: INFO: Pod "downwardapi-volume-1839606c-4cb6-4850-860f-658d2819a23d" satisfied condition "success or failure"
Mar 23 15:43:45.939: INFO: Trying to get logs from node 10.241.69.181 pod downwardapi-volume-1839606c-4cb6-4850-860f-658d2819a23d container client-container: <nil>
STEP: delete the pod
Mar 23 15:43:46.204: INFO: Waiting for pod downwardapi-volume-1839606c-4cb6-4850-860f-658d2819a23d to disappear
Mar 23 15:43:46.237: INFO: Pod downwardapi-volume-1839606c-4cb6-4850-860f-658d2819a23d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:43:46.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3325" for this suite.
Mar 23 15:43:58.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:44:01.565: INFO: namespace downward-api-3325 deletion completed in 15.274618888s

• [SLOW TEST:20.921 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:44:01.565: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 23 15:44:02.740: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Mar 23 15:44:04.936: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720575042, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720575042, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720575043, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720575042, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 15:44:08.033: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 15:44:08.048: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:44:10.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2853" for this suite.
Mar 23 15:44:22.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:44:25.486: INFO: namespace crd-webhook-2853 deletion completed in 15.052109729s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:24.075 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:44:25.641: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Mar 23 15:44:26.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-9151 -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 23 15:44:26.893: INFO: stderr: ""
Mar 23 15:44:26.893: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Mar 23 15:44:26.893: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 23 15:44:26.893: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9151" to be "running and ready, or succeeded"
Mar 23 15:44:26.919: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 25.685279ms
Mar 23 15:44:28.948: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054767989s
Mar 23 15:44:30.972: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.079281852s
Mar 23 15:44:30.972: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 23 15:44:30.972: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Mar 23 15:44:30.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 logs logs-generator logs-generator --namespace=kubectl-9151'
Mar 23 15:44:31.239: INFO: stderr: ""
Mar 23 15:44:31.239: INFO: stdout: "I0323 15:44:28.803200       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/sjmn 233\nI0323 15:44:29.003398       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/xxbz 423\nI0323 15:44:29.203481       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/kvq 578\nI0323 15:44:29.403469       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/btrs 465\nI0323 15:44:29.603521       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/dsv 315\nI0323 15:44:29.803653       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/hwzm 512\nI0323 15:44:30.003489       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/pfvc 272\nI0323 15:44:30.203492       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/5h5t 499\nI0323 15:44:30.403474       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/hd5b 409\nI0323 15:44:30.603531       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/dzmd 530\nI0323 15:44:30.803457       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/jsxp 355\nI0323 15:44:31.003479       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/w66 548\nI0323 15:44:31.203477       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/v26g 570\n"
STEP: limiting log lines
Mar 23 15:44:31.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 logs logs-generator logs-generator --namespace=kubectl-9151 --tail=1'
Mar 23 15:44:31.482: INFO: stderr: ""
Mar 23 15:44:31.482: INFO: stdout: "I0323 15:44:31.403530       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/pq8 492\n"
STEP: limiting log bytes
Mar 23 15:44:31.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 logs logs-generator logs-generator --namespace=kubectl-9151 --limit-bytes=1'
Mar 23 15:44:31.737: INFO: stderr: ""
Mar 23 15:44:31.738: INFO: stdout: "I"
STEP: exposing timestamps
Mar 23 15:44:31.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 logs logs-generator logs-generator --namespace=kubectl-9151 --tail=1 --timestamps'
Mar 23 15:44:32.035: INFO: stderr: ""
Mar 23 15:44:32.035: INFO: stdout: "2020-03-23T10:44:31.803562814-05:00 I0323 15:44:31.803477       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/rldp 484\n"
STEP: restricting to a time range
Mar 23 15:44:34.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 logs logs-generator logs-generator --namespace=kubectl-9151 --since=1s'
Mar 23 15:44:34.748: INFO: stderr: ""
Mar 23 15:44:34.748: INFO: stdout: "I0323 15:44:33.803400       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/vrb5 336\nI0323 15:44:34.003490       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/45l 494\nI0323 15:44:34.203468       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/95g6 468\nI0323 15:44:34.403517       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/99hf 228\nI0323 15:44:34.603524       1 logs_generator.go:76] 29 GET /api/v1/namespaces/default/pods/sjm 309\n"
Mar 23 15:44:34.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 logs logs-generator logs-generator --namespace=kubectl-9151 --since=24h'
Mar 23 15:44:34.971: INFO: stderr: ""
Mar 23 15:44:34.971: INFO: stdout: "I0323 15:44:28.803200       1 logs_generator.go:76] 0 GET /api/v1/namespaces/default/pods/sjmn 233\nI0323 15:44:29.003398       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/xxbz 423\nI0323 15:44:29.203481       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/kvq 578\nI0323 15:44:29.403469       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/btrs 465\nI0323 15:44:29.603521       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/dsv 315\nI0323 15:44:29.803653       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/hwzm 512\nI0323 15:44:30.003489       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/pfvc 272\nI0323 15:44:30.203492       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/5h5t 499\nI0323 15:44:30.403474       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/hd5b 409\nI0323 15:44:30.603531       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/dzmd 530\nI0323 15:44:30.803457       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/jsxp 355\nI0323 15:44:31.003479       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/w66 548\nI0323 15:44:31.203477       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/v26g 570\nI0323 15:44:31.403530       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/pq8 492\nI0323 15:44:31.603499       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/brlf 263\nI0323 15:44:31.803477       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/rldp 484\nI0323 15:44:32.003487       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/mr8m 361\nI0323 15:44:32.203503       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/qzp6 231\nI0323 15:44:32.403486       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/8rv 339\nI0323 15:44:32.603465       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/lnc 396\nI0323 15:44:32.803483       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/stv 284\nI0323 15:44:33.003525       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/g4nv 590\nI0323 15:44:33.203445       1 logs_generator.go:76] 22 POST /api/v1/namespaces/ns/pods/vn9b 347\nI0323 15:44:33.403473       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/x6hz 415\nI0323 15:44:33.603461       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/default/pods/6f8 446\nI0323 15:44:33.803400       1 logs_generator.go:76] 25 POST /api/v1/namespaces/ns/pods/vrb5 336\nI0323 15:44:34.003490       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/45l 494\nI0323 15:44:34.203468       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/95g6 468\nI0323 15:44:34.403517       1 logs_generator.go:76] 28 POST /api/v1/namespaces/ns/pods/99hf 228\nI0323 15:44:34.603524       1 logs_generator.go:76] 29 GET /api/v1/namespaces/default/pods/sjm 309\nI0323 15:44:34.803491       1 logs_generator.go:76] 30 POST /api/v1/namespaces/kube-system/pods/4dt 301\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Mar 23 15:44:34.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete pod logs-generator --namespace=kubectl-9151'
Mar 23 15:44:37.546: INFO: stderr: ""
Mar 23 15:44:37.546: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:44:37.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9151" for this suite.
Mar 23 15:44:49.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:44:52.601: INFO: namespace kubectl-9151 deletion completed in 14.999110226s

• [SLOW TEST:26.960 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:44:52.606: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-6782
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6782 to expose endpoints map[]
Mar 23 15:44:53.241: INFO: Get endpoints failed (19.709822ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar 23 15:44:54.259: INFO: successfully validated that service endpoint-test2 in namespace services-6782 exposes endpoints map[] (1.037708141s elapsed)
STEP: Creating pod pod1 in namespace services-6782
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6782 to expose endpoints map[pod1:[80]]
Mar 23 15:44:57.602: INFO: successfully validated that service endpoint-test2 in namespace services-6782 exposes endpoints map[pod1:[80]] (3.209133333s elapsed)
STEP: Creating pod pod2 in namespace services-6782
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6782 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 23 15:45:01.043: INFO: successfully validated that service endpoint-test2 in namespace services-6782 exposes endpoints map[pod1:[80] pod2:[80]] (3.356628242s elapsed)
STEP: Deleting pod pod1 in namespace services-6782
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6782 to expose endpoints map[pod2:[80]]
Mar 23 15:45:01.403: INFO: successfully validated that service endpoint-test2 in namespace services-6782 exposes endpoints map[pod2:[80]] (302.052864ms elapsed)
STEP: Deleting pod pod2 in namespace services-6782
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6782 to expose endpoints map[]
Mar 23 15:45:02.504: INFO: successfully validated that service endpoint-test2 in namespace services-6782 exposes endpoints map[] (1.027685964s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:45:02.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6782" for this suite.
Mar 23 15:45:16.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:45:19.365: INFO: namespace services-6782 deletion completed in 16.731837205s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:26.759 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:45:19.365: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 15:45:21.284: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 15:45:23.424: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720575121, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720575121, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720575121, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720575121, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 15:45:26.586: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:45:27.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5637" for this suite.
Mar 23 15:45:39.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:45:42.946: INFO: namespace webhook-5637 deletion completed in 15.317857979s
STEP: Destroying namespace "webhook-5637-markers" for this suite.
Mar 23 15:45:55.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:45:58.265: INFO: namespace webhook-5637-markers deletion completed in 15.319232874s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:39.034 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:45:58.403: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 15:45:59.227: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b520d5d6-5b1c-4fa6-89ca-8cebc0ab2c69" in namespace "downward-api-8191" to be "success or failure"
Mar 23 15:45:59.261: INFO: Pod "downwardapi-volume-b520d5d6-5b1c-4fa6-89ca-8cebc0ab2c69": Phase="Pending", Reason="", readiness=false. Elapsed: 33.719024ms
Mar 23 15:46:01.294: INFO: Pod "downwardapi-volume-b520d5d6-5b1c-4fa6-89ca-8cebc0ab2c69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066215661s
Mar 23 15:46:03.331: INFO: Pod "downwardapi-volume-b520d5d6-5b1c-4fa6-89ca-8cebc0ab2c69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.103946711s
STEP: Saw pod success
Mar 23 15:46:03.331: INFO: Pod "downwardapi-volume-b520d5d6-5b1c-4fa6-89ca-8cebc0ab2c69" satisfied condition "success or failure"
Mar 23 15:46:03.370: INFO: Trying to get logs from node 10.241.69.181 pod downwardapi-volume-b520d5d6-5b1c-4fa6-89ca-8cebc0ab2c69 container client-container: <nil>
STEP: delete the pod
Mar 23 15:46:03.930: INFO: Waiting for pod downwardapi-volume-b520d5d6-5b1c-4fa6-89ca-8cebc0ab2c69 to disappear
Mar 23 15:46:03.972: INFO: Pod downwardapi-volume-b520d5d6-5b1c-4fa6-89ca-8cebc0ab2c69 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:46:03.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8191" for this suite.
Mar 23 15:46:16.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:46:18.741: INFO: namespace downward-api-8191 deletion completed in 14.708654159s

• [SLOW TEST:20.338 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:46:18.741: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 15:46:19.274: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff83d792-7435-48f3-8d8a-99080db16123" in namespace "projected-8122" to be "success or failure"
Mar 23 15:46:19.302: INFO: Pod "downwardapi-volume-ff83d792-7435-48f3-8d8a-99080db16123": Phase="Pending", Reason="", readiness=false. Elapsed: 28.572289ms
Mar 23 15:46:21.347: INFO: Pod "downwardapi-volume-ff83d792-7435-48f3-8d8a-99080db16123": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07322013s
Mar 23 15:46:23.372: INFO: Pod "downwardapi-volume-ff83d792-7435-48f3-8d8a-99080db16123": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.098495927s
STEP: Saw pod success
Mar 23 15:46:23.373: INFO: Pod "downwardapi-volume-ff83d792-7435-48f3-8d8a-99080db16123" satisfied condition "success or failure"
Mar 23 15:46:23.398: INFO: Trying to get logs from node 10.241.69.181 pod downwardapi-volume-ff83d792-7435-48f3-8d8a-99080db16123 container client-container: <nil>
STEP: delete the pod
Mar 23 15:46:23.769: INFO: Waiting for pod downwardapi-volume-ff83d792-7435-48f3-8d8a-99080db16123 to disappear
Mar 23 15:46:23.800: INFO: Pod downwardapi-volume-ff83d792-7435-48f3-8d8a-99080db16123 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:46:23.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8122" for this suite.
Mar 23 15:46:37.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:46:40.980: INFO: namespace projected-8122 deletion completed in 17.11508077s

• [SLOW TEST:22.239 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:46:40.980: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 15:46:42.695: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 15:46:44.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720575202, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720575202, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720575202, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720575202, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 15:46:48.081: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:46:50.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9693" for this suite.
Mar 23 15:47:06.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:47:09.293: INFO: namespace webhook-9693 deletion completed in 18.948474769s
STEP: Destroying namespace "webhook-9693-markers" for this suite.
Mar 23 15:47:23.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:47:26.310: INFO: namespace webhook-9693-markers deletion completed in 17.017751796s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:45.475 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:47:26.456: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:47:26.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9662" for this suite.
Mar 23 15:47:39.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:47:42.017: INFO: namespace custom-resource-definition-9662 deletion completed in 14.95965591s

• [SLOW TEST:15.561 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:47:42.017: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 23 15:47:46.884: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:47:47.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4186" for this suite.
Mar 23 15:47:59.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:48:02.095: INFO: namespace container-runtime-4186 deletion completed in 14.868996361s

• [SLOW TEST:20.078 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:48:02.095: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:48:02.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4848" for this suite.
Mar 23 15:48:17.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:48:19.808: INFO: namespace kubelet-test-4848 deletion completed in 16.791603762s

• [SLOW TEST:17.713 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:48:19.812: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 15:48:20.930: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af5fb5a1-a9e1-4555-8ba2-f97e74d270e0" in namespace "projected-2777" to be "success or failure"
Mar 23 15:48:20.961: INFO: Pod "downwardapi-volume-af5fb5a1-a9e1-4555-8ba2-f97e74d270e0": Phase="Pending", Reason="", readiness=false. Elapsed: 31.069699ms
Mar 23 15:48:22.997: INFO: Pod "downwardapi-volume-af5fb5a1-a9e1-4555-8ba2-f97e74d270e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0667804s
Mar 23 15:48:25.022: INFO: Pod "downwardapi-volume-af5fb5a1-a9e1-4555-8ba2-f97e74d270e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.092652298s
STEP: Saw pod success
Mar 23 15:48:25.023: INFO: Pod "downwardapi-volume-af5fb5a1-a9e1-4555-8ba2-f97e74d270e0" satisfied condition "success or failure"
Mar 23 15:48:25.047: INFO: Trying to get logs from node 10.241.69.181 pod downwardapi-volume-af5fb5a1-a9e1-4555-8ba2-f97e74d270e0 container client-container: <nil>
STEP: delete the pod
Mar 23 15:48:25.238: INFO: Waiting for pod downwardapi-volume-af5fb5a1-a9e1-4555-8ba2-f97e74d270e0 to disappear
Mar 23 15:48:25.411: INFO: Pod downwardapi-volume-af5fb5a1-a9e1-4555-8ba2-f97e74d270e0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:48:25.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2777" for this suite.
Mar 23 15:48:39.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:48:42.325: INFO: namespace projected-2777 deletion completed in 16.879968171s

• [SLOW TEST:22.513 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:48:42.326: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4315
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4315
I0323 15:48:42.892463      22 runners.go:184] Created replication controller with name: externalname-service, namespace: services-4315, replica count: 2
Mar 23 15:48:45.943: INFO: Creating new exec pod
I0323 15:48:45.943451      22 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 15:48:49.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=services-4315 execpodv8vh7 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 23 15:48:50.018: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 23 15:48:50.018: INFO: stdout: ""
Mar 23 15:48:50.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=services-4315 execpodv8vh7 -- /bin/sh -x -c nc -zv -t -w 2 172.21.121.0 80'
Mar 23 15:48:50.547: INFO: stderr: "+ nc -zv -t -w 2 172.21.121.0 80\nConnection to 172.21.121.0 80 port [tcp/http] succeeded!\n"
Mar 23 15:48:50.547: INFO: stdout: ""
Mar 23 15:48:50.547: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:48:50.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4315" for this suite.
Mar 23 15:49:04.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:49:07.599: INFO: namespace services-4315 deletion completed in 16.858582155s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:25.273 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:49:07.599: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Mar 23 15:49:12.831: INFO: Successfully updated pod "adopt-release-m9cdx"
STEP: Checking that the Job readopts the Pod
Mar 23 15:49:12.831: INFO: Waiting up to 15m0s for pod "adopt-release-m9cdx" in namespace "job-1224" to be "adopted"
Mar 23 15:49:12.878: INFO: Pod "adopt-release-m9cdx": Phase="Running", Reason="", readiness=true. Elapsed: 47.61366ms
Mar 23 15:49:14.903: INFO: Pod "adopt-release-m9cdx": Phase="Running", Reason="", readiness=true. Elapsed: 2.072388519s
Mar 23 15:49:14.903: INFO: Pod "adopt-release-m9cdx" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Mar 23 15:49:15.504: INFO: Successfully updated pod "adopt-release-m9cdx"
STEP: Checking that the Job releases the Pod
Mar 23 15:49:15.504: INFO: Waiting up to 15m0s for pod "adopt-release-m9cdx" in namespace "job-1224" to be "released"
Mar 23 15:49:15.534: INFO: Pod "adopt-release-m9cdx": Phase="Running", Reason="", readiness=true. Elapsed: 29.919651ms
Mar 23 15:49:17.622: INFO: Pod "adopt-release-m9cdx": Phase="Running", Reason="", readiness=true. Elapsed: 2.118796233s
Mar 23 15:49:17.622: INFO: Pod "adopt-release-m9cdx" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:49:17.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1224" for this suite.
Mar 23 15:50:11.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:50:15.055: INFO: namespace job-1224 deletion completed in 57.39395499s

• [SLOW TEST:67.456 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:50:15.055: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 15:50:15.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 version'
Mar 23 15:50:15.734: INFO: stderr: ""
Mar 23 15:50:15.734: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.2\", GitCommit:\"c97fe5036ef3df2967d086711e6c0c405941e14b\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T19:18:23Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.2\", GitCommit:\"b3bfb5a\", GitTreeState:\"clean\", BuildDate:\"2020-03-02T08:50:52Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:50:15.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7707" for this suite.
Mar 23 15:50:27.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:50:30.625: INFO: namespace kubectl-7707 deletion completed in 14.845057476s

• [SLOW TEST:15.570 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:50:30.625: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 15:50:31.389: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 23 15:50:40.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-4301 create -f -'
Mar 23 15:50:41.975: INFO: stderr: ""
Mar 23 15:50:41.975: INFO: stdout: "e2e-test-crd-publish-openapi-8915-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 23 15:50:41.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-4301 delete e2e-test-crd-publish-openapi-8915-crds test-cr'
Mar 23 15:50:42.168: INFO: stderr: ""
Mar 23 15:50:42.168: INFO: stdout: "e2e-test-crd-publish-openapi-8915-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 23 15:50:42.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-4301 apply -f -'
Mar 23 15:50:43.082: INFO: stderr: ""
Mar 23 15:50:43.083: INFO: stdout: "e2e-test-crd-publish-openapi-8915-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 23 15:50:43.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-4301 delete e2e-test-crd-publish-openapi-8915-crds test-cr'
Mar 23 15:50:43.302: INFO: stderr: ""
Mar 23 15:50:43.302: INFO: stdout: "e2e-test-crd-publish-openapi-8915-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 23 15:50:43.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 explain e2e-test-crd-publish-openapi-8915-crds'
Mar 23 15:50:43.924: INFO: stderr: ""
Mar 23 15:50:43.924: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8915-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:50:52.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4301" for this suite.
Mar 23 15:51:04.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:51:07.689: INFO: namespace crd-publish-openapi-4301 deletion completed in 14.927476609s

• [SLOW TEST:37.064 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:51:07.689: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-9cdcfa20-dbbb-4911-97b5-7b72dc6caa76
Mar 23 15:51:08.134: INFO: Pod name my-hostname-basic-9cdcfa20-dbbb-4911-97b5-7b72dc6caa76: Found 0 pods out of 1
Mar 23 15:51:13.164: INFO: Pod name my-hostname-basic-9cdcfa20-dbbb-4911-97b5-7b72dc6caa76: Found 1 pods out of 1
Mar 23 15:51:13.164: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9cdcfa20-dbbb-4911-97b5-7b72dc6caa76" are running
Mar 23 15:51:13.190: INFO: Pod "my-hostname-basic-9cdcfa20-dbbb-4911-97b5-7b72dc6caa76-5hjtz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-23 15:51:08 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-23 15:51:10 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-23 15:51:10 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-23 15:51:08 +0000 UTC Reason: Message:}])
Mar 23 15:51:13.190: INFO: Trying to dial the pod
Mar 23 15:51:18.277: INFO: Controller my-hostname-basic-9cdcfa20-dbbb-4911-97b5-7b72dc6caa76: Got expected result from replica 1 [my-hostname-basic-9cdcfa20-dbbb-4911-97b5-7b72dc6caa76-5hjtz]: "my-hostname-basic-9cdcfa20-dbbb-4911-97b5-7b72dc6caa76-5hjtz", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:51:18.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4917" for this suite.
Mar 23 15:51:30.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:51:33.176: INFO: namespace replication-controller-4917 deletion completed in 14.866948957s

• [SLOW TEST:25.487 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:51:33.176: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7447
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Mar 23 15:51:33.742: INFO: Found 0 stateful pods, waiting for 3
Mar 23 15:51:43.771: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 15:51:43.771: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 15:51:43.771: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 23 15:51:43.879: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Mar 23 15:51:54.019: INFO: Updating stateful set ss2
Mar 23 15:51:54.061: INFO: Waiting for Pod statefulset-7447/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Mar 23 15:52:04.575: INFO: Found 2 stateful pods, waiting for 3
Mar 23 15:52:14.615: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 15:52:14.615: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 15:52:14.615: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar 23 15:52:24.602: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 15:52:24.602: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 15:52:24.602: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Mar 23 15:52:24.688: INFO: Updating stateful set ss2
Mar 23 15:52:24.753: INFO: Waiting for Pod statefulset-7447/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 23 15:52:34.791: INFO: Waiting for Pod statefulset-7447/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Mar 23 15:52:44.853: INFO: Updating stateful set ss2
Mar 23 15:52:44.908: INFO: Waiting for StatefulSet statefulset-7447/ss2 to complete update
Mar 23 15:52:44.908: INFO: Waiting for Pod statefulset-7447/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 23 15:52:55.023: INFO: Deleting all statefulset in ns statefulset-7447
Mar 23 15:52:55.037: INFO: Scaling statefulset ss2 to 0
Mar 23 15:53:25.132: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 15:53:25.145: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:53:25.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7447" for this suite.
Mar 23 15:53:39.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:53:41.990: INFO: namespace statefulset-7447 deletion completed in 16.727883281s

• [SLOW TEST:128.814 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:53:41.990: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:53:45.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-193" for this suite.
Mar 23 15:54:31.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:54:34.479: INFO: namespace kubelet-test-193 deletion completed in 48.839427877s

• [SLOW TEST:52.489 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:54:34.480: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 23 15:54:34.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-3341'
Mar 23 15:54:35.005: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 23 15:54:35.005: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Mar 23 15:54:39.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete deployment e2e-test-httpd-deployment --namespace=kubectl-3341'
Mar 23 15:54:39.535: INFO: stderr: ""
Mar 23 15:54:39.536: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:54:39.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3341" for this suite.
Mar 23 15:55:01.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:55:04.757: INFO: namespace kubectl-3341 deletion completed in 25.180824822s

• [SLOW TEST:30.278 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:55:04.761: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 15:55:06.516: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 15:55:08.614: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720575706, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720575706, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720575706, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720575706, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 15:55:11.689: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 15:55:11.705: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:55:12.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2824" for this suite.
Mar 23 15:55:25.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:55:27.916: INFO: namespace webhook-2824 deletion completed in 14.951871707s
STEP: Destroying namespace "webhook-2824-markers" for this suite.
Mar 23 15:55:40.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:55:42.914: INFO: namespace webhook-2824-markers deletion completed in 14.998619156s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:38.535 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:55:43.298: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Mar 23 15:55:43.893: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:56:35.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8472" for this suite.
Mar 23 15:56:45.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:56:47.685: INFO: namespace crd-publish-openapi-8472 deletion completed in 12.61305545s

• [SLOW TEST:64.387 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:56:47.686: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-d44d1f03-1cad-4b08-ba00-817d1072dfee
STEP: Creating secret with name secret-projected-all-test-volume-669aa93d-c226-4d62-a9a0-8d28738d0a7a
STEP: Creating a pod to test Check all projections for projected volume plugin
Mar 23 15:56:48.204: INFO: Waiting up to 5m0s for pod "projected-volume-787202da-0d65-4062-b5d4-dc6116b31be5" in namespace "projected-9623" to be "success or failure"
Mar 23 15:56:48.227: INFO: Pod "projected-volume-787202da-0d65-4062-b5d4-dc6116b31be5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.935821ms
Mar 23 15:56:50.252: INFO: Pod "projected-volume-787202da-0d65-4062-b5d4-dc6116b31be5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048221013s
Mar 23 15:56:52.380: INFO: Pod "projected-volume-787202da-0d65-4062-b5d4-dc6116b31be5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.175826766s
STEP: Saw pod success
Mar 23 15:56:52.380: INFO: Pod "projected-volume-787202da-0d65-4062-b5d4-dc6116b31be5" satisfied condition "success or failure"
Mar 23 15:56:52.528: INFO: Trying to get logs from node 10.241.69.181 pod projected-volume-787202da-0d65-4062-b5d4-dc6116b31be5 container projected-all-volume-test: <nil>
STEP: delete the pod
Mar 23 15:56:52.823: INFO: Waiting for pod projected-volume-787202da-0d65-4062-b5d4-dc6116b31be5 to disappear
Mar 23 15:56:52.857: INFO: Pod projected-volume-787202da-0d65-4062-b5d4-dc6116b31be5 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:56:52.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9623" for this suite.
Mar 23 15:57:07.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:57:10.031: INFO: namespace projected-9623 deletion completed in 17.133762808s

• [SLOW TEST:22.345 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:57:10.031: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-v4gtp in namespace proxy-6933
I0323 15:57:10.917567      22 runners.go:184] Created replication controller with name: proxy-service-v4gtp, namespace: proxy-6933, replica count: 1
I0323 15:57:11.968043      22 runners.go:184] proxy-service-v4gtp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 15:57:12.968340      22 runners.go:184] proxy-service-v4gtp Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 15:57:13.968620      22 runners.go:184] proxy-service-v4gtp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0323 15:57:14.968873      22 runners.go:184] proxy-service-v4gtp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0323 15:57:15.969110      22 runners.go:184] proxy-service-v4gtp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0323 15:57:16.969376      22 runners.go:184] proxy-service-v4gtp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0323 15:57:17.969578      22 runners.go:184] proxy-service-v4gtp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0323 15:57:18.969840      22 runners.go:184] proxy-service-v4gtp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0323 15:57:19.970069      22 runners.go:184] proxy-service-v4gtp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0323 15:57:20.970357      22 runners.go:184] proxy-service-v4gtp Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0323 15:57:21.970659      22 runners.go:184] proxy-service-v4gtp Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 15:57:21.984: INFO: setup took 11.177045779s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Mar 23 15:57:22.058: INFO: (0) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 73.099486ms)
Mar 23 15:57:22.059: INFO: (0) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 73.957739ms)
Mar 23 15:57:22.059: INFO: (0) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 74.040824ms)
Mar 23 15:57:22.065: INFO: (0) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 79.630925ms)
Mar 23 15:57:22.065: INFO: (0) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 79.715922ms)
Mar 23 15:57:22.065: INFO: (0) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 79.831859ms)
Mar 23 15:57:22.066: INFO: (0) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 81.212204ms)
Mar 23 15:57:22.066: INFO: (0) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 81.309456ms)
Mar 23 15:57:22.066: INFO: (0) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 81.233654ms)
Mar 23 15:57:22.066: INFO: (0) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 81.15282ms)
Mar 23 15:57:22.066: INFO: (0) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 81.56898ms)
Mar 23 15:57:22.069: INFO: (0) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 84.980473ms)
Mar 23 15:57:22.079: INFO: (0) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 94.571681ms)
Mar 23 15:57:22.080: INFO: (0) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 94.900057ms)
Mar 23 15:57:22.080: INFO: (0) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 95.250557ms)
Mar 23 15:57:22.081: INFO: (0) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 96.051046ms)
Mar 23 15:57:22.126: INFO: (1) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 44.005064ms)
Mar 23 15:57:22.126: INFO: (1) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 43.480825ms)
Mar 23 15:57:22.126: INFO: (1) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 44.652113ms)
Mar 23 15:57:22.135: INFO: (1) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 53.607839ms)
Mar 23 15:57:22.135: INFO: (1) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 52.489959ms)
Mar 23 15:57:22.135: INFO: (1) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 52.794393ms)
Mar 23 15:57:22.135: INFO: (1) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 52.026059ms)
Mar 23 15:57:22.136: INFO: (1) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 52.32639ms)
Mar 23 15:57:22.136: INFO: (1) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 52.841728ms)
Mar 23 15:57:22.136: INFO: (1) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 53.082412ms)
Mar 23 15:57:22.150: INFO: (1) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 66.605957ms)
Mar 23 15:57:22.150: INFO: (1) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 67.037794ms)
Mar 23 15:57:22.150: INFO: (1) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 68.445747ms)
Mar 23 15:57:22.150: INFO: (1) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 67.961384ms)
Mar 23 15:57:22.150: INFO: (1) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 68.394276ms)
Mar 23 15:57:22.150: INFO: (1) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 67.424919ms)
Mar 23 15:57:22.186: INFO: (2) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 35.549488ms)
Mar 23 15:57:22.207: INFO: (2) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 55.661029ms)
Mar 23 15:57:22.207: INFO: (2) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 55.405998ms)
Mar 23 15:57:22.208: INFO: (2) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 55.920748ms)
Mar 23 15:57:22.208: INFO: (2) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 56.801632ms)
Mar 23 15:57:22.209: INFO: (2) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 58.341446ms)
Mar 23 15:57:22.209: INFO: (2) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 57.562137ms)
Mar 23 15:57:22.209: INFO: (2) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 58.033169ms)
Mar 23 15:57:22.209: INFO: (2) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 57.764452ms)
Mar 23 15:57:22.213: INFO: (2) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 61.87953ms)
Mar 23 15:57:22.214: INFO: (2) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 63.439375ms)
Mar 23 15:57:22.219: INFO: (2) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 67.137014ms)
Mar 23 15:57:22.219: INFO: (2) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 67.659057ms)
Mar 23 15:57:22.220: INFO: (2) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 68.825825ms)
Mar 23 15:57:22.220: INFO: (2) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 67.971371ms)
Mar 23 15:57:22.221: INFO: (2) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 69.250567ms)
Mar 23 15:57:22.265: INFO: (3) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 44.515733ms)
Mar 23 15:57:22.298: INFO: (3) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 76.38906ms)
Mar 23 15:57:22.299: INFO: (3) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 76.721204ms)
Mar 23 15:57:22.301: INFO: (3) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 79.546845ms)
Mar 23 15:57:22.301: INFO: (3) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 79.740269ms)
Mar 23 15:57:22.303: INFO: (3) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 82.115843ms)
Mar 23 15:57:22.305: INFO: (3) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 82.989605ms)
Mar 23 15:57:22.305: INFO: (3) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 83.935168ms)
Mar 23 15:57:22.305: INFO: (3) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 83.501277ms)
Mar 23 15:57:22.306: INFO: (3) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 84.734709ms)
Mar 23 15:57:22.306: INFO: (3) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 84.015155ms)
Mar 23 15:57:22.311: INFO: (3) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 89.269666ms)
Mar 23 15:57:22.311: INFO: (3) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 90.147865ms)
Mar 23 15:57:22.311: INFO: (3) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 90.76207ms)
Mar 23 15:57:22.312: INFO: (3) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 90.573793ms)
Mar 23 15:57:22.312: INFO: (3) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 89.885094ms)
Mar 23 15:57:22.343: INFO: (4) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 30.673096ms)
Mar 23 15:57:22.353: INFO: (4) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 40.863879ms)
Mar 23 15:57:22.354: INFO: (4) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 41.467545ms)
Mar 23 15:57:22.358: INFO: (4) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 45.261176ms)
Mar 23 15:57:22.358: INFO: (4) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 45.847154ms)
Mar 23 15:57:22.358: INFO: (4) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 45.835539ms)
Mar 23 15:57:22.364: INFO: (4) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 51.170898ms)
Mar 23 15:57:22.364: INFO: (4) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 51.290552ms)
Mar 23 15:57:22.364: INFO: (4) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 51.652106ms)
Mar 23 15:57:22.364: INFO: (4) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 51.610847ms)
Mar 23 15:57:22.364: INFO: (4) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 51.731429ms)
Mar 23 15:57:22.369: INFO: (4) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 56.4638ms)
Mar 23 15:57:22.385: INFO: (4) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 72.32289ms)
Mar 23 15:57:22.385: INFO: (4) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 72.6865ms)
Mar 23 15:57:22.385: INFO: (4) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 72.662121ms)
Mar 23 15:57:22.385: INFO: (4) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 72.862417ms)
Mar 23 15:57:22.415: INFO: (5) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 29.668296ms)
Mar 23 15:57:22.427: INFO: (5) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 40.685162ms)
Mar 23 15:57:22.427: INFO: (5) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 40.931602ms)
Mar 23 15:57:22.429: INFO: (5) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 42.488142ms)
Mar 23 15:57:22.429: INFO: (5) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 42.64876ms)
Mar 23 15:57:22.429: INFO: (5) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 42.814353ms)
Mar 23 15:57:22.429: INFO: (5) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 43.009629ms)
Mar 23 15:57:22.429: INFO: (5) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 43.071063ms)
Mar 23 15:57:22.429: INFO: (5) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 43.120038ms)
Mar 23 15:57:22.430: INFO: (5) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 43.413717ms)
Mar 23 15:57:22.434: INFO: (5) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 48.130753ms)
Mar 23 15:57:22.439: INFO: (5) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 53.474749ms)
Mar 23 15:57:22.440: INFO: (5) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 53.730902ms)
Mar 23 15:57:22.441: INFO: (5) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 54.858478ms)
Mar 23 15:57:22.442: INFO: (5) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 55.621102ms)
Mar 23 15:57:22.442: INFO: (5) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 56.39404ms)
Mar 23 15:57:22.485: INFO: (6) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 43.0227ms)
Mar 23 15:57:22.495: INFO: (6) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 52.631593ms)
Mar 23 15:57:22.496: INFO: (6) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 53.629075ms)
Mar 23 15:57:22.496: INFO: (6) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 53.431823ms)
Mar 23 15:57:22.497: INFO: (6) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 53.897588ms)
Mar 23 15:57:22.497: INFO: (6) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 53.935849ms)
Mar 23 15:57:22.501: INFO: (6) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 58.093675ms)
Mar 23 15:57:22.501: INFO: (6) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 57.996405ms)
Mar 23 15:57:22.501: INFO: (6) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 58.208827ms)
Mar 23 15:57:22.501: INFO: (6) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 58.189484ms)
Mar 23 15:57:22.510: INFO: (6) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 67.008366ms)
Mar 23 15:57:22.511: INFO: (6) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 67.974672ms)
Mar 23 15:57:22.511: INFO: (6) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 68.17012ms)
Mar 23 15:57:22.511: INFO: (6) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 68.22438ms)
Mar 23 15:57:22.511: INFO: (6) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 68.416866ms)
Mar 23 15:57:22.511: INFO: (6) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 68.596297ms)
Mar 23 15:57:22.543: INFO: (7) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 31.382278ms)
Mar 23 15:57:22.553: INFO: (7) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 40.220972ms)
Mar 23 15:57:22.553: INFO: (7) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 40.495245ms)
Mar 23 15:57:22.553: INFO: (7) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 40.555017ms)
Mar 23 15:57:22.554: INFO: (7) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 40.48189ms)
Mar 23 15:57:22.554: INFO: (7) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 40.639784ms)
Mar 23 15:57:22.554: INFO: (7) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 41.789668ms)
Mar 23 15:57:22.557: INFO: (7) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 44.778687ms)
Mar 23 15:57:22.557: INFO: (7) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 44.1206ms)
Mar 23 15:57:22.559: INFO: (7) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 46.497051ms)
Mar 23 15:57:22.569: INFO: (7) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 56.651909ms)
Mar 23 15:57:22.569: INFO: (7) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 56.25013ms)
Mar 23 15:57:22.674: INFO: (7) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 161.219103ms)
Mar 23 15:57:22.675: INFO: (7) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 162.235419ms)
Mar 23 15:57:22.694: INFO: (7) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 181.253451ms)
Mar 23 15:57:22.694: INFO: (7) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 181.53747ms)
Mar 23 15:57:22.724: INFO: (8) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 29.316126ms)
Mar 23 15:57:22.735: INFO: (8) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 39.877809ms)
Mar 23 15:57:22.736: INFO: (8) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 40.945501ms)
Mar 23 15:57:22.736: INFO: (8) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 41.19449ms)
Mar 23 15:57:22.736: INFO: (8) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 41.042859ms)
Mar 23 15:57:22.736: INFO: (8) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 41.209055ms)
Mar 23 15:57:22.736: INFO: (8) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 41.133589ms)
Mar 23 15:57:22.736: INFO: (8) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 41.615926ms)
Mar 23 15:57:22.748: INFO: (8) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 52.976016ms)
Mar 23 15:57:22.748: INFO: (8) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 52.941485ms)
Mar 23 15:57:22.748: INFO: (8) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 53.019304ms)
Mar 23 15:57:22.749: INFO: (8) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 53.513262ms)
Mar 23 15:57:22.756: INFO: (8) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 60.758764ms)
Mar 23 15:57:22.760: INFO: (8) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 64.787361ms)
Mar 23 15:57:22.760: INFO: (8) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 64.874536ms)
Mar 23 15:57:22.760: INFO: (8) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 65.078481ms)
Mar 23 15:57:22.790: INFO: (9) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 29.970285ms)
Mar 23 15:57:22.805: INFO: (9) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 43.437886ms)
Mar 23 15:57:22.805: INFO: (9) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 43.637035ms)
Mar 23 15:57:22.805: INFO: (9) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 43.536281ms)
Mar 23 15:57:22.805: INFO: (9) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 43.500915ms)
Mar 23 15:57:22.807: INFO: (9) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 45.978802ms)
Mar 23 15:57:22.812: INFO: (9) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 49.916659ms)
Mar 23 15:57:22.813: INFO: (9) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 51.479288ms)
Mar 23 15:57:22.816: INFO: (9) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 54.977505ms)
Mar 23 15:57:22.820: INFO: (9) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 59.531803ms)
Mar 23 15:57:22.820: INFO: (9) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 59.502281ms)
Mar 23 15:57:22.823: INFO: (9) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 60.720483ms)
Mar 23 15:57:22.823: INFO: (9) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 60.798335ms)
Mar 23 15:57:22.823: INFO: (9) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 61.299887ms)
Mar 23 15:57:22.823: INFO: (9) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 62.759205ms)
Mar 23 15:57:22.829: INFO: (9) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 67.929159ms)
Mar 23 15:57:22.867: INFO: (10) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 37.218331ms)
Mar 23 15:57:22.893: INFO: (10) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 63.531296ms)
Mar 23 15:57:22.893: INFO: (10) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 63.565102ms)
Mar 23 15:57:22.893: INFO: (10) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 63.50578ms)
Mar 23 15:57:22.893: INFO: (10) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 63.651476ms)
Mar 23 15:57:22.893: INFO: (10) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 63.870596ms)
Mar 23 15:57:22.893: INFO: (10) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 63.901009ms)
Mar 23 15:57:22.893: INFO: (10) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 63.497652ms)
Mar 23 15:57:22.893: INFO: (10) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 63.785306ms)
Mar 23 15:57:22.893: INFO: (10) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 64.123905ms)
Mar 23 15:57:22.893: INFO: (10) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 63.967575ms)
Mar 23 15:57:22.893: INFO: (10) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 63.915217ms)
Mar 23 15:57:22.898: INFO: (10) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 68.63654ms)
Mar 23 15:57:22.898: INFO: (10) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 68.798299ms)
Mar 23 15:57:22.898: INFO: (10) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 68.862313ms)
Mar 23 15:57:22.898: INFO: (10) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 69.063619ms)
Mar 23 15:57:22.931: INFO: (11) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 31.931896ms)
Mar 23 15:57:22.935: INFO: (11) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 36.230504ms)
Mar 23 15:57:22.935: INFO: (11) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 36.19716ms)
Mar 23 15:57:22.935: INFO: (11) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 36.733155ms)
Mar 23 15:57:22.945: INFO: (11) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 45.931477ms)
Mar 23 15:57:22.947: INFO: (11) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 48.131279ms)
Mar 23 15:57:22.947: INFO: (11) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 48.587804ms)
Mar 23 15:57:22.947: INFO: (11) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 48.292623ms)
Mar 23 15:57:22.947: INFO: (11) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 48.639522ms)
Mar 23 15:57:22.948: INFO: (11) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 48.119629ms)
Mar 23 15:57:22.948: INFO: (11) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 48.208634ms)
Mar 23 15:57:22.960: INFO: (11) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 60.499904ms)
Mar 23 15:57:22.960: INFO: (11) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 60.378386ms)
Mar 23 15:57:22.961: INFO: (11) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 61.786298ms)
Mar 23 15:57:22.961: INFO: (11) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 61.697299ms)
Mar 23 15:57:22.961: INFO: (11) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 62.023115ms)
Mar 23 15:57:23.060: INFO: (12) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 97.62978ms)
Mar 23 15:57:23.060: INFO: (12) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 96.373617ms)
Mar 23 15:57:23.060: INFO: (12) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 97.23928ms)
Mar 23 15:57:23.061: INFO: (12) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 99.239972ms)
Mar 23 15:57:23.061: INFO: (12) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 98.826604ms)
Mar 23 15:57:23.063: INFO: (12) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 101.434354ms)
Mar 23 15:57:23.063: INFO: (12) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 101.39118ms)
Mar 23 15:57:23.063: INFO: (12) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 100.899974ms)
Mar 23 15:57:23.067: INFO: (12) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 103.516613ms)
Mar 23 15:57:23.067: INFO: (12) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 104.076717ms)
Mar 23 15:57:23.067: INFO: (12) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 103.868932ms)
Mar 23 15:57:23.067: INFO: (12) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 104.350496ms)
Mar 23 15:57:23.069: INFO: (12) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 107.259928ms)
Mar 23 15:57:23.069: INFO: (12) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 106.600036ms)
Mar 23 15:57:23.072: INFO: (12) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 110.628709ms)
Mar 23 15:57:23.092: INFO: (12) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 129.070419ms)
Mar 23 15:57:23.153: INFO: (13) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 60.161205ms)
Mar 23 15:57:23.154: INFO: (13) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 61.285397ms)
Mar 23 15:57:23.154: INFO: (13) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 61.093212ms)
Mar 23 15:57:23.154: INFO: (13) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 61.26743ms)
Mar 23 15:57:23.154: INFO: (13) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 61.026445ms)
Mar 23 15:57:23.154: INFO: (13) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 61.805414ms)
Mar 23 15:57:23.154: INFO: (13) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 61.257929ms)
Mar 23 15:57:23.154: INFO: (13) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 61.522991ms)
Mar 23 15:57:23.155: INFO: (13) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 61.479972ms)
Mar 23 15:57:23.156: INFO: (13) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 62.536375ms)
Mar 23 15:57:23.215: INFO: (13) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 121.973217ms)
Mar 23 15:57:23.215: INFO: (13) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 121.82125ms)
Mar 23 15:57:23.218: INFO: (13) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 125.838627ms)
Mar 23 15:57:23.219: INFO: (13) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 125.862711ms)
Mar 23 15:57:23.219: INFO: (13) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 125.832475ms)
Mar 23 15:57:23.219: INFO: (13) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 126.376991ms)
Mar 23 15:57:23.262: INFO: (14) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 42.439437ms)
Mar 23 15:57:23.315: INFO: (14) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 94.94687ms)
Mar 23 15:57:23.318: INFO: (14) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 98.63757ms)
Mar 23 15:57:23.319: INFO: (14) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 99.249311ms)
Mar 23 15:57:23.319: INFO: (14) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 99.322741ms)
Mar 23 15:57:23.319: INFO: (14) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 99.494164ms)
Mar 23 15:57:23.319: INFO: (14) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 99.480698ms)
Mar 23 15:57:23.319: INFO: (14) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 99.694739ms)
Mar 23 15:57:23.319: INFO: (14) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 99.610985ms)
Mar 23 15:57:23.320: INFO: (14) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 100.497885ms)
Mar 23 15:57:23.321: INFO: (14) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 100.824551ms)
Mar 23 15:57:23.326: INFO: (14) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 106.535165ms)
Mar 23 15:57:23.326: INFO: (14) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 106.382134ms)
Mar 23 15:57:23.328: INFO: (14) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 107.862613ms)
Mar 23 15:57:23.328: INFO: (14) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 107.984611ms)
Mar 23 15:57:23.328: INFO: (14) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 108.564175ms)
Mar 23 15:57:23.370: INFO: (15) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 41.479328ms)
Mar 23 15:57:23.397: INFO: (15) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 68.390031ms)
Mar 23 15:57:23.397: INFO: (15) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 67.955581ms)
Mar 23 15:57:23.402: INFO: (15) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 73.482582ms)
Mar 23 15:57:23.402: INFO: (15) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 72.576596ms)
Mar 23 15:57:23.403: INFO: (15) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 74.157774ms)
Mar 23 15:57:23.403: INFO: (15) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 73.980441ms)
Mar 23 15:57:23.403: INFO: (15) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 74.54587ms)
Mar 23 15:57:23.404: INFO: (15) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 75.037833ms)
Mar 23 15:57:23.404: INFO: (15) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 75.561882ms)
Mar 23 15:57:23.404: INFO: (15) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 75.368757ms)
Mar 23 15:57:23.406: INFO: (15) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 77.424883ms)
Mar 23 15:57:23.408: INFO: (15) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 79.144493ms)
Mar 23 15:57:23.408: INFO: (15) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 79.13063ms)
Mar 23 15:57:23.420: INFO: (15) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 91.58535ms)
Mar 23 15:57:23.421: INFO: (15) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 92.719299ms)
Mar 23 15:57:23.457: INFO: (16) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 35.467843ms)
Mar 23 15:57:23.477: INFO: (16) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 55.674185ms)
Mar 23 15:57:23.477: INFO: (16) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 54.11289ms)
Mar 23 15:57:23.481: INFO: (16) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 58.484429ms)
Mar 23 15:57:23.481: INFO: (16) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 59.260779ms)
Mar 23 15:57:23.481: INFO: (16) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 59.747689ms)
Mar 23 15:57:23.481: INFO: (16) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 58.863707ms)
Mar 23 15:57:23.481: INFO: (16) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 60.542741ms)
Mar 23 15:57:23.481: INFO: (16) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 59.980034ms)
Mar 23 15:57:23.481: INFO: (16) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 59.196367ms)
Mar 23 15:57:23.481: INFO: (16) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 58.780879ms)
Mar 23 15:57:23.483: INFO: (16) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 60.587376ms)
Mar 23 15:57:23.490: INFO: (16) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 66.844482ms)
Mar 23 15:57:23.490: INFO: (16) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 66.778368ms)
Mar 23 15:57:23.490: INFO: (16) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 67.507642ms)
Mar 23 15:57:23.490: INFO: (16) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 66.885609ms)
Mar 23 15:57:23.523: INFO: (17) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 32.147043ms)
Mar 23 15:57:23.543: INFO: (17) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 52.571342ms)
Mar 23 15:57:23.543: INFO: (17) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 52.748091ms)
Mar 23 15:57:23.543: INFO: (17) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 52.826703ms)
Mar 23 15:57:23.543: INFO: (17) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 52.910292ms)
Mar 23 15:57:23.545: INFO: (17) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 54.278106ms)
Mar 23 15:57:23.545: INFO: (17) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 54.303664ms)
Mar 23 15:57:23.545: INFO: (17) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 54.337545ms)
Mar 23 15:57:23.546: INFO: (17) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 54.983098ms)
Mar 23 15:57:23.546: INFO: (17) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 55.185831ms)
Mar 23 15:57:23.547: INFO: (17) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 55.996585ms)
Mar 23 15:57:23.547: INFO: (17) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 56.646062ms)
Mar 23 15:57:23.551: INFO: (17) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 60.051327ms)
Mar 23 15:57:23.552: INFO: (17) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 62.120958ms)
Mar 23 15:57:23.558: INFO: (17) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 67.026389ms)
Mar 23 15:57:23.558: INFO: (17) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 67.065974ms)
Mar 23 15:57:23.693: INFO: (18) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 134.660008ms)
Mar 23 15:57:23.695: INFO: (18) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 136.29398ms)
Mar 23 15:57:23.695: INFO: (18) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 136.484751ms)
Mar 23 15:57:23.695: INFO: (18) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 136.449786ms)
Mar 23 15:57:23.695: INFO: (18) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 136.386334ms)
Mar 23 15:57:23.695: INFO: (18) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 136.727921ms)
Mar 23 15:57:23.695: INFO: (18) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 136.790396ms)
Mar 23 15:57:23.695: INFO: (18) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 137.089502ms)
Mar 23 15:57:23.695: INFO: (18) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 137.121965ms)
Mar 23 15:57:23.702: INFO: (18) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 144.025815ms)
Mar 23 15:57:23.703: INFO: (18) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 144.17724ms)
Mar 23 15:57:23.703: INFO: (18) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 144.385472ms)
Mar 23 15:57:23.703: INFO: (18) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 144.277513ms)
Mar 23 15:57:23.703: INFO: (18) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 144.437054ms)
Mar 23 15:57:23.703: INFO: (18) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 144.5784ms)
Mar 23 15:57:23.703: INFO: (18) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 144.722518ms)
Mar 23 15:57:23.740: INFO: (19) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 36.509323ms)
Mar 23 15:57:23.758: INFO: (19) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">test<... (200; 54.394271ms)
Mar 23 15:57:23.758: INFO: (19) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:443/proxy/tlsrewritem... (200; 54.372117ms)
Mar 23 15:57:23.758: INFO: (19) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:462/proxy/: tls qux (200; 54.709977ms)
Mar 23 15:57:23.758: INFO: (19) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:160/proxy/: foo (200; 54.669879ms)
Mar 23 15:57:23.759: INFO: (19) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 55.121777ms)
Mar 23 15:57:23.759: INFO: (19) /api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/http:proxy-service-v4gtp-gzq67:1080/proxy/rewriteme">... (200; 55.044639ms)
Mar 23 15:57:23.759: INFO: (19) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname1/proxy/: tls baz (200; 55.450069ms)
Mar 23 15:57:23.759: INFO: (19) /api/v1/namespaces/proxy-6933/pods/https:proxy-service-v4gtp-gzq67:460/proxy/: tls baz (200; 55.386372ms)
Mar 23 15:57:23.759: INFO: (19) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/: <a href="/api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67/proxy/rewriteme">test</a> (200; 55.527846ms)
Mar 23 15:57:23.759: INFO: (19) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname2/proxy/: bar (200; 55.848129ms)
Mar 23 15:57:23.766: INFO: (19) /api/v1/namespaces/proxy-6933/pods/proxy-service-v4gtp-gzq67:162/proxy/: bar (200; 62.173669ms)
Mar 23 15:57:23.774: INFO: (19) /api/v1/namespaces/proxy-6933/services/proxy-service-v4gtp:portname1/proxy/: foo (200; 69.897136ms)
Mar 23 15:57:23.774: INFO: (19) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname2/proxy/: bar (200; 70.010507ms)
Mar 23 15:57:23.774: INFO: (19) /api/v1/namespaces/proxy-6933/services/https:proxy-service-v4gtp:tlsportname2/proxy/: tls qux (200; 70.039394ms)
Mar 23 15:57:23.774: INFO: (19) /api/v1/namespaces/proxy-6933/services/http:proxy-service-v4gtp:portname1/proxy/: foo (200; 70.199729ms)
STEP: deleting ReplicationController proxy-service-v4gtp in namespace proxy-6933, will wait for the garbage collector to delete the pods
Mar 23 15:57:23.928: INFO: Deleting ReplicationController proxy-service-v4gtp took: 74.006322ms
Mar 23 15:57:24.628: INFO: Terminating ReplicationController proxy-service-v4gtp pods took: 700.327102ms
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:57:27.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6933" for this suite.
Mar 23 15:57:39.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:57:41.524: INFO: namespace proxy-6933 deletion completed in 13.7391975s

• [SLOW TEST:31.493 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:57:41.525: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:57:55.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5203" for this suite.
Mar 23 15:58:08.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 15:58:10.834: INFO: namespace resourcequota-5203 deletion completed in 14.882117154s

• [SLOW TEST:29.310 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 15:58:10.834: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 15:58:11.408: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-2bc1603e-d68f-46fb-80a1-5880462fe26a
STEP: Creating configMap with name cm-test-opt-upd-f30936b0-91b7-47f2-b6c8-d08bde2967ad
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-2bc1603e-d68f-46fb-80a1-5880462fe26a
STEP: Updating configmap cm-test-opt-upd-f30936b0-91b7-47f2-b6c8-d08bde2967ad
STEP: Creating configMap with name cm-test-opt-create-fa8f3cf5-2bea-4f87-8a04-f746b288d5bd
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 15:59:40.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8454" for this suite.
Mar 23 16:00:02.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:00:06.109: INFO: namespace projected-8454 deletion completed in 26.014603967s

• [SLOW TEST:115.275 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:00:06.111: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:00:21.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-345" for this suite.
Mar 23 16:00:41.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:00:44.763: INFO: namespace watch-345 deletion completed in 23.2028387s

• [SLOW TEST:38.652 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:00:44.763: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Mar 23 16:00:46.441: INFO: Waiting up to 5m0s for pod "client-containers-c77b192a-7f99-429a-bfd0-f4d5c9629b2b" in namespace "containers-3030" to be "success or failure"
Mar 23 16:00:46.473: INFO: Pod "client-containers-c77b192a-7f99-429a-bfd0-f4d5c9629b2b": Phase="Pending", Reason="", readiness=false. Elapsed: 31.575206ms
Mar 23 16:00:48.499: INFO: Pod "client-containers-c77b192a-7f99-429a-bfd0-f4d5c9629b2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057480782s
Mar 23 16:00:50.547: INFO: Pod "client-containers-c77b192a-7f99-429a-bfd0-f4d5c9629b2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.106257845s
STEP: Saw pod success
Mar 23 16:00:50.547: INFO: Pod "client-containers-c77b192a-7f99-429a-bfd0-f4d5c9629b2b" satisfied condition "success or failure"
Mar 23 16:00:50.734: INFO: Trying to get logs from node 10.241.69.181 pod client-containers-c77b192a-7f99-429a-bfd0-f4d5c9629b2b container test-container: <nil>
STEP: delete the pod
Mar 23 16:00:51.314: INFO: Waiting for pod client-containers-c77b192a-7f99-429a-bfd0-f4d5c9629b2b to disappear
Mar 23 16:00:51.761: INFO: Pod client-containers-c77b192a-7f99-429a-bfd0-f4d5c9629b2b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:00:51.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3030" for this suite.
Mar 23 16:01:14.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:01:17.848: INFO: namespace containers-3030 deletion completed in 25.94143344s

• [SLOW TEST:33.085 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:01:17.850: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 23 16:01:19.171: INFO: Waiting up to 5m0s for pod "pod-fabed84b-9da3-400d-b060-7d51b55b172a" in namespace "emptydir-1584" to be "success or failure"
Mar 23 16:01:19.220: INFO: Pod "pod-fabed84b-9da3-400d-b060-7d51b55b172a": Phase="Pending", Reason="", readiness=false. Elapsed: 48.779043ms
Mar 23 16:01:21.258: INFO: Pod "pod-fabed84b-9da3-400d-b060-7d51b55b172a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.086814519s
STEP: Saw pod success
Mar 23 16:01:21.258: INFO: Pod "pod-fabed84b-9da3-400d-b060-7d51b55b172a" satisfied condition "success or failure"
Mar 23 16:01:21.287: INFO: Trying to get logs from node 10.241.69.181 pod pod-fabed84b-9da3-400d-b060-7d51b55b172a container test-container: <nil>
STEP: delete the pod
Mar 23 16:01:21.533: INFO: Waiting for pod pod-fabed84b-9da3-400d-b060-7d51b55b172a to disappear
Mar 23 16:01:21.561: INFO: Pod pod-fabed84b-9da3-400d-b060-7d51b55b172a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:01:21.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1584" for this suite.
Mar 23 16:01:35.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:01:39.320: INFO: namespace emptydir-1584 deletion completed in 17.701272223s

• [SLOW TEST:21.471 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:01:39.321: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-qtnw
STEP: Creating a pod to test atomic-volume-subpath
Mar 23 16:01:40.199: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-qtnw" in namespace "subpath-3336" to be "success or failure"
Mar 23 16:01:40.247: INFO: Pod "pod-subpath-test-downwardapi-qtnw": Phase="Pending", Reason="", readiness=false. Elapsed: 48.037828ms
Mar 23 16:01:42.290: INFO: Pod "pod-subpath-test-downwardapi-qtnw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.090452619s
Mar 23 16:01:44.454: INFO: Pod "pod-subpath-test-downwardapi-qtnw": Phase="Running", Reason="", readiness=true. Elapsed: 4.255082776s
Mar 23 16:01:46.495: INFO: Pod "pod-subpath-test-downwardapi-qtnw": Phase="Running", Reason="", readiness=true. Elapsed: 6.295902426s
Mar 23 16:01:48.562: INFO: Pod "pod-subpath-test-downwardapi-qtnw": Phase="Running", Reason="", readiness=true. Elapsed: 8.362741189s
Mar 23 16:01:50.606: INFO: Pod "pod-subpath-test-downwardapi-qtnw": Phase="Running", Reason="", readiness=true. Elapsed: 10.406236544s
Mar 23 16:01:52.668: INFO: Pod "pod-subpath-test-downwardapi-qtnw": Phase="Running", Reason="", readiness=true. Elapsed: 12.468684913s
Mar 23 16:01:54.707: INFO: Pod "pod-subpath-test-downwardapi-qtnw": Phase="Running", Reason="", readiness=true. Elapsed: 14.50788002s
Mar 23 16:01:56.827: INFO: Pod "pod-subpath-test-downwardapi-qtnw": Phase="Running", Reason="", readiness=true. Elapsed: 16.628167076s
Mar 23 16:01:58.862: INFO: Pod "pod-subpath-test-downwardapi-qtnw": Phase="Running", Reason="", readiness=true. Elapsed: 18.662943628s
Mar 23 16:02:00.889: INFO: Pod "pod-subpath-test-downwardapi-qtnw": Phase="Running", Reason="", readiness=true. Elapsed: 20.689740539s
Mar 23 16:02:02.963: INFO: Pod "pod-subpath-test-downwardapi-qtnw": Phase="Running", Reason="", readiness=true. Elapsed: 22.763688814s
Mar 23 16:02:04.994: INFO: Pod "pod-subpath-test-downwardapi-qtnw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.794721031s
STEP: Saw pod success
Mar 23 16:02:04.994: INFO: Pod "pod-subpath-test-downwardapi-qtnw" satisfied condition "success or failure"
Mar 23 16:02:05.017: INFO: Trying to get logs from node 10.241.69.181 pod pod-subpath-test-downwardapi-qtnw container test-container-subpath-downwardapi-qtnw: <nil>
STEP: delete the pod
Mar 23 16:02:05.450: INFO: Waiting for pod pod-subpath-test-downwardapi-qtnw to disappear
Mar 23 16:02:05.488: INFO: Pod pod-subpath-test-downwardapi-qtnw no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-qtnw
Mar 23 16:02:05.489: INFO: Deleting pod "pod-subpath-test-downwardapi-qtnw" in namespace "subpath-3336"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:02:05.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3336" for this suite.
Mar 23 16:02:19.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:02:23.020: INFO: namespace subpath-3336 deletion completed in 17.393132557s

• [SLOW TEST:43.699 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:02:23.022: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-6232/configmap-test-a87187e6-db08-42ab-8e97-f4ff59961646
STEP: Creating a pod to test consume configMaps
Mar 23 16:02:23.830: INFO: Waiting up to 5m0s for pod "pod-configmaps-de051cd2-423f-443b-bebb-ce52b9d7ea9d" in namespace "configmap-6232" to be "success or failure"
Mar 23 16:02:23.870: INFO: Pod "pod-configmaps-de051cd2-423f-443b-bebb-ce52b9d7ea9d": Phase="Pending", Reason="", readiness=false. Elapsed: 40.128492ms
Mar 23 16:02:25.900: INFO: Pod "pod-configmaps-de051cd2-423f-443b-bebb-ce52b9d7ea9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070038944s
Mar 23 16:02:27.951: INFO: Pod "pod-configmaps-de051cd2-423f-443b-bebb-ce52b9d7ea9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.12130451s
STEP: Saw pod success
Mar 23 16:02:27.951: INFO: Pod "pod-configmaps-de051cd2-423f-443b-bebb-ce52b9d7ea9d" satisfied condition "success or failure"
Mar 23 16:02:27.985: INFO: Trying to get logs from node 10.241.69.181 pod pod-configmaps-de051cd2-423f-443b-bebb-ce52b9d7ea9d container env-test: <nil>
STEP: delete the pod
Mar 23 16:02:28.324: INFO: Waiting for pod pod-configmaps-de051cd2-423f-443b-bebb-ce52b9d7ea9d to disappear
Mar 23 16:02:28.367: INFO: Pod pod-configmaps-de051cd2-423f-443b-bebb-ce52b9d7ea9d no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:02:28.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6232" for this suite.
Mar 23 16:02:42.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:02:45.276: INFO: namespace configmap-6232 deletion completed in 16.851388962s

• [SLOW TEST:22.254 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:02:45.277: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:02:46.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-5371" for this suite.
Mar 23 16:02:58.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:03:01.248: INFO: namespace tables-5371 deletion completed in 15.12364811s

• [SLOW TEST:15.971 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:03:01.249: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 16:03:02.152: INFO: Create a RollingUpdate DaemonSet
Mar 23 16:03:02.177: INFO: Check that daemon pods launch on every node of the cluster
Mar 23 16:03:02.242: INFO: Number of nodes with available pods: 0
Mar 23 16:03:02.242: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 16:03:03.305: INFO: Number of nodes with available pods: 0
Mar 23 16:03:03.305: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 16:03:04.331: INFO: Number of nodes with available pods: 0
Mar 23 16:03:04.331: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 16:03:05.321: INFO: Number of nodes with available pods: 2
Mar 23 16:03:05.321: INFO: Node 10.241.69.181 is running more than one daemon pod
Mar 23 16:03:06.405: INFO: Number of nodes with available pods: 3
Mar 23 16:03:06.406: INFO: Number of running nodes: 3, number of available pods: 3
Mar 23 16:03:06.406: INFO: Update the DaemonSet to trigger a rollout
Mar 23 16:03:06.496: INFO: Updating DaemonSet daemon-set
Mar 23 16:03:10.741: INFO: Roll back the DaemonSet before rollout is complete
Mar 23 16:03:10.777: INFO: Updating DaemonSet daemon-set
Mar 23 16:03:10.777: INFO: Make sure DaemonSet rollback is complete
Mar 23 16:03:10.826: INFO: Wrong image for pod: daemon-set-7ghqw. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 23 16:03:10.826: INFO: Pod daemon-set-7ghqw is not available
Mar 23 16:03:11.916: INFO: Wrong image for pod: daemon-set-7ghqw. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 23 16:03:11.916: INFO: Pod daemon-set-7ghqw is not available
Mar 23 16:03:12.918: INFO: Wrong image for pod: daemon-set-7ghqw. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 23 16:03:12.919: INFO: Pod daemon-set-7ghqw is not available
Mar 23 16:03:13.917: INFO: Wrong image for pod: daemon-set-7ghqw. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 23 16:03:13.917: INFO: Pod daemon-set-7ghqw is not available
Mar 23 16:03:14.920: INFO: Wrong image for pod: daemon-set-7ghqw. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 23 16:03:14.920: INFO: Pod daemon-set-7ghqw is not available
Mar 23 16:03:15.924: INFO: Wrong image for pod: daemon-set-7ghqw. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 23 16:03:15.924: INFO: Pod daemon-set-7ghqw is not available
Mar 23 16:03:16.918: INFO: Wrong image for pod: daemon-set-7ghqw. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 23 16:03:16.918: INFO: Pod daemon-set-7ghqw is not available
Mar 23 16:03:17.979: INFO: Wrong image for pod: daemon-set-7ghqw. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 23 16:03:17.979: INFO: Pod daemon-set-7ghqw is not available
Mar 23 16:03:18.917: INFO: Wrong image for pod: daemon-set-7ghqw. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Mar 23 16:03:18.917: INFO: Pod daemon-set-7ghqw is not available
Mar 23 16:03:19.909: INFO: Pod daemon-set-vfm9w is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8252, will wait for the garbage collector to delete the pods
Mar 23 16:03:20.107: INFO: Deleting DaemonSet.extensions daemon-set took: 50.688299ms
Mar 23 16:03:20.607: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.368167ms
Mar 23 16:03:33.933: INFO: Number of nodes with available pods: 0
Mar 23 16:03:33.933: INFO: Number of running nodes: 0, number of available pods: 0
Mar 23 16:03:33.950: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8252/daemonsets","resourceVersion":"78433"},"items":null}

Mar 23 16:03:33.973: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8252/pods","resourceVersion":"78433"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:03:34.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8252" for this suite.
Mar 23 16:03:48.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:03:51.181: INFO: namespace daemonsets-8252 deletion completed in 17.037508273s

• [SLOW TEST:49.932 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:03:51.181: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:04:05.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2024" for this suite.
Mar 23 16:04:17.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:04:21.243: INFO: namespace namespaces-2024 deletion completed in 15.402843085s
STEP: Destroying namespace "nsdeletetest-7483" for this suite.
Mar 23 16:04:21.284: INFO: Namespace nsdeletetest-7483 was already deleted
STEP: Destroying namespace "nsdeletetest-3422" for this suite.
Mar 23 16:04:33.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:04:36.484: INFO: namespace nsdeletetest-3422 deletion completed in 15.199891655s

• [SLOW TEST:45.303 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:04:36.484: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1296.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1296.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1296.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1296.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1296.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1296.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 16:04:52.869: INFO: DNS probes using dns-1296/dns-test-2b51c0e9-e91e-43a6-adc0-afa22e3e6bc8 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:04:52.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1296" for this suite.
Mar 23 16:05:05.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:05:08.045: INFO: namespace dns-1296 deletion completed in 14.988642144s

• [SLOW TEST:31.561 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:05:08.047: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-646
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-646
I0323 16:05:08.821632      22 runners.go:184] Created replication controller with name: externalname-service, namespace: services-646, replica count: 2
I0323 16:05:11.872463      22 runners.go:184] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 16:05:14.872: INFO: Creating new exec pod
I0323 16:05:14.872705      22 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 16:05:20.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=services-646 execpodzpvzh -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Mar 23 16:05:20.894: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 23 16:05:20.894: INFO: stdout: ""
Mar 23 16:05:20.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=services-646 execpodzpvzh -- /bin/sh -x -c nc -zv -t -w 2 172.21.188.22 80'
Mar 23 16:05:21.381: INFO: stderr: "+ nc -zv -t -w 2 172.21.188.22 80\nConnection to 172.21.188.22 80 port [tcp/http] succeeded!\n"
Mar 23 16:05:21.381: INFO: stdout: ""
Mar 23 16:05:21.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=services-646 execpodzpvzh -- /bin/sh -x -c nc -zv -t -w 2 10.241.69.172 31755'
Mar 23 16:05:21.800: INFO: stderr: "+ nc -zv -t -w 2 10.241.69.172 31755\nConnection to 10.241.69.172 31755 port [tcp/31755] succeeded!\n"
Mar 23 16:05:21.800: INFO: stdout: ""
Mar 23 16:05:21.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=services-646 execpodzpvzh -- /bin/sh -x -c nc -zv -t -w 2 10.241.69.181 31755'
Mar 23 16:05:22.248: INFO: stderr: "+ nc -zv -t -w 2 10.241.69.181 31755\nConnection to 10.241.69.181 31755 port [tcp/31755] succeeded!\n"
Mar 23 16:05:22.248: INFO: stdout: ""
Mar 23 16:05:22.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=services-646 execpodzpvzh -- /bin/sh -x -c nc -zv -t -w 2 169.59.196.52 31755'
Mar 23 16:05:22.647: INFO: stderr: "+ nc -zv -t -w 2 169.59.196.52 31755\nConnection to 169.59.196.52 31755 port [tcp/31755] succeeded!\n"
Mar 23 16:05:22.647: INFO: stdout: ""
Mar 23 16:05:22.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=services-646 execpodzpvzh -- /bin/sh -x -c nc -zv -t -w 2 169.59.196.53 31755'
Mar 23 16:05:23.300: INFO: stderr: "+ nc -zv -t -w 2 169.59.196.53 31755\nConnection to 169.59.196.53 31755 port [tcp/31755] succeeded!\n"
Mar 23 16:05:23.300: INFO: stdout: ""
Mar 23 16:05:23.301: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:05:23.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-646" for this suite.
Mar 23 16:05:39.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:05:42.199: INFO: namespace services-646 deletion completed in 18.698683016s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:34.152 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:05:42.199: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 23 16:05:42.950: INFO: Waiting up to 5m0s for pod "pod-22f6ecf7-194f-447d-90c6-3d0fc407185d" in namespace "emptydir-4751" to be "success or failure"
Mar 23 16:05:43.011: INFO: Pod "pod-22f6ecf7-194f-447d-90c6-3d0fc407185d": Phase="Pending", Reason="", readiness=false. Elapsed: 60.754219ms
Mar 23 16:05:45.041: INFO: Pod "pod-22f6ecf7-194f-447d-90c6-3d0fc407185d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.090673858s
Mar 23 16:05:47.079: INFO: Pod "pod-22f6ecf7-194f-447d-90c6-3d0fc407185d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.128753075s
STEP: Saw pod success
Mar 23 16:05:47.079: INFO: Pod "pod-22f6ecf7-194f-447d-90c6-3d0fc407185d" satisfied condition "success or failure"
Mar 23 16:05:47.101: INFO: Trying to get logs from node 10.241.69.181 pod pod-22f6ecf7-194f-447d-90c6-3d0fc407185d container test-container: <nil>
STEP: delete the pod
Mar 23 16:05:47.301: INFO: Waiting for pod pod-22f6ecf7-194f-447d-90c6-3d0fc407185d to disappear
Mar 23 16:05:47.322: INFO: Pod pod-22f6ecf7-194f-447d-90c6-3d0fc407185d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:05:47.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4751" for this suite.
Mar 23 16:06:01.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:06:04.298: INFO: namespace emptydir-4751 deletion completed in 16.936903862s

• [SLOW TEST:22.099 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:06:04.298: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Mar 23 16:06:06.699: INFO: Pod name wrapped-volume-race-aa921522-2c1b-46fc-a8d3-db38b88c4343: Found 0 pods out of 5
Mar 23 16:06:11.777: INFO: Pod name wrapped-volume-race-aa921522-2c1b-46fc-a8d3-db38b88c4343: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-aa921522-2c1b-46fc-a8d3-db38b88c4343 in namespace emptydir-wrapper-368, will wait for the garbage collector to delete the pods
Mar 23 16:06:16.119: INFO: Deleting ReplicationController wrapped-volume-race-aa921522-2c1b-46fc-a8d3-db38b88c4343 took: 81.023027ms
Mar 23 16:06:16.819: INFO: Terminating ReplicationController wrapped-volume-race-aa921522-2c1b-46fc-a8d3-db38b88c4343 pods took: 700.248467ms
STEP: Creating RC which spawns configmap-volume pods
Mar 23 16:06:55.126: INFO: Pod name wrapped-volume-race-a1757738-1d66-4cf5-88e6-0017150f47d8: Found 0 pods out of 5
Mar 23 16:07:00.175: INFO: Pod name wrapped-volume-race-a1757738-1d66-4cf5-88e6-0017150f47d8: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a1757738-1d66-4cf5-88e6-0017150f47d8 in namespace emptydir-wrapper-368, will wait for the garbage collector to delete the pods
Mar 23 16:07:02.645: INFO: Deleting ReplicationController wrapped-volume-race-a1757738-1d66-4cf5-88e6-0017150f47d8 took: 78.021157ms
Mar 23 16:07:03.346: INFO: Terminating ReplicationController wrapped-volume-race-a1757738-1d66-4cf5-88e6-0017150f47d8 pods took: 700.464697ms
STEP: Creating RC which spawns configmap-volume pods
Mar 23 16:07:44.659: INFO: Pod name wrapped-volume-race-b27a45aa-2af2-48cd-a091-a1de9fe3303d: Found 0 pods out of 5
Mar 23 16:07:49.706: INFO: Pod name wrapped-volume-race-b27a45aa-2af2-48cd-a091-a1de9fe3303d: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b27a45aa-2af2-48cd-a091-a1de9fe3303d in namespace emptydir-wrapper-368, will wait for the garbage collector to delete the pods
Mar 23 16:07:50.086: INFO: Deleting ReplicationController wrapped-volume-race-b27a45aa-2af2-48cd-a091-a1de9fe3303d took: 158.846588ms
Mar 23 16:07:50.586: INFO: Terminating ReplicationController wrapped-volume-race-b27a45aa-2af2-48cd-a091-a1de9fe3303d pods took: 500.249953ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:08:41.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-368" for this suite.
Mar 23 16:08:57.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:09:00.392: INFO: namespace emptydir-wrapper-368 deletion completed in 19.290876486s

• [SLOW TEST:176.093 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:09:00.392: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 16:09:01.272: INFO: Waiting up to 5m0s for pod "downwardapi-volume-654e5073-5004-49e5-a3fa-6c7543efec59" in namespace "downward-api-542" to be "success or failure"
Mar 23 16:09:01.309: INFO: Pod "downwardapi-volume-654e5073-5004-49e5-a3fa-6c7543efec59": Phase="Pending", Reason="", readiness=false. Elapsed: 36.487578ms
Mar 23 16:09:03.362: INFO: Pod "downwardapi-volume-654e5073-5004-49e5-a3fa-6c7543efec59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.090082469s
Mar 23 16:09:05.391: INFO: Pod "downwardapi-volume-654e5073-5004-49e5-a3fa-6c7543efec59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.119094567s
STEP: Saw pod success
Mar 23 16:09:05.391: INFO: Pod "downwardapi-volume-654e5073-5004-49e5-a3fa-6c7543efec59" satisfied condition "success or failure"
Mar 23 16:09:05.429: INFO: Trying to get logs from node 10.241.69.181 pod downwardapi-volume-654e5073-5004-49e5-a3fa-6c7543efec59 container client-container: <nil>
STEP: delete the pod
Mar 23 16:09:05.750: INFO: Waiting for pod downwardapi-volume-654e5073-5004-49e5-a3fa-6c7543efec59 to disappear
Mar 23 16:09:05.850: INFO: Pod downwardapi-volume-654e5073-5004-49e5-a3fa-6c7543efec59 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:09:05.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-542" for this suite.
Mar 23 16:09:20.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:09:22.580: INFO: namespace downward-api-542 deletion completed in 16.681238469s

• [SLOW TEST:22.188 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:09:22.581: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 16:09:24.436: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"a74dc390-d253-4d18-8e24-18031437775a", Controller:(*bool)(0xc0032a1f72), BlockOwnerDeletion:(*bool)(0xc0032a1f73)}}
Mar 23 16:09:24.544: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a27293fa-e740-4db3-b517-a585b4dbbc1a", Controller:(*bool)(0xc000709d26), BlockOwnerDeletion:(*bool)(0xc000709d27)}}
Mar 23 16:09:24.625: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"3e525cb8-7dc7-4982-8cf5-a132e6d5d0cd", Controller:(*bool)(0xc002eda1c6), BlockOwnerDeletion:(*bool)(0xc002eda1c7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:09:29.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5204" for this suite.
Mar 23 16:09:41.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:09:44.512: INFO: namespace gc-5204 deletion completed in 14.703466524s

• [SLOW TEST:21.931 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:09:44.512: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 16:09:45.116: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fab456a5-81ca-4150-b769-097e6f6c5aaf" in namespace "projected-3918" to be "success or failure"
Mar 23 16:09:45.153: INFO: Pod "downwardapi-volume-fab456a5-81ca-4150-b769-097e6f6c5aaf": Phase="Pending", Reason="", readiness=false. Elapsed: 37.078649ms
Mar 23 16:09:47.172: INFO: Pod "downwardapi-volume-fab456a5-81ca-4150-b769-097e6f6c5aaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056040549s
Mar 23 16:09:49.200: INFO: Pod "downwardapi-volume-fab456a5-81ca-4150-b769-097e6f6c5aaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.083652024s
STEP: Saw pod success
Mar 23 16:09:49.200: INFO: Pod "downwardapi-volume-fab456a5-81ca-4150-b769-097e6f6c5aaf" satisfied condition "success or failure"
Mar 23 16:09:49.226: INFO: Trying to get logs from node 10.241.69.181 pod downwardapi-volume-fab456a5-81ca-4150-b769-097e6f6c5aaf container client-container: <nil>
STEP: delete the pod
Mar 23 16:09:49.447: INFO: Waiting for pod downwardapi-volume-fab456a5-81ca-4150-b769-097e6f6c5aaf to disappear
Mar 23 16:09:49.486: INFO: Pod downwardapi-volume-fab456a5-81ca-4150-b769-097e6f6c5aaf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:09:49.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3918" for this suite.
Mar 23 16:10:01.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:10:04.293: INFO: namespace projected-3918 deletion completed in 14.757653543s

• [SLOW TEST:19.781 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:10:04.293: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:10:21.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1612" for this suite.
Mar 23 16:10:33.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:10:36.740: INFO: namespace resourcequota-1612 deletion completed in 15.141111849s

• [SLOW TEST:32.446 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:10:36.740: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Mar 23 16:10:37.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 cluster-info'
Mar 23 16:10:37.473: INFO: stderr: ""
Mar 23 16:10:37.473: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:10:37.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3502" for this suite.
Mar 23 16:10:49.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:10:52.735: INFO: namespace kubectl-3502 deletion completed in 15.187300171s

• [SLOW TEST:15.995 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:10:52.735: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:11:04.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5486" for this suite.
Mar 23 16:11:16.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:11:20.057: INFO: namespace resourcequota-5486 deletion completed in 15.359318707s

• [SLOW TEST:27.322 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:11:20.058: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0323 16:11:31.440725      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 23 16:11:31.440: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:11:31.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7881" for this suite.
Mar 23 16:11:47.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:11:51.230: INFO: namespace gc-7881 deletion completed in 19.747278034s

• [SLOW TEST:31.172 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:11:51.230: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-9c8c9cae-f976-4530-8aa7-f0d5631832fb
STEP: Creating a pod to test consume secrets
Mar 23 16:11:51.978: INFO: Waiting up to 5m0s for pod "pod-secrets-bcf6f826-9151-4745-a8cb-04aa1975b4ab" in namespace "secrets-1389" to be "success or failure"
Mar 23 16:11:52.049: INFO: Pod "pod-secrets-bcf6f826-9151-4745-a8cb-04aa1975b4ab": Phase="Pending", Reason="", readiness=false. Elapsed: 70.537661ms
Mar 23 16:11:54.144: INFO: Pod "pod-secrets-bcf6f826-9151-4745-a8cb-04aa1975b4ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.165934933s
Mar 23 16:11:56.204: INFO: Pod "pod-secrets-bcf6f826-9151-4745-a8cb-04aa1975b4ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.226133521s
STEP: Saw pod success
Mar 23 16:11:56.204: INFO: Pod "pod-secrets-bcf6f826-9151-4745-a8cb-04aa1975b4ab" satisfied condition "success or failure"
Mar 23 16:11:56.258: INFO: Trying to get logs from node 10.241.69.181 pod pod-secrets-bcf6f826-9151-4745-a8cb-04aa1975b4ab container secret-volume-test: <nil>
STEP: delete the pod
Mar 23 16:11:56.619: INFO: Waiting for pod pod-secrets-bcf6f826-9151-4745-a8cb-04aa1975b4ab to disappear
Mar 23 16:11:56.667: INFO: Pod pod-secrets-bcf6f826-9151-4745-a8cb-04aa1975b4ab no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:11:56.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1389" for this suite.
Mar 23 16:12:10.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:12:13.725: INFO: namespace secrets-1389 deletion completed in 17.010030051s

• [SLOW TEST:22.496 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:12:13.728: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 16:12:14.775: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:12:19.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5656" for this suite.
Mar 23 16:12:31.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:12:34.508: INFO: namespace custom-resource-definition-5656 deletion completed in 15.013681599s

• [SLOW TEST:20.780 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:12:34.509: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 16:12:35.140: INFO: (0) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 95.690857ms)
Mar 23 16:12:35.178: INFO: (1) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 37.711263ms)
Mar 23 16:12:35.227: INFO: (2) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 48.72566ms)
Mar 23 16:12:35.268: INFO: (3) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 41.439652ms)
Mar 23 16:12:35.304: INFO: (4) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 36.139957ms)
Mar 23 16:12:35.337: INFO: (5) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 33.158204ms)
Mar 23 16:12:35.375: INFO: (6) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 37.333158ms)
Mar 23 16:12:35.404: INFO: (7) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 28.725663ms)
Mar 23 16:12:35.431: INFO: (8) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 26.978595ms)
Mar 23 16:12:35.472: INFO: (9) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 41.134784ms)
Mar 23 16:12:35.504: INFO: (10) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 31.709328ms)
Mar 23 16:12:35.534: INFO: (11) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 29.834422ms)
Mar 23 16:12:35.571: INFO: (12) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 37.160202ms)
Mar 23 16:12:35.604: INFO: (13) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 32.599639ms)
Mar 23 16:12:35.640: INFO: (14) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 35.678197ms)
Mar 23 16:12:35.677: INFO: (15) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 36.882668ms)
Mar 23 16:12:35.716: INFO: (16) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 39.098693ms)
Mar 23 16:12:35.749: INFO: (17) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 33.137052ms)
Mar 23 16:12:35.796: INFO: (18) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 47.347731ms)
Mar 23 16:12:35.828: INFO: (19) /api/v1/nodes/10.241.69.172:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 31.965112ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:12:35.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8962" for this suite.
Mar 23 16:12:47.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:12:49.764: INFO: namespace proxy-8962 deletion completed in 13.891009878s

• [SLOW TEST:15.256 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:12:49.766: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 23 16:12:50.296: INFO: Waiting up to 5m0s for pod "pod-dffc63a5-246c-40c6-a23a-922695ad439d" in namespace "emptydir-7172" to be "success or failure"
Mar 23 16:12:50.327: INFO: Pod "pod-dffc63a5-246c-40c6-a23a-922695ad439d": Phase="Pending", Reason="", readiness=false. Elapsed: 30.780031ms
Mar 23 16:12:52.364: INFO: Pod "pod-dffc63a5-246c-40c6-a23a-922695ad439d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.067772093s
Mar 23 16:12:54.395: INFO: Pod "pod-dffc63a5-246c-40c6-a23a-922695ad439d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.098399413s
STEP: Saw pod success
Mar 23 16:12:54.395: INFO: Pod "pod-dffc63a5-246c-40c6-a23a-922695ad439d" satisfied condition "success or failure"
Mar 23 16:12:54.428: INFO: Trying to get logs from node 10.241.69.181 pod pod-dffc63a5-246c-40c6-a23a-922695ad439d container test-container: <nil>
STEP: delete the pod
Mar 23 16:12:54.608: INFO: Waiting for pod pod-dffc63a5-246c-40c6-a23a-922695ad439d to disappear
Mar 23 16:12:54.645: INFO: Pod pod-dffc63a5-246c-40c6-a23a-922695ad439d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:12:54.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7172" for this suite.
Mar 23 16:13:08.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:13:11.825: INFO: namespace emptydir-7172 deletion completed in 17.118825109s

• [SLOW TEST:22.060 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:13:11.826: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-6666e3ac-0d0e-485e-a142-7191ebee572c
STEP: Creating a pod to test consume secrets
Mar 23 16:13:13.011: INFO: Waiting up to 5m0s for pod "pod-secrets-6df3fa89-0cc6-4de9-b8d1-414a2622ff4a" in namespace "secrets-381" to be "success or failure"
Mar 23 16:13:13.044: INFO: Pod "pod-secrets-6df3fa89-0cc6-4de9-b8d1-414a2622ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 32.279887ms
Mar 23 16:13:15.070: INFO: Pod "pod-secrets-6df3fa89-0cc6-4de9-b8d1-414a2622ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058854214s
Mar 23 16:13:17.101: INFO: Pod "pod-secrets-6df3fa89-0cc6-4de9-b8d1-414a2622ff4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.089667541s
STEP: Saw pod success
Mar 23 16:13:17.101: INFO: Pod "pod-secrets-6df3fa89-0cc6-4de9-b8d1-414a2622ff4a" satisfied condition "success or failure"
Mar 23 16:13:17.130: INFO: Trying to get logs from node 10.241.69.181 pod pod-secrets-6df3fa89-0cc6-4de9-b8d1-414a2622ff4a container secret-volume-test: <nil>
STEP: delete the pod
Mar 23 16:13:17.591: INFO: Waiting for pod pod-secrets-6df3fa89-0cc6-4de9-b8d1-414a2622ff4a to disappear
Mar 23 16:13:17.618: INFO: Pod pod-secrets-6df3fa89-0cc6-4de9-b8d1-414a2622ff4a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:13:17.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-381" for this suite.
Mar 23 16:13:29.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:13:32.821: INFO: namespace secrets-381 deletion completed in 15.11907212s

• [SLOW TEST:20.996 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:13:32.824: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-53
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 23 16:13:33.325: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 23 16:13:58.478: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.197.176:8080/dial?request=hostName&protocol=http&host=172.30.123.236&port=8080&tries=1'] Namespace:pod-network-test-53 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 16:13:58.478: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 16:13:58.769: INFO: Waiting for endpoints: map[]
Mar 23 16:13:58.795: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.197.176:8080/dial?request=hostName&protocol=http&host=172.30.197.175&port=8080&tries=1'] Namespace:pod-network-test-53 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 16:13:58.796: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 16:13:59.085: INFO: Waiting for endpoints: map[]
Mar 23 16:13:59.117: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.197.176:8080/dial?request=hostName&protocol=http&host=172.30.64.252&port=8080&tries=1'] Namespace:pod-network-test-53 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 16:13:59.117: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 16:13:59.390: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:13:59.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-53" for this suite.
Mar 23 16:14:13.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:14:16.406: INFO: namespace pod-network-test-53 deletion completed in 16.969905671s

• [SLOW TEST:43.582 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:14:16.406: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar 23 16:14:16.897: INFO: PodSpec: initContainers in spec.initContainers
Mar 23 16:15:04.744: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9f8c9b13-21e7-4c26-b5d2-0208f048629e", GenerateName:"", Namespace:"init-container-4662", SelfLink:"/api/v1/namespaces/init-container-4662/pods/pod-init-9f8c9b13-21e7-4c26-b5d2-0208f048629e", UID:"060865fe-f1c8-4051-a60a-5c5f782293bd", ResourceVersion:"84003", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63720576856, loc:(*time.Location)(0x84c02a0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"897268291"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.30.197.177/32", "cni.projectcalico.org/podIPs":"172.30.197.177/32", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.197.177\"\n    ],\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-v2g8v", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc000a8e000), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-v2g8v", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003efa1e0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-v2g8v", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003efa320), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-v2g8v", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc003efa0a0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0037b2f98), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.241.69.181", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00330a060), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0037b3050)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0037b3070)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0037b308c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0037b3090), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720576857, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720576857, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720576857, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720576857, loc:(*time.Location)(0x84c02a0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.241.69.181", PodIP:"172.30.197.177", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.197.177"}}, StartTime:(*v1.Time)(0xc003276060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0032780e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003278150)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://ad751134b005992f0e6596caecff8fdfdb15f68d91fdfa3da5fddd2af5a15d0c", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0032760e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003276080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc0037b3114)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:15:04.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4662" for this suite.
Mar 23 16:15:32.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:15:36.405: INFO: namespace init-container-4662 deletion completed in 31.599947451s

• [SLOW TEST:79.999 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:15:36.406: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Mar 23 16:15:42.354: INFO: Successfully updated pod "annotationupdatecd0a7b13-cf19-4559-a577-f8dedc79fb5b"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:15:44.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-690" for this suite.
Mar 23 16:16:08.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:16:12.050: INFO: namespace downward-api-690 deletion completed in 27.510112696s

• [SLOW TEST:35.644 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:16:12.055: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Mar 23 16:16:15.111: INFO: &Pod{ObjectMeta:{send-events-eeb354bf-bf19-46aa-815b-c76f1c0bd859  events-3096 /api/v1/namespaces/events-3096/pods/send-events-eeb354bf-bf19-46aa-815b-c76f1c0bd859 3d8a1d67-5488-4420-baaf-5d0b905e322d 84482 0 2020-03-23 16:16:12 +0000 UTC <nil> <nil> map[name:foo time:722413345] map[cni.projectcalico.org/podIP:172.30.197.179/32 cni.projectcalico.org/podIPs:172.30.197.179/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.197.179"
    ],
    "dns": {}
}] openshift.io/scc:anyuid] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5z45j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5z45j,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5z45j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.181,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c62,c54,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 16:16:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 16:16:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 16:16:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 16:16:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.181,PodIP:172.30.197.179,StartTime:2020-03-23 16:16:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 16:16:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:cri-o://ce16f5f1b3255e768ce07d9f58bad10f6310480b7c2bb32064f87c465e28d893,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.197.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Mar 23 16:16:17.129: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Mar 23 16:16:19.146: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:16:19.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3096" for this suite.
Mar 23 16:17:05.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:17:08.984: INFO: namespace events-3096 deletion completed in 49.708661978s

• [SLOW TEST:56.929 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:17:08.985: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 23 16:17:12.835: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:17:12.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-145" for this suite.
Mar 23 16:17:27.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:17:30.863: INFO: namespace container-runtime-145 deletion completed in 17.760598025s

• [SLOW TEST:21.878 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:17:30.864: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-1108
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 23 16:17:31.390: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 23 16:17:56.714: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.123.237 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1108 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 16:17:56.714: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 16:17:58.048: INFO: Found all expected endpoints: [netserver-0]
Mar 23 16:17:58.077: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.64.254 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1108 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 16:17:58.077: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 16:17:59.602: INFO: Found all expected endpoints: [netserver-1]
Mar 23 16:17:59.783: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.197.181 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1108 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 16:17:59.783: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 16:18:01.133: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:18:01.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1108" for this suite.
Mar 23 16:18:19.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:18:22.430: INFO: namespace pod-network-test-1108 deletion completed in 21.225583825s

• [SLOW TEST:51.567 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:18:22.431: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-7825/secret-test-131f3271-9c4b-4b7f-806a-fac478fffba5
STEP: Creating a pod to test consume secrets
Mar 23 16:18:23.060: INFO: Waiting up to 5m0s for pod "pod-configmaps-163f9b11-ccbc-45e1-93ce-48202678b2d5" in namespace "secrets-7825" to be "success or failure"
Mar 23 16:18:23.087: INFO: Pod "pod-configmaps-163f9b11-ccbc-45e1-93ce-48202678b2d5": Phase="Pending", Reason="", readiness=false. Elapsed: 27.677603ms
Mar 23 16:18:25.116: INFO: Pod "pod-configmaps-163f9b11-ccbc-45e1-93ce-48202678b2d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056539166s
Mar 23 16:18:27.144: INFO: Pod "pod-configmaps-163f9b11-ccbc-45e1-93ce-48202678b2d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.084264089s
STEP: Saw pod success
Mar 23 16:18:27.144: INFO: Pod "pod-configmaps-163f9b11-ccbc-45e1-93ce-48202678b2d5" satisfied condition "success or failure"
Mar 23 16:18:27.172: INFO: Trying to get logs from node 10.241.69.181 pod pod-configmaps-163f9b11-ccbc-45e1-93ce-48202678b2d5 container env-test: <nil>
STEP: delete the pod
Mar 23 16:18:27.428: INFO: Waiting for pod pod-configmaps-163f9b11-ccbc-45e1-93ce-48202678b2d5 to disappear
Mar 23 16:18:27.466: INFO: Pod pod-configmaps-163f9b11-ccbc-45e1-93ce-48202678b2d5 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:18:27.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7825" for this suite.
Mar 23 16:18:39.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:18:42.524: INFO: namespace secrets-7825 deletion completed in 15.005797256s

• [SLOW TEST:20.093 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:18:42.525: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar 23 16:18:43.187: INFO: Waiting up to 5m0s for pod "downward-api-bb9113df-7288-4122-a5e2-5fd9b8bd19f9" in namespace "downward-api-6086" to be "success or failure"
Mar 23 16:18:43.226: INFO: Pod "downward-api-bb9113df-7288-4122-a5e2-5fd9b8bd19f9": Phase="Pending", Reason="", readiness=false. Elapsed: 38.659292ms
Mar 23 16:18:45.265: INFO: Pod "downward-api-bb9113df-7288-4122-a5e2-5fd9b8bd19f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.07857876s
STEP: Saw pod success
Mar 23 16:18:45.266: INFO: Pod "downward-api-bb9113df-7288-4122-a5e2-5fd9b8bd19f9" satisfied condition "success or failure"
Mar 23 16:18:45.485: INFO: Trying to get logs from node 10.241.69.181 pod downward-api-bb9113df-7288-4122-a5e2-5fd9b8bd19f9 container dapi-container: <nil>
STEP: delete the pod
Mar 23 16:18:45.713: INFO: Waiting for pod downward-api-bb9113df-7288-4122-a5e2-5fd9b8bd19f9 to disappear
Mar 23 16:18:45.756: INFO: Pod downward-api-bb9113df-7288-4122-a5e2-5fd9b8bd19f9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:18:45.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6086" for this suite.
Mar 23 16:18:57.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:19:01.195: INFO: namespace downward-api-6086 deletion completed in 15.383674457s

• [SLOW TEST:18.670 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:19:01.197: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 16:19:02.011: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3305e985-4a05-4810-8841-e3f3a7339106" in namespace "projected-555" to be "success or failure"
Mar 23 16:19:02.050: INFO: Pod "downwardapi-volume-3305e985-4a05-4810-8841-e3f3a7339106": Phase="Pending", Reason="", readiness=false. Elapsed: 39.327726ms
Mar 23 16:19:04.077: INFO: Pod "downwardapi-volume-3305e985-4a05-4810-8841-e3f3a7339106": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0657984s
Mar 23 16:19:06.183: INFO: Pod "downwardapi-volume-3305e985-4a05-4810-8841-e3f3a7339106": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.171696747s
STEP: Saw pod success
Mar 23 16:19:06.183: INFO: Pod "downwardapi-volume-3305e985-4a05-4810-8841-e3f3a7339106" satisfied condition "success or failure"
Mar 23 16:19:06.247: INFO: Trying to get logs from node 10.241.69.181 pod downwardapi-volume-3305e985-4a05-4810-8841-e3f3a7339106 container client-container: <nil>
STEP: delete the pod
Mar 23 16:19:06.453: INFO: Waiting for pod downwardapi-volume-3305e985-4a05-4810-8841-e3f3a7339106 to disappear
Mar 23 16:19:06.505: INFO: Pod downwardapi-volume-3305e985-4a05-4810-8841-e3f3a7339106 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:19:06.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-555" for this suite.
Mar 23 16:19:18.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:19:21.599: INFO: namespace projected-555 deletion completed in 15.013209442s

• [SLOW TEST:20.402 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:19:21.599: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-1a44f6b9-fb7a-4ab2-9f74-ac36beca08cc
STEP: Creating a pod to test consume configMaps
Mar 23 16:19:22.229: INFO: Waiting up to 5m0s for pod "pod-configmaps-f9977e3d-3fdb-4d21-929c-08549185354c" in namespace "configmap-895" to be "success or failure"
Mar 23 16:19:22.294: INFO: Pod "pod-configmaps-f9977e3d-3fdb-4d21-929c-08549185354c": Phase="Pending", Reason="", readiness=false. Elapsed: 65.033818ms
Mar 23 16:19:24.336: INFO: Pod "pod-configmaps-f9977e3d-3fdb-4d21-929c-08549185354c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.107346723s
Mar 23 16:19:26.365: INFO: Pod "pod-configmaps-f9977e3d-3fdb-4d21-929c-08549185354c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.13650497s
STEP: Saw pod success
Mar 23 16:19:26.365: INFO: Pod "pod-configmaps-f9977e3d-3fdb-4d21-929c-08549185354c" satisfied condition "success or failure"
Mar 23 16:19:26.390: INFO: Trying to get logs from node 10.241.69.181 pod pod-configmaps-f9977e3d-3fdb-4d21-929c-08549185354c container configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 16:19:26.572: INFO: Waiting for pod pod-configmaps-f9977e3d-3fdb-4d21-929c-08549185354c to disappear
Mar 23 16:19:26.601: INFO: Pod pod-configmaps-f9977e3d-3fdb-4d21-929c-08549185354c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:19:26.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-895" for this suite.
Mar 23 16:19:38.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:19:41.913: INFO: namespace configmap-895 deletion completed in 15.238165921s

• [SLOW TEST:20.314 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:19:41.913: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 23 16:19:47.550: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2464e6cc-6fb6-4e2f-91bc-f09f28dc2db1"
Mar 23 16:19:47.551: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2464e6cc-6fb6-4e2f-91bc-f09f28dc2db1" in namespace "pods-2591" to be "terminated due to deadline exceeded"
Mar 23 16:19:47.572: INFO: Pod "pod-update-activedeadlineseconds-2464e6cc-6fb6-4e2f-91bc-f09f28dc2db1": Phase="Running", Reason="", readiness=true. Elapsed: 21.45662ms
Mar 23 16:19:49.809: INFO: Pod "pod-update-activedeadlineseconds-2464e6cc-6fb6-4e2f-91bc-f09f28dc2db1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.258631877s
Mar 23 16:19:49.809: INFO: Pod "pod-update-activedeadlineseconds-2464e6cc-6fb6-4e2f-91bc-f09f28dc2db1" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:19:49.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2591" for this suite.
Mar 23 16:20:02.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:20:05.918: INFO: namespace pods-2591 deletion completed in 16.0513086s

• [SLOW TEST:24.004 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:20:05.923: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-165
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-165
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-165
Mar 23 16:20:07.308: INFO: Found 0 stateful pods, waiting for 1
Mar 23 16:20:17.331: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 23 16:20:17.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 16:20:18.189: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 16:20:18.189: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 16:20:18.189: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 16:20:18.212: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 23 16:20:28.260: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 16:20:28.260: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 16:20:28.342: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar 23 16:20:28.342: INFO: ss-0  10.241.69.181  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:07 +0000 UTC  }]
Mar 23 16:20:28.342: INFO: 
Mar 23 16:20:28.342: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 23 16:20:29.394: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.976317865s
Mar 23 16:20:30.440: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.924513765s
Mar 23 16:20:31.481: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.878316798s
Mar 23 16:20:32.570: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.836889236s
Mar 23 16:20:33.853: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.747917481s
Mar 23 16:20:35.053: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.465286198s
Mar 23 16:20:36.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.265091107s
Mar 23 16:20:37.129: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.230416797s
Mar 23 16:20:38.156: INFO: Verifying statefulset ss doesn't scale past 3 for another 189.385285ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-165
Mar 23 16:20:39.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:20:39.566: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 16:20:39.566: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 16:20:39.566: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 16:20:39.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:20:40.537: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 23 16:20:40.537: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 16:20:40.537: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 16:20:40.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:20:41.031: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 23 16:20:41.031: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 16:20:41.031: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 16:20:41.071: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 16:20:41.072: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 16:20:41.072: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Mar 23 16:20:41.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 16:20:41.512: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 16:20:41.512: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 16:20:41.512: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 16:20:41.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 16:20:41.984: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 16:20:41.984: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 16:20:41.984: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 16:20:41.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 16:20:42.474: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 16:20:42.474: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 16:20:42.474: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 16:20:42.474: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 16:20:42.492: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 23 16:20:52.582: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 16:20:52.582: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 16:20:52.582: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 16:20:52.836: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar 23 16:20:52.836: INFO: ss-0  10.241.69.181  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:07 +0000 UTC  }]
Mar 23 16:20:52.836: INFO: ss-1  10.241.69.181  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  }]
Mar 23 16:20:52.836: INFO: ss-2  10.241.69.172  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  }]
Mar 23 16:20:52.836: INFO: 
Mar 23 16:20:52.836: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 23 16:20:53.868: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar 23 16:20:53.868: INFO: ss-0  10.241.69.181  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:07 +0000 UTC  }]
Mar 23 16:20:53.868: INFO: ss-1  10.241.69.181  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  }]
Mar 23 16:20:53.868: INFO: ss-2  10.241.69.172  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  }]
Mar 23 16:20:53.868: INFO: 
Mar 23 16:20:53.868: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 23 16:20:54.901: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar 23 16:20:54.901: INFO: ss-0  10.241.69.181  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:07 +0000 UTC  }]
Mar 23 16:20:54.901: INFO: ss-1  10.241.69.181  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  }]
Mar 23 16:20:54.901: INFO: ss-2  10.241.69.172  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  }]
Mar 23 16:20:54.901: INFO: 
Mar 23 16:20:54.901: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 23 16:20:55.927: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar 23 16:20:55.927: INFO: ss-0  10.241.69.181  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:07 +0000 UTC  }]
Mar 23 16:20:55.927: INFO: ss-1  10.241.69.181  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  }]
Mar 23 16:20:55.927: INFO: ss-2  10.241.69.172  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  }]
Mar 23 16:20:55.927: INFO: 
Mar 23 16:20:55.927: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 23 16:20:57.030: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar 23 16:20:57.030: INFO: ss-0  10.241.69.181  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:07 +0000 UTC  }]
Mar 23 16:20:57.030: INFO: ss-1  10.241.69.181  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  }]
Mar 23 16:20:57.030: INFO: ss-2  10.241.69.172  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  }]
Mar 23 16:20:57.030: INFO: 
Mar 23 16:20:57.030: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 23 16:20:58.061: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar 23 16:20:58.061: INFO: ss-2  10.241.69.172  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  }]
Mar 23 16:20:58.061: INFO: 
Mar 23 16:20:58.061: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 23 16:20:59.084: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar 23 16:20:59.084: INFO: ss-2  10.241.69.172  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  }]
Mar 23 16:20:59.084: INFO: 
Mar 23 16:20:59.084: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 23 16:21:00.126: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar 23 16:21:00.126: INFO: ss-2  10.241.69.172  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  }]
Mar 23 16:21:00.126: INFO: 
Mar 23 16:21:00.126: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 23 16:21:01.168: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar 23 16:21:01.168: INFO: ss-2  10.241.69.172  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  }]
Mar 23 16:21:01.168: INFO: 
Mar 23 16:21:01.168: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 23 16:21:02.205: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Mar 23 16:21:02.205: INFO: ss-2  10.241.69.172  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-03-23 16:20:28 +0000 UTC  }]
Mar 23 16:21:02.205: INFO: 
Mar 23 16:21:02.205: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-165
Mar 23 16:21:03.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:21:03.596: INFO: rc: 1
Mar 23 16:21:03.596: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc003f85b00 exit status 1 <nil> <nil> true [0xc0029a4370 0xc0029a4388 0xc0029a43c0] [0xc0029a4370 0xc0029a4388 0xc0029a43c0] [0xc0029a4380 0xc0029a43b0] [0x10efce0 0x10efce0] 0xc00b6bb2c0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Mar 23 16:21:13.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:21:13.752: INFO: rc: 1
Mar 23 16:21:13.753: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f2e270 exit status 1 <nil> <nil> true [0xc002a9e0d8 0xc002a9e0f0 0xc002a9e108] [0xc002a9e0d8 0xc002a9e0f0 0xc002a9e108] [0xc002a9e0e8 0xc002a9e100] [0x10efce0 0x10efce0] 0xc0026bade0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:21:23.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:21:24.043: INFO: rc: 1
Mar 23 16:21:24.043: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f2e690 exit status 1 <nil> <nil> true [0xc002a9e110 0xc002a9e128 0xc002a9e140] [0xc002a9e110 0xc002a9e128 0xc002a9e140] [0xc002a9e120 0xc002a9e138] [0x10efce0 0x10efce0] 0xc0026bb2c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:21:34.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:21:34.210: INFO: rc: 1
Mar 23 16:21:34.210: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f2e9c0 exit status 1 <nil> <nil> true [0xc002a9e148 0xc002a9e160 0xc002a9e178] [0xc002a9e148 0xc002a9e160 0xc002a9e178] [0xc002a9e158 0xc002a9e170] [0x10efce0 0x10efce0] 0xc0026bb740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:21:44.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:21:44.407: INFO: rc: 1
Mar 23 16:21:44.407: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f2ef00 exit status 1 <nil> <nil> true [0xc002a9e180 0xc002a9e198 0xc002a9e1b0] [0xc002a9e180 0xc002a9e198 0xc002a9e1b0] [0xc002a9e190 0xc002a9e1a8] [0x10efce0 0x10efce0] 0xc0026bbaa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:21:54.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:21:54.701: INFO: rc: 1
Mar 23 16:21:54.701: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f2f3b0 exit status 1 <nil> <nil> true [0xc002a9e1b8 0xc002a9e1d0 0xc002a9e1e8] [0xc002a9e1b8 0xc002a9e1d0 0xc002a9e1e8] [0xc002a9e1c8 0xc002a9e1e0] [0x10efce0 0x10efce0] 0xc0026bbf80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:22:04.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:22:04.928: INFO: rc: 1
Mar 23 16:22:04.928: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003a65740 exit status 1 <nil> <nil> true [0xc001ce8258 0xc001ce82a0 0xc001ce82d0] [0xc001ce8258 0xc001ce82a0 0xc001ce82d0] [0xc001ce8290 0xc001ce82c0] [0x10efce0 0x10efce0] 0xc002053500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:22:14.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:22:15.108: INFO: rc: 1
Mar 23 16:22:15.109: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002f2f7d0 exit status 1 <nil> <nil> true [0xc002a9e1f0 0xc002a9e208 0xc002a9e220] [0xc002a9e1f0 0xc002a9e208 0xc002a9e220] [0xc002a9e200 0xc002a9e218] [0x10efce0 0x10efce0] 0xc002ee6300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:22:25.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:22:25.313: INFO: rc: 1
Mar 23 16:22:25.313: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003f85e60 exit status 1 <nil> <nil> true [0xc0029a43c8 0xc0029a43e0 0xc0029a4408] [0xc0029a43c8 0xc0029a43e0 0xc0029a4408] [0xc0029a43d8 0xc0029a43f8] [0x10efce0 0x10efce0] 0xc00b6bb620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:22:35.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:22:35.479: INFO: rc: 1
Mar 23 16:22:35.479: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0034f21b0 exit status 1 <nil> <nil> true [0xc0029a4418 0xc0029a4430 0xc0029a4448] [0xc0029a4418 0xc0029a4430 0xc0029a4448] [0xc0029a4428 0xc0029a4440] [0x10efce0 0x10efce0] 0xc00b6bbb00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:22:45.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:22:45.862: INFO: rc: 1
Mar 23 16:22:45.862: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00758c300 exit status 1 <nil> <nil> true [0xc0029a4020 0xc0029a4058 0xc0029a4080] [0xc0029a4020 0xc0029a4058 0xc0029a4080] [0xc0029a4050 0xc0029a4068] [0x10efce0 0x10efce0] 0xc0026ba2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:22:55.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:22:56.068: INFO: rc: 1
Mar 23 16:22:56.069: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003f84330 exit status 1 <nil> <nil> true [0xc001ce8000 0xc001ce8018 0xc001ce8030] [0xc001ce8000 0xc001ce8018 0xc001ce8030] [0xc001ce8010 0xc001ce8028] [0x10efce0 0x10efce0] 0xc00b6ba2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:23:06.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:23:06.448: INFO: rc: 1
Mar 23 16:23:06.448: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00abfe330 exit status 1 <nil> <nil> true [0xc002a9e000 0xc002a9e018 0xc002a9e030] [0xc002a9e000 0xc002a9e018 0xc002a9e030] [0xc002a9e010 0xc002a9e028] [0x10efce0 0x10efce0] 0xc002052300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:23:16.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:23:16.665: INFO: rc: 1
Mar 23 16:23:16.666: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003f846c0 exit status 1 <nil> <nil> true [0xc001ce8038 0xc001ce8050 0xc001ce8068] [0xc001ce8038 0xc001ce8050 0xc001ce8068] [0xc001ce8048 0xc001ce8060] [0x10efce0 0x10efce0] 0xc00b6ba660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:23:26.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:23:26.829: INFO: rc: 1
Mar 23 16:23:26.829: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00abfe690 exit status 1 <nil> <nil> true [0xc002a9e038 0xc002a9e050 0xc002a9e068] [0xc002a9e038 0xc002a9e050 0xc002a9e068] [0xc002a9e048 0xc002a9e060] [0x10efce0 0x10efce0] 0xc0020526c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:23:36.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:23:37.027: INFO: rc: 1
Mar 23 16:23:37.028: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003f84a20 exit status 1 <nil> <nil> true [0xc001ce8070 0xc001ce8088 0xc001ce80a0] [0xc001ce8070 0xc001ce8088 0xc001ce80a0] [0xc001ce8080 0xc001ce8098] [0x10efce0 0x10efce0] 0xc00b6baa20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:23:47.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:23:47.177: INFO: rc: 1
Mar 23 16:23:47.177: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00abfe9f0 exit status 1 <nil> <nil> true [0xc002a9e070 0xc002a9e088 0xc002a9e0a0] [0xc002a9e070 0xc002a9e088 0xc002a9e0a0] [0xc002a9e080 0xc002a9e098] [0x10efce0 0x10efce0] 0xc002052a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:23:57.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:23:57.456: INFO: rc: 1
Mar 23 16:23:57.456: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00758c720 exit status 1 <nil> <nil> true [0xc0029a4090 0xc0029a40e0 0xc0029a4190] [0xc0029a4090 0xc0029a40e0 0xc0029a4190] [0xc0029a40d0 0xc0029a4168] [0x10efce0 0x10efce0] 0xc0026ba660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:24:07.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:24:07.627: INFO: rc: 1
Mar 23 16:24:07.627: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00abfee10 exit status 1 <nil> <nil> true [0xc002a9e0a8 0xc002a9e0c0 0xc002a9e0d8] [0xc002a9e0a8 0xc002a9e0c0 0xc002a9e0d8] [0xc002a9e0b8 0xc002a9e0d0] [0x10efce0 0x10efce0] 0xc002052de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:24:17.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:24:17.793: INFO: rc: 1
Mar 23 16:24:17.793: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00758cab0 exit status 1 <nil> <nil> true [0xc0029a41a0 0xc0029a41d0 0xc0029a41e8] [0xc0029a41a0 0xc0029a41d0 0xc0029a41e8] [0xc0029a41c0 0xc0029a41e0] [0x10efce0 0x10efce0] 0xc0026ba9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:24:27.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:24:28.007: INFO: rc: 1
Mar 23 16:24:28.007: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003f84db0 exit status 1 <nil> <nil> true [0xc001ce80a8 0xc001ce80c0 0xc001ce80e0] [0xc001ce80a8 0xc001ce80c0 0xc001ce80e0] [0xc001ce80b8 0xc001ce80d8] [0x10efce0 0x10efce0] 0xc00b6bade0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:24:38.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:24:38.208: INFO: rc: 1
Mar 23 16:24:38.208: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00abff200 exit status 1 <nil> <nil> true [0xc002a9e0e0 0xc002a9e0f8 0xc002a9e110] [0xc002a9e0e0 0xc002a9e0f8 0xc002a9e110] [0xc002a9e0f0 0xc002a9e108] [0x10efce0 0x10efce0] 0xc0020532c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:24:48.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:24:48.402: INFO: rc: 1
Mar 23 16:24:48.402: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00abfe300 exit status 1 <nil> <nil> true [0xc002a9e008 0xc002a9e020 0xc002a9e038] [0xc002a9e008 0xc002a9e020 0xc002a9e038] [0xc002a9e018 0xc002a9e030] [0x10efce0 0x10efce0] 0xc002052300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:24:58.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:24:58.576: INFO: rc: 1
Mar 23 16:24:58.576: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00758c330 exit status 1 <nil> <nil> true [0xc0029a4000 0xc0029a4050 0xc0029a4068] [0xc0029a4000 0xc0029a4050 0xc0029a4068] [0xc0029a4048 0xc0029a4060] [0x10efce0 0x10efce0] 0xc0026ba2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:25:08.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:25:08.777: INFO: rc: 1
Mar 23 16:25:08.777: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00abfe6c0 exit status 1 <nil> <nil> true [0xc002a9e040 0xc002a9e058 0xc002a9e070] [0xc002a9e040 0xc002a9e058 0xc002a9e070] [0xc002a9e050 0xc002a9e068] [0x10efce0 0x10efce0] 0xc0020526c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:25:18.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:25:18.943: INFO: rc: 1
Mar 23 16:25:18.943: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003f84360 exit status 1 <nil> <nil> true [0xc001ce8000 0xc001ce8018 0xc001ce8030] [0xc001ce8000 0xc001ce8018 0xc001ce8030] [0xc001ce8010 0xc001ce8028] [0x10efce0 0x10efce0] 0xc00b6ba2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:25:28.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:25:29.225: INFO: rc: 1
Mar 23 16:25:29.225: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00758c6f0 exit status 1 <nil> <nil> true [0xc0029a4080 0xc0029a40d0 0xc0029a4168] [0xc0029a4080 0xc0029a40d0 0xc0029a4168] [0xc0029a40a8 0xc0029a4128] [0x10efce0 0x10efce0] 0xc0026ba660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:25:39.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:25:39.461: INFO: rc: 1
Mar 23 16:25:39.461: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00758cae0 exit status 1 <nil> <nil> true [0xc0029a4190 0xc0029a41c0 0xc0029a41e0] [0xc0029a4190 0xc0029a41c0 0xc0029a41e0] [0xc0029a41b0 0xc0029a41d8] [0x10efce0 0x10efce0] 0xc0026ba9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:25:49.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:25:49.623: INFO: rc: 1
Mar 23 16:25:49.623: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00276e330 exit status 1 <nil> <nil> true [0xc002bf0010 0xc002bf0078 0xc002bf00f8] [0xc002bf0010 0xc002bf0078 0xc002bf00f8] [0xc002bf0068 0xc002bf00b0] [0x10efce0 0x10efce0] 0xc002ee62a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:25:59.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:25:59.820: INFO: rc: 1
Mar 23 16:25:59.820: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00758ce70 exit status 1 <nil> <nil> true [0xc0029a41e8 0xc0029a4220 0xc0029a4238] [0xc0029a41e8 0xc0029a4220 0xc0029a4238] [0xc0029a4210 0xc0029a4230] [0x10efce0 0x10efce0] 0xc0026bafc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Mar 23 16:26:09.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-165 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:26:10.048: INFO: rc: 1
Mar 23 16:26:10.048: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
Mar 23 16:26:10.048: INFO: Scaling statefulset ss to 0
Mar 23 16:26:10.142: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 23 16:26:10.159: INFO: Deleting all statefulset in ns statefulset-165
Mar 23 16:26:10.179: INFO: Scaling statefulset ss to 0
Mar 23 16:26:10.240: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 16:26:10.257: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:26:10.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-165" for this suite.
Mar 23 16:26:24.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:26:27.435: INFO: namespace statefulset-165 deletion completed in 16.956993432s

• [SLOW TEST:381.512 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:26:27.435: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:27:28.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-153" for this suite.
Mar 23 16:27:52.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:27:56.053: INFO: namespace container-probe-153 deletion completed in 27.88113401s

• [SLOW TEST:88.618 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:27:56.054: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0323 16:27:58.033687      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 23 16:27:58.033: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:27:58.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8301" for this suite.
Mar 23 16:28:10.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:28:13.489: INFO: namespace gc-8301 deletion completed in 15.34606403s

• [SLOW TEST:17.436 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:28:13.491: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Mar 23 16:28:14.733: INFO: Waiting up to 5m0s for pod "pod-542e4cc8-cb63-4ec6-b378-e268cb3b3073" in namespace "emptydir-9919" to be "success or failure"
Mar 23 16:28:14.892: INFO: Pod "pod-542e4cc8-cb63-4ec6-b378-e268cb3b3073": Phase="Pending", Reason="", readiness=false. Elapsed: 159.063601ms
Mar 23 16:28:16.946: INFO: Pod "pod-542e4cc8-cb63-4ec6-b378-e268cb3b3073": Phase="Pending", Reason="", readiness=false. Elapsed: 2.21319259s
Mar 23 16:28:18.972: INFO: Pod "pod-542e4cc8-cb63-4ec6-b378-e268cb3b3073": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.238912909s
STEP: Saw pod success
Mar 23 16:28:18.972: INFO: Pod "pod-542e4cc8-cb63-4ec6-b378-e268cb3b3073" satisfied condition "success or failure"
Mar 23 16:28:19.002: INFO: Trying to get logs from node 10.241.69.181 pod pod-542e4cc8-cb63-4ec6-b378-e268cb3b3073 container test-container: <nil>
STEP: delete the pod
Mar 23 16:28:19.448: INFO: Waiting for pod pod-542e4cc8-cb63-4ec6-b378-e268cb3b3073 to disappear
Mar 23 16:28:19.471: INFO: Pod pod-542e4cc8-cb63-4ec6-b378-e268cb3b3073 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:28:19.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9919" for this suite.
Mar 23 16:28:31.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:28:34.774: INFO: namespace emptydir-9919 deletion completed in 15.252862135s

• [SLOW TEST:21.283 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:28:34.775: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 16:28:35.329: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-7daea706-df68-4bd7-9ddb-f0bf4231bcf7
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-7daea706-df68-4bd7-9ddb-f0bf4231bcf7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:29:58.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1556" for this suite.
Mar 23 16:30:25.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:30:28.350: INFO: namespace projected-1556 deletion completed in 29.482084612s

• [SLOW TEST:113.576 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:30:28.351: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4371
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4371
STEP: creating replication controller externalsvc in namespace services-4371
I0323 16:30:28.996073      22 runners.go:184] Created replication controller with name: externalsvc, namespace: services-4371, replica count: 2
I0323 16:30:32.046610      22 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Mar 23 16:30:32.145: INFO: Creating new exec pod
Mar 23 16:30:36.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=services-4371 execpodfjghw -- /bin/sh -x -c nslookup nodeport-service'
Mar 23 16:30:37.178: INFO: stderr: "+ nslookup nodeport-service\n"
Mar 23 16:30:37.178: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-4371.svc.cluster.local\tcanonical name = externalsvc.services-4371.svc.cluster.local.\nName:\texternalsvc.services-4371.svc.cluster.local\nAddress: 172.21.98.60\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4371, will wait for the garbage collector to delete the pods
Mar 23 16:30:37.410: INFO: Deleting ReplicationController externalsvc took: 149.896651ms
Mar 23 16:30:38.410: INFO: Terminating ReplicationController externalsvc pods took: 1.000255745s
Mar 23 16:30:47.893: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:30:47.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4371" for this suite.
Mar 23 16:31:04.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:31:07.496: INFO: namespace services-4371 deletion completed in 19.496233835s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:39.145 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:31:07.496: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 16:31:08.476: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-7bcda737-1a40-4482-a332-18178e191e6d
STEP: Creating configMap with name cm-test-opt-upd-f1ea49b1-bb0e-4c57-8f90-54dd5284c60a
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7bcda737-1a40-4482-a332-18178e191e6d
STEP: Updating configmap cm-test-opt-upd-f1ea49b1-bb0e-4c57-8f90-54dd5284c60a
STEP: Creating configMap with name cm-test-opt-create-a017bb53-629b-42d9-865e-51a38523c33a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:32:41.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-457" for this suite.
Mar 23 16:33:05.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:33:08.804: INFO: namespace configmap-457 deletion completed in 27.373590739s

• [SLOW TEST:121.308 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:33:08.807: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-e226cdc4-028a-4f8b-90e1-0e55d49d0c85
STEP: Creating a pod to test consume configMaps
Mar 23 16:33:09.684: INFO: Waiting up to 5m0s for pod "pod-configmaps-f745fa56-4049-4658-8be2-5b25f12e34c8" in namespace "configmap-4" to be "success or failure"
Mar 23 16:33:09.742: INFO: Pod "pod-configmaps-f745fa56-4049-4658-8be2-5b25f12e34c8": Phase="Pending", Reason="", readiness=false. Elapsed: 58.254878ms
Mar 23 16:33:11.798: INFO: Pod "pod-configmaps-f745fa56-4049-4658-8be2-5b25f12e34c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.114215207s
Mar 23 16:33:13.829: INFO: Pod "pod-configmaps-f745fa56-4049-4658-8be2-5b25f12e34c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.145146307s
STEP: Saw pod success
Mar 23 16:33:13.829: INFO: Pod "pod-configmaps-f745fa56-4049-4658-8be2-5b25f12e34c8" satisfied condition "success or failure"
Mar 23 16:33:13.864: INFO: Trying to get logs from node 10.241.69.181 pod pod-configmaps-f745fa56-4049-4658-8be2-5b25f12e34c8 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 16:33:14.078: INFO: Waiting for pod pod-configmaps-f745fa56-4049-4658-8be2-5b25f12e34c8 to disappear
Mar 23 16:33:14.112: INFO: Pod pod-configmaps-f745fa56-4049-4658-8be2-5b25f12e34c8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:33:14.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4" for this suite.
Mar 23 16:33:28.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:33:31.242: INFO: namespace configmap-4 deletion completed in 17.067760957s

• [SLOW TEST:22.435 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:33:31.243: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Mar 23 16:33:32.479: INFO: Waiting up to 5m0s for pod "var-expansion-c7a6103e-0a66-47a0-b5ec-59c78509b150" in namespace "var-expansion-8921" to be "success or failure"
Mar 23 16:33:32.514: INFO: Pod "var-expansion-c7a6103e-0a66-47a0-b5ec-59c78509b150": Phase="Pending", Reason="", readiness=false. Elapsed: 35.197177ms
Mar 23 16:33:34.542: INFO: Pod "var-expansion-c7a6103e-0a66-47a0-b5ec-59c78509b150": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.063178049s
STEP: Saw pod success
Mar 23 16:33:34.542: INFO: Pod "var-expansion-c7a6103e-0a66-47a0-b5ec-59c78509b150" satisfied condition "success or failure"
Mar 23 16:33:34.570: INFO: Trying to get logs from node 10.241.69.181 pod var-expansion-c7a6103e-0a66-47a0-b5ec-59c78509b150 container dapi-container: <nil>
STEP: delete the pod
Mar 23 16:33:34.755: INFO: Waiting for pod var-expansion-c7a6103e-0a66-47a0-b5ec-59c78509b150 to disappear
Mar 23 16:33:35.005: INFO: Pod var-expansion-c7a6103e-0a66-47a0-b5ec-59c78509b150 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:33:35.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8921" for this suite.
Mar 23 16:33:49.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:33:52.309: INFO: namespace var-expansion-8921 deletion completed in 17.189382316s

• [SLOW TEST:21.067 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:33:52.310: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar 23 16:33:52.944: INFO: Waiting up to 5m0s for pod "downward-api-45fa79a0-29e9-4e4e-b5e4-8c5b81b1453c" in namespace "downward-api-6882" to be "success or failure"
Mar 23 16:33:52.978: INFO: Pod "downward-api-45fa79a0-29e9-4e4e-b5e4-8c5b81b1453c": Phase="Pending", Reason="", readiness=false. Elapsed: 34.401708ms
Mar 23 16:33:55.007: INFO: Pod "downward-api-45fa79a0-29e9-4e4e-b5e4-8c5b81b1453c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063120412s
Mar 23 16:33:57.038: INFO: Pod "downward-api-45fa79a0-29e9-4e4e-b5e4-8c5b81b1453c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.094438962s
STEP: Saw pod success
Mar 23 16:33:57.038: INFO: Pod "downward-api-45fa79a0-29e9-4e4e-b5e4-8c5b81b1453c" satisfied condition "success or failure"
Mar 23 16:33:57.092: INFO: Trying to get logs from node 10.241.69.181 pod downward-api-45fa79a0-29e9-4e4e-b5e4-8c5b81b1453c container dapi-container: <nil>
STEP: delete the pod
Mar 23 16:33:57.347: INFO: Waiting for pod downward-api-45fa79a0-29e9-4e4e-b5e4-8c5b81b1453c to disappear
Mar 23 16:33:57.482: INFO: Pod downward-api-45fa79a0-29e9-4e4e-b5e4-8c5b81b1453c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:33:57.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6882" for this suite.
Mar 23 16:34:09.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:34:12.786: INFO: namespace downward-api-6882 deletion completed in 15.226172921s

• [SLOW TEST:20.477 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:34:12.788: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Mar 23 16:34:18.091: INFO: Successfully updated pod "pod-update-89d59385-18d8-4618-bbfd-3728f451b2be"
STEP: verifying the updated pod is in kubernetes
Mar 23 16:34:18.138: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:34:18.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1023" for this suite.
Mar 23 16:34:42.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:34:44.809: INFO: namespace pods-1023 deletion completed in 26.627917997s

• [SLOW TEST:32.021 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:34:44.810: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-46e304db-17dd-4d40-b995-d6fdf0d4d3aa
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:34:45.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-726" for this suite.
Mar 23 16:34:57.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:35:00.059: INFO: namespace configmap-726 deletion completed in 14.791818592s

• [SLOW TEST:15.250 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:35:00.059: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 23 16:35:00.771: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2946 /api/v1/namespaces/watch-2946/configmaps/e2e-watch-test-configmap-a ee70f26f-64fa-43aa-a402-658473f8527d 91346 0 2020-03-23 16:35:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 23 16:35:00.771: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2946 /api/v1/namespaces/watch-2946/configmaps/e2e-watch-test-configmap-a ee70f26f-64fa-43aa-a402-658473f8527d 91346 0 2020-03-23 16:35:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Mar 23 16:35:10.846: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2946 /api/v1/namespaces/watch-2946/configmaps/e2e-watch-test-configmap-a ee70f26f-64fa-43aa-a402-658473f8527d 91393 0 2020-03-23 16:35:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 23 16:35:10.846: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2946 /api/v1/namespaces/watch-2946/configmaps/e2e-watch-test-configmap-a ee70f26f-64fa-43aa-a402-658473f8527d 91393 0 2020-03-23 16:35:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 23 16:35:20.906: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2946 /api/v1/namespaces/watch-2946/configmaps/e2e-watch-test-configmap-a ee70f26f-64fa-43aa-a402-658473f8527d 91434 0 2020-03-23 16:35:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 23 16:35:20.906: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2946 /api/v1/namespaces/watch-2946/configmaps/e2e-watch-test-configmap-a ee70f26f-64fa-43aa-a402-658473f8527d 91434 0 2020-03-23 16:35:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Mar 23 16:35:30.995: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2946 /api/v1/namespaces/watch-2946/configmaps/e2e-watch-test-configmap-a ee70f26f-64fa-43aa-a402-658473f8527d 91478 0 2020-03-23 16:35:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 23 16:35:30.995: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2946 /api/v1/namespaces/watch-2946/configmaps/e2e-watch-test-configmap-a ee70f26f-64fa-43aa-a402-658473f8527d 91478 0 2020-03-23 16:35:00 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 23 16:35:41.036: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2946 /api/v1/namespaces/watch-2946/configmaps/e2e-watch-test-configmap-b c6977be6-af7b-4d19-82a3-4311c438202a 91529 0 2020-03-23 16:35:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 23 16:35:41.036: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2946 /api/v1/namespaces/watch-2946/configmaps/e2e-watch-test-configmap-b c6977be6-af7b-4d19-82a3-4311c438202a 91529 0 2020-03-23 16:35:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Mar 23 16:35:51.127: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2946 /api/v1/namespaces/watch-2946/configmaps/e2e-watch-test-configmap-b c6977be6-af7b-4d19-82a3-4311c438202a 91579 0 2020-03-23 16:35:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 23 16:35:51.127: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2946 /api/v1/namespaces/watch-2946/configmaps/e2e-watch-test-configmap-b c6977be6-af7b-4d19-82a3-4311c438202a 91579 0 2020-03-23 16:35:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:36:01.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2946" for this suite.
Mar 23 16:36:13.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:36:16.033: INFO: namespace watch-2946 deletion completed in 14.722253003s

• [SLOW TEST:75.974 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:36:16.033: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0323 16:36:57.072799      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 23 16:36:57.073: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:36:57.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9053" for this suite.
Mar 23 16:37:13.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:37:15.976: INFO: namespace gc-9053 deletion completed in 18.864957502s

• [SLOW TEST:59.943 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:37:15.977: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Mar 23 16:37:16.437: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-012311840 proxy --unix-socket=/tmp/kubectl-proxy-unix650473519/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:37:16.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9475" for this suite.
Mar 23 16:37:28.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:37:31.813: INFO: namespace kubectl-9475 deletion completed in 15.239624541s

• [SLOW TEST:15.837 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:37:31.813: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 23 16:37:32.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-3568'
Mar 23 16:37:32.465: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 23 16:37:32.465: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Mar 23 16:37:34.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete deployment e2e-test-httpd-deployment --namespace=kubectl-3568'
Mar 23 16:37:35.062: INFO: stderr: ""
Mar 23 16:37:35.062: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:37:35.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3568" for this suite.
Mar 23 16:37:49.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:37:52.293: INFO: namespace kubectl-3568 deletion completed in 17.16068998s

• [SLOW TEST:20.480 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:37:52.294: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 16:37:54.037: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 16:37:56.206: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720578274, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720578274, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720578274, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720578274, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 16:37:59.263: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:37:59.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6657" for this suite.
Mar 23 16:38:13.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:38:16.756: INFO: namespace webhook-6657 deletion completed in 17.00684418s
STEP: Destroying namespace "webhook-6657-markers" for this suite.
Mar 23 16:38:28.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:38:31.488: INFO: namespace webhook-6657-markers deletion completed in 14.731848797s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:39.316 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:38:31.612: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-68d4718b-b460-4d2f-9d29-c9d9aca0bd4b
STEP: Creating a pod to test consume secrets
Mar 23 16:38:32.305: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-433cd573-4ace-4efc-9ea0-dd7f3a47b9ce" in namespace "projected-5146" to be "success or failure"
Mar 23 16:38:32.334: INFO: Pod "pod-projected-secrets-433cd573-4ace-4efc-9ea0-dd7f3a47b9ce": Phase="Pending", Reason="", readiness=false. Elapsed: 28.636363ms
Mar 23 16:38:34.362: INFO: Pod "pod-projected-secrets-433cd573-4ace-4efc-9ea0-dd7f3a47b9ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.056811071s
STEP: Saw pod success
Mar 23 16:38:34.362: INFO: Pod "pod-projected-secrets-433cd573-4ace-4efc-9ea0-dd7f3a47b9ce" satisfied condition "success or failure"
Mar 23 16:38:34.399: INFO: Trying to get logs from node 10.241.69.181 pod pod-projected-secrets-433cd573-4ace-4efc-9ea0-dd7f3a47b9ce container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 23 16:38:34.885: INFO: Waiting for pod pod-projected-secrets-433cd573-4ace-4efc-9ea0-dd7f3a47b9ce to disappear
Mar 23 16:38:34.927: INFO: Pod pod-projected-secrets-433cd573-4ace-4efc-9ea0-dd7f3a47b9ce no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:38:34.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5146" for this suite.
Mar 23 16:38:47.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:38:49.761: INFO: namespace projected-5146 deletion completed in 14.764214158s

• [SLOW TEST:18.149 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:38:49.761: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 16:38:50.228: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:38:52.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9429" for this suite.
Mar 23 16:39:43.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:39:45.599: INFO: namespace pods-9429 deletion completed in 52.681077534s

• [SLOW TEST:55.838 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:39:45.599: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:40:03.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4744" for this suite.
Mar 23 16:40:19.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:40:22.558: INFO: namespace resourcequota-4744 deletion completed in 19.186978555s

• [SLOW TEST:36.959 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:40:22.559: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Mar 23 16:40:23.068: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 16:40:33.003: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:41:09.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6460" for this suite.
Mar 23 16:41:21.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:41:24.698: INFO: namespace crd-publish-openapi-6460 deletion completed in 15.226920849s

• [SLOW TEST:62.139 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:41:24.698: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Mar 23 16:41:25.246: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 23 16:42:25.429: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 16:42:25.474: INFO: Starting informer...
STEP: Starting pod...
Mar 23 16:42:25.820: INFO: Pod is running on 10.241.69.181. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Mar 23 16:42:26.148: INFO: Pod wasn't evicted. Proceeding
Mar 23 16:42:26.148: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Mar 23 16:43:41.437: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:43:41.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-2575" for this suite.
Mar 23 16:44:05.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:44:08.614: INFO: namespace taint-single-pod-2575 deletion completed in 27.141383753s

• [SLOW TEST:163.916 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:44:08.616: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 16:44:09.133: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Mar 23 16:44:19.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-3666 create -f -'
Mar 23 16:44:20.436: INFO: stderr: ""
Mar 23 16:44:20.436: INFO: stdout: "e2e-test-crd-publish-openapi-3246-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 23 16:44:20.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-3666 delete e2e-test-crd-publish-openapi-3246-crds test-cr'
Mar 23 16:44:20.637: INFO: stderr: ""
Mar 23 16:44:20.637: INFO: stdout: "e2e-test-crd-publish-openapi-3246-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 23 16:44:20.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-3666 apply -f -'
Mar 23 16:44:21.595: INFO: stderr: ""
Mar 23 16:44:21.595: INFO: stdout: "e2e-test-crd-publish-openapi-3246-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 23 16:44:21.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-3666 delete e2e-test-crd-publish-openapi-3246-crds test-cr'
Mar 23 16:44:21.969: INFO: stderr: ""
Mar 23 16:44:21.969: INFO: stdout: "e2e-test-crd-publish-openapi-3246-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Mar 23 16:44:21.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 explain e2e-test-crd-publish-openapi-3246-crds'
Mar 23 16:44:22.302: INFO: stderr: ""
Mar 23 16:44:22.302: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3246-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:44:31.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3666" for this suite.
Mar 23 16:44:43.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:44:46.696: INFO: namespace crd-publish-openapi-3666 deletion completed in 15.039975272s

• [SLOW TEST:38.081 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:44:46.697: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-c3225c2a-a2a5-4edd-8886-2f9a8e47477b
STEP: Creating a pod to test consume configMaps
Mar 23 16:44:48.222: INFO: Waiting up to 5m0s for pod "pod-configmaps-c88a0a59-8e59-40db-b570-2d56bd6c621c" in namespace "configmap-2956" to be "success or failure"
Mar 23 16:44:48.258: INFO: Pod "pod-configmaps-c88a0a59-8e59-40db-b570-2d56bd6c621c": Phase="Pending", Reason="", readiness=false. Elapsed: 35.491266ms
Mar 23 16:44:50.290: INFO: Pod "pod-configmaps-c88a0a59-8e59-40db-b570-2d56bd6c621c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.067558755s
STEP: Saw pod success
Mar 23 16:44:50.290: INFO: Pod "pod-configmaps-c88a0a59-8e59-40db-b570-2d56bd6c621c" satisfied condition "success or failure"
Mar 23 16:44:50.322: INFO: Trying to get logs from node 10.241.69.181 pod pod-configmaps-c88a0a59-8e59-40db-b570-2d56bd6c621c container configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 16:44:50.583: INFO: Waiting for pod pod-configmaps-c88a0a59-8e59-40db-b570-2d56bd6c621c to disappear
Mar 23 16:44:50.619: INFO: Pod pod-configmaps-c88a0a59-8e59-40db-b570-2d56bd6c621c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:44:50.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2956" for this suite.
Mar 23 16:45:02.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:45:06.409: INFO: namespace configmap-2956 deletion completed in 15.731757783s

• [SLOW TEST:19.712 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:45:06.410: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:45:23.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5485" for this suite.
Mar 23 16:45:37.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:45:40.779: INFO: namespace resourcequota-5485 deletion completed in 17.100815054s

• [SLOW TEST:34.370 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:45:40.780: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 16:46:07.690: INFO: Container started at 2020-03-23 16:45:43 +0000 UTC, pod became ready at 2020-03-23 16:46:05 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:46:07.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9359" for this suite.
Mar 23 16:46:49.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:46:53.132: INFO: namespace container-probe-9359 deletion completed in 45.390344648s

• [SLOW TEST:72.352 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:46:53.132: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2003
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2003
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2003
Mar 23 16:46:53.779: INFO: Found 0 stateful pods, waiting for 1
Mar 23 16:47:03.810: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 23 16:47:03.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-2003 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 16:47:04.699: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 16:47:04.699: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 16:47:04.699: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 16:47:04.758: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 23 16:47:14.790: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 16:47:14.790: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 16:47:14.906: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998095s
Mar 23 16:47:15.933: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.956674221s
Mar 23 16:47:16.967: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.929613013s
Mar 23 16:47:18.005: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.895205885s
Mar 23 16:47:19.036: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.856964319s
Mar 23 16:47:20.072: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.825789842s
Mar 23 16:47:21.098: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.790763243s
Mar 23 16:47:22.126: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.763774648s
Mar 23 16:47:23.381: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.73651919s
Mar 23 16:47:24.405: INFO: Verifying statefulset ss doesn't scale past 1 for another 481.75137ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2003
Mar 23 16:47:25.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-2003 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:47:25.839: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 16:47:25.839: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 16:47:25.839: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 16:47:25.877: INFO: Found 1 stateful pods, waiting for 3
Mar 23 16:47:35.911: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 16:47:35.911: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 16:47:35.911: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Mar 23 16:47:35.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-2003 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 16:47:36.769: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 16:47:36.769: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 16:47:36.769: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 16:47:36.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-2003 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 16:47:37.181: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 16:47:37.181: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 16:47:37.181: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 16:47:37.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-2003 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 16:47:37.573: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 16:47:37.573: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 16:47:37.573: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 16:47:37.573: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 16:47:37.588: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 23 16:47:47.637: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 16:47:47.637: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 16:47:47.637: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 16:47:47.706: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998051s
Mar 23 16:47:48.736: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.969677243s
Mar 23 16:47:49.798: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.938960536s
Mar 23 16:47:50.838: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.877366855s
Mar 23 16:47:51.871: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.836808909s
Mar 23 16:47:52.900: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.803848001s
Mar 23 16:47:53.939: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.775179125s
Mar 23 16:47:55.036: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.736031569s
Mar 23 16:47:56.067: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.639038955s
Mar 23 16:47:57.095: INFO: Verifying statefulset ss doesn't scale past 3 for another 607.7236ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2003
Mar 23 16:47:58.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-2003 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:47:58.567: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 16:47:58.567: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 16:47:58.567: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 16:47:58.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-2003 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:47:58.985: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 16:47:58.985: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 16:47:58.985: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 16:47:58.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-2003 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 16:47:59.434: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 16:47:59.434: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 16:47:59.434: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 16:47:59.434: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 23 16:48:39.527: INFO: Deleting all statefulset in ns statefulset-2003
Mar 23 16:48:39.544: INFO: Scaling statefulset ss to 0
Mar 23 16:48:39.607: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 16:48:39.621: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:48:39.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2003" for this suite.
Mar 23 16:48:53.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:48:56.727: INFO: namespace statefulset-2003 deletion completed in 16.948750958s

• [SLOW TEST:123.595 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:48:56.729: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-80f0a822-e78a-4016-a4d1-9591ba83a11e
STEP: Creating a pod to test consume configMaps
Mar 23 16:48:57.497: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9bc8c54a-1a65-4a2e-92af-63d7be66e182" in namespace "projected-9301" to be "success or failure"
Mar 23 16:48:57.537: INFO: Pod "pod-projected-configmaps-9bc8c54a-1a65-4a2e-92af-63d7be66e182": Phase="Pending", Reason="", readiness=false. Elapsed: 40.793878ms
Mar 23 16:48:59.560: INFO: Pod "pod-projected-configmaps-9bc8c54a-1a65-4a2e-92af-63d7be66e182": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063843472s
Mar 23 16:49:01.617: INFO: Pod "pod-projected-configmaps-9bc8c54a-1a65-4a2e-92af-63d7be66e182": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.120648271s
STEP: Saw pod success
Mar 23 16:49:01.618: INFO: Pod "pod-projected-configmaps-9bc8c54a-1a65-4a2e-92af-63d7be66e182" satisfied condition "success or failure"
Mar 23 16:49:01.647: INFO: Trying to get logs from node 10.241.69.181 pod pod-projected-configmaps-9bc8c54a-1a65-4a2e-92af-63d7be66e182 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 16:49:01.854: INFO: Waiting for pod pod-projected-configmaps-9bc8c54a-1a65-4a2e-92af-63d7be66e182 to disappear
Mar 23 16:49:01.898: INFO: Pod pod-projected-configmaps-9bc8c54a-1a65-4a2e-92af-63d7be66e182 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:49:01.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9301" for this suite.
Mar 23 16:49:14.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:49:17.018: INFO: namespace projected-9301 deletion completed in 15.046963503s

• [SLOW TEST:20.289 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:49:17.018: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 23 16:49:26.102: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 23 16:49:26.139: INFO: Pod pod-with-prestop-http-hook still exists
Mar 23 16:49:28.140: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 23 16:49:28.192: INFO: Pod pod-with-prestop-http-hook still exists
Mar 23 16:49:30.140: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 23 16:49:30.171: INFO: Pod pod-with-prestop-http-hook still exists
Mar 23 16:49:32.140: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 23 16:49:32.177: INFO: Pod pod-with-prestop-http-hook still exists
Mar 23 16:49:34.140: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 23 16:49:34.161: INFO: Pod pod-with-prestop-http-hook still exists
Mar 23 16:49:36.140: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 23 16:49:36.171: INFO: Pod pod-with-prestop-http-hook still exists
Mar 23 16:49:38.140: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 23 16:49:38.164: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:49:38.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1727" for this suite.
Mar 23 16:50:00.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:50:03.290: INFO: namespace container-lifecycle-hook-1727 deletion completed in 25.029158148s

• [SLOW TEST:46.272 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:50:03.290: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Mar 23 16:50:04.940: INFO: Waiting up to 5m0s for pod "client-containers-9cc0c87a-5cb8-4366-98f4-425502e624da" in namespace "containers-9157" to be "success or failure"
Mar 23 16:50:04.959: INFO: Pod "client-containers-9cc0c87a-5cb8-4366-98f4-425502e624da": Phase="Pending", Reason="", readiness=false. Elapsed: 18.666637ms
Mar 23 16:50:06.990: INFO: Pod "client-containers-9cc0c87a-5cb8-4366-98f4-425502e624da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049848874s
Mar 23 16:50:09.020: INFO: Pod "client-containers-9cc0c87a-5cb8-4366-98f4-425502e624da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079907057s
STEP: Saw pod success
Mar 23 16:50:09.020: INFO: Pod "client-containers-9cc0c87a-5cb8-4366-98f4-425502e624da" satisfied condition "success or failure"
Mar 23 16:50:09.102: INFO: Trying to get logs from node 10.241.69.181 pod client-containers-9cc0c87a-5cb8-4366-98f4-425502e624da container test-container: <nil>
STEP: delete the pod
Mar 23 16:50:09.304: INFO: Waiting for pod client-containers-9cc0c87a-5cb8-4366-98f4-425502e624da to disappear
Mar 23 16:50:09.352: INFO: Pod client-containers-9cc0c87a-5cb8-4366-98f4-425502e624da no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:50:09.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9157" for this suite.
Mar 23 16:50:23.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:50:26.642: INFO: namespace containers-9157 deletion completed in 17.219159038s

• [SLOW TEST:23.351 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:50:26.642: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:50:27.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8491" for this suite.
Mar 23 16:50:39.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:50:42.402: INFO: namespace resourcequota-8491 deletion completed in 14.956073515s

• [SLOW TEST:15.760 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:50:42.402: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Mar 23 16:50:48.087: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:50:49.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8168" for this suite.
Mar 23 16:51:15.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:51:18.461: INFO: namespace replicaset-8168 deletion completed in 29.139546774s

• [SLOW TEST:36.059 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:51:18.461: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 16:51:20.125: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 16:51:22.214: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720579080, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720579080, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720579080, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720579080, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 16:51:25.288: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:51:25.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9530" for this suite.
Mar 23 16:51:39.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:51:42.963: INFO: namespace webhook-9530 deletion completed in 17.256835736s
STEP: Destroying namespace "webhook-9530-markers" for this suite.
Mar 23 16:51:57.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:52:00.220: INFO: namespace webhook-9530-markers deletion completed in 17.25699941s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:41.905 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:52:00.367: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:52:08.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9760" for this suite.
Mar 23 16:52:20.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:52:24.243: INFO: namespace resourcequota-9760 deletion completed in 15.702089319s

• [SLOW TEST:23.876 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:52:24.248: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar 23 16:52:24.800: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 23 16:52:25.121: INFO: Waiting for terminating namespaces to be deleted...
Mar 23 16:52:25.192: INFO: 
Logging pods the kubelet thinks is on node 10.241.69.172 before test
Mar 23 16:52:25.340: INFO: calico-node-6f276 from calico-system started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.340: INFO: 	Container calico-node ready: true, restart count 0
Mar 23 16:52:25.340: INFO: ibmcloud-block-storage-driver-rxcc5 from kube-system started at 2020-03-23 13:20:43 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.340: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 23 16:52:25.341: INFO: calico-typha-7d46744f5d-5mvhh from calico-system started at 2020-03-23 13:21:09 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.341: INFO: 	Container calico-typha ready: true, restart count 0
Mar 23 16:52:25.341: INFO: cluster-samples-operator-6b5459b7b9-fwhtx from openshift-cluster-samples-operator started at 2020-03-23 14:57:52 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.341: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar 23 16:52:25.341: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar 23 16:52:25.341: INFO: thanos-querier-866887d744-whk26 from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (4 container statuses recorded)
Mar 23 16:52:25.341: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 16:52:25.341: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar 23 16:52:25.341: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 16:52:25.341: INFO: 	Container thanos-querier ready: true, restart count 0
Mar 23 16:52:25.341: INFO: packageserver-5c7d978cbb-llzxd from openshift-operator-lifecycle-manager started at 2020-03-23 13:31:00 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.341: INFO: 	Container packageserver ready: true, restart count 0
Mar 23 16:52:25.341: INFO: openshift-kube-proxy-lv75h from openshift-kube-proxy started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.341: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 16:52:25.341: INFO: node-exporter-2c5jl from openshift-monitoring started at 2020-03-23 13:20:40 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.342: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 16:52:25.342: INFO: 	Container node-exporter ready: true, restart count 0
Mar 23 16:52:25.342: INFO: grafana-5bdf758c94-j7bvk from openshift-monitoring started at 2020-03-23 13:28:09 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.342: INFO: 	Container grafana ready: true, restart count 0
Mar 23 16:52:25.342: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar 23 16:52:25.342: INFO: sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-9fzqx from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.342: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Mar 23 16:52:25.342: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 16:52:25.342: INFO: console-5b755cbdcd-7sqjq from openshift-console started at 2020-03-23 13:22:24 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.342: INFO: 	Container console ready: true, restart count 0
Mar 23 16:52:25.342: INFO: multus-b8pkh from openshift-multus started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.342: INFO: 	Container kube-multus ready: true, restart count 0
Mar 23 16:52:25.342: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-03-23 13:28:54 +0000 UTC (3 container statuses recorded)
Mar 23 16:52:25.342: INFO: 	Container alertmanager ready: true, restart count 0
Mar 23 16:52:25.342: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar 23 16:52:25.342: INFO: 	Container config-reloader ready: true, restart count 0
Mar 23 16:52:25.342: INFO: multus-admission-controller-shsp6 from openshift-multus started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.342: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar 23 16:52:25.342: INFO: image-registry-684f4746d9-ptcpt from openshift-image-registry started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.342: INFO: 	Container registry ready: true, restart count 0
Mar 23 16:52:25.343: INFO: node-ca-c6cdr from openshift-image-registry started at 2020-03-23 13:21:48 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.343: INFO: 	Container node-ca ready: true, restart count 0
Mar 23 16:52:25.343: INFO: router-default-58f5c9568-cqkqx from openshift-ingress started at 2020-03-23 13:22:00 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.343: INFO: 	Container router ready: true, restart count 0
Mar 23 16:52:25.343: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-03-23 13:29:34 +0000 UTC (7 container statuses recorded)
Mar 23 16:52:25.343: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 16:52:25.343: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 16:52:25.343: INFO: 	Container prometheus ready: true, restart count 1
Mar 23 16:52:25.343: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 23 16:52:25.343: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar 23 16:52:25.343: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 23 16:52:25.343: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar 23 16:52:25.343: INFO: sonobuoy-e2e-job-b545dbbb0c064966 from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.343: INFO: 	Container e2e ready: true, restart count 0
Mar 23 16:52:25.343: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 16:52:25.343: INFO: ibm-master-proxy-static-10.241.69.172 from kube-system started at 2020-03-23 13:20:33 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.343: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 23 16:52:25.343: INFO: 	Container pause ready: true, restart count 0
Mar 23 16:52:25.343: INFO: ibm-keepalived-watcher-zqvkl from kube-system started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.343: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 23 16:52:25.343: INFO: vpn-774dcbf8fc-mqtz2 from kube-system started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.344: INFO: 	Container vpn ready: true, restart count 0
Mar 23 16:52:25.344: INFO: prometheus-adapter-6445ff7666-kwdxl from openshift-monitoring started at 2020-03-23 13:28:03 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.344: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar 23 16:52:25.344: INFO: tuned-zh9qp from openshift-cluster-node-tuning-operator started at 2020-03-23 13:21:11 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.344: INFO: 	Container tuned ready: true, restart count 0
Mar 23 16:52:25.344: INFO: dns-default-fmdtj from openshift-dns started at 2020-03-23 13:21:48 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.344: INFO: 	Container dns ready: true, restart count 0
Mar 23 16:52:25.344: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar 23 16:52:25.344: INFO: redhat-operators-5d5b56f44f-9c86l from openshift-marketplace started at 2020-03-23 14:22:14 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.344: INFO: 	Container redhat-operators ready: true, restart count 0
Mar 23 16:52:25.344: INFO: ibm-cloud-provider-ip-169-47-106-243-5c45fc569-dx5w9 from ibm-system started at 2020-03-23 16:42:26 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.344: INFO: 	Container ibm-cloud-provider-ip-169-47-106-243 ready: true, restart count 0
Mar 23 16:52:25.344: INFO: 
Logging pods the kubelet thinks is on node 10.241.69.181 before test
Mar 23 16:52:25.474: INFO: openshift-kube-proxy-hgttr from openshift-kube-proxy started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.474: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 16:52:25.474: INFO: calico-node-skv44 from calico-system started at 2020-03-23 13:21:48 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.474: INFO: 	Container calico-node ready: true, restart count 0
Mar 23 16:52:25.474: INFO: dns-default-98lms from openshift-dns started at 2020-03-23 16:42:37 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.474: INFO: 	Container dns ready: true, restart count 0
Mar 23 16:52:25.474: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar 23 16:52:25.474: INFO: sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-m6fcz from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.474: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Mar 23 16:52:25.474: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 16:52:25.474: INFO: ibm-master-proxy-static-10.241.69.181 from kube-system started at 2020-03-23 13:21:17 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.474: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 23 16:52:25.474: INFO: 	Container pause ready: true, restart count 0
Mar 23 16:52:25.474: INFO: community-operators-c75c8c9f6-jjk8l from openshift-marketplace started at 2020-03-23 16:42:26 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.474: INFO: 	Container community-operators ready: true, restart count 0
Mar 23 16:52:25.474: INFO: ibm-keepalived-watcher-2klqj from kube-system started at 2020-03-23 13:21:48 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.475: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 23 16:52:25.475: INFO: node-exporter-k4sbd from openshift-monitoring started at 2020-03-23 13:21:48 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.475: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 16:52:25.475: INFO: 	Container node-exporter ready: true, restart count 0
Mar 23 16:52:25.475: INFO: calico-typha-7d46744f5d-qnz4h from calico-system started at 2020-03-23 13:23:09 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.475: INFO: 	Container calico-typha ready: true, restart count 0
Mar 23 16:52:25.475: INFO: tuned-n9xl7 from openshift-cluster-node-tuning-operator started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.475: INFO: 	Container tuned ready: true, restart count 0
Mar 23 16:52:25.475: INFO: multus-596gk from openshift-multus started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.475: INFO: 	Container kube-multus ready: true, restart count 0
Mar 23 16:52:25.475: INFO: multus-admission-controller-k89c8 from openshift-multus started at 2020-03-23 16:43:07 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.475: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar 23 16:52:25.475: INFO: node-ca-bpcf9 from openshift-image-registry started at 2020-03-23 16:42:37 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.475: INFO: 	Container node-ca ready: true, restart count 0
Mar 23 16:52:25.475: INFO: certified-operators-6998665554-shpdk from openshift-marketplace started at 2020-03-23 16:42:26 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.475: INFO: 	Container certified-operators ready: true, restart count 0
Mar 23 16:52:25.475: INFO: registry-pvc-permissions-gptrd from openshift-image-registry started at 2020-03-23 13:24:16 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.475: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar 23 16:52:25.475: INFO: ibmcloud-block-storage-driver-rwjct from kube-system started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.475: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 23 16:52:25.476: INFO: 
Logging pods the kubelet thinks is on node 10.241.69.184 before test
Mar 23 16:52:25.689: INFO: cluster-storage-operator-7f448d8d78-tb4bw from openshift-cluster-storage-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.690: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Mar 23 16:52:25.690: INFO: cluster-node-tuning-operator-878d4d68-48859 from openshift-cluster-node-tuning-operator started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.690: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar 23 16:52:25.690: INFO: openshift-state-metrics-7479448b69-fsvs4 from openshift-monitoring started at 2020-03-23 13:20:33 +0000 UTC (3 container statuses recorded)
Mar 23 16:52:25.690: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar 23 16:52:25.690: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar 23 16:52:25.690: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar 23 16:52:25.690: INFO: ibm-master-proxy-static-10.241.69.184 from kube-system started at 2020-03-23 13:18:49 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.691: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 23 16:52:25.691: INFO: 	Container pause ready: true, restart count 0
Mar 23 16:52:25.691: INFO: ibm-keepalived-watcher-xc448 from kube-system started at 2020-03-23 13:18:55 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.691: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 23 16:52:25.691: INFO: service-ca-operator-69984d88bd-kcxc5 from openshift-service-ca-operator started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.691: INFO: 	Container operator ready: true, restart count 0
Mar 23 16:52:25.691: INFO: calico-kube-controllers-85c89dd878-ddpfj from calico-system started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.691: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 23 16:52:25.691: INFO: kube-state-metrics-6bcc97c9d6-vbfvx from openshift-monitoring started at 2020-03-23 13:20:32 +0000 UTC (3 container statuses recorded)
Mar 23 16:52:25.691: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar 23 16:52:25.692: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar 23 16:52:25.692: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 23 16:52:25.692: INFO: dns-default-pvrkk from openshift-dns started at 2020-03-23 13:21:47 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.692: INFO: 	Container dns ready: true, restart count 0
Mar 23 16:52:25.692: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar 23 16:52:25.692: INFO: thanos-querier-866887d744-7rdmw from openshift-monitoring started at 2020-03-23 13:29:10 +0000 UTC (4 container statuses recorded)
Mar 23 16:52:25.692: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 16:52:25.692: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar 23 16:52:25.692: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 16:52:25.692: INFO: 	Container thanos-querier ready: true, restart count 0
Mar 23 16:52:25.693: INFO: openshift-service-catalog-apiserver-operator-5845fbf887-jlsvg from openshift-service-catalog-apiserver-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.693: INFO: 	Container operator ready: true, restart count 1
Mar 23 16:52:25.693: INFO: catalog-operator-bb7df57d7-xkrxt from openshift-operator-lifecycle-manager started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.693: INFO: 	Container catalog-operator ready: true, restart count 0
Mar 23 16:52:25.693: INFO: multus-admission-controller-ckp4z from openshift-multus started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.693: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar 23 16:52:25.693: INFO: olm-operator-9d666b5b9-s6wnm from openshift-operator-lifecycle-manager started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.693: INFO: 	Container olm-operator ready: true, restart count 0
Mar 23 16:52:25.693: INFO: ibm-storage-watcher-cdf8f7d4c-pnk9m from kube-system started at 2020-03-23 13:20:08 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.693: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar 23 16:52:25.693: INFO: cluster-image-registry-operator-5d7d64d769-n6l5f from openshift-image-registry started at 2020-03-23 13:20:07 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.694: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar 23 16:52:25.694: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Mar 23 16:52:25.694: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-03-23 14:58:09 +0000 UTC (7 container statuses recorded)
Mar 23 16:52:25.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 16:52:25.694: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 16:52:25.694: INFO: 	Container prometheus ready: true, restart count 1
Mar 23 16:52:25.694: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 23 16:52:25.694: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar 23 16:52:25.694: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 23 16:52:25.694: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar 23 16:52:25.694: INFO: tigera-operator-c7555ddb8-fggcz from tigera-operator started at 2020-03-23 13:18:58 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.695: INFO: 	Container tigera-operator ready: true, restart count 0
Mar 23 16:52:25.695: INFO: ibmcloud-block-storage-plugin-5c7dcf4bdb-jm274 from kube-system started at 2020-03-23 13:19:01 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.695: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar 23 16:52:25.695: INFO: console-operator-597c74c496-7g9c9 from openshift-console-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.695: INFO: 	Container console-operator ready: true, restart count 0
Mar 23 16:52:25.695: INFO: node-exporter-xwc46 from openshift-monitoring started at 2020-03-23 13:20:33 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.695: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 16:52:25.695: INFO: 	Container node-exporter ready: true, restart count 0
Mar 23 16:52:25.695: INFO: tuned-fjdv5 from openshift-cluster-node-tuning-operator started at 2020-03-23 13:21:11 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.695: INFO: 	Container tuned ready: true, restart count 0
Mar 23 16:52:25.696: INFO: router-default-58f5c9568-vkljz from openshift-ingress started at 2020-03-23 13:22:01 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.696: INFO: 	Container router ready: true, restart count 0
Mar 23 16:52:25.696: INFO: packageserver-5c7d978cbb-s4z4h from openshift-operator-lifecycle-manager started at 2020-03-23 13:30:54 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.696: INFO: 	Container packageserver ready: true, restart count 0
Mar 23 16:52:25.696: INFO: cluster-monitoring-operator-5b6ff66676-fjscj from openshift-monitoring started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.696: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Mar 23 16:52:25.696: INFO: console-5b755cbdcd-pllqp from openshift-console started at 2020-03-23 13:22:44 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.696: INFO: 	Container console ready: true, restart count 0
Mar 23 16:52:25.696: INFO: marketplace-operator-554cffcfd-d5zjt from openshift-marketplace started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.696: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar 23 16:52:25.696: INFO: ingress-operator-754bbd5fbf-q5d8c from openshift-ingress-operator started at 2020-03-23 13:20:07 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.697: INFO: 	Container ingress-operator ready: true, restart count 0
Mar 23 16:52:25.697: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 16:52:25.697: INFO: telemeter-client-79bc5978cc-hnfmp from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (3 container statuses recorded)
Mar 23 16:52:25.697: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 16:52:25.697: INFO: 	Container reload ready: true, restart count 0
Mar 23 16:52:25.697: INFO: 	Container telemeter-client ready: true, restart count 0
Mar 23 16:52:25.697: INFO: ibmcloud-block-storage-driver-9xhmg from kube-system started at 2020-03-23 13:19:01 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.697: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 23 16:52:25.697: INFO: cloud-credential-operator-65466dfbf8-nvzxn from openshift-cloud-credential-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.697: INFO: 	Container manager ready: true, restart count 0
Mar 23 16:52:25.698: INFO: network-operator-6f9f45bfbb-2gt5t from openshift-network-operator started at 2020-03-23 13:19:00 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.698: INFO: 	Container network-operator ready: true, restart count 0
Mar 23 16:52:25.698: INFO: multus-g6kbd from openshift-multus started at 2020-03-23 13:19:31 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.698: INFO: 	Container kube-multus ready: true, restart count 0
Mar 23 16:52:25.698: INFO: downloads-7fdfb77b95-56xsf from openshift-console started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.698: INFO: 	Container download-server ready: true, restart count 0
Mar 23 16:52:25.698: INFO: dns-operator-6cc86f84cd-qd8jq from openshift-dns-operator started at 2020-03-23 13:20:08 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.698: INFO: 	Container dns-operator ready: true, restart count 0
Mar 23 16:52:25.698: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 16:52:25.698: INFO: configmap-cabundle-injector-78bbf44c6b-rnztt from openshift-service-ca started at 2020-03-23 13:20:35 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.698: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Mar 23 16:52:25.699: INFO: sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-tk2rn from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 16:52:25.699: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Mar 23 16:52:25.699: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 16:52:25.699: INFO: ibm-cloud-provider-ip-169-47-106-243-5c45fc569-hq4zt from ibm-system started at 2020-03-23 15:19:18 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.699: INFO: 	Container ibm-cloud-provider-ip-169-47-106-243 ready: true, restart count 0
Mar 23 16:52:25.699: INFO: calico-node-jxjt7 from calico-system started at 2020-03-23 13:19:14 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.699: INFO: 	Container calico-node ready: true, restart count 0
Mar 23 16:52:25.699: INFO: downloads-7fdfb77b95-668gj from openshift-console started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.699: INFO: 	Container download-server ready: true, restart count 0
Mar 23 16:52:25.699: INFO: node-ca-mvtq9 from openshift-image-registry started at 2020-03-23 13:21:49 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.700: INFO: 	Container node-ca ready: true, restart count 0
Mar 23 16:52:25.700: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-03-23 14:58:08 +0000 UTC (3 container statuses recorded)
Mar 23 16:52:25.700: INFO: 	Container alertmanager ready: true, restart count 0
Mar 23 16:52:25.700: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar 23 16:52:25.700: INFO: 	Container config-reloader ready: true, restart count 0
Mar 23 16:52:25.700: INFO: ibm-file-plugin-bd78b44b5-7xzx5 from kube-system started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.700: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar 23 16:52:25.700: INFO: service-serving-cert-signer-598cdff956-pxfdm from openshift-service-ca started at 2020-03-23 13:20:35 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.700: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Mar 23 16:52:25.700: INFO: openshift-service-catalog-controller-manager-operator-5487z68jd from openshift-service-catalog-controller-manager-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.700: INFO: 	Container operator ready: true, restart count 1
Mar 23 16:52:25.700: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-03-23 13:28:39 +0000 UTC (3 container statuses recorded)
Mar 23 16:52:25.700: INFO: 	Container alertmanager ready: true, restart count 0
Mar 23 16:52:25.700: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar 23 16:52:25.700: INFO: 	Container config-reloader ready: true, restart count 0
Mar 23 16:52:25.700: INFO: prometheus-operator-54f44d89d8-xfwmx from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.700: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar 23 16:52:25.700: INFO: prometheus-adapter-6445ff7666-7lcjj from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.700: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar 23 16:52:25.700: INFO: calico-typha-7d46744f5d-c2gxq from calico-system started at 2020-03-23 13:19:14 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.700: INFO: 	Container calico-typha ready: true, restart count 1
Mar 23 16:52:25.700: INFO: openshift-kube-proxy-9lb45 from openshift-kube-proxy started at 2020-03-23 13:19:38 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.700: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 16:52:25.700: INFO: apiservice-cabundle-injector-6b4f956cf4-rtrzx from openshift-service-ca started at 2020-03-23 13:20:35 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.700: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Mar 23 16:52:25.700: INFO: sonobuoy from sonobuoy started at 2020-03-23 14:35:40 +0000 UTC (1 container statuses recorded)
Mar 23 16:52:25.700: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node 10.241.69.172
STEP: verifying the node has the label node 10.241.69.181
STEP: verifying the node has the label node 10.241.69.184
Mar 23 16:52:26.746: INFO: Pod calico-kube-controllers-85c89dd878-ddpfj requesting resource cpu=0m on Node 10.241.69.184
Mar 23 16:52:26.747: INFO: Pod calico-node-6f276 requesting resource cpu=0m on Node 10.241.69.172
Mar 23 16:52:26.747: INFO: Pod calico-node-jxjt7 requesting resource cpu=0m on Node 10.241.69.184
Mar 23 16:52:26.747: INFO: Pod calico-node-skv44 requesting resource cpu=0m on Node 10.241.69.181
Mar 23 16:52:26.747: INFO: Pod calico-typha-7d46744f5d-5mvhh requesting resource cpu=0m on Node 10.241.69.172
Mar 23 16:52:26.747: INFO: Pod calico-typha-7d46744f5d-c2gxq requesting resource cpu=0m on Node 10.241.69.184
Mar 23 16:52:26.747: INFO: Pod calico-typha-7d46744f5d-qnz4h requesting resource cpu=0m on Node 10.241.69.181
Mar 23 16:52:26.747: INFO: Pod ibm-cloud-provider-ip-169-47-106-243-5c45fc569-dx5w9 requesting resource cpu=5m on Node 10.241.69.172
Mar 23 16:52:26.747: INFO: Pod ibm-cloud-provider-ip-169-47-106-243-5c45fc569-hq4zt requesting resource cpu=5m on Node 10.241.69.184
Mar 23 16:52:26.747: INFO: Pod ibm-file-plugin-bd78b44b5-7xzx5 requesting resource cpu=50m on Node 10.241.69.184
Mar 23 16:52:26.747: INFO: Pod ibm-keepalived-watcher-2klqj requesting resource cpu=5m on Node 10.241.69.181
Mar 23 16:52:26.747: INFO: Pod ibm-keepalived-watcher-xc448 requesting resource cpu=5m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod ibm-keepalived-watcher-zqvkl requesting resource cpu=5m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod ibm-master-proxy-static-10.241.69.172 requesting resource cpu=25m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod ibm-master-proxy-static-10.241.69.181 requesting resource cpu=25m on Node 10.241.69.181
Mar 23 16:52:26.748: INFO: Pod ibm-master-proxy-static-10.241.69.184 requesting resource cpu=25m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod ibm-storage-watcher-cdf8f7d4c-pnk9m requesting resource cpu=50m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod ibmcloud-block-storage-driver-9xhmg requesting resource cpu=0m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod ibmcloud-block-storage-driver-rwjct requesting resource cpu=0m on Node 10.241.69.181
Mar 23 16:52:26.748: INFO: Pod ibmcloud-block-storage-driver-rxcc5 requesting resource cpu=0m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod ibmcloud-block-storage-plugin-5c7dcf4bdb-jm274 requesting resource cpu=50m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod vpn-774dcbf8fc-mqtz2 requesting resource cpu=5m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod cloud-credential-operator-65466dfbf8-nvzxn requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod cluster-node-tuning-operator-878d4d68-48859 requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod tuned-fjdv5 requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod tuned-n9xl7 requesting resource cpu=10m on Node 10.241.69.181
Mar 23 16:52:26.748: INFO: Pod tuned-zh9qp requesting resource cpu=10m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod cluster-samples-operator-6b5459b7b9-fwhtx requesting resource cpu=20m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod cluster-storage-operator-7f448d8d78-tb4bw requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod console-operator-597c74c496-7g9c9 requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod console-5b755cbdcd-7sqjq requesting resource cpu=10m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod console-5b755cbdcd-pllqp requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod downloads-7fdfb77b95-56xsf requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod downloads-7fdfb77b95-668gj requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod dns-operator-6cc86f84cd-qd8jq requesting resource cpu=20m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod dns-default-98lms requesting resource cpu=110m on Node 10.241.69.181
Mar 23 16:52:26.748: INFO: Pod dns-default-fmdtj requesting resource cpu=110m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod dns-default-pvrkk requesting resource cpu=110m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod cluster-image-registry-operator-5d7d64d769-n6l5f requesting resource cpu=20m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod image-registry-684f4746d9-ptcpt requesting resource cpu=100m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod node-ca-bpcf9 requesting resource cpu=10m on Node 10.241.69.181
Mar 23 16:52:26.748: INFO: Pod node-ca-c6cdr requesting resource cpu=10m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod node-ca-mvtq9 requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod ingress-operator-754bbd5fbf-q5d8c requesting resource cpu=20m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod router-default-58f5c9568-cqkqx requesting resource cpu=100m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod router-default-58f5c9568-vkljz requesting resource cpu=100m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod openshift-kube-proxy-9lb45 requesting resource cpu=0m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod openshift-kube-proxy-hgttr requesting resource cpu=0m on Node 10.241.69.181
Mar 23 16:52:26.748: INFO: Pod openshift-kube-proxy-lv75h requesting resource cpu=0m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod certified-operators-6998665554-shpdk requesting resource cpu=10m on Node 10.241.69.181
Mar 23 16:52:26.748: INFO: Pod community-operators-c75c8c9f6-jjk8l requesting resource cpu=10m on Node 10.241.69.181
Mar 23 16:52:26.748: INFO: Pod marketplace-operator-554cffcfd-d5zjt requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod redhat-operators-5d5b56f44f-9c86l requesting resource cpu=10m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod alertmanager-main-0 requesting resource cpu=100m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod alertmanager-main-1 requesting resource cpu=100m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod alertmanager-main-2 requesting resource cpu=100m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod cluster-monitoring-operator-5b6ff66676-fjscj requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod grafana-5bdf758c94-j7bvk requesting resource cpu=100m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod kube-state-metrics-6bcc97c9d6-vbfvx requesting resource cpu=30m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod node-exporter-2c5jl requesting resource cpu=10m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod node-exporter-k4sbd requesting resource cpu=10m on Node 10.241.69.181
Mar 23 16:52:26.748: INFO: Pod node-exporter-xwc46 requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod openshift-state-metrics-7479448b69-fsvs4 requesting resource cpu=120m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod prometheus-adapter-6445ff7666-7lcjj requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod prometheus-adapter-6445ff7666-kwdxl requesting resource cpu=10m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod prometheus-k8s-0 requesting resource cpu=430m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod prometheus-k8s-1 requesting resource cpu=430m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod prometheus-operator-54f44d89d8-xfwmx requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod telemeter-client-79bc5978cc-hnfmp requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod thanos-querier-866887d744-7rdmw requesting resource cpu=40m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod thanos-querier-866887d744-whk26 requesting resource cpu=40m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod multus-596gk requesting resource cpu=10m on Node 10.241.69.181
Mar 23 16:52:26.748: INFO: Pod multus-admission-controller-ckp4z requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod multus-admission-controller-k89c8 requesting resource cpu=10m on Node 10.241.69.181
Mar 23 16:52:26.748: INFO: Pod multus-admission-controller-shsp6 requesting resource cpu=10m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod multus-b8pkh requesting resource cpu=10m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod multus-g6kbd requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod network-operator-6f9f45bfbb-2gt5t requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod catalog-operator-bb7df57d7-xkrxt requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod olm-operator-9d666b5b9-s6wnm requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod packageserver-5c7d978cbb-llzxd requesting resource cpu=10m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod packageserver-5c7d978cbb-s4z4h requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod service-ca-operator-69984d88bd-kcxc5 requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod apiservice-cabundle-injector-6b4f956cf4-rtrzx requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod configmap-cabundle-injector-78bbf44c6b-rnztt requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod service-serving-cert-signer-598cdff956-pxfdm requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod openshift-service-catalog-apiserver-operator-5845fbf887-jlsvg requesting resource cpu=0m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod openshift-service-catalog-controller-manager-operator-5487z68jd requesting resource cpu=10m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod sonobuoy-e2e-job-b545dbbb0c064966 requesting resource cpu=0m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-9fzqx requesting resource cpu=0m on Node 10.241.69.172
Mar 23 16:52:26.748: INFO: Pod sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-m6fcz requesting resource cpu=0m on Node 10.241.69.181
Mar 23 16:52:26.748: INFO: Pod sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-tk2rn requesting resource cpu=0m on Node 10.241.69.184
Mar 23 16:52:26.748: INFO: Pod tigera-operator-c7555ddb8-fggcz requesting resource cpu=0m on Node 10.241.69.184
STEP: Starting Pods to consume most of the cluster CPU.
Mar 23 16:52:26.748: INFO: Creating a pod which consumes cpu=1946m on Node 10.241.69.172
Mar 23 16:52:27.023: INFO: Creating a pod which consumes cpu=2590m on Node 10.241.69.181
Mar 23 16:52:27.111: INFO: Creating a pod which consumes cpu=1662m on Node 10.241.69.184
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5099cbd8-c141-4b92-90cd-2182e0af787f.15fefd0c1604360b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7084/filler-pod-5099cbd8-c141-4b92-90cd-2182e0af787f to 10.241.69.184]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5099cbd8-c141-4b92-90cd-2182e0af787f.15fefd0c71944a9b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5099cbd8-c141-4b92-90cd-2182e0af787f.15fefd0c84a3e891], Reason = [Created], Message = [Created container filler-pod-5099cbd8-c141-4b92-90cd-2182e0af787f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5099cbd8-c141-4b92-90cd-2182e0af787f.15fefd0c874a9ac4], Reason = [Started], Message = [Started container filler-pod-5099cbd8-c141-4b92-90cd-2182e0af787f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d10a310-0d14-42e4-a41a-706d61ab8ed1.15fefd0c119524a9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7084/filler-pod-9d10a310-0d14-42e4-a41a-706d61ab8ed1 to 10.241.69.181]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d10a310-0d14-42e4-a41a-706d61ab8ed1.15fefd0c59189840], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d10a310-0d14-42e4-a41a-706d61ab8ed1.15fefd0c67b745fe], Reason = [Created], Message = [Created container filler-pod-9d10a310-0d14-42e4-a41a-706d61ab8ed1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9d10a310-0d14-42e4-a41a-706d61ab8ed1.15fefd0c69c54dc4], Reason = [Started], Message = [Started container filler-pod-9d10a310-0d14-42e4-a41a-706d61ab8ed1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d17dc962-7f82-4df4-a9eb-902c7ebeac2b.15fefd0c0ba0ba9a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7084/filler-pod-d17dc962-7f82-4df4-a9eb-902c7ebeac2b to 10.241.69.172]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d17dc962-7f82-4df4-a9eb-902c7ebeac2b.15fefd0c6dac1775], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d17dc962-7f82-4df4-a9eb-902c7ebeac2b.15fefd0c7e191516], Reason = [Created], Message = [Created container filler-pod-d17dc962-7f82-4df4-a9eb-902c7ebeac2b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d17dc962-7f82-4df4-a9eb-902c7ebeac2b.15fefd0c807f1364], Reason = [Started], Message = [Started container filler-pod-d17dc962-7f82-4df4-a9eb-902c7ebeac2b]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15fefd0d174c524f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15fefd0d1c2a198a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.241.69.172
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.241.69.181
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.241.69.184
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:52:33.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7084" for this suite.
Mar 23 16:52:47.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:52:50.314: INFO: namespace sched-pred-7084 deletion completed in 16.969852746s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:26.066 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:52:50.315: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar 23 16:52:50.866: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:52:56.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6533" for this suite.
Mar 23 16:53:10.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:53:13.310: INFO: namespace init-container-6533 deletion completed in 16.755627183s

• [SLOW TEST:22.995 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:53:13.310: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-ec8f22d9-ad3c-4e70-81d8-f85fcf93aa0c
STEP: Creating a pod to test consume configMaps
Mar 23 16:53:14.148: INFO: Waiting up to 5m0s for pod "pod-configmaps-649bf501-4b6c-459b-9cb8-2ba4c11de1a5" in namespace "configmap-201" to be "success or failure"
Mar 23 16:53:14.176: INFO: Pod "pod-configmaps-649bf501-4b6c-459b-9cb8-2ba4c11de1a5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.363568ms
Mar 23 16:53:16.208: INFO: Pod "pod-configmaps-649bf501-4b6c-459b-9cb8-2ba4c11de1a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059976876s
Mar 23 16:53:18.230: INFO: Pod "pod-configmaps-649bf501-4b6c-459b-9cb8-2ba4c11de1a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.082110327s
STEP: Saw pod success
Mar 23 16:53:18.230: INFO: Pod "pod-configmaps-649bf501-4b6c-459b-9cb8-2ba4c11de1a5" satisfied condition "success or failure"
Mar 23 16:53:18.260: INFO: Trying to get logs from node 10.241.69.181 pod pod-configmaps-649bf501-4b6c-459b-9cb8-2ba4c11de1a5 container configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 16:53:18.462: INFO: Waiting for pod pod-configmaps-649bf501-4b6c-459b-9cb8-2ba4c11de1a5 to disappear
Mar 23 16:53:18.487: INFO: Pod pod-configmaps-649bf501-4b6c-459b-9cb8-2ba4c11de1a5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:53:18.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-201" for this suite.
Mar 23 16:53:32.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:53:35.272: INFO: namespace configmap-201 deletion completed in 16.748642675s

• [SLOW TEST:21.962 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:53:35.275: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 16:53:35.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 create -f - --namespace=kubectl-5147'
Mar 23 16:53:36.564: INFO: stderr: ""
Mar 23 16:53:36.564: INFO: stdout: "replicationcontroller/redis-master created\n"
Mar 23 16:53:36.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 create -f - --namespace=kubectl-5147'
Mar 23 16:53:37.263: INFO: stderr: ""
Mar 23 16:53:37.263: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Mar 23 16:53:38.494: INFO: Selector matched 1 pods for map[app:redis]
Mar 23 16:53:38.494: INFO: Found 0 / 1
Mar 23 16:53:39.289: INFO: Selector matched 1 pods for map[app:redis]
Mar 23 16:53:39.289: INFO: Found 1 / 1
Mar 23 16:53:39.289: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 23 16:53:39.314: INFO: Selector matched 1 pods for map[app:redis]
Mar 23 16:53:39.315: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 23 16:53:39.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 describe pod redis-master-w65dv --namespace=kubectl-5147'
Mar 23 16:53:39.558: INFO: stderr: ""
Mar 23 16:53:39.558: INFO: stdout: "Name:         redis-master-w65dv\nNamespace:    kubectl-5147\nPriority:     0\nNode:         10.241.69.181/10.241.69.181\nStart Time:   Mon, 23 Mar 2020 16:53:36 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 172.30.197.178/32\n              cni.projectcalico.org/podIPs: 172.30.197.178/32\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.30.197.178\"\n                    ],\n                    \"dns\": {}\n                }]\n              openshift.io/scc: privileged\nStatus:       Running\nIP:           172.30.197.178\nIPs:\n  IP:           172.30.197.178\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   cri-o://bd49d381980c7f8f6f89063ed1a41d31c1bfae6e85cb1e2a99f01c4dc7691eb4\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 23 Mar 2020 16:53:38 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ggvxk (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-ggvxk:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-ggvxk\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                    Message\n  ----    ------     ----       ----                    -------\n  Normal  Scheduled  <unknown>  default-scheduler       Successfully assigned kubectl-5147/redis-master-w65dv to 10.241.69.181\n  Normal  Pulled     1s         kubelet, 10.241.69.181  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    1s         kubelet, 10.241.69.181  Created container redis-master\n  Normal  Started    1s         kubelet, 10.241.69.181  Started container redis-master\n"
Mar 23 16:53:39.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 describe rc redis-master --namespace=kubectl-5147'
Mar 23 16:53:39.811: INFO: stderr: ""
Mar 23 16:53:39.811: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-5147\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-w65dv\n"
Mar 23 16:53:39.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 describe service redis-master --namespace=kubectl-5147'
Mar 23 16:53:40.062: INFO: stderr: ""
Mar 23 16:53:40.062: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-5147\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.21.220.124\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.30.197.178:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 23 16:53:40.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 describe node 10.241.69.172'
Mar 23 16:53:40.460: INFO: stderr: ""
Mar 23 16:53:40.460: INFO: stdout: "Name:               10.241.69.172\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-south\n                    failure-domain.beta.kubernetes.io/zone=dal12\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=169.59.196.52\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.241.69.172\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_7_64\n                    ibm-cloud.kubernetes.io/region=us-south\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-bpsb4s1d0soj8fpsuur0-kubee2epvgv-default-00000228\n                    ibm-cloud.kubernetes.io/worker-pool-id=bpsb4s1d0soj8fpsuur0-6f0e773\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.3.5_1514_openshift\n                    ibm-cloud.kubernetes.io/zone=dal12\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.241.69.172\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=2722938\n                    publicVLAN=2722936\n                    topology.kubernetes.io/region=us-south\n                    topology.kubernetes.io/zone=dal12\nAnnotations:        projectcalico.org/IPv4Address: 10.241.69.172/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.64.192\nCreationTimestamp:  Mon, 23 Mar 2020 13:20:40 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 23 Mar 2020 13:21:32 +0000   Mon, 23 Mar 2020 13:21:32 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 23 Mar 2020 16:52:44 +0000   Mon, 23 Mar 2020 13:20:40 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 23 Mar 2020 16:52:44 +0000   Mon, 23 Mar 2020 13:20:40 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 23 Mar 2020 16:52:44 +0000   Mon, 23 Mar 2020 13:20:40 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 23 Mar 2020 16:52:44 +0000   Mon, 23 Mar 2020 13:21:30 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.241.69.172\n  ExternalIP:  169.59.196.52\n  Hostname:    10.241.69.172\nCapacity:\n cpu:                4\n ephemeral-storage:  103078840Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16260928Ki\n pods:               110\nAllocatable:\n cpu:                3910m\n ephemeral-storage:  100275095474\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             13484864Ki\n pods:               110\nSystem Info:\n Machine ID:                              792901d2887b48b792e6923540d209c7\n System UUID:                             46486757-0F6A-FE8F-616E-802B0DF2F289\n Boot ID:                                 ffc67723-8ca1-4e6b-8058-bf92c5202ac9\n Kernel Version:                          3.10.0-1062.12.1.el7.x86_64\n OS Image:                                Red Hat\n Operating System:                        linux\n Architecture:                            amd64\n Container Runtime Version:               cri-o://1.16.3-20.dev.rhaos4.3.git11c04e3.el7\n Kubelet Version:                         v1.16.2\n Kube-Proxy Version:                      v1.16.2\nProviderID:                               ibm://fee034388aa6435883a1f720010ab3a2///bpsb4s1d0soj8fpsuur0/kube-bpsb4s1d0soj8fpsuur0-kubee2epvgv-default-00000228\nNon-terminated Pods:                      (27 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                           calico-node-6f276                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h33m\n  calico-system                           calico-typha-7d46744f5d-5mvhh                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h32m\n  ibm-system                              ibm-cloud-provider-ip-169-47-106-243-5c45fc569-dx5w9       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         11m\n  kube-system                             ibm-keepalived-watcher-zqvkl                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         3h33m\n  kube-system                             ibm-master-proxy-static-10.241.69.172                      25m (0%)      300m (7%)   32M (0%)         512M (3%)      3h31m\n  kube-system                             ibmcloud-block-storage-driver-rxcc5                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h32m\n  kube-system                             vpn-774dcbf8fc-mqtz2                                       5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         115m\n  openshift-cluster-node-tuning-operator  tuned-zh9qp                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h32m\n  openshift-cluster-samples-operator      cluster-samples-operator-6b5459b7b9-fwhtx                  20m (0%)      0 (0%)      0 (0%)           0 (0%)         115m\n  openshift-console                       console-5b755cbdcd-7sqjq                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         3h31m\n  openshift-dns                           dns-default-fmdtj                                          110m (2%)     0 (0%)      70Mi (0%)        512Mi (3%)     3h31m\n  openshift-image-registry                image-registry-684f4746d9-ptcpt                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         115m\n  openshift-image-registry                node-ca-c6cdr                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h31m\n  openshift-ingress                       router-default-58f5c9568-cqkqx                             100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         3h31m\n  openshift-kube-proxy                    openshift-kube-proxy-lv75h                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h33m\n  openshift-marketplace                   redhat-operators-5d5b56f44f-9c86l                          10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         151m\n  openshift-monitoring                    alertmanager-main-0                                        100m (2%)     100m (2%)   225Mi (1%)       25Mi (0%)      3h24m\n  openshift-monitoring                    grafana-5bdf758c94-j7bvk                                   100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         3h25m\n  openshift-monitoring                    node-exporter-2c5jl                                        10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         3h33m\n  openshift-monitoring                    prometheus-adapter-6445ff7666-kwdxl                        10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         3h25m\n  openshift-monitoring                    prometheus-k8s-0                                           430m (10%)    200m (5%)   1134Mi (8%)      50Mi (0%)      3h24m\n  openshift-monitoring                    thanos-querier-866887d744-whk26                            40m (1%)      0 (0%)      72Mi (0%)        0 (0%)         115m\n  openshift-multus                        multus-admission-controller-shsp6                          10m (0%)      0 (0%)      0 (0%)           0 (0%)         3h31m\n  openshift-multus                        multus-b8pkh                                               10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         3h33m\n  openshift-operator-lifecycle-manager    packageserver-5c7d978cbb-llzxd                             10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h22m\n  sonobuoy                                sonobuoy-e2e-job-b545dbbb0c064966                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         137m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-9fzqx    0 (0%)        0 (0%)      0 (0%)           0 (0%)         137m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1130m (28%)      600m (15%)\n  memory             2732562Ki (20%)  1127514112 (8%)\n  ephemeral-storage  0 (0%)           0 (0%)\nEvents:              <none>\n"
Mar 23 16:53:40.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 describe namespace kubectl-5147'
Mar 23 16:53:40.692: INFO: stderr: ""
Mar 23 16:53:40.692: INFO: stdout: "Name:         kubectl-5147\nLabels:       e2e-framework=kubectl\n              e2e-run=571c74f6-e0ed-4913-8ac9-e1f6a3e1baf7\nAnnotations:  openshift.io/sa.scc.mcs: s0:c66,c40\n              openshift.io/sa.scc.supplemental-groups: 1004370000/10000\n              openshift.io/sa.scc.uid-range: 1004370000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:53:40.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5147" for this suite.
Mar 23 16:54:02.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:54:05.502: INFO: namespace kubectl-5147 deletion completed in 24.768145663s

• [SLOW TEST:30.228 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:54:05.504: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-5bcfba9a-9386-4a78-970f-e3eaf8a5fa3c
STEP: Creating a pod to test consume secrets
Mar 23 16:54:06.233: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-69feb724-e2e1-4d21-aa22-a42385a39537" in namespace "projected-5863" to be "success or failure"
Mar 23 16:54:06.261: INFO: Pod "pod-projected-secrets-69feb724-e2e1-4d21-aa22-a42385a39537": Phase="Pending", Reason="", readiness=false. Elapsed: 27.644599ms
Mar 23 16:54:08.288: INFO: Pod "pod-projected-secrets-69feb724-e2e1-4d21-aa22-a42385a39537": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053973437s
Mar 23 16:54:10.313: INFO: Pod "pod-projected-secrets-69feb724-e2e1-4d21-aa22-a42385a39537": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.079321782s
STEP: Saw pod success
Mar 23 16:54:10.313: INFO: Pod "pod-projected-secrets-69feb724-e2e1-4d21-aa22-a42385a39537" satisfied condition "success or failure"
Mar 23 16:54:10.349: INFO: Trying to get logs from node 10.241.69.181 pod pod-projected-secrets-69feb724-e2e1-4d21-aa22-a42385a39537 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 23 16:54:10.502: INFO: Waiting for pod pod-projected-secrets-69feb724-e2e1-4d21-aa22-a42385a39537 to disappear
Mar 23 16:54:10.542: INFO: Pod pod-projected-secrets-69feb724-e2e1-4d21-aa22-a42385a39537 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:54:10.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5863" for this suite.
Mar 23 16:54:22.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:54:25.614: INFO: namespace projected-5863 deletion completed in 15.03824856s

• [SLOW TEST:20.111 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:54:25.615: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 16:54:27.558: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 16:54:29.634: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720579267, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720579267, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720579267, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720579267, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 16:54:32.721: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Mar 23 16:54:32.817: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:54:32.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7321" for this suite.
Mar 23 16:54:45.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:54:47.651: INFO: namespace webhook-7321 deletion completed in 14.714735018s
STEP: Destroying namespace "webhook-7321-markers" for this suite.
Mar 23 16:54:57.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:55:00.740: INFO: namespace webhook-7321-markers deletion completed in 13.088732226s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:35.291 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:55:00.907: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 16:55:01.888: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 23 16:55:02.004: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 23 16:55:07.023: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 23 16:55:07.023: INFO: Creating deployment "test-rolling-update-deployment"
Mar 23 16:55:07.045: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 23 16:55:07.092: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 23 16:55:09.141: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 23 16:55:09.161: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720579307, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720579307, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720579307, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720579307, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 16:55:11.176: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar 23 16:55:11.233: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5328 /apis/apps/v1/namespaces/deployment-5328/deployments/test-rolling-update-deployment 68769da7-2133-4497-9cd7-f7dc25ec480e 99856 1 2020-03-23 16:55:07 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002822e08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-23 16:55:07 +0000 UTC,LastTransitionTime:2020-03-23 16:55:07 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-03-23 16:55:10 +0000 UTC,LastTransitionTime:2020-03-23 16:55:07 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 23 16:55:11.262: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-5328 /apis/apps/v1/namespaces/deployment-5328/replicasets/test-rolling-update-deployment-55d946486 b9099bbd-abfe-4373-96c1-8ce9f4fd1b5d 99844 1 2020-03-23 16:55:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 68769da7-2133-4497-9cd7-f7dc25ec480e 0xc002823320 0xc002823321}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002823388 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 23 16:55:11.262: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 23 16:55:11.262: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5328 /apis/apps/v1/namespaces/deployment-5328/replicasets/test-rolling-update-controller a6e2223b-026f-49ef-85cf-2f367ca10666 99855 2 2020-03-23 16:55:01 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 68769da7-2133-4497-9cd7-f7dc25ec480e 0xc002823247 0xc002823248}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0028232b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 16:55:11.300: INFO: Pod "test-rolling-update-deployment-55d946486-8dgw9" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-8dgw9 test-rolling-update-deployment-55d946486- deployment-5328 /api/v1/namespaces/deployment-5328/pods/test-rolling-update-deployment-55d946486-8dgw9 cbaabe2b-ebd1-494a-ac72-74056baf0d65 99843 0 2020-03-23 16:55:07 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:172.30.197.182/32 cni.projectcalico.org/podIPs:172.30.197.182/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.197.182"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 b9099bbd-abfe-4373-96c1-8ce9f4fd1b5d 0xc002823820 0xc002823821}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-z5pn8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-z5pn8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-z5pn8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.181,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-4s5dk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 16:55:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 16:55:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 16:55:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 16:55:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.181,PodIP:172.30.197.182,StartTime:2020-03-23 16:55:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 16:55:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://42dc7b9511f726489432cc497450885e3701292396b38e41570fada3e9c79fe2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.197.182,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:55:11.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5328" for this suite.
Mar 23 16:55:23.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:55:26.068: INFO: namespace deployment-5328 deletion completed in 14.712393406s

• [SLOW TEST:25.162 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:55:26.069: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6334.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6334.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6334.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6334.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6334.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6334.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6334.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6334.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6334.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6334.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6334.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6334.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6334.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 236.171.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.171.236_udp@PTR;check="$$(dig +tcp +noall +answer +search 236.171.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.171.236_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6334.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6334.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6334.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6334.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6334.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6334.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6334.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6334.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6334.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6334.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6334.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6334.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6334.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 236.171.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.171.236_udp@PTR;check="$$(dig +tcp +noall +answer +search 236.171.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.171.236_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 16:55:30.797: INFO: Unable to read wheezy_udp@dns-test-service.dns-6334.svc.cluster.local from pod dns-6334/dns-test-a6a42a23-7024-498d-9469-a215ab02b673: the server could not find the requested resource (get pods dns-test-a6a42a23-7024-498d-9469-a215ab02b673)
Mar 23 16:55:30.831: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6334.svc.cluster.local from pod dns-6334/dns-test-a6a42a23-7024-498d-9469-a215ab02b673: the server could not find the requested resource (get pods dns-test-a6a42a23-7024-498d-9469-a215ab02b673)
Mar 23 16:55:30.862: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6334.svc.cluster.local from pod dns-6334/dns-test-a6a42a23-7024-498d-9469-a215ab02b673: the server could not find the requested resource (get pods dns-test-a6a42a23-7024-498d-9469-a215ab02b673)
Mar 23 16:55:30.893: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6334.svc.cluster.local from pod dns-6334/dns-test-a6a42a23-7024-498d-9469-a215ab02b673: the server could not find the requested resource (get pods dns-test-a6a42a23-7024-498d-9469-a215ab02b673)
Mar 23 16:55:31.255: INFO: Unable to read jessie_tcp@dns-test-service.dns-6334.svc.cluster.local from pod dns-6334/dns-test-a6a42a23-7024-498d-9469-a215ab02b673: the server could not find the requested resource (get pods dns-test-a6a42a23-7024-498d-9469-a215ab02b673)
Mar 23 16:55:31.309: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6334.svc.cluster.local from pod dns-6334/dns-test-a6a42a23-7024-498d-9469-a215ab02b673: the server could not find the requested resource (get pods dns-test-a6a42a23-7024-498d-9469-a215ab02b673)
Mar 23 16:55:31.352: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6334.svc.cluster.local from pod dns-6334/dns-test-a6a42a23-7024-498d-9469-a215ab02b673: the server could not find the requested resource (get pods dns-test-a6a42a23-7024-498d-9469-a215ab02b673)
Mar 23 16:55:31.689: INFO: Lookups using dns-6334/dns-test-a6a42a23-7024-498d-9469-a215ab02b673 failed for: [wheezy_udp@dns-test-service.dns-6334.svc.cluster.local wheezy_tcp@dns-test-service.dns-6334.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6334.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6334.svc.cluster.local jessie_tcp@dns-test-service.dns-6334.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6334.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6334.svc.cluster.local]

Mar 23 16:55:37.481: INFO: DNS probes using dns-6334/dns-test-a6a42a23-7024-498d-9469-a215ab02b673 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:55:37.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6334" for this suite.
Mar 23 16:55:50.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:55:52.999: INFO: namespace dns-6334 deletion completed in 15.024088076s

• [SLOW TEST:26.930 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:55:52.999: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Mar 23 16:55:53.638: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:56:46.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7928" for this suite.
Mar 23 16:56:56.589: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:56:59.174: INFO: namespace crd-publish-openapi-7928 deletion completed in 12.693599782s

• [SLOW TEST:66.174 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:56:59.174: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-1955
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Mar 23 16:56:59.663: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Mar 23 16:57:20.711: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.197.184:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1955 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 16:57:20.711: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 16:57:21.011: INFO: Found all expected endpoints: [netserver-0]
Mar 23 16:57:21.037: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.64.204:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1955 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 16:57:21.037: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 16:57:21.264: INFO: Found all expected endpoints: [netserver-1]
Mar 23 16:57:21.293: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.123.241:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1955 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 23 16:57:21.293: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 16:57:21.564: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:57:21.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1955" for this suite.
Mar 23 16:57:33.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:57:36.690: INFO: namespace pod-network-test-1955 deletion completed in 15.087160626s

• [SLOW TEST:37.517 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:57:36.691: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 16:57:37.281: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb4a3680-9baa-4adb-860b-d869a0c040dc" in namespace "projected-3857" to be "success or failure"
Mar 23 16:57:37.313: INFO: Pod "downwardapi-volume-eb4a3680-9baa-4adb-860b-d869a0c040dc": Phase="Pending", Reason="", readiness=false. Elapsed: 31.075147ms
Mar 23 16:57:39.337: INFO: Pod "downwardapi-volume-eb4a3680-9baa-4adb-860b-d869a0c040dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.05545693s
STEP: Saw pod success
Mar 23 16:57:39.337: INFO: Pod "downwardapi-volume-eb4a3680-9baa-4adb-860b-d869a0c040dc" satisfied condition "success or failure"
Mar 23 16:57:39.368: INFO: Trying to get logs from node 10.241.69.181 pod downwardapi-volume-eb4a3680-9baa-4adb-860b-d869a0c040dc container client-container: <nil>
STEP: delete the pod
Mar 23 16:57:39.567: INFO: Waiting for pod downwardapi-volume-eb4a3680-9baa-4adb-860b-d869a0c040dc to disappear
Mar 23 16:57:39.596: INFO: Pod downwardapi-volume-eb4a3680-9baa-4adb-860b-d869a0c040dc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:57:39.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3857" for this suite.
Mar 23 16:57:51.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:57:54.801: INFO: namespace projected-3857 deletion completed in 15.17145254s

• [SLOW TEST:18.110 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:57:54.802: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:58:25.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5809" for this suite.
Mar 23 16:58:39.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:58:42.742: INFO: namespace container-runtime-5809 deletion completed in 16.86050859s

• [SLOW TEST:47.940 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:58:42.742: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Mar 23 16:58:43.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 api-versions'
Mar 23 16:58:43.383: INFO: stderr: ""
Mar 23 16:58:43.383: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.openshift.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:58:43.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4295" for this suite.
Mar 23 16:58:55.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:58:58.642: INFO: namespace kubectl-4295 deletion completed in 15.208008487s

• [SLOW TEST:15.900 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:58:58.642: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Mar 23 16:58:59.321: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-705 /api/v1/namespaces/watch-705/configmaps/e2e-watch-test-label-changed 74d2f29f-3751-4d6b-b9b0-29b50143a2bb 101505 0 2020-03-23 16:58:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 23 16:58:59.322: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-705 /api/v1/namespaces/watch-705/configmaps/e2e-watch-test-label-changed 74d2f29f-3751-4d6b-b9b0-29b50143a2bb 101511 0 2020-03-23 16:58:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 23 16:58:59.322: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-705 /api/v1/namespaces/watch-705/configmaps/e2e-watch-test-label-changed 74d2f29f-3751-4d6b-b9b0-29b50143a2bb 101514 0 2020-03-23 16:58:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Mar 23 16:59:09.603: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-705 /api/v1/namespaces/watch-705/configmaps/e2e-watch-test-label-changed 74d2f29f-3751-4d6b-b9b0-29b50143a2bb 101563 0 2020-03-23 16:58:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 23 16:59:09.604: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-705 /api/v1/namespaces/watch-705/configmaps/e2e-watch-test-label-changed 74d2f29f-3751-4d6b-b9b0-29b50143a2bb 101564 0 2020-03-23 16:58:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar 23 16:59:09.604: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-705 /api/v1/namespaces/watch-705/configmaps/e2e-watch-test-label-changed 74d2f29f-3751-4d6b-b9b0-29b50143a2bb 101565 0 2020-03-23 16:58:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:59:09.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-705" for this suite.
Mar 23 16:59:21.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:59:24.650: INFO: namespace watch-705 deletion completed in 14.991552471s

• [SLOW TEST:26.008 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:59:24.650: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 16:59:25.060: INFO: Creating deployment "test-recreate-deployment"
Mar 23 16:59:25.085: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 23 16:59:25.174: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 23 16:59:27.218: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 23 16:59:27.256: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720579565, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720579565, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720579565, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720579565, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 16:59:29.279: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 23 16:59:29.345: INFO: Updating deployment test-recreate-deployment
Mar 23 16:59:29.345: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar 23 16:59:30.155: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4830 /apis/apps/v1/namespaces/deployment-4830/deployments/test-recreate-deployment 94204604-5a85-4fc2-a227-fec302ce27a5 101757 2 2020-03-23 16:59:25 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001f84068 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-03-23 16:59:29 +0000 UTC,LastTransitionTime:2020-03-23 16:59:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-03-23 16:59:30 +0000 UTC,LastTransitionTime:2020-03-23 16:59:25 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 23 16:59:30.193: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-4830 /apis/apps/v1/namespaces/deployment-4830/replicasets/test-recreate-deployment-5f94c574ff eec7f873-6d93-4b02-be51-c75208d385d9 101754 1 2020-03-23 16:59:29 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 94204604-5a85-4fc2-a227-fec302ce27a5 0xc002b5bd17 0xc002b5bd18}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002b5bd78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 16:59:30.193: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 23 16:59:30.193: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-4830 /apis/apps/v1/namespaces/deployment-4830/replicasets/test-recreate-deployment-68fc85c7bb 97f5584b-1e28-4500-80cc-e4f21a53c952 101742 2 2020-03-23 16:59:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 94204604-5a85-4fc2-a227-fec302ce27a5 0xc002b5bde7 0xc002b5bde8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002b5be48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 16:59:30.232: INFO: Pod "test-recreate-deployment-5f94c574ff-qfwqz" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-qfwqz test-recreate-deployment-5f94c574ff- deployment-4830 /api/v1/namespaces/deployment-4830/pods/test-recreate-deployment-5f94c574ff-qfwqz a09d0117-8178-46b6-b4e6-569a6400f27b 101756 0 2020-03-23 16:59:29 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff eec7f873-6d93-4b02-be51-c75208d385d9 0xc0037b31c7 0xc0037b31c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gqn8s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gqn8s,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gqn8s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.181,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-k8ss6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 16:59:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 16:59:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 16:59:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 16:59:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.181,PodIP:,StartTime:2020-03-23 16:59:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:59:30.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4830" for this suite.
Mar 23 16:59:42.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 16:59:44.742: INFO: namespace deployment-4830 deletion completed in 14.45822816s

• [SLOW TEST:20.092 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 16:59:44.743: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 16:59:49.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6043" for this suite.
Mar 23 17:00:17.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:00:20.304: INFO: namespace containers-6043 deletion completed in 30.910230828s

• [SLOW TEST:35.562 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:00:20.305: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:00:20.959: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 23 17:00:25.989: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 23 17:00:25.990: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar 23 17:00:26.261: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7549 /apis/apps/v1/namespaces/deployment-7549/deployments/test-cleanup-deployment 0b4fe6eb-50be-4746-86da-d9f684becd21 102174 1 2020-03-23 17:00:26 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0024c9b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Mar 23 17:00:26.339: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-7549 /apis/apps/v1/namespaces/deployment-7549/replicasets/test-cleanup-deployment-65db99849b 538219ce-c9a0-4719-b885-890765e741d3 102177 1 2020-03-23 17:00:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 0b4fe6eb-50be-4746-86da-d9f684becd21 0xc0024c9fd7 0xc0024c9fd8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002dae038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 17:00:26.339: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar 23 17:00:26.340: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7549 /apis/apps/v1/namespaces/deployment-7549/replicasets/test-cleanup-controller f90e6208-8971-45c7-861e-e9661dcb2139 102175 1 2020-03-23 17:00:20 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 0b4fe6eb-50be-4746-86da-d9f684becd21 0xc0024c9f07 0xc0024c9f08}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0024c9f68 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 23 17:00:26.372: INFO: Pod "test-cleanup-controller-4m9wr" is available:
&Pod{ObjectMeta:{test-cleanup-controller-4m9wr test-cleanup-controller- deployment-7549 /api/v1/namespaces/deployment-7549/pods/test-cleanup-controller-4m9wr 5312ed64-0a06-47a7-ad4f-776562021135 102158 0 2020-03-23 17:00:21 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/podIP:172.30.197.133/32 cni.projectcalico.org/podIPs:172.30.197.133/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.197.133"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-cleanup-controller f90e6208-8971-45c7-861e-e9661dcb2139 0xc002dae4a7 0xc002dae4a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-65k4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-65k4d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-65k4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.181,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 17:00:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 17:00:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 17:00:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 17:00:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.181,PodIP:172.30.197.133,StartTime:2020-03-23 17:00:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 17:00:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://aa6a3b0b9bd67282f6b088d5912717947e429701d3b848a654d707e7aebdd9ab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.197.133,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:00:26.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7549" for this suite.
Mar 23 17:00:38.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:00:41.492: INFO: namespace deployment-7549 deletion completed in 15.067751188s

• [SLOW TEST:21.187 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:00:41.492: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7315
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7315
STEP: Creating statefulset with conflicting port in namespace statefulset-7315
STEP: Waiting until pod test-pod will start running in namespace statefulset-7315
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7315
Mar 23 17:00:46.227: INFO: Observed stateful pod in namespace: statefulset-7315, name: ss-0, uid: 2d0f4e10-8a4e-4fcc-9cbd-13a61895e7a8, status phase: Failed. Waiting for statefulset controller to delete.
Mar 23 17:00:46.244: INFO: Observed stateful pod in namespace: statefulset-7315, name: ss-0, uid: 2d0f4e10-8a4e-4fcc-9cbd-13a61895e7a8, status phase: Failed. Waiting for statefulset controller to delete.
Mar 23 17:00:46.285: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7315
STEP: Removing pod with conflicting port in namespace statefulset-7315
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7315 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 23 17:00:50.567: INFO: Deleting all statefulset in ns statefulset-7315
Mar 23 17:00:50.584: INFO: Scaling statefulset ss to 0
Mar 23 17:01:00.979: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 17:01:00.998: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:01:01.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7315" for this suite.
Mar 23 17:01:19.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:01:22.001: INFO: namespace statefulset-7315 deletion completed in 20.813164107s

• [SLOW TEST:40.509 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:01:22.001: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Mar 23 17:01:22.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 create -f - --namespace=kubectl-1855'
Mar 23 17:01:23.366: INFO: stderr: ""
Mar 23 17:01:23.366: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 23 17:01:23.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1855'
Mar 23 17:01:23.525: INFO: stderr: ""
Mar 23 17:01:23.525: INFO: stdout: "update-demo-nautilus-xqgkd "
STEP: Replicas for name=update-demo: expected=2 actual=1
Mar 23 17:01:28.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1855'
Mar 23 17:01:28.705: INFO: stderr: ""
Mar 23 17:01:28.705: INFO: stdout: "update-demo-nautilus-hdlq2 update-demo-nautilus-xqgkd "
Mar 23 17:01:28.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-hdlq2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1855'
Mar 23 17:01:28.874: INFO: stderr: ""
Mar 23 17:01:28.874: INFO: stdout: "true"
Mar 23 17:01:28.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-hdlq2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1855'
Mar 23 17:01:29.084: INFO: stderr: ""
Mar 23 17:01:29.084: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 17:01:29.084: INFO: validating pod update-demo-nautilus-hdlq2
Mar 23 17:01:29.142: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 17:01:29.142: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 17:01:29.142: INFO: update-demo-nautilus-hdlq2 is verified up and running
Mar 23 17:01:29.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-xqgkd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1855'
Mar 23 17:01:29.274: INFO: stderr: ""
Mar 23 17:01:29.274: INFO: stdout: "true"
Mar 23 17:01:29.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-xqgkd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1855'
Mar 23 17:01:29.503: INFO: stderr: ""
Mar 23 17:01:29.503: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 17:01:29.503: INFO: validating pod update-demo-nautilus-xqgkd
Mar 23 17:01:29.553: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 17:01:29.553: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 17:01:29.553: INFO: update-demo-nautilus-xqgkd is verified up and running
STEP: using delete to clean up resources
Mar 23 17:01:29.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete --grace-period=0 --force -f - --namespace=kubectl-1855'
Mar 23 17:01:29.710: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 17:01:29.710: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 23 17:01:29.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1855'
Mar 23 17:01:29.892: INFO: stderr: "No resources found in kubectl-1855 namespace.\n"
Mar 23 17:01:29.892: INFO: stdout: ""
Mar 23 17:01:29.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -l name=update-demo --namespace=kubectl-1855 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 23 17:01:30.055: INFO: stderr: ""
Mar 23 17:01:30.055: INFO: stdout: "update-demo-nautilus-hdlq2\nupdate-demo-nautilus-xqgkd\n"
Mar 23 17:01:30.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1855'
Mar 23 17:01:30.811: INFO: stderr: "No resources found in kubectl-1855 namespace.\n"
Mar 23 17:01:30.811: INFO: stdout: ""
Mar 23 17:01:30.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -l name=update-demo --namespace=kubectl-1855 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 23 17:01:30.954: INFO: stderr: ""
Mar 23 17:01:30.954: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:01:30.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1855" for this suite.
Mar 23 17:01:53.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:01:56.168: INFO: namespace kubectl-1855 deletion completed in 25.138725753s

• [SLOW TEST:34.167 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:01:56.169: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 17:01:56.896: INFO: Waiting up to 5m0s for pod "downwardapi-volume-550f8365-7f72-4255-bdba-75d20284eac9" in namespace "downward-api-6882" to be "success or failure"
Mar 23 17:01:56.936: INFO: Pod "downwardapi-volume-550f8365-7f72-4255-bdba-75d20284eac9": Phase="Pending", Reason="", readiness=false. Elapsed: 39.476387ms
Mar 23 17:01:58.958: INFO: Pod "downwardapi-volume-550f8365-7f72-4255-bdba-75d20284eac9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061860567s
Mar 23 17:02:00.983: INFO: Pod "downwardapi-volume-550f8365-7f72-4255-bdba-75d20284eac9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.086540647s
STEP: Saw pod success
Mar 23 17:02:00.983: INFO: Pod "downwardapi-volume-550f8365-7f72-4255-bdba-75d20284eac9" satisfied condition "success or failure"
Mar 23 17:02:01.008: INFO: Trying to get logs from node 10.241.69.181 pod downwardapi-volume-550f8365-7f72-4255-bdba-75d20284eac9 container client-container: <nil>
STEP: delete the pod
Mar 23 17:02:01.217: INFO: Waiting for pod downwardapi-volume-550f8365-7f72-4255-bdba-75d20284eac9 to disappear
Mar 23 17:02:01.246: INFO: Pod downwardapi-volume-550f8365-7f72-4255-bdba-75d20284eac9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:02:01.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6882" for this suite.
Mar 23 17:02:13.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:02:16.423: INFO: namespace downward-api-6882 deletion completed in 15.099720105s

• [SLOW TEST:20.254 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:02:16.423: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:02:21.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-127" for this suite.
Mar 23 17:03:15.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:03:18.868: INFO: namespace kubelet-test-127 deletion completed in 57.388129247s

• [SLOW TEST:62.445 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:03:18.872: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4627
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Mar 23 17:03:19.341: INFO: Found 0 stateful pods, waiting for 3
Mar 23 17:03:29.395: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 17:03:29.395: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 17:03:29.395: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 17:03:29.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-4627 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 17:03:30.249: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 17:03:30.249: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 17:03:30.249: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Mar 23 17:03:40.425: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Mar 23 17:03:50.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-4627 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 17:03:51.051: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 17:03:51.051: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 17:03:51.051: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 17:04:21.213: INFO: Waiting for StatefulSet statefulset-4627/ss2 to complete update
Mar 23 17:04:21.214: INFO: Waiting for Pod statefulset-4627/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Mar 23 17:04:31.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-4627 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 17:04:31.824: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 17:04:31.824: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 17:04:31.824: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 17:04:41.967: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Mar 23 17:04:52.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=statefulset-4627 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 17:04:52.849: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 17:04:52.849: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 17:04:52.849: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 17:05:03.029: INFO: Waiting for StatefulSet statefulset-4627/ss2 to complete update
Mar 23 17:05:03.029: INFO: Waiting for Pod statefulset-4627/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 23 17:05:03.029: INFO: Waiting for Pod statefulset-4627/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 23 17:05:03.029: INFO: Waiting for Pod statefulset-4627/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 23 17:05:13.079: INFO: Waiting for StatefulSet statefulset-4627/ss2 to complete update
Mar 23 17:05:13.079: INFO: Waiting for Pod statefulset-4627/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Mar 23 17:05:23.092: INFO: Waiting for StatefulSet statefulset-4627/ss2 to complete update
Mar 23 17:05:23.093: INFO: Waiting for Pod statefulset-4627/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Mar 23 17:05:33.093: INFO: Deleting all statefulset in ns statefulset-4627
Mar 23 17:05:33.118: INFO: Scaling statefulset ss2 to 0
Mar 23 17:06:13.227: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 17:06:13.246: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:06:13.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4627" for this suite.
Mar 23 17:06:27.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:06:30.428: INFO: namespace statefulset-4627 deletion completed in 16.980091038s

• [SLOW TEST:191.557 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:06:30.433: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 23 17:06:32.390: INFO: Waiting up to 5m0s for pod "pod-0789b77f-1f95-4e5c-ab76-c48ded106cac" in namespace "emptydir-8125" to be "success or failure"
Mar 23 17:06:32.418: INFO: Pod "pod-0789b77f-1f95-4e5c-ab76-c48ded106cac": Phase="Pending", Reason="", readiness=false. Elapsed: 27.54458ms
Mar 23 17:06:34.457: INFO: Pod "pod-0789b77f-1f95-4e5c-ab76-c48ded106cac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.066712643s
STEP: Saw pod success
Mar 23 17:06:34.457: INFO: Pod "pod-0789b77f-1f95-4e5c-ab76-c48ded106cac" satisfied condition "success or failure"
Mar 23 17:06:34.484: INFO: Trying to get logs from node 10.241.69.181 pod pod-0789b77f-1f95-4e5c-ab76-c48ded106cac container test-container: <nil>
STEP: delete the pod
Mar 23 17:06:34.677: INFO: Waiting for pod pod-0789b77f-1f95-4e5c-ab76-c48ded106cac to disappear
Mar 23 17:06:34.703: INFO: Pod pod-0789b77f-1f95-4e5c-ab76-c48ded106cac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:06:34.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8125" for this suite.
Mar 23 17:06:47.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:06:49.997: INFO: namespace emptydir-8125 deletion completed in 15.008375371s

• [SLOW TEST:19.565 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:06:50.002: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:06:50.561: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 23 17:06:55.586: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Mar 23 17:06:55.586: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 23 17:06:57.614: INFO: Creating deployment "test-rollover-deployment"
Mar 23 17:06:57.651: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 23 17:06:59.684: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 23 17:06:59.731: INFO: Ensure that both replica sets have 1 created replica
Mar 23 17:06:59.781: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 23 17:06:59.823: INFO: Updating deployment test-rollover-deployment
Mar 23 17:06:59.823: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 23 17:07:01.863: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 23 17:07:01.916: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 23 17:07:02.009: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 17:07:02.009: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580020, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 17:07:04.073: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 17:07:04.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580022, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 17:07:06.041: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 17:07:06.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580022, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 17:07:08.073: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 17:07:08.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580022, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 17:07:10.054: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 17:07:10.054: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580022, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 17:07:12.050: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 17:07:12.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580022, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580017, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 17:07:14.045: INFO: 
Mar 23 17:07:14.045: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Mar 23 17:07:14.097: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1967 /apis/apps/v1/namespaces/deployment-1967/deployments/test-rollover-deployment 86f838a9-fc2f-4afa-b916-923e8725bb30 105198 2 2020-03-23 17:06:57 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002b5be28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-03-23 17:06:57 +0000 UTC,LastTransitionTime:2020-03-23 17:06:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-03-23 17:07:12 +0000 UTC,LastTransitionTime:2020-03-23 17:06:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 23 17:07:14.117: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-1967 /apis/apps/v1/namespaces/deployment-1967/replicasets/test-rollover-deployment-7d7dc6548c bfb73902-fbbe-440a-aa68-697cf3f411b7 105186 2 2020-03-23 17:06:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 86f838a9-fc2f-4afa-b916-923e8725bb30 0xc002f95d67 0xc002f95d68}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002f95dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 23 17:07:14.117: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 23 17:07:14.117: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1967 /apis/apps/v1/namespaces/deployment-1967/replicasets/test-rollover-controller ed4349cd-c4e5-461a-b663-afc232cc4f58 105197 2 2020-03-23 17:06:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 86f838a9-fc2f-4afa-b916-923e8725bb30 0xc002f95c97 0xc002f95c98}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002f95cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 17:07:14.117: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-1967 /apis/apps/v1/namespaces/deployment-1967/replicasets/test-rollover-deployment-f6c94f66c 2941b38e-9bdb-4f4c-bf99-d96050d5975b 105105 2 2020-03-23 17:06:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 86f838a9-fc2f-4afa-b916-923e8725bb30 0xc002f95e30 0xc002f95e31}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002f95ea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 17:07:14.138: INFO: Pod "test-rollover-deployment-7d7dc6548c-mmz9s" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-mmz9s test-rollover-deployment-7d7dc6548c- deployment-1967 /api/v1/namespaces/deployment-1967/pods/test-rollover-deployment-7d7dc6548c-mmz9s 920050f6-2cc2-40d7-a7e0-496158d30990 105136 0 2020-03-23 17:06:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:172.30.197.129/32 cni.projectcalico.org/podIPs:172.30.197.129/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.197.129"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c bfb73902-fbbe-440a-aa68-697cf3f411b7 0xc001cc6427 0xc001cc6428}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-j68h2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-j68h2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-j68h2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.241.69.181,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-jz9m6,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 17:07:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 17:07:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 17:07:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-03-23 17:07:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.241.69.181,PodIP:172.30.197.129,StartTime:2020-03-23 17:07:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-03-23 17:07:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://bd57705b87c740d9422181176d9065ddc2267968e5e9e7067b18608bfd4b3b7a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.197.129,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:07:14.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1967" for this suite.
Mar 23 17:07:26.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:07:28.997: INFO: namespace deployment-1967 deletion completed in 14.800811744s

• [SLOW TEST:38.996 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:07:28.998: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-1560
STEP: creating replication controller nodeport-test in namespace services-1560
I0323 17:07:29.662688      22 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-1560, replica count: 2
I0323 17:07:32.713205      22 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 17:07:35.713: INFO: Creating new exec pod
I0323 17:07:35.713504      22 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 17:07:38.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=services-1560 execpod2lh5d -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Mar 23 17:07:39.532: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 23 17:07:39.532: INFO: stdout: ""
Mar 23 17:07:39.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=services-1560 execpod2lh5d -- /bin/sh -x -c nc -zv -t -w 2 172.21.27.197 80'
Mar 23 17:07:39.983: INFO: stderr: "+ nc -zv -t -w 2 172.21.27.197 80\nConnection to 172.21.27.197 80 port [tcp/http] succeeded!\n"
Mar 23 17:07:39.983: INFO: stdout: ""
Mar 23 17:07:39.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=services-1560 execpod2lh5d -- /bin/sh -x -c nc -zv -t -w 2 10.241.69.172 31213'
Mar 23 17:07:40.507: INFO: stderr: "+ nc -zv -t -w 2 10.241.69.172 31213\nConnection to 10.241.69.172 31213 port [tcp/31213] succeeded!\n"
Mar 23 17:07:40.507: INFO: stdout: ""
Mar 23 17:07:40.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=services-1560 execpod2lh5d -- /bin/sh -x -c nc -zv -t -w 2 10.241.69.181 31213'
Mar 23 17:07:41.143: INFO: stderr: "+ nc -zv -t -w 2 10.241.69.181 31213\nConnection to 10.241.69.181 31213 port [tcp/31213] succeeded!\n"
Mar 23 17:07:41.143: INFO: stdout: ""
Mar 23 17:07:41.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=services-1560 execpod2lh5d -- /bin/sh -x -c nc -zv -t -w 2 169.59.196.52 31213'
Mar 23 17:07:41.597: INFO: stderr: "+ nc -zv -t -w 2 169.59.196.52 31213\nConnection to 169.59.196.52 31213 port [tcp/31213] succeeded!\n"
Mar 23 17:07:41.597: INFO: stdout: ""
Mar 23 17:07:41.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 exec --namespace=services-1560 execpod2lh5d -- /bin/sh -x -c nc -zv -t -w 2 169.59.196.53 31213'
Mar 23 17:07:42.175: INFO: stderr: "+ nc -zv -t -w 2 169.59.196.53 31213\nConnection to 169.59.196.53 31213 port [tcp/31213] succeeded!\n"
Mar 23 17:07:42.175: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:07:42.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1560" for this suite.
Mar 23 17:07:54.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:07:56.918: INFO: namespace services-1560 deletion completed in 14.696119602s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:27.921 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:07:56.919: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Mar 23 17:07:57.399: INFO: Waiting up to 5m0s for pod "var-expansion-10b9a9b8-ddee-4be2-9dfa-0501cbbc8471" in namespace "var-expansion-5019" to be "success or failure"
Mar 23 17:07:57.432: INFO: Pod "var-expansion-10b9a9b8-ddee-4be2-9dfa-0501cbbc8471": Phase="Pending", Reason="", readiness=false. Elapsed: 32.670515ms
Mar 23 17:07:59.459: INFO: Pod "var-expansion-10b9a9b8-ddee-4be2-9dfa-0501cbbc8471": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059422402s
Mar 23 17:08:01.498: INFO: Pod "var-expansion-10b9a9b8-ddee-4be2-9dfa-0501cbbc8471": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.098722898s
STEP: Saw pod success
Mar 23 17:08:01.498: INFO: Pod "var-expansion-10b9a9b8-ddee-4be2-9dfa-0501cbbc8471" satisfied condition "success or failure"
Mar 23 17:08:01.544: INFO: Trying to get logs from node 10.241.69.181 pod var-expansion-10b9a9b8-ddee-4be2-9dfa-0501cbbc8471 container dapi-container: <nil>
STEP: delete the pod
Mar 23 17:08:01.906: INFO: Waiting for pod var-expansion-10b9a9b8-ddee-4be2-9dfa-0501cbbc8471 to disappear
Mar 23 17:08:01.944: INFO: Pod var-expansion-10b9a9b8-ddee-4be2-9dfa-0501cbbc8471 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:08:01.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5019" for this suite.
Mar 23 17:08:14.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:08:16.653: INFO: namespace var-expansion-5019 deletion completed in 14.64551058s

• [SLOW TEST:19.734 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:08:16.653: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:08:17.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1943" for this suite.
Mar 23 17:08:29.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:08:31.876: INFO: namespace services-1943 deletion completed in 14.71790744s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:15.223 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:08:31.877: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:08:32.953: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Mar 23 17:08:33.029: INFO: Number of nodes with available pods: 0
Mar 23 17:08:33.029: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Mar 23 17:08:33.326: INFO: Number of nodes with available pods: 0
Mar 23 17:08:33.326: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:08:34.347: INFO: Number of nodes with available pods: 0
Mar 23 17:08:34.347: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:08:35.353: INFO: Number of nodes with available pods: 0
Mar 23 17:08:35.353: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:08:36.346: INFO: Number of nodes with available pods: 1
Mar 23 17:08:36.346: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Mar 23 17:08:36.491: INFO: Number of nodes with available pods: 1
Mar 23 17:08:36.491: INFO: Number of running nodes: 0, number of available pods: 1
Mar 23 17:08:37.510: INFO: Number of nodes with available pods: 0
Mar 23 17:08:37.510: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 23 17:08:37.566: INFO: Number of nodes with available pods: 0
Mar 23 17:08:37.566: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:08:38.760: INFO: Number of nodes with available pods: 0
Mar 23 17:08:38.760: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:08:39.593: INFO: Number of nodes with available pods: 0
Mar 23 17:08:39.593: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:08:40.600: INFO: Number of nodes with available pods: 0
Mar 23 17:08:40.600: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:08:41.599: INFO: Number of nodes with available pods: 0
Mar 23 17:08:41.599: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:08:42.587: INFO: Number of nodes with available pods: 0
Mar 23 17:08:42.587: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:08:43.588: INFO: Number of nodes with available pods: 0
Mar 23 17:08:43.588: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:08:44.599: INFO: Number of nodes with available pods: 0
Mar 23 17:08:44.599: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:08:45.602: INFO: Number of nodes with available pods: 0
Mar 23 17:08:45.602: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:08:46.590: INFO: Number of nodes with available pods: 0
Mar 23 17:08:46.590: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:08:47.591: INFO: Number of nodes with available pods: 1
Mar 23 17:08:47.591: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8762, will wait for the garbage collector to delete the pods
Mar 23 17:08:47.725: INFO: Deleting DaemonSet.extensions daemon-set took: 37.010588ms
Mar 23 17:08:48.225: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.279766ms
Mar 23 17:08:53.967: INFO: Number of nodes with available pods: 0
Mar 23 17:08:53.967: INFO: Number of running nodes: 0, number of available pods: 0
Mar 23 17:08:53.987: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8762/daemonsets","resourceVersion":"106049"},"items":null}

Mar 23 17:08:54.014: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8762/pods","resourceVersion":"106049"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:08:54.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8762" for this suite.
Mar 23 17:09:08.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:09:11.860: INFO: namespace daemonsets-8762 deletion completed in 17.540785824s

• [SLOW TEST:39.984 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:09:11.860: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Mar 23 17:09:12.522: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 23 17:09:17.578: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:09:18.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-747" for this suite.
Mar 23 17:09:29.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:09:31.970: INFO: namespace replication-controller-747 deletion completed in 13.164794881s

• [SLOW TEST:20.110 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:09:31.971: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:09:33.525: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ebb03113-a7d7-4491-ade6-d4effc7596dc" in namespace "security-context-test-8321" to be "success or failure"
Mar 23 17:09:33.558: INFO: Pod "busybox-readonly-false-ebb03113-a7d7-4491-ade6-d4effc7596dc": Phase="Pending", Reason="", readiness=false. Elapsed: 32.119081ms
Mar 23 17:09:35.582: INFO: Pod "busybox-readonly-false-ebb03113-a7d7-4491-ade6-d4effc7596dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057029438s
Mar 23 17:09:37.601: INFO: Pod "busybox-readonly-false-ebb03113-a7d7-4491-ade6-d4effc7596dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.075581369s
Mar 23 17:09:37.601: INFO: Pod "busybox-readonly-false-ebb03113-a7d7-4491-ade6-d4effc7596dc" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:09:37.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8321" for this suite.
Mar 23 17:09:47.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:09:50.588: INFO: namespace security-context-test-8321 deletion completed in 12.948000154s

• [SLOW TEST:18.618 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:09:50.589: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Mar 23 17:09:51.098: INFO: Waiting up to 5m0s for pod "downward-api-db64339b-04c6-4420-9715-9a10b3cfe035" in namespace "downward-api-295" to be "success or failure"
Mar 23 17:09:51.142: INFO: Pod "downward-api-db64339b-04c6-4420-9715-9a10b3cfe035": Phase="Pending", Reason="", readiness=false. Elapsed: 44.032295ms
Mar 23 17:09:53.184: INFO: Pod "downward-api-db64339b-04c6-4420-9715-9a10b3cfe035": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.085831076s
STEP: Saw pod success
Mar 23 17:09:53.184: INFO: Pod "downward-api-db64339b-04c6-4420-9715-9a10b3cfe035" satisfied condition "success or failure"
Mar 23 17:09:53.224: INFO: Trying to get logs from node 10.241.69.181 pod downward-api-db64339b-04c6-4420-9715-9a10b3cfe035 container dapi-container: <nil>
STEP: delete the pod
Mar 23 17:09:53.413: INFO: Waiting for pod downward-api-db64339b-04c6-4420-9715-9a10b3cfe035 to disappear
Mar 23 17:09:53.441: INFO: Pod downward-api-db64339b-04c6-4420-9715-9a10b3cfe035 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:09:53.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-295" for this suite.
Mar 23 17:10:05.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:10:08.832: INFO: namespace downward-api-295 deletion completed in 15.333139114s

• [SLOW TEST:18.243 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:10:08.833: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Mar 23 17:10:19.886: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:10:19.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0323 17:10:19.886548      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-5805" for this suite.
Mar 23 17:10:32.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:10:35.169: INFO: namespace gc-5805 deletion completed in 15.241293398s

• [SLOW TEST:26.337 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:10:35.170: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar 23 17:10:35.609: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 23 17:10:35.850: INFO: Waiting for terminating namespaces to be deleted...
Mar 23 17:10:35.886: INFO: 
Logging pods the kubelet thinks is on node 10.241.69.172 before test
Mar 23 17:10:36.063: INFO: multus-b8pkh from openshift-multus started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container kube-multus ready: true, restart count 0
Mar 23 17:10:36.063: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-03-23 13:28:54 +0000 UTC (3 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container alertmanager ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 	Container config-reloader ready: true, restart count 0
Mar 23 17:10:36.063: INFO: multus-admission-controller-shsp6 from openshift-multus started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar 23 17:10:36.063: INFO: image-registry-684f4746d9-ptcpt from openshift-image-registry started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container registry ready: true, restart count 0
Mar 23 17:10:36.063: INFO: node-ca-c6cdr from openshift-image-registry started at 2020-03-23 13:21:48 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container node-ca ready: true, restart count 0
Mar 23 17:10:36.063: INFO: router-default-58f5c9568-cqkqx from openshift-ingress started at 2020-03-23 13:22:00 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container router ready: true, restart count 0
Mar 23 17:10:36.063: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-03-23 13:29:34 +0000 UTC (7 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 	Container prometheus ready: true, restart count 1
Mar 23 17:10:36.063: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar 23 17:10:36.063: INFO: sonobuoy-e2e-job-b545dbbb0c064966 from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container e2e ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 17:10:36.063: INFO: ibm-master-proxy-static-10.241.69.172 from kube-system started at 2020-03-23 13:20:33 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 	Container pause ready: true, restart count 0
Mar 23 17:10:36.063: INFO: ibm-keepalived-watcher-zqvkl from kube-system started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 23 17:10:36.063: INFO: vpn-774dcbf8fc-mqtz2 from kube-system started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container vpn ready: true, restart count 0
Mar 23 17:10:36.063: INFO: prometheus-adapter-6445ff7666-kwdxl from openshift-monitoring started at 2020-03-23 13:28:03 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar 23 17:10:36.063: INFO: tuned-zh9qp from openshift-cluster-node-tuning-operator started at 2020-03-23 13:21:11 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container tuned ready: true, restart count 0
Mar 23 17:10:36.063: INFO: dns-default-fmdtj from openshift-dns started at 2020-03-23 13:21:48 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container dns ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar 23 17:10:36.063: INFO: redhat-operators-5d5b56f44f-9c86l from openshift-marketplace started at 2020-03-23 14:22:14 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container redhat-operators ready: true, restart count 0
Mar 23 17:10:36.063: INFO: ibm-cloud-provider-ip-169-47-106-243-5c45fc569-dx5w9 from ibm-system started at 2020-03-23 16:42:26 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container ibm-cloud-provider-ip-169-47-106-243 ready: true, restart count 0
Mar 23 17:10:36.063: INFO: calico-node-6f276 from calico-system started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container calico-node ready: true, restart count 0
Mar 23 17:10:36.063: INFO: ibmcloud-block-storage-driver-rxcc5 from kube-system started at 2020-03-23 13:20:43 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 23 17:10:36.063: INFO: calico-typha-7d46744f5d-5mvhh from calico-system started at 2020-03-23 13:21:09 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container calico-typha ready: true, restart count 0
Mar 23 17:10:36.063: INFO: cluster-samples-operator-6b5459b7b9-fwhtx from openshift-cluster-samples-operator started at 2020-03-23 14:57:52 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar 23 17:10:36.063: INFO: thanos-querier-866887d744-whk26 from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (4 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 	Container thanos-querier ready: true, restart count 0
Mar 23 17:10:36.063: INFO: openshift-kube-proxy-lv75h from openshift-kube-proxy started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 17:10:36.063: INFO: node-exporter-2c5jl from openshift-monitoring started at 2020-03-23 13:20:40 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 	Container node-exporter ready: true, restart count 0
Mar 23 17:10:36.063: INFO: grafana-5bdf758c94-j7bvk from openshift-monitoring started at 2020-03-23 13:28:09 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container grafana ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar 23 17:10:36.063: INFO: sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-9fzqx from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Mar 23 17:10:36.063: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 17:10:36.063: INFO: console-5b755cbdcd-7sqjq from openshift-console started at 2020-03-23 13:22:24 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container console ready: true, restart count 0
Mar 23 17:10:36.063: INFO: packageserver-5c7d978cbb-llzxd from openshift-operator-lifecycle-manager started at 2020-03-23 13:31:00 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.063: INFO: 	Container packageserver ready: true, restart count 0
Mar 23 17:10:36.063: INFO: 
Logging pods the kubelet thinks is on node 10.241.69.181 before test
Mar 23 17:10:36.140: INFO: ibm-master-proxy-static-10.241.69.181 from kube-system started at 2020-03-23 13:21:17 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.140: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 23 17:10:36.140: INFO: 	Container pause ready: true, restart count 0
Mar 23 17:10:36.140: INFO: community-operators-c75c8c9f6-jjk8l from openshift-marketplace started at 2020-03-23 16:42:26 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.140: INFO: 	Container community-operators ready: true, restart count 0
Mar 23 17:10:36.140: INFO: sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-m6fcz from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.140: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Mar 23 17:10:36.140: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 17:10:36.140: INFO: calico-typha-7d46744f5d-qnz4h from calico-system started at 2020-03-23 13:23:09 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.140: INFO: 	Container calico-typha ready: true, restart count 0
Mar 23 17:10:36.140: INFO: tuned-n9xl7 from openshift-cluster-node-tuning-operator started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.140: INFO: 	Container tuned ready: true, restart count 0
Mar 23 17:10:36.140: INFO: multus-596gk from openshift-multus started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.140: INFO: 	Container kube-multus ready: true, restart count 0
Mar 23 17:10:36.140: INFO: ibm-keepalived-watcher-2klqj from kube-system started at 2020-03-23 13:21:48 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.140: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 23 17:10:36.140: INFO: node-exporter-k4sbd from openshift-monitoring started at 2020-03-23 13:21:48 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.140: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:10:36.140: INFO: 	Container node-exporter ready: true, restart count 0
Mar 23 17:10:36.140: INFO: multus-admission-controller-k89c8 from openshift-multus started at 2020-03-23 16:43:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.140: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar 23 17:10:36.140: INFO: certified-operators-6998665554-shpdk from openshift-marketplace started at 2020-03-23 16:42:26 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.140: INFO: 	Container certified-operators ready: true, restart count 0
Mar 23 17:10:36.140: INFO: registry-pvc-permissions-gptrd from openshift-image-registry started at 2020-03-23 13:24:16 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.140: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar 23 17:10:36.140: INFO: node-ca-bpcf9 from openshift-image-registry started at 2020-03-23 16:42:37 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.140: INFO: 	Container node-ca ready: true, restart count 0
Mar 23 17:10:36.140: INFO: ibmcloud-block-storage-driver-rwjct from kube-system started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.140: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 23 17:10:36.140: INFO: openshift-kube-proxy-hgttr from openshift-kube-proxy started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.140: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 17:10:36.140: INFO: calico-node-skv44 from calico-system started at 2020-03-23 13:21:48 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.140: INFO: 	Container calico-node ready: true, restart count 0
Mar 23 17:10:36.140: INFO: dns-default-98lms from openshift-dns started at 2020-03-23 16:42:37 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.140: INFO: 	Container dns ready: true, restart count 0
Mar 23 17:10:36.140: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar 23 17:10:36.140: INFO: 
Logging pods the kubelet thinks is on node 10.241.69.184 before test
Mar 23 17:10:36.306: INFO: marketplace-operator-554cffcfd-d5zjt from openshift-marketplace started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar 23 17:10:36.306: INFO: ingress-operator-754bbd5fbf-q5d8c from openshift-ingress-operator started at 2020-03-23 13:20:07 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container ingress-operator ready: true, restart count 0
Mar 23 17:10:36.306: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:10:36.306: INFO: telemeter-client-79bc5978cc-hnfmp from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (3 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:10:36.306: INFO: 	Container reload ready: true, restart count 0
Mar 23 17:10:36.306: INFO: 	Container telemeter-client ready: true, restart count 0
Mar 23 17:10:36.306: INFO: ibmcloud-block-storage-driver-9xhmg from kube-system started at 2020-03-23 13:19:01 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 23 17:10:36.306: INFO: cloud-credential-operator-65466dfbf8-nvzxn from openshift-cloud-credential-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container manager ready: true, restart count 0
Mar 23 17:10:36.306: INFO: network-operator-6f9f45bfbb-2gt5t from openshift-network-operator started at 2020-03-23 13:19:00 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container network-operator ready: true, restart count 0
Mar 23 17:10:36.306: INFO: multus-g6kbd from openshift-multus started at 2020-03-23 13:19:31 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container kube-multus ready: true, restart count 0
Mar 23 17:10:36.306: INFO: downloads-7fdfb77b95-56xsf from openshift-console started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container download-server ready: true, restart count 0
Mar 23 17:10:36.306: INFO: dns-operator-6cc86f84cd-qd8jq from openshift-dns-operator started at 2020-03-23 13:20:08 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container dns-operator ready: true, restart count 0
Mar 23 17:10:36.306: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:10:36.306: INFO: configmap-cabundle-injector-78bbf44c6b-rnztt from openshift-service-ca started at 2020-03-23 13:20:35 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Mar 23 17:10:36.306: INFO: sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-tk2rn from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Mar 23 17:10:36.306: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 17:10:36.306: INFO: ibm-cloud-provider-ip-169-47-106-243-5c45fc569-hq4zt from ibm-system started at 2020-03-23 15:19:18 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container ibm-cloud-provider-ip-169-47-106-243 ready: true, restart count 0
Mar 23 17:10:36.306: INFO: calico-node-jxjt7 from calico-system started at 2020-03-23 13:19:14 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container calico-node ready: true, restart count 0
Mar 23 17:10:36.306: INFO: downloads-7fdfb77b95-668gj from openshift-console started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container download-server ready: true, restart count 0
Mar 23 17:10:36.306: INFO: node-ca-mvtq9 from openshift-image-registry started at 2020-03-23 13:21:49 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container node-ca ready: true, restart count 0
Mar 23 17:10:36.306: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-03-23 14:58:08 +0000 UTC (3 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container alertmanager ready: true, restart count 0
Mar 23 17:10:36.306: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar 23 17:10:36.306: INFO: 	Container config-reloader ready: true, restart count 0
Mar 23 17:10:36.306: INFO: ibm-file-plugin-bd78b44b5-7xzx5 from kube-system started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar 23 17:10:36.306: INFO: service-serving-cert-signer-598cdff956-pxfdm from openshift-service-ca started at 2020-03-23 13:20:35 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Mar 23 17:10:36.306: INFO: openshift-service-catalog-controller-manager-operator-5487z68jd from openshift-service-catalog-controller-manager-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container operator ready: true, restart count 1
Mar 23 17:10:36.306: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-03-23 13:28:39 +0000 UTC (3 container statuses recorded)
Mar 23 17:10:36.306: INFO: 	Container alertmanager ready: true, restart count 0
Mar 23 17:10:36.306: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container config-reloader ready: true, restart count 0
Mar 23 17:10:36.307: INFO: prometheus-operator-54f44d89d8-xfwmx from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar 23 17:10:36.307: INFO: prometheus-adapter-6445ff7666-7lcjj from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar 23 17:10:36.307: INFO: calico-typha-7d46744f5d-c2gxq from calico-system started at 2020-03-23 13:19:14 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container calico-typha ready: true, restart count 1
Mar 23 17:10:36.307: INFO: openshift-kube-proxy-9lb45 from openshift-kube-proxy started at 2020-03-23 13:19:38 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 17:10:36.307: INFO: apiservice-cabundle-injector-6b4f956cf4-rtrzx from openshift-service-ca started at 2020-03-23 13:20:35 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Mar 23 17:10:36.307: INFO: sonobuoy from sonobuoy started at 2020-03-23 14:35:40 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 23 17:10:36.307: INFO: cluster-storage-operator-7f448d8d78-tb4bw from openshift-cluster-storage-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Mar 23 17:10:36.307: INFO: cluster-node-tuning-operator-878d4d68-48859 from openshift-cluster-node-tuning-operator started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar 23 17:10:36.307: INFO: openshift-state-metrics-7479448b69-fsvs4 from openshift-monitoring started at 2020-03-23 13:20:33 +0000 UTC (3 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar 23 17:10:36.307: INFO: dns-default-pvrkk from openshift-dns started at 2020-03-23 13:21:47 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container dns ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar 23 17:10:36.307: INFO: thanos-querier-866887d744-7rdmw from openshift-monitoring started at 2020-03-23 13:29:10 +0000 UTC (4 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container thanos-querier ready: true, restart count 0
Mar 23 17:10:36.307: INFO: ibm-master-proxy-static-10.241.69.184 from kube-system started at 2020-03-23 13:18:49 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container pause ready: true, restart count 0
Mar 23 17:10:36.307: INFO: ibm-keepalived-watcher-xc448 from kube-system started at 2020-03-23 13:18:55 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 23 17:10:36.307: INFO: service-ca-operator-69984d88bd-kcxc5 from openshift-service-ca-operator started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container operator ready: true, restart count 0
Mar 23 17:10:36.307: INFO: calico-kube-controllers-85c89dd878-ddpfj from calico-system started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 23 17:10:36.307: INFO: kube-state-metrics-6bcc97c9d6-vbfvx from openshift-monitoring started at 2020-03-23 13:20:32 +0000 UTC (3 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 23 17:10:36.307: INFO: openshift-service-catalog-apiserver-operator-5845fbf887-jlsvg from openshift-service-catalog-apiserver-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container operator ready: true, restart count 1
Mar 23 17:10:36.307: INFO: catalog-operator-bb7df57d7-xkrxt from openshift-operator-lifecycle-manager started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container catalog-operator ready: true, restart count 0
Mar 23 17:10:36.307: INFO: multus-admission-controller-ckp4z from openshift-multus started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar 23 17:10:36.307: INFO: olm-operator-9d666b5b9-s6wnm from openshift-operator-lifecycle-manager started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container olm-operator ready: true, restart count 0
Mar 23 17:10:36.307: INFO: ibm-storage-watcher-cdf8f7d4c-pnk9m from kube-system started at 2020-03-23 13:20:08 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar 23 17:10:36.307: INFO: cluster-image-registry-operator-5d7d64d769-n6l5f from openshift-image-registry started at 2020-03-23 13:20:07 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Mar 23 17:10:36.307: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-03-23 14:58:09 +0000 UTC (7 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container prometheus ready: true, restart count 1
Mar 23 17:10:36.307: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar 23 17:10:36.307: INFO: tigera-operator-c7555ddb8-fggcz from tigera-operator started at 2020-03-23 13:18:58 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container tigera-operator ready: true, restart count 0
Mar 23 17:10:36.307: INFO: ibmcloud-block-storage-plugin-5c7dcf4bdb-jm274 from kube-system started at 2020-03-23 13:19:01 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar 23 17:10:36.307: INFO: console-operator-597c74c496-7g9c9 from openshift-console-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container console-operator ready: true, restart count 0
Mar 23 17:10:36.307: INFO: node-exporter-xwc46 from openshift-monitoring started at 2020-03-23 13:20:33 +0000 UTC (2 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:10:36.307: INFO: 	Container node-exporter ready: true, restart count 0
Mar 23 17:10:36.307: INFO: tuned-fjdv5 from openshift-cluster-node-tuning-operator started at 2020-03-23 13:21:11 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container tuned ready: true, restart count 0
Mar 23 17:10:36.307: INFO: router-default-58f5c9568-vkljz from openshift-ingress started at 2020-03-23 13:22:01 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container router ready: true, restart count 0
Mar 23 17:10:36.307: INFO: packageserver-5c7d978cbb-s4z4h from openshift-operator-lifecycle-manager started at 2020-03-23 13:30:54 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container packageserver ready: true, restart count 0
Mar 23 17:10:36.307: INFO: cluster-monitoring-operator-5b6ff66676-fjscj from openshift-monitoring started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Mar 23 17:10:36.307: INFO: console-5b755cbdcd-pllqp from openshift-console started at 2020-03-23 13:22:44 +0000 UTC (1 container statuses recorded)
Mar 23 17:10:36.307: INFO: 	Container console ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-007a69a1-5e1c-4911-a950-3400f545210a 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-007a69a1-5e1c-4911-a950-3400f545210a off the node 10.241.69.181
STEP: verifying the node doesn't have the label kubernetes.io/e2e-007a69a1-5e1c-4911-a950-3400f545210a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:10:55.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9375" for this suite.
Mar 23 17:11:21.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:11:24.560: INFO: namespace sched-pred-9375 deletion completed in 29.306702598s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:49.391 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:11:24.561: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-cd9a8871-fdd4-4e8c-952f-c24b17d2e39d
STEP: Creating a pod to test consume secrets
Mar 23 17:11:25.323: INFO: Waiting up to 5m0s for pod "pod-secrets-40a0561b-1345-4c14-a016-1d08f063135b" in namespace "secrets-2291" to be "success or failure"
Mar 23 17:11:25.379: INFO: Pod "pod-secrets-40a0561b-1345-4c14-a016-1d08f063135b": Phase="Pending", Reason="", readiness=false. Elapsed: 55.367337ms
Mar 23 17:11:27.407: INFO: Pod "pod-secrets-40a0561b-1345-4c14-a016-1d08f063135b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.083501819s
Mar 23 17:11:29.542: INFO: Pod "pod-secrets-40a0561b-1345-4c14-a016-1d08f063135b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.218370814s
STEP: Saw pod success
Mar 23 17:11:29.542: INFO: Pod "pod-secrets-40a0561b-1345-4c14-a016-1d08f063135b" satisfied condition "success or failure"
Mar 23 17:11:29.569: INFO: Trying to get logs from node 10.241.69.181 pod pod-secrets-40a0561b-1345-4c14-a016-1d08f063135b container secret-volume-test: <nil>
STEP: delete the pod
Mar 23 17:11:29.809: INFO: Waiting for pod pod-secrets-40a0561b-1345-4c14-a016-1d08f063135b to disappear
Mar 23 17:11:29.842: INFO: Pod pod-secrets-40a0561b-1345-4c14-a016-1d08f063135b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:11:29.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2291" for this suite.
Mar 23 17:11:44.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:11:46.825: INFO: namespace secrets-2291 deletion completed in 16.944256413s

• [SLOW TEST:22.264 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:11:46.826: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Mar 23 17:11:47.205: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Mar 23 17:12:28.735: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
Mar 23 17:12:38.510: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:13:13.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1159" for this suite.
Mar 23 17:13:26.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:13:29.257: INFO: namespace crd-publish-openapi-1159 deletion completed in 15.196064561s

• [SLOW TEST:102.431 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:13:29.259: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 23 17:13:29.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-3155'
Mar 23 17:13:30.082: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 23 17:13:30.082: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Mar 23 17:13:32.199: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-7tnpq]
Mar 23 17:13:32.199: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-7tnpq" in namespace "kubectl-3155" to be "running and ready"
Mar 23 17:13:32.224: INFO: Pod "e2e-test-httpd-rc-7tnpq": Phase="Pending", Reason="", readiness=false. Elapsed: 25.644458ms
Mar 23 17:13:34.255: INFO: Pod "e2e-test-httpd-rc-7tnpq": Phase="Running", Reason="", readiness=true. Elapsed: 2.056468969s
Mar 23 17:13:34.255: INFO: Pod "e2e-test-httpd-rc-7tnpq" satisfied condition "running and ready"
Mar 23 17:13:34.255: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-7tnpq]
Mar 23 17:13:34.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 logs rc/e2e-test-httpd-rc --namespace=kubectl-3155'
Mar 23 17:13:34.978: INFO: stderr: ""
Mar 23 17:13:34.978: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.197.164. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.197.164. Set the 'ServerName' directive globally to suppress this message\n[Mon Mar 23 17:13:32.173302 2020] [mpm_event:notice] [pid 1:tid 140217359944552] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Mon Mar 23 17:13:32.173411 2020] [core:notice] [pid 1:tid 140217359944552] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Mar 23 17:13:34.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete rc e2e-test-httpd-rc --namespace=kubectl-3155'
Mar 23 17:13:35.196: INFO: stderr: ""
Mar 23 17:13:35.196: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:13:35.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3155" for this suite.
Mar 23 17:13:55.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:13:57.980: INFO: namespace kubectl-3155 deletion completed in 22.745472845s

• [SLOW TEST:28.721 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:13:57.980: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 17:13:59.472: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c91ab5a3-b0d2-459d-836c-9f10eba293a8" in namespace "downward-api-569" to be "success or failure"
Mar 23 17:13:59.496: INFO: Pod "downwardapi-volume-c91ab5a3-b0d2-459d-836c-9f10eba293a8": Phase="Pending", Reason="", readiness=false. Elapsed: 23.530011ms
Mar 23 17:14:01.517: INFO: Pod "downwardapi-volume-c91ab5a3-b0d2-459d-836c-9f10eba293a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044331279s
Mar 23 17:14:03.544: INFO: Pod "downwardapi-volume-c91ab5a3-b0d2-459d-836c-9f10eba293a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071746199s
STEP: Saw pod success
Mar 23 17:14:03.544: INFO: Pod "downwardapi-volume-c91ab5a3-b0d2-459d-836c-9f10eba293a8" satisfied condition "success or failure"
Mar 23 17:14:03.570: INFO: Trying to get logs from node 10.241.69.181 pod downwardapi-volume-c91ab5a3-b0d2-459d-836c-9f10eba293a8 container client-container: <nil>
STEP: delete the pod
Mar 23 17:14:03.771: INFO: Waiting for pod downwardapi-volume-c91ab5a3-b0d2-459d-836c-9f10eba293a8 to disappear
Mar 23 17:14:03.804: INFO: Pod downwardapi-volume-c91ab5a3-b0d2-459d-836c-9f10eba293a8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:14:03.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-569" for this suite.
Mar 23 17:14:15.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:14:18.633: INFO: namespace downward-api-569 deletion completed in 14.790868066s

• [SLOW TEST:20.653 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:14:18.633: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Mar 23 17:14:25.400: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0323 17:14:25.400511      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:14:25.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3238" for this suite.
Mar 23 17:14:39.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:14:42.407: INFO: namespace gc-3238 deletion completed in 16.968267061s

• [SLOW TEST:23.774 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:14:42.410: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-l82v
STEP: Creating a pod to test atomic-volume-subpath
Mar 23 17:14:42.847: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-l82v" in namespace "subpath-2345" to be "success or failure"
Mar 23 17:14:42.869: INFO: Pod "pod-subpath-test-configmap-l82v": Phase="Pending", Reason="", readiness=false. Elapsed: 22.054018ms
Mar 23 17:14:44.890: INFO: Pod "pod-subpath-test-configmap-l82v": Phase="Running", Reason="", readiness=true. Elapsed: 2.043484638s
Mar 23 17:14:46.921: INFO: Pod "pod-subpath-test-configmap-l82v": Phase="Running", Reason="", readiness=true. Elapsed: 4.073905555s
Mar 23 17:14:48.940: INFO: Pod "pod-subpath-test-configmap-l82v": Phase="Running", Reason="", readiness=true. Elapsed: 6.093126618s
Mar 23 17:14:50.968: INFO: Pod "pod-subpath-test-configmap-l82v": Phase="Running", Reason="", readiness=true. Elapsed: 8.120893903s
Mar 23 17:14:52.997: INFO: Pod "pod-subpath-test-configmap-l82v": Phase="Running", Reason="", readiness=true. Elapsed: 10.150044504s
Mar 23 17:14:55.024: INFO: Pod "pod-subpath-test-configmap-l82v": Phase="Running", Reason="", readiness=true. Elapsed: 12.176649362s
Mar 23 17:14:57.062: INFO: Pod "pod-subpath-test-configmap-l82v": Phase="Running", Reason="", readiness=true. Elapsed: 14.21460464s
Mar 23 17:14:59.100: INFO: Pod "pod-subpath-test-configmap-l82v": Phase="Running", Reason="", readiness=true. Elapsed: 16.252793174s
Mar 23 17:15:01.123: INFO: Pod "pod-subpath-test-configmap-l82v": Phase="Running", Reason="", readiness=true. Elapsed: 18.276377476s
Mar 23 17:15:03.145: INFO: Pod "pod-subpath-test-configmap-l82v": Phase="Running", Reason="", readiness=true. Elapsed: 20.29851548s
Mar 23 17:15:05.167: INFO: Pod "pod-subpath-test-configmap-l82v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.319842664s
STEP: Saw pod success
Mar 23 17:15:05.167: INFO: Pod "pod-subpath-test-configmap-l82v" satisfied condition "success or failure"
Mar 23 17:15:05.189: INFO: Trying to get logs from node 10.241.69.181 pod pod-subpath-test-configmap-l82v container test-container-subpath-configmap-l82v: <nil>
STEP: delete the pod
Mar 23 17:15:05.352: INFO: Waiting for pod pod-subpath-test-configmap-l82v to disappear
Mar 23 17:15:05.385: INFO: Pod pod-subpath-test-configmap-l82v no longer exists
STEP: Deleting pod pod-subpath-test-configmap-l82v
Mar 23 17:15:05.385: INFO: Deleting pod "pod-subpath-test-configmap-l82v" in namespace "subpath-2345"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:15:05.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2345" for this suite.
Mar 23 17:15:19.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:15:22.489: INFO: namespace subpath-2345 deletion completed in 16.829805771s

• [SLOW TEST:40.080 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:15:22.489: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-a16f8383-7ed0-4f75-a841-8b45185d0763
STEP: Creating a pod to test consume secrets
Mar 23 17:15:23.066: INFO: Waiting up to 5m0s for pod "pod-secrets-af4c8555-1fa9-44bd-bc05-6f17d1809b3e" in namespace "secrets-6213" to be "success or failure"
Mar 23 17:15:23.121: INFO: Pod "pod-secrets-af4c8555-1fa9-44bd-bc05-6f17d1809b3e": Phase="Pending", Reason="", readiness=false. Elapsed: 55.430961ms
Mar 23 17:15:25.150: INFO: Pod "pod-secrets-af4c8555-1fa9-44bd-bc05-6f17d1809b3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.08409175s
Mar 23 17:15:27.169: INFO: Pod "pod-secrets-af4c8555-1fa9-44bd-bc05-6f17d1809b3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.103397453s
STEP: Saw pod success
Mar 23 17:15:27.169: INFO: Pod "pod-secrets-af4c8555-1fa9-44bd-bc05-6f17d1809b3e" satisfied condition "success or failure"
Mar 23 17:15:27.189: INFO: Trying to get logs from node 10.241.69.181 pod pod-secrets-af4c8555-1fa9-44bd-bc05-6f17d1809b3e container secret-volume-test: <nil>
STEP: delete the pod
Mar 23 17:15:27.319: INFO: Waiting for pod pod-secrets-af4c8555-1fa9-44bd-bc05-6f17d1809b3e to disappear
Mar 23 17:15:27.341: INFO: Pod pod-secrets-af4c8555-1fa9-44bd-bc05-6f17d1809b3e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:15:27.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6213" for this suite.
Mar 23 17:15:39.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:15:42.183: INFO: namespace secrets-6213 deletion completed in 14.789009351s

• [SLOW TEST:19.694 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:15:42.183: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3077.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3077.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3077.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3077.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3077.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3077.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3077.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3077.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3077.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3077.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3077.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3077.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3077.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3077.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3077.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3077.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3077.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3077.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 17:15:46.751: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3077.svc.cluster.local from pod dns-3077/dns-test-20fd60e5-6f64-40f4-bb00-f99b964d735d: the server could not find the requested resource (get pods dns-test-20fd60e5-6f64-40f4-bb00-f99b964d735d)
Mar 23 17:15:46.848: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3077.svc.cluster.local from pod dns-3077/dns-test-20fd60e5-6f64-40f4-bb00-f99b964d735d: the server could not find the requested resource (get pods dns-test-20fd60e5-6f64-40f4-bb00-f99b964d735d)
Mar 23 17:15:46.944: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3077.svc.cluster.local from pod dns-3077/dns-test-20fd60e5-6f64-40f4-bb00-f99b964d735d: the server could not find the requested resource (get pods dns-test-20fd60e5-6f64-40f4-bb00-f99b964d735d)
Mar 23 17:15:46.975: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3077.svc.cluster.local from pod dns-3077/dns-test-20fd60e5-6f64-40f4-bb00-f99b964d735d: the server could not find the requested resource (get pods dns-test-20fd60e5-6f64-40f4-bb00-f99b964d735d)
Mar 23 17:15:47.030: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3077.svc.cluster.local from pod dns-3077/dns-test-20fd60e5-6f64-40f4-bb00-f99b964d735d: the server could not find the requested resource (get pods dns-test-20fd60e5-6f64-40f4-bb00-f99b964d735d)
Mar 23 17:15:47.067: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3077.svc.cluster.local from pod dns-3077/dns-test-20fd60e5-6f64-40f4-bb00-f99b964d735d: the server could not find the requested resource (get pods dns-test-20fd60e5-6f64-40f4-bb00-f99b964d735d)
Mar 23 17:15:47.145: INFO: Lookups using dns-3077/dns-test-20fd60e5-6f64-40f4-bb00-f99b964d735d failed for: [wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3077.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3077.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3077.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3077.svc.cluster.local jessie_udp@dns-test-service-2.dns-3077.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3077.svc.cluster.local]

Mar 23 17:15:52.582: INFO: DNS probes using dns-3077/dns-test-20fd60e5-6f64-40f4-bb00-f99b964d735d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:15:52.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3077" for this suite.
Mar 23 17:16:04.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:16:07.608: INFO: namespace dns-3077 deletion completed in 14.813851818s

• [SLOW TEST:25.425 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:16:07.608: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Mar 23 17:16:07.993: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:16:12.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1424" for this suite.
Mar 23 17:16:33.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:16:36.084: INFO: namespace init-container-1424 deletion completed in 23.099108409s

• [SLOW TEST:28.476 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:16:36.084: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 23 17:16:36.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-6998'
Mar 23 17:16:36.614: INFO: stderr: ""
Mar 23 17:16:36.614: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Mar 23 17:16:41.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pod e2e-test-httpd-pod --namespace=kubectl-6998 -o json'
Mar 23 17:16:41.898: INFO: stderr: ""
Mar 23 17:16:41.898: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.30.197.179/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.197.179/32\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.197.179\\\"\\n    ],\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2020-03-23T17:16:36Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6998\",\n        \"resourceVersion\": \"109673\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6998/pods/e2e-test-httpd-pod\",\n        \"uid\": \"dc557e38-f1c5-43d6-9eb6-758fd94c7e01\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-wkkcc\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.241.69.181\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c69,c39\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-wkkcc\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-wkkcc\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-23T17:16:36Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-23T17:16:38Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-23T17:16:38Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-03-23T17:16:36Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://6352809f3cf18aa156a7c931636f1b4b8bdf7153a54bb2900a4a5ac004df1ebe\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-03-23T17:16:38Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.241.69.181\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.197.179\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.197.179\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-03-23T17:16:36Z\"\n    }\n}\n"
STEP: replace the image in the pod
Mar 23 17:16:41.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 replace -f - --namespace=kubectl-6998'
Mar 23 17:16:42.789: INFO: stderr: ""
Mar 23 17:16:42.789: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Mar 23 17:16:42.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete pods e2e-test-httpd-pod --namespace=kubectl-6998'
Mar 23 17:16:47.681: INFO: stderr: ""
Mar 23 17:16:47.681: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:16:47.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6998" for this suite.
Mar 23 17:17:00.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:17:02.775: INFO: namespace kubectl-6998 deletion completed in 15.007360461s

• [SLOW TEST:26.691 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:17:02.775: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 17:17:03.989: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 17:17:06.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580624, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580624, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580624, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720580623, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 17:17:09.151: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:17:09.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-646" for this suite.
Mar 23 17:17:23.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:17:26.711: INFO: namespace webhook-646 deletion completed in 16.84381008s
STEP: Destroying namespace "webhook-646-markers" for this suite.
Mar 23 17:17:40.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:17:43.433: INFO: namespace webhook-646-markers deletion completed in 16.722666935s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:40.909 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:17:43.685: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Mar 23 17:17:44.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=kubectl-4497 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar 23 17:17:48.436: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar 23 17:17:48.436: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:17:50.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4497" for this suite.
Mar 23 17:18:04.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:18:07.541: INFO: namespace kubectl-4497 deletion completed in 16.752869423s

• [SLOW TEST:23.856 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:18:07.542: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Mar 23 17:18:08.080: INFO: Waiting up to 5m0s for pod "pod-e5f2021e-060e-462a-8e12-89aabc118a77" in namespace "emptydir-2282" to be "success or failure"
Mar 23 17:18:08.147: INFO: Pod "pod-e5f2021e-060e-462a-8e12-89aabc118a77": Phase="Pending", Reason="", readiness=false. Elapsed: 67.56638ms
Mar 23 17:18:10.177: INFO: Pod "pod-e5f2021e-060e-462a-8e12-89aabc118a77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.097462672s
Mar 23 17:18:12.265: INFO: Pod "pod-e5f2021e-060e-462a-8e12-89aabc118a77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.18556943s
STEP: Saw pod success
Mar 23 17:18:12.266: INFO: Pod "pod-e5f2021e-060e-462a-8e12-89aabc118a77" satisfied condition "success or failure"
Mar 23 17:18:12.294: INFO: Trying to get logs from node 10.241.69.181 pod pod-e5f2021e-060e-462a-8e12-89aabc118a77 container test-container: <nil>
STEP: delete the pod
Mar 23 17:18:12.506: INFO: Waiting for pod pod-e5f2021e-060e-462a-8e12-89aabc118a77 to disappear
Mar 23 17:18:12.542: INFO: Pod pod-e5f2021e-060e-462a-8e12-89aabc118a77 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:18:12.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2282" for this suite.
Mar 23 17:18:26.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:18:29.469: INFO: namespace emptydir-2282 deletion completed in 16.887929922s

• [SLOW TEST:21.927 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:18:29.469: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Mar 23 17:18:34.864: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3626 pod-service-account-a6015e9f-1277-488a-be97-bd6ee85dfb12 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Mar 23 17:18:35.425: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3626 pod-service-account-a6015e9f-1277-488a-be97-bd6ee85dfb12 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Mar 23 17:18:35.826: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3626 pod-service-account-a6015e9f-1277-488a-be97-bd6ee85dfb12 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:18:36.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3626" for this suite.
Mar 23 17:18:48.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:18:51.155: INFO: namespace svcaccounts-3626 deletion completed in 14.800884338s

• [SLOW TEST:21.686 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:18:51.155: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Mar 23 17:18:51.975: INFO: Number of nodes with available pods: 0
Mar 23 17:18:51.975: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:18:53.135: INFO: Number of nodes with available pods: 0
Mar 23 17:18:53.135: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:18:54.060: INFO: Number of nodes with available pods: 0
Mar 23 17:18:54.060: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:18:55.043: INFO: Number of nodes with available pods: 1
Mar 23 17:18:55.043: INFO: Node 10.241.69.172 is running more than one daemon pod
Mar 23 17:18:56.035: INFO: Number of nodes with available pods: 3
Mar 23 17:18:56.036: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 23 17:18:56.186: INFO: Number of nodes with available pods: 2
Mar 23 17:18:56.186: INFO: Node 10.241.69.181 is running more than one daemon pod
Mar 23 17:18:57.252: INFO: Number of nodes with available pods: 2
Mar 23 17:18:57.252: INFO: Node 10.241.69.181 is running more than one daemon pod
Mar 23 17:18:58.243: INFO: Number of nodes with available pods: 2
Mar 23 17:18:58.243: INFO: Node 10.241.69.181 is running more than one daemon pod
Mar 23 17:18:59.300: INFO: Number of nodes with available pods: 3
Mar 23 17:18:59.300: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1638, will wait for the garbage collector to delete the pods
Mar 23 17:18:59.466: INFO: Deleting DaemonSet.extensions daemon-set took: 42.703127ms
Mar 23 17:19:00.167: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.602389ms
Mar 23 17:19:09.492: INFO: Number of nodes with available pods: 0
Mar 23 17:19:09.492: INFO: Number of running nodes: 0, number of available pods: 0
Mar 23 17:19:09.508: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1638/daemonsets","resourceVersion":"110926"},"items":null}

Mar 23 17:19:09.535: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1638/pods","resourceVersion":"110926"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:19:09.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1638" for this suite.
Mar 23 17:19:21.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:19:24.572: INFO: namespace daemonsets-1638 deletion completed in 14.855504223s

• [SLOW TEST:33.417 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:19:24.572: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-3532a60c-46fa-4eaa-b27e-b144e6d67bf7
STEP: Creating a pod to test consume secrets
Mar 23 17:19:25.353: INFO: Waiting up to 5m0s for pod "pod-secrets-8869de00-a36f-4cc5-94c0-e689094d8c82" in namespace "secrets-9313" to be "success or failure"
Mar 23 17:19:25.423: INFO: Pod "pod-secrets-8869de00-a36f-4cc5-94c0-e689094d8c82": Phase="Pending", Reason="", readiness=false. Elapsed: 70.172696ms
Mar 23 17:19:27.454: INFO: Pod "pod-secrets-8869de00-a36f-4cc5-94c0-e689094d8c82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.100913958s
STEP: Saw pod success
Mar 23 17:19:27.454: INFO: Pod "pod-secrets-8869de00-a36f-4cc5-94c0-e689094d8c82" satisfied condition "success or failure"
Mar 23 17:19:27.485: INFO: Trying to get logs from node 10.241.69.181 pod pod-secrets-8869de00-a36f-4cc5-94c0-e689094d8c82 container secret-env-test: <nil>
STEP: delete the pod
Mar 23 17:19:27.637: INFO: Waiting for pod pod-secrets-8869de00-a36f-4cc5-94c0-e689094d8c82 to disappear
Mar 23 17:19:27.667: INFO: Pod pod-secrets-8869de00-a36f-4cc5-94c0-e689094d8c82 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:19:27.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9313" for this suite.
Mar 23 17:19:37.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:19:40.157: INFO: namespace secrets-9313 deletion completed in 12.455075831s

• [SLOW TEST:15.585 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:19:40.158: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Mar 23 17:19:42.620: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:19:42.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4224" for this suite.
Mar 23 17:19:54.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:19:57.645: INFO: namespace container-runtime-4224 deletion completed in 14.894067069s

• [SLOW TEST:17.488 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:19:57.646: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-5dcaa0a0-3a6d-4101-b5b5-0ac08e2a0007
STEP: Creating a pod to test consume secrets
Mar 23 17:19:58.145: INFO: Waiting up to 5m0s for pod "pod-secrets-897dd1f2-380e-49d9-ac00-ccaf36837bc6" in namespace "secrets-8264" to be "success or failure"
Mar 23 17:19:58.165: INFO: Pod "pod-secrets-897dd1f2-380e-49d9-ac00-ccaf36837bc6": Phase="Pending", Reason="", readiness=false. Elapsed: 20.310681ms
Mar 23 17:20:00.191: INFO: Pod "pod-secrets-897dd1f2-380e-49d9-ac00-ccaf36837bc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046776036s
Mar 23 17:20:02.347: INFO: Pod "pod-secrets-897dd1f2-380e-49d9-ac00-ccaf36837bc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.20257818s
STEP: Saw pod success
Mar 23 17:20:02.347: INFO: Pod "pod-secrets-897dd1f2-380e-49d9-ac00-ccaf36837bc6" satisfied condition "success or failure"
Mar 23 17:20:02.380: INFO: Trying to get logs from node 10.241.69.181 pod pod-secrets-897dd1f2-380e-49d9-ac00-ccaf36837bc6 container secret-volume-test: <nil>
STEP: delete the pod
Mar 23 17:20:02.719: INFO: Waiting for pod pod-secrets-897dd1f2-380e-49d9-ac00-ccaf36837bc6 to disappear
Mar 23 17:20:02.752: INFO: Pod pod-secrets-897dd1f2-380e-49d9-ac00-ccaf36837bc6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:20:02.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8264" for this suite.
Mar 23 17:20:17.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:20:19.664: INFO: namespace secrets-8264 deletion completed in 16.847723081s

• [SLOW TEST:22.019 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:20:19.668: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:20:20.151: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-8e2c9fd6-d55f-4cf5-83db-1c063a250068" in namespace "security-context-test-8700" to be "success or failure"
Mar 23 17:20:20.174: INFO: Pod "busybox-privileged-false-8e2c9fd6-d55f-4cf5-83db-1c063a250068": Phase="Pending", Reason="", readiness=false. Elapsed: 23.091538ms
Mar 23 17:20:22.231: INFO: Pod "busybox-privileged-false-8e2c9fd6-d55f-4cf5-83db-1c063a250068": Phase="Pending", Reason="", readiness=false. Elapsed: 2.08001314s
Mar 23 17:20:24.257: INFO: Pod "busybox-privileged-false-8e2c9fd6-d55f-4cf5-83db-1c063a250068": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.105498755s
Mar 23 17:20:24.257: INFO: Pod "busybox-privileged-false-8e2c9fd6-d55f-4cf5-83db-1c063a250068" satisfied condition "success or failure"
Mar 23 17:20:24.318: INFO: Got logs for pod "busybox-privileged-false-8e2c9fd6-d55f-4cf5-83db-1c063a250068": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:20:24.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8700" for this suite.
Mar 23 17:20:38.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:20:41.619: INFO: namespace security-context-test-8700 deletion completed in 17.245250746s

• [SLOW TEST:21.952 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:20:41.620: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1641.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1641.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1641.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1641.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 17:20:46.299: INFO: DNS probes using dns-test-43f688f4-f447-4972-9d02-14e26ac41aba succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1641.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1641.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1641.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1641.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 17:20:50.694: INFO: File wheezy_udp@dns-test-service-3.dns-1641.svc.cluster.local from pod  dns-1641/dns-test-41bd5a96-607b-4537-80c9-9390d44c5ea7 contains 'foo.example.com.
' instead of 'bar.example.com.'
Mar 23 17:20:50.726: INFO: Lookups using dns-1641/dns-test-41bd5a96-607b-4537-80c9-9390d44c5ea7 failed for: [wheezy_udp@dns-test-service-3.dns-1641.svc.cluster.local]

Mar 23 17:20:55.816: INFO: DNS probes using dns-test-41bd5a96-607b-4537-80c9-9390d44c5ea7 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1641.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1641.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1641.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1641.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 17:21:00.345: INFO: DNS probes using dns-test-ec7e1c89-125a-4714-ba61-b837f905742c succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:21:00.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1641" for this suite.
Mar 23 17:21:14.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:21:17.239: INFO: namespace dns-1641 deletion completed in 16.662768095s

• [SLOW TEST:35.619 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:21:17.241: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:21:17.547: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:21:21.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5123" for this suite.
Mar 23 17:22:20.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:22:22.948: INFO: namespace pods-5123 deletion completed in 1m0.911255045s

• [SLOW TEST:65.708 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:22:22.948: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-b4957c11-a22c-4616-978d-d4264624f69b
STEP: Creating a pod to test consume secrets
Mar 23 17:22:23.771: INFO: Waiting up to 5m0s for pod "pod-secrets-01cdcfde-124d-4609-9299-b181bea93bf6" in namespace "secrets-8235" to be "success or failure"
Mar 23 17:22:23.794: INFO: Pod "pod-secrets-01cdcfde-124d-4609-9299-b181bea93bf6": Phase="Pending", Reason="", readiness=false. Elapsed: 23.359765ms
Mar 23 17:22:25.819: INFO: Pod "pod-secrets-01cdcfde-124d-4609-9299-b181bea93bf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047502957s
Mar 23 17:22:27.882: INFO: Pod "pod-secrets-01cdcfde-124d-4609-9299-b181bea93bf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.110668584s
STEP: Saw pod success
Mar 23 17:22:27.882: INFO: Pod "pod-secrets-01cdcfde-124d-4609-9299-b181bea93bf6" satisfied condition "success or failure"
Mar 23 17:22:27.911: INFO: Trying to get logs from node 10.241.69.181 pod pod-secrets-01cdcfde-124d-4609-9299-b181bea93bf6 container secret-volume-test: <nil>
STEP: delete the pod
Mar 23 17:22:28.217: INFO: Waiting for pod pod-secrets-01cdcfde-124d-4609-9299-b181bea93bf6 to disappear
Mar 23 17:22:28.299: INFO: Pod pod-secrets-01cdcfde-124d-4609-9299-b181bea93bf6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:22:28.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8235" for this suite.
Mar 23 17:22:40.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:22:43.541: INFO: namespace secrets-8235 deletion completed in 15.191308942s
STEP: Destroying namespace "secret-namespace-6750" for this suite.
Mar 23 17:22:55.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:22:58.830: INFO: namespace secret-namespace-6750 deletion completed in 15.289534054s

• [SLOW TEST:35.882 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:22:58.831: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Mar 23 17:22:59.873: INFO: Waiting up to 5m0s for pod "pod-0edf4dfa-3474-436d-abd9-543203c53891" in namespace "emptydir-855" to be "success or failure"
Mar 23 17:23:00.158: INFO: Pod "pod-0edf4dfa-3474-436d-abd9-543203c53891": Phase="Pending", Reason="", readiness=false. Elapsed: 284.591255ms
Mar 23 17:23:02.198: INFO: Pod "pod-0edf4dfa-3474-436d-abd9-543203c53891": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.32495666s
STEP: Saw pod success
Mar 23 17:23:02.198: INFO: Pod "pod-0edf4dfa-3474-436d-abd9-543203c53891" satisfied condition "success or failure"
Mar 23 17:23:02.226: INFO: Trying to get logs from node 10.241.69.181 pod pod-0edf4dfa-3474-436d-abd9-543203c53891 container test-container: <nil>
STEP: delete the pod
Mar 23 17:23:02.362: INFO: Waiting for pod pod-0edf4dfa-3474-436d-abd9-543203c53891 to disappear
Mar 23 17:23:02.392: INFO: Pod pod-0edf4dfa-3474-436d-abd9-543203c53891 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:23:02.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-855" for this suite.
Mar 23 17:23:14.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:23:17.179: INFO: namespace emptydir-855 deletion completed in 14.721071408s

• [SLOW TEST:18.348 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:23:17.179: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:23:17.563: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Mar 23 17:23:26.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-249 create -f -'
Mar 23 17:23:27.352: INFO: stderr: ""
Mar 23 17:23:27.352: INFO: stdout: "e2e-test-crd-publish-openapi-9574-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 23 17:23:27.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-249 delete e2e-test-crd-publish-openapi-9574-crds test-foo'
Mar 23 17:23:27.548: INFO: stderr: ""
Mar 23 17:23:27.548: INFO: stdout: "e2e-test-crd-publish-openapi-9574-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 23 17:23:27.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-249 apply -f -'
Mar 23 17:23:27.927: INFO: stderr: ""
Mar 23 17:23:27.927: INFO: stdout: "e2e-test-crd-publish-openapi-9574-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 23 17:23:27.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-249 delete e2e-test-crd-publish-openapi-9574-crds test-foo'
Mar 23 17:23:28.179: INFO: stderr: ""
Mar 23 17:23:28.179: INFO: stdout: "e2e-test-crd-publish-openapi-9574-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Mar 23 17:23:28.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-249 create -f -'
Mar 23 17:23:28.472: INFO: rc: 1
Mar 23 17:23:28.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-249 apply -f -'
Mar 23 17:23:29.036: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Mar 23 17:23:29.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-249 create -f -'
Mar 23 17:23:29.634: INFO: rc: 1
Mar 23 17:23:29.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 --namespace=crd-publish-openapi-249 apply -f -'
Mar 23 17:23:30.184: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Mar 23 17:23:30.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 explain e2e-test-crd-publish-openapi-9574-crds'
Mar 23 17:23:30.969: INFO: stderr: ""
Mar 23 17:23:30.970: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9574-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Mar 23 17:23:30.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 explain e2e-test-crd-publish-openapi-9574-crds.metadata'
Mar 23 17:23:31.609: INFO: stderr: ""
Mar 23 17:23:31.609: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9574-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 23 17:23:31.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 explain e2e-test-crd-publish-openapi-9574-crds.spec'
Mar 23 17:23:31.971: INFO: stderr: ""
Mar 23 17:23:31.971: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9574-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 23 17:23:31.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 explain e2e-test-crd-publish-openapi-9574-crds.spec.bars'
Mar 23 17:23:32.834: INFO: stderr: ""
Mar 23 17:23:32.834: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9574-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Mar 23 17:23:32.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 explain e2e-test-crd-publish-openapi-9574-crds.spec.bars2'
Mar 23 17:23:33.514: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:23:42.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-249" for this suite.
Mar 23 17:23:54.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:23:57.972: INFO: namespace crd-publish-openapi-249 deletion completed in 15.113111239s

• [SLOW TEST:40.792 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:23:57.972: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Mar 23 17:24:06.860: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 23 17:24:06.888: INFO: Pod pod-with-poststart-http-hook still exists
Mar 23 17:24:08.888: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 23 17:24:09.150: INFO: Pod pod-with-poststart-http-hook still exists
Mar 23 17:24:10.888: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 23 17:24:10.962: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:24:10.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4599" for this suite.
Mar 23 17:24:31.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:24:33.803: INFO: namespace container-lifecycle-hook-4599 deletion completed in 22.787844367s

• [SLOW TEST:35.832 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:24:33.804: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-aea1317b-834c-4583-8001-8bf6db9946fc
STEP: Creating a pod to test consume configMaps
Mar 23 17:24:34.744: INFO: Waiting up to 5m0s for pod "pod-configmaps-ae045d7b-db33-4053-afad-8870ee8ab4df" in namespace "configmap-4606" to be "success or failure"
Mar 23 17:24:34.802: INFO: Pod "pod-configmaps-ae045d7b-db33-4053-afad-8870ee8ab4df": Phase="Pending", Reason="", readiness=false. Elapsed: 58.01197ms
Mar 23 17:24:36.827: INFO: Pod "pod-configmaps-ae045d7b-db33-4053-afad-8870ee8ab4df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.082955579s
Mar 23 17:24:38.853: INFO: Pod "pod-configmaps-ae045d7b-db33-4053-afad-8870ee8ab4df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.108760415s
STEP: Saw pod success
Mar 23 17:24:38.853: INFO: Pod "pod-configmaps-ae045d7b-db33-4053-afad-8870ee8ab4df" satisfied condition "success or failure"
Mar 23 17:24:38.878: INFO: Trying to get logs from node 10.241.69.181 pod pod-configmaps-ae045d7b-db33-4053-afad-8870ee8ab4df container configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 17:24:39.064: INFO: Waiting for pod pod-configmaps-ae045d7b-db33-4053-afad-8870ee8ab4df to disappear
Mar 23 17:24:39.105: INFO: Pod pod-configmaps-ae045d7b-db33-4053-afad-8870ee8ab4df no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:24:39.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4606" for this suite.
Mar 23 17:24:49.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:24:51.959: INFO: namespace configmap-4606 deletion completed in 12.806997154s

• [SLOW TEST:18.155 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:24:51.959: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:24:52.395: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-5f360f62-5ab3-4ff7-ac2d-2610a08c3268
STEP: Creating secret with name s-test-opt-upd-6826ef19-3fc1-456e-b60b-753c01305ba6
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-5f360f62-5ab3-4ff7-ac2d-2610a08c3268
STEP: Updating secret s-test-opt-upd-6826ef19-3fc1-456e-b60b-753c01305ba6
STEP: Creating secret with name s-test-opt-create-7bb55621-54cc-4fb4-b8b3-606c569d3f7c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:26:27.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5608" for this suite.
Mar 23 17:27:01.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:27:04.134: INFO: namespace projected-5608 deletion completed in 36.859793869s

• [SLOW TEST:132.175 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:27:04.134: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1034, will wait for the garbage collector to delete the pods
Mar 23 17:27:08.937: INFO: Deleting Job.batch foo took: 83.21822ms
Mar 23 17:27:09.437: INFO: Terminating Job.batch foo pods took: 500.424628ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:27:47.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1034" for this suite.
Mar 23 17:27:59.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:28:02.937: INFO: namespace job-1034 deletion completed in 15.424831735s

• [SLOW TEST:58.803 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:28:02.937: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:28:03.366: INFO: Creating ReplicaSet my-hostname-basic-c8683b99-6a8a-4649-9714-855784d3d28b
Mar 23 17:28:03.495: INFO: Pod name my-hostname-basic-c8683b99-6a8a-4649-9714-855784d3d28b: Found 0 pods out of 1
Mar 23 17:28:08.525: INFO: Pod name my-hostname-basic-c8683b99-6a8a-4649-9714-855784d3d28b: Found 1 pods out of 1
Mar 23 17:28:08.525: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c8683b99-6a8a-4649-9714-855784d3d28b" is running
Mar 23 17:28:08.551: INFO: Pod "my-hostname-basic-c8683b99-6a8a-4649-9714-855784d3d28b-jl2jg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-23 17:28:03 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-23 17:28:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-23 17:28:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-03-23 17:28:03 +0000 UTC Reason: Message:}])
Mar 23 17:28:08.551: INFO: Trying to dial the pod
Mar 23 17:28:13.684: INFO: Controller my-hostname-basic-c8683b99-6a8a-4649-9714-855784d3d28b: Got expected result from replica 1 [my-hostname-basic-c8683b99-6a8a-4649-9714-855784d3d28b-jl2jg]: "my-hostname-basic-c8683b99-6a8a-4649-9714-855784d3d28b-jl2jg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:28:13.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3002" for this suite.
Mar 23 17:28:27.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:28:30.463: INFO: namespace replicaset-3002 deletion completed in 16.649983712s

• [SLOW TEST:27.526 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:28:30.463: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:28:30.859: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:28:31.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7546" for this suite.
Mar 23 17:28:43.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:28:46.560: INFO: namespace custom-resource-definition-7546 deletion completed in 14.970861292s

• [SLOW TEST:16.097 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:28:46.560: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Mar 23 17:28:47.268: INFO: Waiting up to 5m0s for pod "pod-61878231-54df-4ce2-bf41-af1565a3d932" in namespace "emptydir-8336" to be "success or failure"
Mar 23 17:28:47.331: INFO: Pod "pod-61878231-54df-4ce2-bf41-af1565a3d932": Phase="Pending", Reason="", readiness=false. Elapsed: 63.753603ms
Mar 23 17:28:49.363: INFO: Pod "pod-61878231-54df-4ce2-bf41-af1565a3d932": Phase="Pending", Reason="", readiness=false. Elapsed: 2.095348232s
Mar 23 17:28:51.397: INFO: Pod "pod-61878231-54df-4ce2-bf41-af1565a3d932": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.129710565s
STEP: Saw pod success
Mar 23 17:28:51.397: INFO: Pod "pod-61878231-54df-4ce2-bf41-af1565a3d932" satisfied condition "success or failure"
Mar 23 17:28:51.429: INFO: Trying to get logs from node 10.241.69.181 pod pod-61878231-54df-4ce2-bf41-af1565a3d932 container test-container: <nil>
STEP: delete the pod
Mar 23 17:28:51.821: INFO: Waiting for pod pod-61878231-54df-4ce2-bf41-af1565a3d932 to disappear
Mar 23 17:28:51.870: INFO: Pod pod-61878231-54df-4ce2-bf41-af1565a3d932 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:28:51.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8336" for this suite.
Mar 23 17:29:04.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:29:06.802: INFO: namespace emptydir-8336 deletion completed in 14.870051335s

• [SLOW TEST:20.242 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:29:06.803: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 17:29:07.174: INFO: Waiting up to 5m0s for pod "downwardapi-volume-674b23d0-b2a5-4025-8abf-81ea8687f7a9" in namespace "projected-4629" to be "success or failure"
Mar 23 17:29:07.210: INFO: Pod "downwardapi-volume-674b23d0-b2a5-4025-8abf-81ea8687f7a9": Phase="Pending", Reason="", readiness=false. Elapsed: 35.661007ms
Mar 23 17:29:09.236: INFO: Pod "downwardapi-volume-674b23d0-b2a5-4025-8abf-81ea8687f7a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.06241243s
STEP: Saw pod success
Mar 23 17:29:09.236: INFO: Pod "downwardapi-volume-674b23d0-b2a5-4025-8abf-81ea8687f7a9" satisfied condition "success or failure"
Mar 23 17:29:09.254: INFO: Trying to get logs from node 10.241.69.181 pod downwardapi-volume-674b23d0-b2a5-4025-8abf-81ea8687f7a9 container client-container: <nil>
STEP: delete the pod
Mar 23 17:29:09.398: INFO: Waiting for pod downwardapi-volume-674b23d0-b2a5-4025-8abf-81ea8687f7a9 to disappear
Mar 23 17:29:09.426: INFO: Pod downwardapi-volume-674b23d0-b2a5-4025-8abf-81ea8687f7a9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:29:09.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4629" for this suite.
Mar 23 17:29:19.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:29:21.917: INFO: namespace projected-4629 deletion completed in 12.449436903s

• [SLOW TEST:15.114 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:29:21.918: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:29:22.273: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Creating first CR 
Mar 23 17:29:23.074: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-23T17:29:23Z generation:1 name:name1 resourceVersion:115188 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:5d08abdf-3611-43fc-a61c-238d6f1a215b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Mar 23 17:29:33.110: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-23T17:29:33Z generation:1 name:name2 resourceVersion:115229 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:98905236-93fe-41f5-a2de-f181cd98afe9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Mar 23 17:29:43.144: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-23T17:29:23Z generation:2 name:name1 resourceVersion:115272 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:5d08abdf-3611-43fc-a61c-238d6f1a215b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Mar 23 17:29:53.176: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-23T17:29:33Z generation:2 name:name2 resourceVersion:115313 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:98905236-93fe-41f5-a2de-f181cd98afe9] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Mar 23 17:30:03.365: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-23T17:29:23Z generation:2 name:name1 resourceVersion:115353 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:5d08abdf-3611-43fc-a61c-238d6f1a215b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Mar 23 17:30:13.428: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-03-23T17:29:33Z generation:2 name:name2 resourceVersion:115396 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:98905236-93fe-41f5-a2de-f181cd98afe9] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:30:23.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2741" for this suite.
Mar 23 17:30:36.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:30:39.039: INFO: namespace crd-watch-2741 deletion completed in 14.999788278s

• [SLOW TEST:77.121 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:30:39.039: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 17:30:40.373: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 17:30:42.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720581440, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720581440, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720581440, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720581440, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 17:30:45.511: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:30:45.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5358" for this suite.
Mar 23 17:31:00.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:31:03.378: INFO: namespace webhook-5358 deletion completed in 17.203625641s
STEP: Destroying namespace "webhook-5358-markers" for this suite.
Mar 23 17:31:15.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:31:18.142: INFO: namespace webhook-5358-markers deletion completed in 14.764159016s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:39.238 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:31:18.278: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar 23 17:31:18.657: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 23 17:31:19.285: INFO: Waiting for terminating namespaces to be deleted...
Mar 23 17:31:19.362: INFO: 
Logging pods the kubelet thinks is on node 10.241.69.172 before test
Mar 23 17:31:19.541: INFO: node-ca-c6cdr from openshift-image-registry started at 2020-03-23 13:21:48 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container node-ca ready: true, restart count 0
Mar 23 17:31:19.541: INFO: router-default-58f5c9568-cqkqx from openshift-ingress started at 2020-03-23 13:22:00 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container router ready: true, restart count 0
Mar 23 17:31:19.541: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-03-23 13:29:34 +0000 UTC (7 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 	Container prometheus ready: true, restart count 1
Mar 23 17:31:19.541: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar 23 17:31:19.541: INFO: sonobuoy-e2e-job-b545dbbb0c064966 from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container e2e ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 17:31:19.541: INFO: ibm-master-proxy-static-10.241.69.172 from kube-system started at 2020-03-23 13:20:33 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 	Container pause ready: true, restart count 0
Mar 23 17:31:19.541: INFO: ibm-keepalived-watcher-zqvkl from kube-system started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 23 17:31:19.541: INFO: vpn-774dcbf8fc-mqtz2 from kube-system started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container vpn ready: true, restart count 0
Mar 23 17:31:19.541: INFO: prometheus-adapter-6445ff7666-kwdxl from openshift-monitoring started at 2020-03-23 13:28:03 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar 23 17:31:19.541: INFO: tuned-zh9qp from openshift-cluster-node-tuning-operator started at 2020-03-23 13:21:11 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container tuned ready: true, restart count 0
Mar 23 17:31:19.541: INFO: dns-default-fmdtj from openshift-dns started at 2020-03-23 13:21:48 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container dns ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar 23 17:31:19.541: INFO: redhat-operators-5d5b56f44f-9c86l from openshift-marketplace started at 2020-03-23 14:22:14 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container redhat-operators ready: true, restart count 0
Mar 23 17:31:19.541: INFO: ibm-cloud-provider-ip-169-47-106-243-5c45fc569-dx5w9 from ibm-system started at 2020-03-23 16:42:26 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container ibm-cloud-provider-ip-169-47-106-243 ready: true, restart count 0
Mar 23 17:31:19.541: INFO: calico-node-6f276 from calico-system started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container calico-node ready: true, restart count 0
Mar 23 17:31:19.541: INFO: ibmcloud-block-storage-driver-rxcc5 from kube-system started at 2020-03-23 13:20:43 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 23 17:31:19.541: INFO: calico-typha-7d46744f5d-5mvhh from calico-system started at 2020-03-23 13:21:09 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container calico-typha ready: true, restart count 0
Mar 23 17:31:19.541: INFO: cluster-samples-operator-6b5459b7b9-fwhtx from openshift-cluster-samples-operator started at 2020-03-23 14:57:52 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar 23 17:31:19.541: INFO: thanos-querier-866887d744-whk26 from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (4 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 	Container thanos-querier ready: true, restart count 0
Mar 23 17:31:19.541: INFO: openshift-kube-proxy-lv75h from openshift-kube-proxy started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 17:31:19.541: INFO: node-exporter-2c5jl from openshift-monitoring started at 2020-03-23 13:20:40 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 	Container node-exporter ready: true, restart count 0
Mar 23 17:31:19.541: INFO: grafana-5bdf758c94-j7bvk from openshift-monitoring started at 2020-03-23 13:28:09 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container grafana ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar 23 17:31:19.541: INFO: sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-9fzqx from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Mar 23 17:31:19.541: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 17:31:19.541: INFO: console-5b755cbdcd-7sqjq from openshift-console started at 2020-03-23 13:22:24 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container console ready: true, restart count 0
Mar 23 17:31:19.541: INFO: packageserver-5c7d978cbb-llzxd from openshift-operator-lifecycle-manager started at 2020-03-23 13:31:00 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container packageserver ready: true, restart count 0
Mar 23 17:31:19.541: INFO: multus-b8pkh from openshift-multus started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container kube-multus ready: true, restart count 0
Mar 23 17:31:19.541: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-03-23 13:28:54 +0000 UTC (3 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container alertmanager ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 	Container config-reloader ready: true, restart count 0
Mar 23 17:31:19.541: INFO: multus-admission-controller-shsp6 from openshift-multus started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar 23 17:31:19.541: INFO: image-registry-684f4746d9-ptcpt from openshift-image-registry started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.541: INFO: 	Container registry ready: true, restart count 0
Mar 23 17:31:19.541: INFO: 
Logging pods the kubelet thinks is on node 10.241.69.181 before test
Mar 23 17:31:19.702: INFO: multus-admission-controller-k89c8 from openshift-multus started at 2020-03-23 16:43:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.702: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar 23 17:31:19.702: INFO: certified-operators-6998665554-shpdk from openshift-marketplace started at 2020-03-23 16:42:26 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.702: INFO: 	Container certified-operators ready: true, restart count 0
Mar 23 17:31:19.702: INFO: registry-pvc-permissions-gptrd from openshift-image-registry started at 2020-03-23 13:24:16 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.702: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar 23 17:31:19.702: INFO: node-ca-bpcf9 from openshift-image-registry started at 2020-03-23 16:42:37 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.702: INFO: 	Container node-ca ready: true, restart count 0
Mar 23 17:31:19.702: INFO: ibmcloud-block-storage-driver-rwjct from kube-system started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.702: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 23 17:31:19.702: INFO: openshift-kube-proxy-hgttr from openshift-kube-proxy started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.703: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 17:31:19.703: INFO: calico-node-skv44 from calico-system started at 2020-03-23 13:21:48 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.703: INFO: 	Container calico-node ready: true, restart count 0
Mar 23 17:31:19.703: INFO: dns-default-98lms from openshift-dns started at 2020-03-23 16:42:37 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.703: INFO: 	Container dns ready: true, restart count 0
Mar 23 17:31:19.703: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar 23 17:31:19.703: INFO: ibm-master-proxy-static-10.241.69.181 from kube-system started at 2020-03-23 13:21:17 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.703: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 23 17:31:19.703: INFO: 	Container pause ready: true, restart count 0
Mar 23 17:31:19.703: INFO: community-operators-c75c8c9f6-jjk8l from openshift-marketplace started at 2020-03-23 16:42:26 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.703: INFO: 	Container community-operators ready: true, restart count 0
Mar 23 17:31:19.703: INFO: sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-m6fcz from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.703: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Mar 23 17:31:19.703: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 17:31:19.703: INFO: tuned-n9xl7 from openshift-cluster-node-tuning-operator started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.703: INFO: 	Container tuned ready: true, restart count 0
Mar 23 17:31:19.704: INFO: multus-596gk from openshift-multus started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.704: INFO: 	Container kube-multus ready: true, restart count 0
Mar 23 17:31:19.704: INFO: ibm-keepalived-watcher-2klqj from kube-system started at 2020-03-23 13:21:48 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.704: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 23 17:31:19.704: INFO: node-exporter-k4sbd from openshift-monitoring started at 2020-03-23 13:21:48 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.704: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:31:19.704: INFO: 	Container node-exporter ready: true, restart count 0
Mar 23 17:31:19.704: INFO: calico-typha-7d46744f5d-qnz4h from calico-system started at 2020-03-23 13:23:09 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.704: INFO: 	Container calico-typha ready: true, restart count 0
Mar 23 17:31:19.704: INFO: 
Logging pods the kubelet thinks is on node 10.241.69.184 before test
Mar 23 17:31:19.878: INFO: tigera-operator-c7555ddb8-fggcz from tigera-operator started at 2020-03-23 13:18:58 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container tigera-operator ready: true, restart count 0
Mar 23 17:31:19.878: INFO: ibmcloud-block-storage-plugin-5c7dcf4bdb-jm274 from kube-system started at 2020-03-23 13:19:01 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar 23 17:31:19.878: INFO: console-operator-597c74c496-7g9c9 from openshift-console-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container console-operator ready: true, restart count 0
Mar 23 17:31:19.878: INFO: node-exporter-xwc46 from openshift-monitoring started at 2020-03-23 13:20:33 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:31:19.878: INFO: 	Container node-exporter ready: true, restart count 0
Mar 23 17:31:19.878: INFO: tuned-fjdv5 from openshift-cluster-node-tuning-operator started at 2020-03-23 13:21:11 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container tuned ready: true, restart count 0
Mar 23 17:31:19.878: INFO: router-default-58f5c9568-vkljz from openshift-ingress started at 2020-03-23 13:22:01 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container router ready: true, restart count 0
Mar 23 17:31:19.878: INFO: packageserver-5c7d978cbb-s4z4h from openshift-operator-lifecycle-manager started at 2020-03-23 13:30:54 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container packageserver ready: true, restart count 0
Mar 23 17:31:19.878: INFO: cluster-monitoring-operator-5b6ff66676-fjscj from openshift-monitoring started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Mar 23 17:31:19.878: INFO: console-5b755cbdcd-pllqp from openshift-console started at 2020-03-23 13:22:44 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container console ready: true, restart count 0
Mar 23 17:31:19.878: INFO: marketplace-operator-554cffcfd-d5zjt from openshift-marketplace started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar 23 17:31:19.878: INFO: ingress-operator-754bbd5fbf-q5d8c from openshift-ingress-operator started at 2020-03-23 13:20:07 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container ingress-operator ready: true, restart count 0
Mar 23 17:31:19.878: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:31:19.878: INFO: telemeter-client-79bc5978cc-hnfmp from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (3 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:31:19.878: INFO: 	Container reload ready: true, restart count 0
Mar 23 17:31:19.878: INFO: 	Container telemeter-client ready: true, restart count 0
Mar 23 17:31:19.878: INFO: ibmcloud-block-storage-driver-9xhmg from kube-system started at 2020-03-23 13:19:01 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 23 17:31:19.878: INFO: cloud-credential-operator-65466dfbf8-nvzxn from openshift-cloud-credential-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container manager ready: true, restart count 0
Mar 23 17:31:19.878: INFO: network-operator-6f9f45bfbb-2gt5t from openshift-network-operator started at 2020-03-23 13:19:00 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container network-operator ready: true, restart count 0
Mar 23 17:31:19.878: INFO: multus-g6kbd from openshift-multus started at 2020-03-23 13:19:31 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container kube-multus ready: true, restart count 0
Mar 23 17:31:19.878: INFO: downloads-7fdfb77b95-56xsf from openshift-console started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container download-server ready: true, restart count 0
Mar 23 17:31:19.878: INFO: dns-operator-6cc86f84cd-qd8jq from openshift-dns-operator started at 2020-03-23 13:20:08 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container dns-operator ready: true, restart count 0
Mar 23 17:31:19.878: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:31:19.878: INFO: configmap-cabundle-injector-78bbf44c6b-rnztt from openshift-service-ca started at 2020-03-23 13:20:35 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Mar 23 17:31:19.878: INFO: sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-tk2rn from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Mar 23 17:31:19.878: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 17:31:19.878: INFO: ibm-cloud-provider-ip-169-47-106-243-5c45fc569-hq4zt from ibm-system started at 2020-03-23 15:19:18 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.878: INFO: 	Container ibm-cloud-provider-ip-169-47-106-243 ready: true, restart count 0
Mar 23 17:31:19.879: INFO: calico-node-jxjt7 from calico-system started at 2020-03-23 13:19:14 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container calico-node ready: true, restart count 0
Mar 23 17:31:19.879: INFO: downloads-7fdfb77b95-668gj from openshift-console started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container download-server ready: true, restart count 0
Mar 23 17:31:19.879: INFO: node-ca-mvtq9 from openshift-image-registry started at 2020-03-23 13:21:49 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container node-ca ready: true, restart count 0
Mar 23 17:31:19.879: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-03-23 14:58:08 +0000 UTC (3 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container alertmanager ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container config-reloader ready: true, restart count 0
Mar 23 17:31:19.879: INFO: ibm-file-plugin-bd78b44b5-7xzx5 from kube-system started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar 23 17:31:19.879: INFO: service-serving-cert-signer-598cdff956-pxfdm from openshift-service-ca started at 2020-03-23 13:20:35 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Mar 23 17:31:19.879: INFO: openshift-service-catalog-controller-manager-operator-5487z68jd from openshift-service-catalog-controller-manager-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container operator ready: true, restart count 1
Mar 23 17:31:19.879: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-03-23 13:28:39 +0000 UTC (3 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container alertmanager ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container config-reloader ready: true, restart count 0
Mar 23 17:31:19.879: INFO: prometheus-operator-54f44d89d8-xfwmx from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar 23 17:31:19.879: INFO: prometheus-adapter-6445ff7666-7lcjj from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar 23 17:31:19.879: INFO: calico-typha-7d46744f5d-c2gxq from calico-system started at 2020-03-23 13:19:14 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container calico-typha ready: true, restart count 1
Mar 23 17:31:19.879: INFO: openshift-kube-proxy-9lb45 from openshift-kube-proxy started at 2020-03-23 13:19:38 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 17:31:19.879: INFO: apiservice-cabundle-injector-6b4f956cf4-rtrzx from openshift-service-ca started at 2020-03-23 13:20:35 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Mar 23 17:31:19.879: INFO: sonobuoy from sonobuoy started at 2020-03-23 14:35:40 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 23 17:31:19.879: INFO: cluster-storage-operator-7f448d8d78-tb4bw from openshift-cluster-storage-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Mar 23 17:31:19.879: INFO: cluster-node-tuning-operator-878d4d68-48859 from openshift-cluster-node-tuning-operator started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar 23 17:31:19.879: INFO: openshift-state-metrics-7479448b69-fsvs4 from openshift-monitoring started at 2020-03-23 13:20:33 +0000 UTC (3 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar 23 17:31:19.879: INFO: ibm-master-proxy-static-10.241.69.184 from kube-system started at 2020-03-23 13:18:49 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container pause ready: true, restart count 0
Mar 23 17:31:19.879: INFO: ibm-keepalived-watcher-xc448 from kube-system started at 2020-03-23 13:18:55 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 23 17:31:19.879: INFO: service-ca-operator-69984d88bd-kcxc5 from openshift-service-ca-operator started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container operator ready: true, restart count 0
Mar 23 17:31:19.879: INFO: calico-kube-controllers-85c89dd878-ddpfj from calico-system started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 23 17:31:19.879: INFO: kube-state-metrics-6bcc97c9d6-vbfvx from openshift-monitoring started at 2020-03-23 13:20:32 +0000 UTC (3 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 23 17:31:19.879: INFO: dns-default-pvrkk from openshift-dns started at 2020-03-23 13:21:47 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container dns ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar 23 17:31:19.879: INFO: thanos-querier-866887d744-7rdmw from openshift-monitoring started at 2020-03-23 13:29:10 +0000 UTC (4 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container thanos-querier ready: true, restart count 0
Mar 23 17:31:19.879: INFO: openshift-service-catalog-apiserver-operator-5845fbf887-jlsvg from openshift-service-catalog-apiserver-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container operator ready: true, restart count 1
Mar 23 17:31:19.879: INFO: catalog-operator-bb7df57d7-xkrxt from openshift-operator-lifecycle-manager started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container catalog-operator ready: true, restart count 0
Mar 23 17:31:19.879: INFO: multus-admission-controller-ckp4z from openshift-multus started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar 23 17:31:19.879: INFO: olm-operator-9d666b5b9-s6wnm from openshift-operator-lifecycle-manager started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container olm-operator ready: true, restart count 0
Mar 23 17:31:19.879: INFO: ibm-storage-watcher-cdf8f7d4c-pnk9m from kube-system started at 2020-03-23 13:20:08 +0000 UTC (1 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar 23 17:31:19.879: INFO: cluster-image-registry-operator-5d7d64d769-n6l5f from openshift-image-registry started at 2020-03-23 13:20:07 +0000 UTC (2 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Mar 23 17:31:19.879: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-03-23 14:58:09 +0000 UTC (7 container statuses recorded)
Mar 23 17:31:19.879: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container prometheus ready: true, restart count 1
Mar 23 17:31:19.879: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 23 17:31:19.879: INFO: 	Container thanos-sidecar ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15feff2b3f9c3610], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15feff2b43dc1cf6], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:31:21.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3397" for this suite.
Mar 23 17:31:33.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:31:36.166: INFO: namespace sched-pred-3397 deletion completed in 15.011743686s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:17.888 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:31:36.168: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-674c6f88-a3db-40ff-867b-ead3953ca804
STEP: Creating a pod to test consume secrets
Mar 23 17:31:36.926: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a546231a-934a-4d58-beda-d0cc50761353" in namespace "projected-9645" to be "success or failure"
Mar 23 17:31:36.990: INFO: Pod "pod-projected-secrets-a546231a-934a-4d58-beda-d0cc50761353": Phase="Pending", Reason="", readiness=false. Elapsed: 62.76315ms
Mar 23 17:31:39.014: INFO: Pod "pod-projected-secrets-a546231a-934a-4d58-beda-d0cc50761353": Phase="Pending", Reason="", readiness=false. Elapsed: 2.087201108s
Mar 23 17:31:41.055: INFO: Pod "pod-projected-secrets-a546231a-934a-4d58-beda-d0cc50761353": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.128526887s
STEP: Saw pod success
Mar 23 17:31:41.056: INFO: Pod "pod-projected-secrets-a546231a-934a-4d58-beda-d0cc50761353" satisfied condition "success or failure"
Mar 23 17:31:41.089: INFO: Trying to get logs from node 10.241.69.181 pod pod-projected-secrets-a546231a-934a-4d58-beda-d0cc50761353 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 23 17:31:41.394: INFO: Waiting for pod pod-projected-secrets-a546231a-934a-4d58-beda-d0cc50761353 to disappear
Mar 23 17:31:41.417: INFO: Pod pod-projected-secrets-a546231a-934a-4d58-beda-d0cc50761353 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:31:41.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9645" for this suite.
Mar 23 17:31:53.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:31:56.676: INFO: namespace projected-9645 deletion completed in 15.203336774s

• [SLOW TEST:20.508 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:31:56.677: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:31:57.219: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-513dc2e7-e2ad-4100-a0b0-024404b47a8e
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:32:01.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7256" for this suite.
Mar 23 17:32:51.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:32:54.957: INFO: namespace configmap-7256 deletion completed in 53.261357252s

• [SLOW TEST:58.281 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:32:54.958: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:33:06.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7215" for this suite.
Mar 23 17:33:19.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:33:21.445: INFO: namespace resourcequota-7215 deletion completed in 14.672938531s

• [SLOW TEST:26.487 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:33:21.449: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 17:33:22.516: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 17:33:24.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720581602, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720581602, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720581602, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720581602, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 17:33:27.662: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:33:27.680: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-99-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:33:28.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5942" for this suite.
Mar 23 17:33:40.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:33:43.447: INFO: namespace webhook-5942 deletion completed in 14.794846591s
STEP: Destroying namespace "webhook-5942-markers" for this suite.
Mar 23 17:33:55.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:33:58.155: INFO: namespace webhook-5942-markers deletion completed in 14.707414691s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:36.851 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:33:58.300: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Mar 23 17:33:58.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 create -f - --namespace=kubectl-5742'
Mar 23 17:33:59.561: INFO: stderr: ""
Mar 23 17:33:59.561: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 23 17:33:59.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5742'
Mar 23 17:33:59.720: INFO: stderr: ""
Mar 23 17:33:59.720: INFO: stdout: "update-demo-nautilus-gk5l6 update-demo-nautilus-xqfkv "
Mar 23 17:33:59.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-gk5l6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5742'
Mar 23 17:33:59.893: INFO: stderr: ""
Mar 23 17:33:59.893: INFO: stdout: ""
Mar 23 17:33:59.893: INFO: update-demo-nautilus-gk5l6 is created but not running
Mar 23 17:34:04.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5742'
Mar 23 17:34:05.134: INFO: stderr: ""
Mar 23 17:34:05.134: INFO: stdout: "update-demo-nautilus-gk5l6 update-demo-nautilus-xqfkv "
Mar 23 17:34:05.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-gk5l6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5742'
Mar 23 17:34:05.271: INFO: stderr: ""
Mar 23 17:34:05.272: INFO: stdout: "true"
Mar 23 17:34:05.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-gk5l6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5742'
Mar 23 17:34:05.429: INFO: stderr: ""
Mar 23 17:34:05.429: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 17:34:05.429: INFO: validating pod update-demo-nautilus-gk5l6
Mar 23 17:34:05.471: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 17:34:05.471: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 17:34:05.471: INFO: update-demo-nautilus-gk5l6 is verified up and running
Mar 23 17:34:05.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-xqfkv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5742'
Mar 23 17:34:05.625: INFO: stderr: ""
Mar 23 17:34:05.625: INFO: stdout: "true"
Mar 23 17:34:05.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-nautilus-xqfkv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5742'
Mar 23 17:34:05.799: INFO: stderr: ""
Mar 23 17:34:05.799: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 23 17:34:05.799: INFO: validating pod update-demo-nautilus-xqfkv
Mar 23 17:34:05.862: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 17:34:05.862: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 17:34:05.862: INFO: update-demo-nautilus-xqfkv is verified up and running
STEP: rolling-update to new replication controller
Mar 23 17:34:05.866: INFO: scanned /root for discovery docs: <nil>
Mar 23 17:34:05.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-5742'
Mar 23 17:34:29.303: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 23 17:34:29.303: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Mar 23 17:34:29.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5742'
Mar 23 17:34:29.482: INFO: stderr: ""
Mar 23 17:34:29.482: INFO: stdout: "update-demo-kitten-gvvd6 update-demo-kitten-wlnk7 "
Mar 23 17:34:29.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-kitten-gvvd6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5742'
Mar 23 17:34:29.613: INFO: stderr: ""
Mar 23 17:34:29.613: INFO: stdout: "true"
Mar 23 17:34:29.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-kitten-gvvd6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5742'
Mar 23 17:34:29.753: INFO: stderr: ""
Mar 23 17:34:29.753: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 23 17:34:29.753: INFO: validating pod update-demo-kitten-gvvd6
Mar 23 17:34:29.815: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 23 17:34:29.815: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 23 17:34:29.815: INFO: update-demo-kitten-gvvd6 is verified up and running
Mar 23 17:34:29.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-kitten-wlnk7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5742'
Mar 23 17:34:29.995: INFO: stderr: ""
Mar 23 17:34:29.995: INFO: stdout: "true"
Mar 23 17:34:29.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods update-demo-kitten-wlnk7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5742'
Mar 23 17:34:30.123: INFO: stderr: ""
Mar 23 17:34:30.123: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 23 17:34:30.123: INFO: validating pod update-demo-kitten-wlnk7
Mar 23 17:34:30.162: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 23 17:34:30.162: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 23 17:34:30.162: INFO: update-demo-kitten-wlnk7 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:34:30.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5742" for this suite.
Mar 23 17:35:14.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:35:17.069: INFO: namespace kubectl-5742 deletion completed in 46.867058249s

• [SLOW TEST:78.769 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:35:17.069: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Mar 23 17:35:17.701: INFO: Waiting up to 5m0s for pod "pod-265635a4-bb49-403b-a474-4b8a3392fd98" in namespace "emptydir-5993" to be "success or failure"
Mar 23 17:35:17.738: INFO: Pod "pod-265635a4-bb49-403b-a474-4b8a3392fd98": Phase="Pending", Reason="", readiness=false. Elapsed: 36.366192ms
Mar 23 17:35:19.766: INFO: Pod "pod-265635a4-bb49-403b-a474-4b8a3392fd98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065065488s
Mar 23 17:35:21.803: INFO: Pod "pod-265635a4-bb49-403b-a474-4b8a3392fd98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.101396792s
STEP: Saw pod success
Mar 23 17:35:21.803: INFO: Pod "pod-265635a4-bb49-403b-a474-4b8a3392fd98" satisfied condition "success or failure"
Mar 23 17:35:21.845: INFO: Trying to get logs from node 10.241.69.181 pod pod-265635a4-bb49-403b-a474-4b8a3392fd98 container test-container: <nil>
STEP: delete the pod
Mar 23 17:35:22.046: INFO: Waiting for pod pod-265635a4-bb49-403b-a474-4b8a3392fd98 to disappear
Mar 23 17:35:22.094: INFO: Pod pod-265635a4-bb49-403b-a474-4b8a3392fd98 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:35:22.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5993" for this suite.
Mar 23 17:35:34.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:35:37.341: INFO: namespace emptydir-5993 deletion completed in 15.178585594s

• [SLOW TEST:20.272 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:35:37.341: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:35:38.427: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-b3e7ba11-827b-4465-b248-665868692074" in namespace "security-context-test-2391" to be "success or failure"
Mar 23 17:35:38.538: INFO: Pod "alpine-nnp-false-b3e7ba11-827b-4465-b248-665868692074": Phase="Pending", Reason="", readiness=false. Elapsed: 111.274569ms
Mar 23 17:35:40.714: INFO: Pod "alpine-nnp-false-b3e7ba11-827b-4465-b248-665868692074": Phase="Pending", Reason="", readiness=false. Elapsed: 2.28769048s
Mar 23 17:35:42.757: INFO: Pod "alpine-nnp-false-b3e7ba11-827b-4465-b248-665868692074": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.330084591s
Mar 23 17:35:42.757: INFO: Pod "alpine-nnp-false-b3e7ba11-827b-4465-b248-665868692074" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:35:43.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2391" for this suite.
Mar 23 17:35:55.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:35:57.744: INFO: namespace security-context-test-2391 deletion completed in 14.63657539s

• [SLOW TEST:20.403 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:35:57.746: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 23 17:35:57.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-8959'
Mar 23 17:35:58.140: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 23 17:35:58.140: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Mar 23 17:35:58.163: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Mar 23 17:35:58.213: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Mar 23 17:35:58.236: INFO: scanned /root for discovery docs: <nil>
Mar 23 17:35:58.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-8959'
Mar 23 17:36:14.869: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 23 17:36:14.869: INFO: stdout: "Created e2e-test-httpd-rc-db703e9f733ca9adb1d66a8bbea5d721\nScaling up e2e-test-httpd-rc-db703e9f733ca9adb1d66a8bbea5d721 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-db703e9f733ca9adb1d66a8bbea5d721 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-db703e9f733ca9adb1d66a8bbea5d721 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Mar 23 17:36:14.869: INFO: stdout: "Created e2e-test-httpd-rc-db703e9f733ca9adb1d66a8bbea5d721\nScaling up e2e-test-httpd-rc-db703e9f733ca9adb1d66a8bbea5d721 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-db703e9f733ca9adb1d66a8bbea5d721 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-db703e9f733ca9adb1d66a8bbea5d721 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Mar 23 17:36:14.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-8959'
Mar 23 17:36:15.027: INFO: stderr: ""
Mar 23 17:36:15.027: INFO: stdout: "e2e-test-httpd-rc-db703e9f733ca9adb1d66a8bbea5d721-brlvm e2e-test-httpd-rc-prx9b "
STEP: Replicas for run=e2e-test-httpd-rc: expected=1 actual=2
Mar 23 17:36:20.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-8959'
Mar 23 17:36:20.258: INFO: stderr: ""
Mar 23 17:36:20.258: INFO: stdout: "e2e-test-httpd-rc-db703e9f733ca9adb1d66a8bbea5d721-brlvm "
Mar 23 17:36:20.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods e2e-test-httpd-rc-db703e9f733ca9adb1d66a8bbea5d721-brlvm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8959'
Mar 23 17:36:20.396: INFO: stderr: ""
Mar 23 17:36:20.396: INFO: stdout: "true"
Mar 23 17:36:20.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 get pods e2e-test-httpd-rc-db703e9f733ca9adb1d66a8bbea5d721-brlvm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8959'
Mar 23 17:36:20.533: INFO: stderr: ""
Mar 23 17:36:20.533: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Mar 23 17:36:20.533: INFO: e2e-test-httpd-rc-db703e9f733ca9adb1d66a8bbea5d721-brlvm is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Mar 23 17:36:20.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete rc e2e-test-httpd-rc --namespace=kubectl-8959'
Mar 23 17:36:20.760: INFO: stderr: ""
Mar 23 17:36:20.760: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:36:20.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8959" for this suite.
Mar 23 17:36:40.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:36:43.547: INFO: namespace kubectl-8959 deletion completed in 22.738229268s

• [SLOW TEST:45.801 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:36:43.547: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8816.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8816.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 17:36:48.355: INFO: DNS probes using dns-8816/dns-test-02bd0780-6d6b-4fc8-ab9d-d1478652ec10 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:36:48.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8816" for this suite.
Mar 23 17:37:02.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:37:06.190: INFO: namespace dns-8816 deletion completed in 17.693761029s

• [SLOW TEST:22.643 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:37:06.191: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Mar 23 17:37:06.978: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8f5f8bfe-9b10-45c8-a4bb-21b15be85766" in namespace "downward-api-6049" to be "success or failure"
Mar 23 17:37:07.028: INFO: Pod "downwardapi-volume-8f5f8bfe-9b10-45c8-a4bb-21b15be85766": Phase="Pending", Reason="", readiness=false. Elapsed: 50.058854ms
Mar 23 17:37:09.061: INFO: Pod "downwardapi-volume-8f5f8bfe-9b10-45c8-a4bb-21b15be85766": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.083662344s
STEP: Saw pod success
Mar 23 17:37:09.062: INFO: Pod "downwardapi-volume-8f5f8bfe-9b10-45c8-a4bb-21b15be85766" satisfied condition "success or failure"
Mar 23 17:37:09.098: INFO: Trying to get logs from node 10.241.69.181 pod downwardapi-volume-8f5f8bfe-9b10-45c8-a4bb-21b15be85766 container client-container: <nil>
STEP: delete the pod
Mar 23 17:37:09.320: INFO: Waiting for pod downwardapi-volume-8f5f8bfe-9b10-45c8-a4bb-21b15be85766 to disappear
Mar 23 17:37:09.389: INFO: Pod downwardapi-volume-8f5f8bfe-9b10-45c8-a4bb-21b15be85766 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:37:09.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6049" for this suite.
Mar 23 17:37:19.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:37:22.181: INFO: namespace downward-api-6049 deletion completed in 12.707422867s

• [SLOW TEST:15.990 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:37:22.181: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Mar 23 17:37:22.690: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 23 17:37:22.885: INFO: Waiting for terminating namespaces to be deleted...
Mar 23 17:37:22.931: INFO: 
Logging pods the kubelet thinks is on node 10.241.69.172 before test
Mar 23 17:37:23.190: INFO: node-ca-c6cdr from openshift-image-registry started at 2020-03-23 13:21:48 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container node-ca ready: true, restart count 0
Mar 23 17:37:23.190: INFO: router-default-58f5c9568-cqkqx from openshift-ingress started at 2020-03-23 13:22:00 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container router ready: true, restart count 0
Mar 23 17:37:23.190: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-03-23 13:29:34 +0000 UTC (7 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 	Container prometheus ready: true, restart count 1
Mar 23 17:37:23.190: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar 23 17:37:23.190: INFO: sonobuoy-e2e-job-b545dbbb0c064966 from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container e2e ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 17:37:23.190: INFO: ibm-master-proxy-static-10.241.69.172 from kube-system started at 2020-03-23 13:20:33 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 	Container pause ready: true, restart count 0
Mar 23 17:37:23.190: INFO: ibm-keepalived-watcher-zqvkl from kube-system started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 23 17:37:23.190: INFO: vpn-774dcbf8fc-mqtz2 from kube-system started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container vpn ready: true, restart count 0
Mar 23 17:37:23.190: INFO: prometheus-adapter-6445ff7666-kwdxl from openshift-monitoring started at 2020-03-23 13:28:03 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar 23 17:37:23.190: INFO: tuned-zh9qp from openshift-cluster-node-tuning-operator started at 2020-03-23 13:21:11 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container tuned ready: true, restart count 0
Mar 23 17:37:23.190: INFO: dns-default-fmdtj from openshift-dns started at 2020-03-23 13:21:48 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container dns ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar 23 17:37:23.190: INFO: redhat-operators-5d5b56f44f-9c86l from openshift-marketplace started at 2020-03-23 14:22:14 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container redhat-operators ready: true, restart count 0
Mar 23 17:37:23.190: INFO: ibm-cloud-provider-ip-169-47-106-243-5c45fc569-dx5w9 from ibm-system started at 2020-03-23 16:42:26 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container ibm-cloud-provider-ip-169-47-106-243 ready: true, restart count 0
Mar 23 17:37:23.190: INFO: calico-node-6f276 from calico-system started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container calico-node ready: true, restart count 0
Mar 23 17:37:23.190: INFO: ibmcloud-block-storage-driver-rxcc5 from kube-system started at 2020-03-23 13:20:43 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 23 17:37:23.190: INFO: calico-typha-7d46744f5d-5mvhh from calico-system started at 2020-03-23 13:21:09 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container calico-typha ready: true, restart count 0
Mar 23 17:37:23.190: INFO: cluster-samples-operator-6b5459b7b9-fwhtx from openshift-cluster-samples-operator started at 2020-03-23 14:57:52 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Mar 23 17:37:23.190: INFO: thanos-querier-866887d744-whk26 from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (4 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 	Container thanos-querier ready: true, restart count 0
Mar 23 17:37:23.190: INFO: packageserver-5c7d978cbb-llzxd from openshift-operator-lifecycle-manager started at 2020-03-23 13:31:00 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container packageserver ready: true, restart count 0
Mar 23 17:37:23.190: INFO: openshift-kube-proxy-lv75h from openshift-kube-proxy started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 17:37:23.190: INFO: node-exporter-2c5jl from openshift-monitoring started at 2020-03-23 13:20:40 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 	Container node-exporter ready: true, restart count 0
Mar 23 17:37:23.190: INFO: grafana-5bdf758c94-j7bvk from openshift-monitoring started at 2020-03-23 13:28:09 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container grafana ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 	Container grafana-proxy ready: true, restart count 0
Mar 23 17:37:23.190: INFO: sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-9fzqx from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container sonobuoy-worker ready: true, restart count 3
Mar 23 17:37:23.190: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 17:37:23.190: INFO: console-5b755cbdcd-7sqjq from openshift-console started at 2020-03-23 13:22:24 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container console ready: true, restart count 0
Mar 23 17:37:23.190: INFO: multus-b8pkh from openshift-multus started at 2020-03-23 13:20:40 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container kube-multus ready: true, restart count 0
Mar 23 17:37:23.190: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-03-23 13:28:54 +0000 UTC (3 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container alertmanager ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 	Container config-reloader ready: true, restart count 0
Mar 23 17:37:23.190: INFO: multus-admission-controller-shsp6 from openshift-multus started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar 23 17:37:23.190: INFO: image-registry-684f4746d9-ptcpt from openshift-image-registry started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.190: INFO: 	Container registry ready: true, restart count 0
Mar 23 17:37:23.190: INFO: 
Logging pods the kubelet thinks is on node 10.241.69.181 before test
Mar 23 17:37:23.284: INFO: ibmcloud-block-storage-driver-rwjct from kube-system started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.284: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 23 17:37:23.284: INFO: openshift-kube-proxy-hgttr from openshift-kube-proxy started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.284: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 17:37:23.284: INFO: calico-node-skv44 from calico-system started at 2020-03-23 13:21:48 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.284: INFO: 	Container calico-node ready: true, restart count 0
Mar 23 17:37:23.284: INFO: dns-default-98lms from openshift-dns started at 2020-03-23 16:42:37 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.284: INFO: 	Container dns ready: true, restart count 0
Mar 23 17:37:23.284: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar 23 17:37:23.284: INFO: ibm-master-proxy-static-10.241.69.181 from kube-system started at 2020-03-23 13:21:17 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.284: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 23 17:37:23.284: INFO: 	Container pause ready: true, restart count 0
Mar 23 17:37:23.284: INFO: community-operators-c75c8c9f6-jjk8l from openshift-marketplace started at 2020-03-23 16:42:26 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.284: INFO: 	Container community-operators ready: true, restart count 0
Mar 23 17:37:23.284: INFO: sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-m6fcz from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.284: INFO: 	Container sonobuoy-worker ready: true, restart count 3
Mar 23 17:37:23.284: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 17:37:23.284: INFO: tuned-n9xl7 from openshift-cluster-node-tuning-operator started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.284: INFO: 	Container tuned ready: true, restart count 0
Mar 23 17:37:23.284: INFO: multus-596gk from openshift-multus started at 2020-03-23 13:21:47 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.284: INFO: 	Container kube-multus ready: true, restart count 0
Mar 23 17:37:23.284: INFO: ibm-keepalived-watcher-2klqj from kube-system started at 2020-03-23 13:21:48 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.284: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 23 17:37:23.284: INFO: node-exporter-k4sbd from openshift-monitoring started at 2020-03-23 13:21:48 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.284: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:37:23.284: INFO: 	Container node-exporter ready: true, restart count 0
Mar 23 17:37:23.284: INFO: calico-typha-7d46744f5d-qnz4h from calico-system started at 2020-03-23 13:23:09 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.284: INFO: 	Container calico-typha ready: true, restart count 0
Mar 23 17:37:23.284: INFO: multus-admission-controller-k89c8 from openshift-multus started at 2020-03-23 16:43:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.284: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar 23 17:37:23.284: INFO: certified-operators-6998665554-shpdk from openshift-marketplace started at 2020-03-23 16:42:26 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.284: INFO: 	Container certified-operators ready: true, restart count 0
Mar 23 17:37:23.284: INFO: registry-pvc-permissions-gptrd from openshift-image-registry started at 2020-03-23 13:24:16 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.284: INFO: 	Container pvc-permissions ready: false, restart count 0
Mar 23 17:37:23.284: INFO: node-ca-bpcf9 from openshift-image-registry started at 2020-03-23 16:42:37 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.284: INFO: 	Container node-ca ready: true, restart count 0
Mar 23 17:37:23.284: INFO: 
Logging pods the kubelet thinks is on node 10.241.69.184 before test
Mar 23 17:37:23.547: INFO: marketplace-operator-554cffcfd-d5zjt from openshift-marketplace started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.547: INFO: 	Container marketplace-operator ready: true, restart count 0
Mar 23 17:37:23.547: INFO: ingress-operator-754bbd5fbf-q5d8c from openshift-ingress-operator started at 2020-03-23 13:20:07 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.547: INFO: 	Container ingress-operator ready: true, restart count 0
Mar 23 17:37:23.547: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:37:23.547: INFO: telemeter-client-79bc5978cc-hnfmp from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (3 container statuses recorded)
Mar 23 17:37:23.547: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:37:23.547: INFO: 	Container reload ready: true, restart count 0
Mar 23 17:37:23.547: INFO: 	Container telemeter-client ready: true, restart count 0
Mar 23 17:37:23.547: INFO: ibmcloud-block-storage-driver-9xhmg from kube-system started at 2020-03-23 13:19:01 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.547: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Mar 23 17:37:23.547: INFO: cloud-credential-operator-65466dfbf8-nvzxn from openshift-cloud-credential-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.547: INFO: 	Container manager ready: true, restart count 0
Mar 23 17:37:23.547: INFO: network-operator-6f9f45bfbb-2gt5t from openshift-network-operator started at 2020-03-23 13:19:00 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.547: INFO: 	Container network-operator ready: true, restart count 0
Mar 23 17:37:23.547: INFO: multus-g6kbd from openshift-multus started at 2020-03-23 13:19:31 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.547: INFO: 	Container kube-multus ready: true, restart count 0
Mar 23 17:37:23.547: INFO: downloads-7fdfb77b95-56xsf from openshift-console started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container download-server ready: true, restart count 0
Mar 23 17:37:23.548: INFO: dns-operator-6cc86f84cd-qd8jq from openshift-dns-operator started at 2020-03-23 13:20:08 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container dns-operator ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:37:23.548: INFO: configmap-cabundle-injector-78bbf44c6b-rnztt from openshift-service-ca started at 2020-03-23 13:20:35 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Mar 23 17:37:23.548: INFO: sonobuoy-systemd-logs-daemon-set-1a463d54114d46b2-tk2rn from sonobuoy started at 2020-03-23 14:35:49 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container sonobuoy-worker ready: true, restart count 3
Mar 23 17:37:23.548: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 17:37:23.548: INFO: ibm-cloud-provider-ip-169-47-106-243-5c45fc569-hq4zt from ibm-system started at 2020-03-23 15:19:18 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container ibm-cloud-provider-ip-169-47-106-243 ready: true, restart count 0
Mar 23 17:37:23.548: INFO: calico-node-jxjt7 from calico-system started at 2020-03-23 13:19:14 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container calico-node ready: true, restart count 0
Mar 23 17:37:23.548: INFO: downloads-7fdfb77b95-668gj from openshift-console started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container download-server ready: true, restart count 0
Mar 23 17:37:23.548: INFO: node-ca-mvtq9 from openshift-image-registry started at 2020-03-23 13:21:49 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container node-ca ready: true, restart count 0
Mar 23 17:37:23.548: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-03-23 14:58:08 +0000 UTC (3 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container alertmanager ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container config-reloader ready: true, restart count 0
Mar 23 17:37:23.548: INFO: ibm-file-plugin-bd78b44b5-7xzx5 from kube-system started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Mar 23 17:37:23.548: INFO: service-serving-cert-signer-598cdff956-pxfdm from openshift-service-ca started at 2020-03-23 13:20:35 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Mar 23 17:37:23.548: INFO: openshift-service-catalog-controller-manager-operator-5487z68jd from openshift-service-catalog-controller-manager-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container operator ready: true, restart count 1
Mar 23 17:37:23.548: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-03-23 13:28:39 +0000 UTC (3 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container alertmanager ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container config-reloader ready: true, restart count 0
Mar 23 17:37:23.548: INFO: prometheus-operator-54f44d89d8-xfwmx from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container prometheus-operator ready: true, restart count 0
Mar 23 17:37:23.548: INFO: prometheus-adapter-6445ff7666-7lcjj from openshift-monitoring started at 2020-03-23 14:57:52 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container prometheus-adapter ready: true, restart count 0
Mar 23 17:37:23.548: INFO: calico-typha-7d46744f5d-c2gxq from calico-system started at 2020-03-23 13:19:14 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container calico-typha ready: true, restart count 1
Mar 23 17:37:23.548: INFO: openshift-kube-proxy-9lb45 from openshift-kube-proxy started at 2020-03-23 13:19:38 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 17:37:23.548: INFO: apiservice-cabundle-injector-6b4f956cf4-rtrzx from openshift-service-ca started at 2020-03-23 13:20:35 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Mar 23 17:37:23.548: INFO: sonobuoy from sonobuoy started at 2020-03-23 14:35:40 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 23 17:37:23.548: INFO: cluster-storage-operator-7f448d8d78-tb4bw from openshift-cluster-storage-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Mar 23 17:37:23.548: INFO: cluster-node-tuning-operator-878d4d68-48859 from openshift-cluster-node-tuning-operator started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Mar 23 17:37:23.548: INFO: openshift-state-metrics-7479448b69-fsvs4 from openshift-monitoring started at 2020-03-23 13:20:33 +0000 UTC (3 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Mar 23 17:37:23.548: INFO: ibm-master-proxy-static-10.241.69.184 from kube-system started at 2020-03-23 13:18:49 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container pause ready: true, restart count 0
Mar 23 17:37:23.548: INFO: ibm-keepalived-watcher-xc448 from kube-system started at 2020-03-23 13:18:55 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container keepalived-watcher ready: true, restart count 0
Mar 23 17:37:23.548: INFO: service-ca-operator-69984d88bd-kcxc5 from openshift-service-ca-operator started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container operator ready: true, restart count 0
Mar 23 17:37:23.548: INFO: calico-kube-controllers-85c89dd878-ddpfj from calico-system started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Mar 23 17:37:23.548: INFO: kube-state-metrics-6bcc97c9d6-vbfvx from openshift-monitoring started at 2020-03-23 13:20:32 +0000 UTC (3 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container kube-state-metrics ready: true, restart count 0
Mar 23 17:37:23.548: INFO: dns-default-pvrkk from openshift-dns started at 2020-03-23 13:21:47 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container dns ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container dns-node-resolver ready: true, restart count 0
Mar 23 17:37:23.548: INFO: thanos-querier-866887d744-7rdmw from openshift-monitoring started at 2020-03-23 13:29:10 +0000 UTC (4 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container oauth-proxy ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container thanos-querier ready: true, restart count 0
Mar 23 17:37:23.548: INFO: openshift-service-catalog-apiserver-operator-5845fbf887-jlsvg from openshift-service-catalog-apiserver-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container operator ready: true, restart count 1
Mar 23 17:37:23.548: INFO: catalog-operator-bb7df57d7-xkrxt from openshift-operator-lifecycle-manager started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container catalog-operator ready: true, restart count 0
Mar 23 17:37:23.548: INFO: multus-admission-controller-ckp4z from openshift-multus started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container multus-admission-controller ready: true, restart count 0
Mar 23 17:37:23.548: INFO: olm-operator-9d666b5b9-s6wnm from openshift-operator-lifecycle-manager started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container olm-operator ready: true, restart count 0
Mar 23 17:37:23.548: INFO: ibm-storage-watcher-cdf8f7d4c-pnk9m from kube-system started at 2020-03-23 13:20:08 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Mar 23 17:37:23.548: INFO: cluster-image-registry-operator-5d7d64d769-n6l5f from openshift-image-registry started at 2020-03-23 13:20:07 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Mar 23 17:37:23.548: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-03-23 14:58:09 +0000 UTC (7 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container prom-label-proxy ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container prometheus ready: true, restart count 1
Mar 23 17:37:23.548: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container prometheus-proxy ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container thanos-sidecar ready: true, restart count 0
Mar 23 17:37:23.548: INFO: tigera-operator-c7555ddb8-fggcz from tigera-operator started at 2020-03-23 13:18:58 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container tigera-operator ready: true, restart count 0
Mar 23 17:37:23.548: INFO: ibmcloud-block-storage-plugin-5c7dcf4bdb-jm274 from kube-system started at 2020-03-23 13:19:01 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Mar 23 17:37:23.548: INFO: console-operator-597c74c496-7g9c9 from openshift-console-operator started at 2020-03-23 13:20:06 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container console-operator ready: true, restart count 0
Mar 23 17:37:23.548: INFO: node-exporter-xwc46 from openshift-monitoring started at 2020-03-23 13:20:33 +0000 UTC (2 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Mar 23 17:37:23.548: INFO: 	Container node-exporter ready: true, restart count 0
Mar 23 17:37:23.548: INFO: tuned-fjdv5 from openshift-cluster-node-tuning-operator started at 2020-03-23 13:21:11 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container tuned ready: true, restart count 0
Mar 23 17:37:23.548: INFO: router-default-58f5c9568-vkljz from openshift-ingress started at 2020-03-23 13:22:01 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container router ready: true, restart count 0
Mar 23 17:37:23.548: INFO: packageserver-5c7d978cbb-s4z4h from openshift-operator-lifecycle-manager started at 2020-03-23 13:30:54 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container packageserver ready: true, restart count 0
Mar 23 17:37:23.548: INFO: cluster-monitoring-operator-5b6ff66676-fjscj from openshift-monitoring started at 2020-03-23 13:20:07 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Mar 23 17:37:23.548: INFO: console-5b755cbdcd-pllqp from openshift-console started at 2020-03-23 13:22:44 +0000 UTC (1 container statuses recorded)
Mar 23 17:37:23.548: INFO: 	Container console ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e1bcdd2d-12fd-4df5-9bcb-3b507b752609 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-e1bcdd2d-12fd-4df5-9bcb-3b507b752609 off the node 10.241.69.181
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e1bcdd2d-12fd-4df5-9bcb-3b507b752609
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:37:30.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3066" for this suite.
Mar 23 17:37:52.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:37:55.679: INFO: namespace sched-pred-3066 deletion completed in 24.885136766s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:33.498 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:37:55.679: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:37:56.240: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-11f8f1b4-6136-439a-aa7e-452369356fd9
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-11f8f1b4-6136-439a-aa7e-452369356fd9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:39:19.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2589" for this suite.
Mar 23 17:39:39.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:39:42.177: INFO: namespace configmap-2589 deletion completed in 22.577866335s

• [SLOW TEST:106.498 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:39:42.178: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-a329d312-3df0-4867-9450-42eac5f8cbd9
STEP: Creating a pod to test consume configMaps
Mar 23 17:39:42.653: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ef7d9ffb-3243-45dc-9c54-0cb8e1c1d995" in namespace "projected-129" to be "success or failure"
Mar 23 17:39:42.681: INFO: Pod "pod-projected-configmaps-ef7d9ffb-3243-45dc-9c54-0cb8e1c1d995": Phase="Pending", Reason="", readiness=false. Elapsed: 28.565765ms
Mar 23 17:39:44.704: INFO: Pod "pod-projected-configmaps-ef7d9ffb-3243-45dc-9c54-0cb8e1c1d995": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051378854s
Mar 23 17:39:46.724: INFO: Pod "pod-projected-configmaps-ef7d9ffb-3243-45dc-9c54-0cb8e1c1d995": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071354656s
STEP: Saw pod success
Mar 23 17:39:46.724: INFO: Pod "pod-projected-configmaps-ef7d9ffb-3243-45dc-9c54-0cb8e1c1d995" satisfied condition "success or failure"
Mar 23 17:39:46.744: INFO: Trying to get logs from node 10.241.69.181 pod pod-projected-configmaps-ef7d9ffb-3243-45dc-9c54-0cb8e1c1d995 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Mar 23 17:39:46.868: INFO: Waiting for pod pod-projected-configmaps-ef7d9ffb-3243-45dc-9c54-0cb8e1c1d995 to disappear
Mar 23 17:39:46.888: INFO: Pod pod-projected-configmaps-ef7d9ffb-3243-45dc-9c54-0cb8e1c1d995 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:39:46.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-129" for this suite.
Mar 23 17:39:59.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:40:01.514: INFO: namespace projected-129 deletion completed in 14.568937567s

• [SLOW TEST:19.337 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:40:01.515: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-pt6c
STEP: Creating a pod to test atomic-volume-subpath
Mar 23 17:40:02.071: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pt6c" in namespace "subpath-2823" to be "success or failure"
Mar 23 17:40:02.098: INFO: Pod "pod-subpath-test-projected-pt6c": Phase="Pending", Reason="", readiness=false. Elapsed: 26.36732ms
Mar 23 17:40:04.131: INFO: Pod "pod-subpath-test-projected-pt6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059709699s
Mar 23 17:40:06.159: INFO: Pod "pod-subpath-test-projected-pt6c": Phase="Running", Reason="", readiness=true. Elapsed: 4.087868084s
Mar 23 17:40:08.183: INFO: Pod "pod-subpath-test-projected-pt6c": Phase="Running", Reason="", readiness=true. Elapsed: 6.112152951s
Mar 23 17:40:10.207: INFO: Pod "pod-subpath-test-projected-pt6c": Phase="Running", Reason="", readiness=true. Elapsed: 8.136125273s
Mar 23 17:40:12.234: INFO: Pod "pod-subpath-test-projected-pt6c": Phase="Running", Reason="", readiness=true. Elapsed: 10.163260779s
Mar 23 17:40:14.275: INFO: Pod "pod-subpath-test-projected-pt6c": Phase="Running", Reason="", readiness=true. Elapsed: 12.204028763s
Mar 23 17:40:16.313: INFO: Pod "pod-subpath-test-projected-pt6c": Phase="Running", Reason="", readiness=true. Elapsed: 14.241544053s
Mar 23 17:40:18.336: INFO: Pod "pod-subpath-test-projected-pt6c": Phase="Running", Reason="", readiness=true. Elapsed: 16.264975036s
Mar 23 17:40:20.370: INFO: Pod "pod-subpath-test-projected-pt6c": Phase="Running", Reason="", readiness=true. Elapsed: 18.298899477s
Mar 23 17:40:22.390: INFO: Pod "pod-subpath-test-projected-pt6c": Phase="Running", Reason="", readiness=true. Elapsed: 20.318688521s
Mar 23 17:40:24.419: INFO: Pod "pod-subpath-test-projected-pt6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.348001971s
STEP: Saw pod success
Mar 23 17:40:24.419: INFO: Pod "pod-subpath-test-projected-pt6c" satisfied condition "success or failure"
Mar 23 17:40:24.443: INFO: Trying to get logs from node 10.241.69.181 pod pod-subpath-test-projected-pt6c container test-container-subpath-projected-pt6c: <nil>
STEP: delete the pod
Mar 23 17:40:24.852: INFO: Waiting for pod pod-subpath-test-projected-pt6c to disappear
Mar 23 17:40:24.889: INFO: Pod pod-subpath-test-projected-pt6c no longer exists
STEP: Deleting pod pod-subpath-test-projected-pt6c
Mar 23 17:40:24.889: INFO: Deleting pod "pod-subpath-test-projected-pt6c" in namespace "subpath-2823"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:40:24.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2823" for this suite.
Mar 23 17:40:37.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:40:39.631: INFO: namespace subpath-2823 deletion completed in 14.663948416s

• [SLOW TEST:38.116 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:40:39.631: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-4cf66a62-83d2-4fbc-98c3-360de6a7d2a3
STEP: Creating a pod to test consume secrets
Mar 23 17:40:41.044: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eb8556aa-a8a1-41e0-b181-b88a4390fd03" in namespace "projected-8510" to be "success or failure"
Mar 23 17:40:41.089: INFO: Pod "pod-projected-secrets-eb8556aa-a8a1-41e0-b181-b88a4390fd03": Phase="Pending", Reason="", readiness=false. Elapsed: 45.194014ms
Mar 23 17:40:43.107: INFO: Pod "pod-projected-secrets-eb8556aa-a8a1-41e0-b181-b88a4390fd03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.063174237s
Mar 23 17:40:45.128: INFO: Pod "pod-projected-secrets-eb8556aa-a8a1-41e0-b181-b88a4390fd03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.084218725s
STEP: Saw pod success
Mar 23 17:40:45.128: INFO: Pod "pod-projected-secrets-eb8556aa-a8a1-41e0-b181-b88a4390fd03" satisfied condition "success or failure"
Mar 23 17:40:45.151: INFO: Trying to get logs from node 10.241.69.181 pod pod-projected-secrets-eb8556aa-a8a1-41e0-b181-b88a4390fd03 container projected-secret-volume-test: <nil>
STEP: delete the pod
Mar 23 17:40:45.260: INFO: Waiting for pod pod-projected-secrets-eb8556aa-a8a1-41e0-b181-b88a4390fd03 to disappear
Mar 23 17:40:45.281: INFO: Pod pod-projected-secrets-eb8556aa-a8a1-41e0-b181-b88a4390fd03 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:40:45.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8510" for this suite.
Mar 23 17:40:57.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:41:00.228: INFO: namespace projected-8510 deletion completed in 14.888780964s

• [SLOW TEST:20.597 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:41:00.228: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8828.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8828.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8828.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8828.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8828.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8828.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Mar 23 17:41:05.332: INFO: DNS probes using dns-8828/dns-test-98452a62-60d8-4346-9e50-6cfb3cc8452e succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:41:05.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8828" for this suite.
Mar 23 17:41:15.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:41:18.371: INFO: namespace dns-8828 deletion completed in 12.703642306s

• [SLOW TEST:18.143 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:41:18.371: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:41:21.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8131" for this suite.
Mar 23 17:42:09.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:42:11.854: INFO: namespace kubelet-test-8131 deletion completed in 50.652008952s

• [SLOW TEST:53.482 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:42:11.854: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Mar 23 17:42:12.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1816'
Mar 23 17:42:12.302: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 23 17:42:12.302: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Mar 23 17:42:12.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-012311840 delete jobs e2e-test-httpd-job --namespace=kubectl-1816'
Mar 23 17:42:12.559: INFO: stderr: ""
Mar 23 17:42:12.559: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:42:12.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1816" for this suite.
Mar 23 17:42:22.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:42:25.168: INFO: namespace kubectl-1816 deletion completed in 12.517650717s

• [SLOW TEST:13.315 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:42:25.168: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Mar 23 17:42:33.951: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 17:42:33.980: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 23 17:42:35.980: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 17:42:36.008: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 23 17:42:37.980: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 17:42:38.003: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 23 17:42:39.980: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 17:42:40.022: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 23 17:42:41.980: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 17:42:42.003: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 23 17:42:43.980: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 17:42:44.006: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 23 17:42:45.980: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 17:42:46.001: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 23 17:42:47.980: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 17:42:48.001: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:42:48.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4470" for this suite.
Mar 23 17:43:28.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:43:30.949: INFO: namespace container-lifecycle-hook-4470 deletion completed in 42.846172312s

• [SLOW TEST:65.781 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:43:30.949: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-6564d8aa-2b77-4cba-b6d6-84f283dc391d in namespace container-probe-504
Mar 23 17:43:35.565: INFO: Started pod liveness-6564d8aa-2b77-4cba-b6d6-84f283dc391d in namespace container-probe-504
STEP: checking the pod's current state and verifying that restartCount is present
Mar 23 17:43:35.589: INFO: Initial restart count of pod liveness-6564d8aa-2b77-4cba-b6d6-84f283dc391d is 0
Mar 23 17:43:53.971: INFO: Restart count of pod container-probe-504/liveness-6564d8aa-2b77-4cba-b6d6-84f283dc391d is now 1 (18.381528156s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:43:54.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-504" for this suite.
Mar 23 17:44:08.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:44:10.765: INFO: namespace container-probe-504 deletion completed in 16.654497432s

• [SLOW TEST:39.816 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:44:10.765: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Mar 23 17:44:11.707: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Mar 23 17:44:13.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720582251, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720582251, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720582252, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720582251, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 17:44:16.818: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:44:16.836: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:44:17.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2599" for this suite.
Mar 23 17:44:27.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:44:30.291: INFO: namespace crd-webhook-2599 deletion completed in 12.441611785s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:19.619 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Mar 23 17:44:30.384: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Mar 23 17:44:31.331: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 17:44:33.396: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720582271, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720582271, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63720582271, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63720582271, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Mar 23 17:44:36.471: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Mar 23 17:44:36.486: INFO: >>> kubeConfig: /tmp/kubeconfig-012311840
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2614-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Mar 23 17:44:37.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2097" for this suite.
Mar 23 17:44:49.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:44:52.398: INFO: namespace webhook-2097 deletion completed in 14.84225645s
STEP: Destroying namespace "webhook-2097-markers" for this suite.
Mar 23 17:45:04.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 23 17:45:07.352: INFO: namespace webhook-2097-markers deletion completed in 14.953749166s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:37.099 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSMar 23 17:45:07.483: INFO: Running AfterSuite actions on all nodes
Mar 23 17:45:07.483: INFO: Running AfterSuite actions on node 1
Mar 23 17:45:07.483: INFO: Skipping dumping logs from cluster

Ran 276 of 4897 Specs in 11324.569 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4621 Skipped
PASS

Ginkgo ran 1 suite in 3h8m46.538166648s
Test Suite Passed
