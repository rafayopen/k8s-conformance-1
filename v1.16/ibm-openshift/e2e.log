I0217 16:30:49.764396      26 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-559420064
I0217 16:30:49.764578      26 e2e.go:92] Starting e2e run "30c90592-a32f-44bd-9829-57f58a215716" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1581957047 - Will randomize all specs
Will run 276 of 4897 specs

Feb 17 16:30:49.779: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 16:30:49.784: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Feb 17 16:30:49.837: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Feb 17 16:30:49.910: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Feb 17 16:30:49.910: INFO: expected 8 pod replicas in namespace 'kube-system', 8 are Running and Ready.
Feb 17 16:30:49.910: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Feb 17 16:30:49.933: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Feb 17 16:30:49.933: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Feb 17 16:30:49.933: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Feb 17 16:30:49.933: INFO: e2e test version: v1.16.2
Feb 17 16:30:49.938: INFO: kube-apiserver version: v1.16.2
Feb 17 16:30:49.938: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 16:30:49.955: INFO: Cluster IP family: ipv4
SSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:30:49.955: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename resourcequota
Feb 17 16:30:50.092: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:31:07.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6289" for this suite.
Feb 17 16:31:15.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:31:19.382: INFO: namespace resourcequota-6289 deletion completed in 12.103955974s

• [SLOW TEST:29.427 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:31:19.382: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 16:31:19.605: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd764f34-6026-4f0e-9e38-c6ffb1271d2d" in namespace "projected-2761" to be "success or failure"
Feb 17 16:31:19.624: INFO: Pod "downwardapi-volume-dd764f34-6026-4f0e-9e38-c6ffb1271d2d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.997817ms
Feb 17 16:31:21.638: INFO: Pod "downwardapi-volume-dd764f34-6026-4f0e-9e38-c6ffb1271d2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033055011s
Feb 17 16:31:23.650: INFO: Pod "downwardapi-volume-dd764f34-6026-4f0e-9e38-c6ffb1271d2d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045385283s
Feb 17 16:31:25.664: INFO: Pod "downwardapi-volume-dd764f34-6026-4f0e-9e38-c6ffb1271d2d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.059051666s
Feb 17 16:31:27.678: INFO: Pod "downwardapi-volume-dd764f34-6026-4f0e-9e38-c6ffb1271d2d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.073273818s
Feb 17 16:31:29.691: INFO: Pod "downwardapi-volume-dd764f34-6026-4f0e-9e38-c6ffb1271d2d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.085577017s
Feb 17 16:31:31.708: INFO: Pod "downwardapi-volume-dd764f34-6026-4f0e-9e38-c6ffb1271d2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.103122701s
STEP: Saw pod success
Feb 17 16:31:31.708: INFO: Pod "downwardapi-volume-dd764f34-6026-4f0e-9e38-c6ffb1271d2d" satisfied condition "success or failure"
Feb 17 16:31:31.727: INFO: Trying to get logs from node 10.45.66.177 pod downwardapi-volume-dd764f34-6026-4f0e-9e38-c6ffb1271d2d container client-container: <nil>
STEP: delete the pod
Feb 17 16:31:31.863: INFO: Waiting for pod downwardapi-volume-dd764f34-6026-4f0e-9e38-c6ffb1271d2d to disappear
Feb 17 16:31:31.875: INFO: Pod downwardapi-volume-dd764f34-6026-4f0e-9e38-c6ffb1271d2d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:31:31.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2761" for this suite.
Feb 17 16:31:39.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:31:42.009: INFO: namespace projected-2761 deletion completed in 10.119326128s

• [SLOW TEST:22.627 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:31:42.010: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-759b65de-2d0a-45af-9295-a2cc7fece219
STEP: Creating secret with name secret-projected-all-test-volume-5ba36798-0d30-4cf3-acc0-8d591b8a48ae
STEP: Creating a pod to test Check all projections for projected volume plugin
Feb 17 16:31:42.247: INFO: Waiting up to 5m0s for pod "projected-volume-e3e54122-bbdd-46b2-8f80-b8aab8bfc79d" in namespace "projected-9296" to be "success or failure"
Feb 17 16:31:42.263: INFO: Pod "projected-volume-e3e54122-bbdd-46b2-8f80-b8aab8bfc79d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.159755ms
Feb 17 16:31:44.283: INFO: Pod "projected-volume-e3e54122-bbdd-46b2-8f80-b8aab8bfc79d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036127623s
Feb 17 16:31:46.297: INFO: Pod "projected-volume-e3e54122-bbdd-46b2-8f80-b8aab8bfc79d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050072161s
Feb 17 16:31:48.310: INFO: Pod "projected-volume-e3e54122-bbdd-46b2-8f80-b8aab8bfc79d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063090285s
Feb 17 16:31:50.323: INFO: Pod "projected-volume-e3e54122-bbdd-46b2-8f80-b8aab8bfc79d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.076170316s
Feb 17 16:31:52.336: INFO: Pod "projected-volume-e3e54122-bbdd-46b2-8f80-b8aab8bfc79d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.089127399s
Feb 17 16:31:54.349: INFO: Pod "projected-volume-e3e54122-bbdd-46b2-8f80-b8aab8bfc79d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.102171305s
STEP: Saw pod success
Feb 17 16:31:54.349: INFO: Pod "projected-volume-e3e54122-bbdd-46b2-8f80-b8aab8bfc79d" satisfied condition "success or failure"
Feb 17 16:31:54.361: INFO: Trying to get logs from node 10.45.66.178 pod projected-volume-e3e54122-bbdd-46b2-8f80-b8aab8bfc79d container projected-all-volume-test: <nil>
STEP: delete the pod
Feb 17 16:31:54.462: INFO: Waiting for pod projected-volume-e3e54122-bbdd-46b2-8f80-b8aab8bfc79d to disappear
Feb 17 16:31:54.474: INFO: Pod projected-volume-e3e54122-bbdd-46b2-8f80-b8aab8bfc79d no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:31:54.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9296" for this suite.
Feb 17 16:32:02.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:32:04.665: INFO: namespace projected-9296 deletion completed in 10.176084676s

• [SLOW TEST:22.655 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:32:04.665: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 16:32:05.877: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 16:32:07.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:32:09.934: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:32:11.930: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:32:13.929: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:32:15.930: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:32:17.939: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 16:32:19.930: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717553925, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 16:32:22.963: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:32:33.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9977" for this suite.
Feb 17 16:32:41.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:32:43.679: INFO: namespace webhook-9977 deletion completed in 10.174237114s
STEP: Destroying namespace "webhook-9977-markers" for this suite.
Feb 17 16:32:51.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:32:54.026: INFO: namespace webhook-9977-markers deletion completed in 10.347012515s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:49.432 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:32:54.098: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2771.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2771.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2771.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2771.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2771.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2771.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 16:33:20.563: INFO: DNS probes using dns-2771/dns-test-9c667d14-49d1-4115-a7bc-53461fdf2045 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:33:20.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2771" for this suite.
Feb 17 16:33:28.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:33:30.789: INFO: namespace dns-2771 deletion completed in 10.157471946s

• [SLOW TEST:36.692 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:33:30.790: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-2l9d
STEP: Creating a pod to test atomic-volume-subpath
Feb 17 16:33:31.049: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-2l9d" in namespace "subpath-2670" to be "success or failure"
Feb 17 16:33:31.064: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.783199ms
Feb 17 16:33:33.078: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028161777s
Feb 17 16:33:35.091: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041109633s
Feb 17 16:33:37.104: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054340044s
Feb 17 16:33:39.118: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.068963503s
Feb 17 16:33:41.132: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.082532924s
Feb 17 16:33:43.146: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Running", Reason="", readiness=true. Elapsed: 12.096598582s
Feb 17 16:33:45.160: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Running", Reason="", readiness=true. Elapsed: 14.110060886s
Feb 17 16:33:47.172: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Running", Reason="", readiness=true. Elapsed: 16.122517026s
Feb 17 16:33:49.185: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Running", Reason="", readiness=true. Elapsed: 18.135321807s
Feb 17 16:33:51.199: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Running", Reason="", readiness=true. Elapsed: 20.149743109s
Feb 17 16:33:53.215: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Running", Reason="", readiness=true. Elapsed: 22.165177493s
Feb 17 16:33:55.229: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Running", Reason="", readiness=true. Elapsed: 24.179555585s
Feb 17 16:33:57.242: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Running", Reason="", readiness=true. Elapsed: 26.192502003s
Feb 17 16:33:59.256: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Running", Reason="", readiness=true. Elapsed: 28.206133768s
Feb 17 16:34:01.270: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Running", Reason="", readiness=true. Elapsed: 30.220071062s
Feb 17 16:34:03.283: INFO: Pod "pod-subpath-test-downwardapi-2l9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.233406316s
STEP: Saw pod success
Feb 17 16:34:03.283: INFO: Pod "pod-subpath-test-downwardapi-2l9d" satisfied condition "success or failure"
Feb 17 16:34:03.299: INFO: Trying to get logs from node 10.45.66.189 pod pod-subpath-test-downwardapi-2l9d container test-container-subpath-downwardapi-2l9d: <nil>
STEP: delete the pod
Feb 17 16:34:03.420: INFO: Waiting for pod pod-subpath-test-downwardapi-2l9d to disappear
Feb 17 16:34:03.436: INFO: Pod pod-subpath-test-downwardapi-2l9d no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-2l9d
Feb 17 16:34:03.437: INFO: Deleting pod "pod-subpath-test-downwardapi-2l9d" in namespace "subpath-2670"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:34:03.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2670" for this suite.
Feb 17 16:34:11.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:34:13.596: INFO: namespace subpath-2670 deletion completed in 10.125746582s

• [SLOW TEST:42.806 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:34:13.596: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6430.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6430.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 16:34:38.039: INFO: DNS probes using dns-6430/dns-test-9a201145-3191-4958-be01-59ef29c76e12 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:34:38.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6430" for this suite.
Feb 17 16:34:46.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:34:48.237: INFO: namespace dns-6430 deletion completed in 10.13674522s

• [SLOW TEST:34.642 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:34:48.239: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:34:48.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5127" for this suite.
Feb 17 16:34:56.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:34:58.540: INFO: namespace services-5127 deletion completed in 10.119568343s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:10.301 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:34:58.540: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Feb 17 16:35:06.790: INFO: Pod pod-hostip-aef4efa0-ee18-4faf-bb01-258eead9dd34 has hostIP: 10.45.66.177
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:35:06.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1306" for this suite.
Feb 17 16:35:38.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:35:40.966: INFO: namespace pods-1306 deletion completed in 34.159065944s

• [SLOW TEST:42.426 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:35:40.970: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-8473f0fc-3a85-46ac-9e82-d409b6060493
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:35:41.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2694" for this suite.
Feb 17 16:35:49.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:35:51.286: INFO: namespace configmap-2694 deletion completed in 10.138691534s

• [SLOW TEST:10.316 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:35:51.291: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 17 16:36:13.636: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 16:36:13.648: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 16:36:15.649: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 16:36:15.661: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 16:36:17.649: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 16:36:17.661: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 16:36:19.649: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 16:36:19.664: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 16:36:21.650: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 16:36:21.664: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 16:36:23.649: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 16:36:23.666: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 16:36:25.649: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 16:36:25.661: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 16:36:27.649: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 16:36:27.662: INFO: Pod pod-with-prestop-exec-hook still exists
Feb 17 16:36:29.649: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Feb 17 16:36:29.666: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:36:29.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-37" for this suite.
Feb 17 16:36:55.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:36:57.920: INFO: namespace container-lifecycle-hook-37 deletion completed in 28.155938888s

• [SLOW TEST:66.630 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:36:57.921: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 16:36:58.064: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:37:12.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9129" for this suite.
Feb 17 16:37:56.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:37:58.444: INFO: namespace pods-9129 deletion completed in 46.143166989s

• [SLOW TEST:60.524 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:37:58.445: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 16:37:58.566: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Feb 17 16:37:59.682: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:38:00.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8992" for this suite.
Feb 17 16:38:08.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:38:10.887: INFO: namespace replication-controller-8992 deletion completed in 10.158927972s

• [SLOW TEST:12.442 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:38:10.888: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-6189
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Feb 17 16:38:11.106: INFO: Found 0 stateful pods, waiting for 3
Feb 17 16:38:21.120: INFO: Found 2 stateful pods, waiting for 3
Feb 17 16:38:31.120: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 16:38:31.120: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 16:38:31.120: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 17 16:38:41.121: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 16:38:41.121: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 16:38:41.121: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 17 16:38:51.120: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 16:38:51.120: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 16:38:51.120: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 16:38:51.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-6189 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 16:38:51.770: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 16:38:51.770: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 16:38:51.770: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 17 16:39:01.860: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Feb 17 16:39:11.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-6189 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 16:39:12.292: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 16:39:12.292: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 16:39:12.292: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 16:39:22.370: INFO: Waiting for StatefulSet statefulset-6189/ss2 to complete update
Feb 17 16:39:22.370: INFO: Waiting for Pod statefulset-6189/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 16:39:22.370: INFO: Waiting for Pod statefulset-6189/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 16:39:32.395: INFO: Waiting for StatefulSet statefulset-6189/ss2 to complete update
Feb 17 16:39:32.395: INFO: Waiting for Pod statefulset-6189/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 16:39:32.395: INFO: Waiting for Pod statefulset-6189/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 16:39:42.399: INFO: Waiting for StatefulSet statefulset-6189/ss2 to complete update
Feb 17 16:39:42.399: INFO: Waiting for Pod statefulset-6189/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 16:39:52.397: INFO: Waiting for StatefulSet statefulset-6189/ss2 to complete update
Feb 17 16:39:52.397: INFO: Waiting for Pod statefulset-6189/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 16:40:02.395: INFO: Waiting for StatefulSet statefulset-6189/ss2 to complete update
Feb 17 16:40:02.395: INFO: Waiting for Pod statefulset-6189/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 16:40:12.396: INFO: Waiting for StatefulSet statefulset-6189/ss2 to complete update
Feb 17 16:40:22.396: INFO: Waiting for StatefulSet statefulset-6189/ss2 to complete update
STEP: Rolling back to a previous revision
Feb 17 16:40:32.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-6189 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 16:40:32.755: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 16:40:32.755: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 16:40:32.755: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 16:40:42.849: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Feb 17 16:40:52.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-6189 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 16:40:54.260: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 16:40:54.260: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 16:40:54.260: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 16:40:54.327: INFO: Waiting for StatefulSet statefulset-6189/ss2 to complete update
Feb 17 16:40:54.327: INFO: Waiting for Pod statefulset-6189/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 16:40:54.327: INFO: Waiting for Pod statefulset-6189/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 16:40:54.327: INFO: Waiting for Pod statefulset-6189/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 16:41:04.352: INFO: Waiting for StatefulSet statefulset-6189/ss2 to complete update
Feb 17 16:41:04.352: INFO: Waiting for Pod statefulset-6189/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 16:41:04.352: INFO: Waiting for Pod statefulset-6189/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 16:41:14.353: INFO: Waiting for StatefulSet statefulset-6189/ss2 to complete update
Feb 17 16:41:14.353: INFO: Waiting for Pod statefulset-6189/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 16:41:24.352: INFO: Waiting for StatefulSet statefulset-6189/ss2 to complete update
Feb 17 16:41:24.352: INFO: Waiting for Pod statefulset-6189/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Feb 17 16:41:34.354: INFO: Waiting for StatefulSet statefulset-6189/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 17 16:41:44.353: INFO: Deleting all statefulset in ns statefulset-6189
Feb 17 16:41:44.364: INFO: Scaling statefulset ss2 to 0
Feb 17 16:42:04.419: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 16:42:04.430: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:42:04.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6189" for this suite.
Feb 17 16:42:12.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:42:14.687: INFO: namespace statefulset-6189 deletion completed in 10.183288225s

• [SLOW TEST:243.799 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:42:14.687: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Feb 17 16:42:31.029: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6704 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 16:42:31.030: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 16:42:31.278: INFO: Exec stderr: ""
Feb 17 16:42:31.279: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6704 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 16:42:31.279: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 16:42:31.508: INFO: Exec stderr: ""
Feb 17 16:42:31.508: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6704 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 16:42:31.508: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 16:42:31.727: INFO: Exec stderr: ""
Feb 17 16:42:31.727: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6704 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 16:42:31.727: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 16:42:31.932: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Feb 17 16:42:31.932: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6704 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 16:42:31.932: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 16:42:32.163: INFO: Exec stderr: ""
Feb 17 16:42:32.163: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6704 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 16:42:32.163: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 16:42:32.391: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Feb 17 16:42:32.392: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6704 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 16:42:32.392: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 16:42:32.601: INFO: Exec stderr: ""
Feb 17 16:42:32.601: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6704 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 16:42:32.601: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 16:42:32.798: INFO: Exec stderr: ""
Feb 17 16:42:32.798: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6704 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 16:42:32.798: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 16:42:33.015: INFO: Exec stderr: ""
Feb 17 16:42:33.016: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6704 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 16:42:33.016: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 16:42:33.245: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:42:33.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6704" for this suite.
Feb 17 16:43:23.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:43:25.486: INFO: namespace e2e-kubelet-etc-hosts-6704 deletion completed in 52.217896758s

• [SLOW TEST:70.799 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:43:25.486: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-704f6836-0ed4-4600-8776-bc55bc561654
STEP: Creating a pod to test consume secrets
Feb 17 16:43:25.694: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-12e30553-9bc9-4ff8-8e09-b3291ab16f92" in namespace "projected-358" to be "success or failure"
Feb 17 16:43:25.708: INFO: Pod "pod-projected-secrets-12e30553-9bc9-4ff8-8e09-b3291ab16f92": Phase="Pending", Reason="", readiness=false. Elapsed: 13.878766ms
Feb 17 16:43:27.721: INFO: Pod "pod-projected-secrets-12e30553-9bc9-4ff8-8e09-b3291ab16f92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026782388s
Feb 17 16:43:29.734: INFO: Pod "pod-projected-secrets-12e30553-9bc9-4ff8-8e09-b3291ab16f92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03966065s
Feb 17 16:43:31.747: INFO: Pod "pod-projected-secrets-12e30553-9bc9-4ff8-8e09-b3291ab16f92": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053000888s
Feb 17 16:43:33.763: INFO: Pod "pod-projected-secrets-12e30553-9bc9-4ff8-8e09-b3291ab16f92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.06895816s
STEP: Saw pod success
Feb 17 16:43:33.763: INFO: Pod "pod-projected-secrets-12e30553-9bc9-4ff8-8e09-b3291ab16f92" satisfied condition "success or failure"
Feb 17 16:43:33.775: INFO: Trying to get logs from node 10.45.66.189 pod pod-projected-secrets-12e30553-9bc9-4ff8-8e09-b3291ab16f92 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 17 16:43:33.893: INFO: Waiting for pod pod-projected-secrets-12e30553-9bc9-4ff8-8e09-b3291ab16f92 to disappear
Feb 17 16:43:33.906: INFO: Pod pod-projected-secrets-12e30553-9bc9-4ff8-8e09-b3291ab16f92 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:43:33.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-358" for this suite.
Feb 17 16:43:41.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:43:44.143: INFO: namespace projected-358 deletion completed in 10.209360675s

• [SLOW TEST:18.657 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:43:44.144: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 16:43:44.314: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 17 16:43:51.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-4392 create -f -'
Feb 17 16:43:52.231: INFO: stderr: ""
Feb 17 16:43:52.231: INFO: stdout: "e2e-test-crd-publish-openapi-9503-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 17 16:43:52.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-4392 delete e2e-test-crd-publish-openapi-9503-crds test-cr'
Feb 17 16:43:52.447: INFO: stderr: ""
Feb 17 16:43:52.447: INFO: stdout: "e2e-test-crd-publish-openapi-9503-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Feb 17 16:43:52.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-4392 apply -f -'
Feb 17 16:43:53.032: INFO: stderr: ""
Feb 17 16:43:53.032: INFO: stdout: "e2e-test-crd-publish-openapi-9503-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Feb 17 16:43:53.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-4392 delete e2e-test-crd-publish-openapi-9503-crds test-cr'
Feb 17 16:43:53.169: INFO: stderr: ""
Feb 17 16:43:53.169: INFO: stdout: "e2e-test-crd-publish-openapi-9503-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 17 16:43:53.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 explain e2e-test-crd-publish-openapi-9503-crds'
Feb 17 16:43:53.702: INFO: stderr: ""
Feb 17 16:43:53.703: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9503-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:44:00.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4392" for this suite.
Feb 17 16:44:08.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:44:10.776: INFO: namespace crd-publish-openapi-4392 deletion completed in 10.096695132s

• [SLOW TEST:26.633 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:44:10.777: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-2430/secret-test-a2201ead-3c10-420f-b4cb-09f0df260102
STEP: Creating a pod to test consume secrets
Feb 17 16:44:10.981: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9c5194c-7939-4a40-b5e3-966895774bfe" in namespace "secrets-2430" to be "success or failure"
Feb 17 16:44:10.988: INFO: Pod "pod-configmaps-e9c5194c-7939-4a40-b5e3-966895774bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.586809ms
Feb 17 16:44:12.996: INFO: Pod "pod-configmaps-e9c5194c-7939-4a40-b5e3-966895774bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014402154s
Feb 17 16:44:15.003: INFO: Pod "pod-configmaps-e9c5194c-7939-4a40-b5e3-966895774bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021500078s
Feb 17 16:44:17.011: INFO: Pod "pod-configmaps-e9c5194c-7939-4a40-b5e3-966895774bfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.029324216s
Feb 17 16:44:19.018: INFO: Pod "pod-configmaps-e9c5194c-7939-4a40-b5e3-966895774bfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.036572186s
STEP: Saw pod success
Feb 17 16:44:19.018: INFO: Pod "pod-configmaps-e9c5194c-7939-4a40-b5e3-966895774bfe" satisfied condition "success or failure"
Feb 17 16:44:19.026: INFO: Trying to get logs from node 10.45.66.178 pod pod-configmaps-e9c5194c-7939-4a40-b5e3-966895774bfe container env-test: <nil>
STEP: delete the pod
Feb 17 16:44:19.110: INFO: Waiting for pod pod-configmaps-e9c5194c-7939-4a40-b5e3-966895774bfe to disappear
Feb 17 16:44:19.116: INFO: Pod pod-configmaps-e9c5194c-7939-4a40-b5e3-966895774bfe no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:44:19.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2430" for this suite.
Feb 17 16:44:27.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:44:29.221: INFO: namespace secrets-2430 deletion completed in 10.091452034s

• [SLOW TEST:18.444 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:44:29.222: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 16:44:29.417: INFO: Waiting up to 5m0s for pod "downwardapi-volume-679d1558-4994-42c2-8f45-d44be7b82e4b" in namespace "downward-api-7668" to be "success or failure"
Feb 17 16:44:29.425: INFO: Pod "downwardapi-volume-679d1558-4994-42c2-8f45-d44be7b82e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.266417ms
Feb 17 16:44:31.432: INFO: Pod "downwardapi-volume-679d1558-4994-42c2-8f45-d44be7b82e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014760739s
Feb 17 16:44:33.440: INFO: Pod "downwardapi-volume-679d1558-4994-42c2-8f45-d44be7b82e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022356866s
Feb 17 16:44:35.448: INFO: Pod "downwardapi-volume-679d1558-4994-42c2-8f45-d44be7b82e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030347758s
Feb 17 16:44:37.455: INFO: Pod "downwardapi-volume-679d1558-4994-42c2-8f45-d44be7b82e4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.037512659s
STEP: Saw pod success
Feb 17 16:44:37.455: INFO: Pod "downwardapi-volume-679d1558-4994-42c2-8f45-d44be7b82e4b" satisfied condition "success or failure"
Feb 17 16:44:37.462: INFO: Trying to get logs from node 10.45.66.177 pod downwardapi-volume-679d1558-4994-42c2-8f45-d44be7b82e4b container client-container: <nil>
STEP: delete the pod
Feb 17 16:44:37.569: INFO: Waiting for pod downwardapi-volume-679d1558-4994-42c2-8f45-d44be7b82e4b to disappear
Feb 17 16:44:37.576: INFO: Pod downwardapi-volume-679d1558-4994-42c2-8f45-d44be7b82e4b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:44:37.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7668" for this suite.
Feb 17 16:44:45.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:44:47.692: INFO: namespace downward-api-7668 deletion completed in 10.096961945s

• [SLOW TEST:18.470 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:44:47.693: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:45:11.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1652" for this suite.
Feb 17 16:45:19.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:45:21.963: INFO: namespace job-1652 deletion completed in 10.095360835s

• [SLOW TEST:34.270 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:45:21.964: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 16:45:30.223: INFO: Waiting up to 5m0s for pod "client-envvars-99696008-8ad3-4ac5-8566-85ae827f1a45" in namespace "pods-3615" to be "success or failure"
Feb 17 16:45:30.229: INFO: Pod "client-envvars-99696008-8ad3-4ac5-8566-85ae827f1a45": Phase="Pending", Reason="", readiness=false. Elapsed: 5.809901ms
Feb 17 16:45:32.237: INFO: Pod "client-envvars-99696008-8ad3-4ac5-8566-85ae827f1a45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013588676s
Feb 17 16:45:34.245: INFO: Pod "client-envvars-99696008-8ad3-4ac5-8566-85ae827f1a45": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021265606s
Feb 17 16:45:36.252: INFO: Pod "client-envvars-99696008-8ad3-4ac5-8566-85ae827f1a45": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028353036s
Feb 17 16:45:38.260: INFO: Pod "client-envvars-99696008-8ad3-4ac5-8566-85ae827f1a45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.037029551s
STEP: Saw pod success
Feb 17 16:45:38.260: INFO: Pod "client-envvars-99696008-8ad3-4ac5-8566-85ae827f1a45" satisfied condition "success or failure"
Feb 17 16:45:38.267: INFO: Trying to get logs from node 10.45.66.178 pod client-envvars-99696008-8ad3-4ac5-8566-85ae827f1a45 container env3cont: <nil>
STEP: delete the pod
Feb 17 16:45:38.315: INFO: Waiting for pod client-envvars-99696008-8ad3-4ac5-8566-85ae827f1a45 to disappear
Feb 17 16:45:38.324: INFO: Pod client-envvars-99696008-8ad3-4ac5-8566-85ae827f1a45 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:45:38.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3615" for this suite.
Feb 17 16:46:10.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:46:12.463: INFO: namespace pods-3615 deletion completed in 34.123749714s

• [SLOW TEST:50.499 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:46:12.463: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:46:36.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2690" for this suite.
Feb 17 16:46:45.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:46:47.108: INFO: namespace namespaces-2690 deletion completed in 10.103062309s
STEP: Destroying namespace "nsdeletetest-8137" for this suite.
Feb 17 16:46:47.120: INFO: Namespace nsdeletetest-8137 was already deleted
STEP: Destroying namespace "nsdeletetest-53" for this suite.
Feb 17 16:46:55.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:46:57.216: INFO: namespace nsdeletetest-53 deletion completed in 10.096689822s

• [SLOW TEST:44.754 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:46:57.218: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-e6d53595-0930-4d78-a5be-784bdd9a5c1b in namespace container-probe-2435
Feb 17 16:47:05.445: INFO: Started pod busybox-e6d53595-0930-4d78-a5be-784bdd9a5c1b in namespace container-probe-2435
STEP: checking the pod's current state and verifying that restartCount is present
Feb 17 16:47:05.452: INFO: Initial restart count of pod busybox-e6d53595-0930-4d78-a5be-784bdd9a5c1b is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:51:06.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2435" for this suite.
Feb 17 16:51:14.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:51:16.563: INFO: namespace container-probe-2435 deletion completed in 10.102331259s

• [SLOW TEST:259.346 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:51:16.563: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Feb 17 16:51:25.367: INFO: Successfully updated pod "labelsupdate377144cb-5f68-4be8-9270-4d8477d28c89"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:51:29.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2168" for this suite.
Feb 17 16:51:43.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:51:45.565: INFO: namespace downward-api-2168 deletion completed in 16.098188019s

• [SLOW TEST:29.002 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:51:45.568: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-d0bb9bf2-1774-4585-961c-5e0ebe1d4bdb
STEP: Creating a pod to test consume configMaps
Feb 17 16:51:45.788: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1cf46e9e-b981-4c49-9ff7-6b12c8dbe315" in namespace "projected-2833" to be "success or failure"
Feb 17 16:51:45.795: INFO: Pod "pod-projected-configmaps-1cf46e9e-b981-4c49-9ff7-6b12c8dbe315": Phase="Pending", Reason="", readiness=false. Elapsed: 6.813526ms
Feb 17 16:51:47.803: INFO: Pod "pod-projected-configmaps-1cf46e9e-b981-4c49-9ff7-6b12c8dbe315": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014436005s
Feb 17 16:51:49.812: INFO: Pod "pod-projected-configmaps-1cf46e9e-b981-4c49-9ff7-6b12c8dbe315": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023928432s
Feb 17 16:51:51.822: INFO: Pod "pod-projected-configmaps-1cf46e9e-b981-4c49-9ff7-6b12c8dbe315": Phase="Pending", Reason="", readiness=false. Elapsed: 6.034196686s
Feb 17 16:51:53.832: INFO: Pod "pod-projected-configmaps-1cf46e9e-b981-4c49-9ff7-6b12c8dbe315": Phase="Pending", Reason="", readiness=false. Elapsed: 8.044162975s
Feb 17 16:51:55.841: INFO: Pod "pod-projected-configmaps-1cf46e9e-b981-4c49-9ff7-6b12c8dbe315": Phase="Pending", Reason="", readiness=false. Elapsed: 10.052686744s
Feb 17 16:51:57.852: INFO: Pod "pod-projected-configmaps-1cf46e9e-b981-4c49-9ff7-6b12c8dbe315": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.063759285s
STEP: Saw pod success
Feb 17 16:51:57.852: INFO: Pod "pod-projected-configmaps-1cf46e9e-b981-4c49-9ff7-6b12c8dbe315" satisfied condition "success or failure"
Feb 17 16:51:57.859: INFO: Trying to get logs from node 10.45.66.178 pod pod-projected-configmaps-1cf46e9e-b981-4c49-9ff7-6b12c8dbe315 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 16:51:57.938: INFO: Waiting for pod pod-projected-configmaps-1cf46e9e-b981-4c49-9ff7-6b12c8dbe315 to disappear
Feb 17 16:51:57.946: INFO: Pod pod-projected-configmaps-1cf46e9e-b981-4c49-9ff7-6b12c8dbe315 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:51:57.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2833" for this suite.
Feb 17 16:52:06.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:52:08.064: INFO: namespace projected-2833 deletion completed in 10.098315907s

• [SLOW TEST:22.496 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:52:08.064: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-f6h6
STEP: Creating a pod to test atomic-volume-subpath
Feb 17 16:52:08.299: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-f6h6" in namespace "subpath-793" to be "success or failure"
Feb 17 16:52:08.305: INFO: Pod "pod-subpath-test-configmap-f6h6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.4063ms
Feb 17 16:52:10.315: INFO: Pod "pod-subpath-test-configmap-f6h6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015868431s
Feb 17 16:52:12.322: INFO: Pod "pod-subpath-test-configmap-f6h6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023357638s
Feb 17 16:52:14.330: INFO: Pod "pod-subpath-test-configmap-f6h6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031023997s
Feb 17 16:52:16.338: INFO: Pod "pod-subpath-test-configmap-f6h6": Phase="Running", Reason="", readiness=true. Elapsed: 8.039300511s
Feb 17 16:52:18.346: INFO: Pod "pod-subpath-test-configmap-f6h6": Phase="Running", Reason="", readiness=true. Elapsed: 10.046787284s
Feb 17 16:52:20.354: INFO: Pod "pod-subpath-test-configmap-f6h6": Phase="Running", Reason="", readiness=true. Elapsed: 12.054735875s
Feb 17 16:52:22.366: INFO: Pod "pod-subpath-test-configmap-f6h6": Phase="Running", Reason="", readiness=true. Elapsed: 14.066675889s
Feb 17 16:52:24.373: INFO: Pod "pod-subpath-test-configmap-f6h6": Phase="Running", Reason="", readiness=true. Elapsed: 16.074283577s
Feb 17 16:52:26.381: INFO: Pod "pod-subpath-test-configmap-f6h6": Phase="Running", Reason="", readiness=true. Elapsed: 18.082156401s
Feb 17 16:52:28.391: INFO: Pod "pod-subpath-test-configmap-f6h6": Phase="Running", Reason="", readiness=true. Elapsed: 20.091817208s
Feb 17 16:52:30.398: INFO: Pod "pod-subpath-test-configmap-f6h6": Phase="Running", Reason="", readiness=true. Elapsed: 22.099206774s
Feb 17 16:52:32.414: INFO: Pod "pod-subpath-test-configmap-f6h6": Phase="Running", Reason="", readiness=true. Elapsed: 24.114777546s
Feb 17 16:52:34.422: INFO: Pod "pod-subpath-test-configmap-f6h6": Phase="Running", Reason="", readiness=true. Elapsed: 26.123034546s
Feb 17 16:52:36.429: INFO: Pod "pod-subpath-test-configmap-f6h6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.130462589s
STEP: Saw pod success
Feb 17 16:52:36.430: INFO: Pod "pod-subpath-test-configmap-f6h6" satisfied condition "success or failure"
Feb 17 16:52:36.436: INFO: Trying to get logs from node 10.45.66.189 pod pod-subpath-test-configmap-f6h6 container test-container-subpath-configmap-f6h6: <nil>
STEP: delete the pod
Feb 17 16:52:36.488: INFO: Waiting for pod pod-subpath-test-configmap-f6h6 to disappear
Feb 17 16:52:36.494: INFO: Pod pod-subpath-test-configmap-f6h6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-f6h6
Feb 17 16:52:36.494: INFO: Deleting pod "pod-subpath-test-configmap-f6h6" in namespace "subpath-793"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:52:36.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-793" for this suite.
Feb 17 16:52:44.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:52:46.612: INFO: namespace subpath-793 deletion completed in 10.094303703s

• [SLOW TEST:38.549 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:52:46.615: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-84738de0-e24d-4b8d-9bd9-f1ae959f2dfa
STEP: Creating a pod to test consume secrets
Feb 17 16:52:46.842: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8f7caf1d-507e-4509-8c35-f5217a4c3471" in namespace "projected-5890" to be "success or failure"
Feb 17 16:52:46.849: INFO: Pod "pod-projected-secrets-8f7caf1d-507e-4509-8c35-f5217a4c3471": Phase="Pending", Reason="", readiness=false. Elapsed: 6.87351ms
Feb 17 16:52:48.858: INFO: Pod "pod-projected-secrets-8f7caf1d-507e-4509-8c35-f5217a4c3471": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015596234s
Feb 17 16:52:50.872: INFO: Pod "pod-projected-secrets-8f7caf1d-507e-4509-8c35-f5217a4c3471": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029769446s
Feb 17 16:52:52.879: INFO: Pod "pod-projected-secrets-8f7caf1d-507e-4509-8c35-f5217a4c3471": Phase="Pending", Reason="", readiness=false. Elapsed: 6.037040623s
Feb 17 16:52:54.886: INFO: Pod "pod-projected-secrets-8f7caf1d-507e-4509-8c35-f5217a4c3471": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.04408797s
STEP: Saw pod success
Feb 17 16:52:54.886: INFO: Pod "pod-projected-secrets-8f7caf1d-507e-4509-8c35-f5217a4c3471" satisfied condition "success or failure"
Feb 17 16:52:54.893: INFO: Trying to get logs from node 10.45.66.178 pod pod-projected-secrets-8f7caf1d-507e-4509-8c35-f5217a4c3471 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 17 16:52:54.942: INFO: Waiting for pod pod-projected-secrets-8f7caf1d-507e-4509-8c35-f5217a4c3471 to disappear
Feb 17 16:52:54.949: INFO: Pod pod-projected-secrets-8f7caf1d-507e-4509-8c35-f5217a4c3471 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:52:54.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5890" for this suite.
Feb 17 16:53:03.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:53:05.068: INFO: namespace projected-5890 deletion completed in 10.100871601s

• [SLOW TEST:18.453 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:53:05.068: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:53:13.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4080" for this suite.
Feb 17 16:54:01.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:54:03.414: INFO: namespace kubelet-test-4080 deletion completed in 50.107292419s

• [SLOW TEST:58.346 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:54:03.417: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:54:08.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5967" for this suite.
Feb 17 16:54:16.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:54:18.691: INFO: namespace watch-5967 deletion completed in 10.153627743s

• [SLOW TEST:15.274 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:54:18.691: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 16:54:18.829: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-fb13a5b5-fe5b-4a39-b38e-48b4ed530588
STEP: Creating configMap with name cm-test-opt-upd-ccde3d1c-e735-4b7b-9e82-631492ce6d78
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-fb13a5b5-fe5b-4a39-b38e-48b4ed530588
STEP: Updating configmap cm-test-opt-upd-ccde3d1c-e735-4b7b-9e82-631492ce6d78
STEP: Creating configMap with name cm-test-opt-create-24d62a14-80bc-4772-aeec-cbb631000d97
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:54:29.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3996" for this suite.
Feb 17 16:55:01.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:55:03.351: INFO: namespace projected-3996 deletion completed in 34.100324811s

• [SLOW TEST:44.660 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:55:03.351: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-0afbe5b4-b99a-4575-ad2c-f9d210391f8a
STEP: Creating a pod to test consume configMaps
Feb 17 16:55:03.563: INFO: Waiting up to 5m0s for pod "pod-configmaps-f3db8d63-6374-4038-b867-d2f26735b461" in namespace "configmap-7472" to be "success or failure"
Feb 17 16:55:03.575: INFO: Pod "pod-configmaps-f3db8d63-6374-4038-b867-d2f26735b461": Phase="Pending", Reason="", readiness=false. Elapsed: 12.156728ms
Feb 17 16:55:05.583: INFO: Pod "pod-configmaps-f3db8d63-6374-4038-b867-d2f26735b461": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019675165s
Feb 17 16:55:07.591: INFO: Pod "pod-configmaps-f3db8d63-6374-4038-b867-d2f26735b461": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02828139s
Feb 17 16:55:09.602: INFO: Pod "pod-configmaps-f3db8d63-6374-4038-b867-d2f26735b461": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038573423s
Feb 17 16:55:11.609: INFO: Pod "pod-configmaps-f3db8d63-6374-4038-b867-d2f26735b461": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.045694855s
STEP: Saw pod success
Feb 17 16:55:11.609: INFO: Pod "pod-configmaps-f3db8d63-6374-4038-b867-d2f26735b461" satisfied condition "success or failure"
Feb 17 16:55:11.615: INFO: Trying to get logs from node 10.45.66.189 pod pod-configmaps-f3db8d63-6374-4038-b867-d2f26735b461 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 16:55:11.695: INFO: Waiting for pod pod-configmaps-f3db8d63-6374-4038-b867-d2f26735b461 to disappear
Feb 17 16:55:11.701: INFO: Pod pod-configmaps-f3db8d63-6374-4038-b867-d2f26735b461 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:55:11.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7472" for this suite.
Feb 17 16:55:19.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:55:21.821: INFO: namespace configmap-7472 deletion completed in 10.10563269s

• [SLOW TEST:18.469 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:55:21.821: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-b1b85f5b-9e9d-40c9-b3cd-f8aa1d58ee61
STEP: Creating a pod to test consume configMaps
Feb 17 16:55:22.070: INFO: Waiting up to 5m0s for pod "pod-configmaps-e10f55c9-3998-4963-afdd-1a7b55c389bc" in namespace "configmap-982" to be "success or failure"
Feb 17 16:55:22.077: INFO: Pod "pod-configmaps-e10f55c9-3998-4963-afdd-1a7b55c389bc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.371894ms
Feb 17 16:55:24.085: INFO: Pod "pod-configmaps-e10f55c9-3998-4963-afdd-1a7b55c389bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01540523s
Feb 17 16:55:26.093: INFO: Pod "pod-configmaps-e10f55c9-3998-4963-afdd-1a7b55c389bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023188017s
Feb 17 16:55:28.110: INFO: Pod "pod-configmaps-e10f55c9-3998-4963-afdd-1a7b55c389bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040028088s
Feb 17 16:55:30.117: INFO: Pod "pod-configmaps-e10f55c9-3998-4963-afdd-1a7b55c389bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.046967369s
STEP: Saw pod success
Feb 17 16:55:30.117: INFO: Pod "pod-configmaps-e10f55c9-3998-4963-afdd-1a7b55c389bc" satisfied condition "success or failure"
Feb 17 16:55:30.124: INFO: Trying to get logs from node 10.45.66.178 pod pod-configmaps-e10f55c9-3998-4963-afdd-1a7b55c389bc container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 16:55:30.175: INFO: Waiting for pod pod-configmaps-e10f55c9-3998-4963-afdd-1a7b55c389bc to disappear
Feb 17 16:55:30.183: INFO: Pod pod-configmaps-e10f55c9-3998-4963-afdd-1a7b55c389bc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:55:30.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-982" for this suite.
Feb 17 16:55:38.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:55:40.315: INFO: namespace configmap-982 deletion completed in 10.116881659s

• [SLOW TEST:18.494 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:55:40.315: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Feb 17 16:56:11.132: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:56:11.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0217 16:56:11.132757      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3664" for this suite.
Feb 17 16:56:19.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:56:21.252: INFO: namespace gc-3664 deletion completed in 10.102472811s

• [SLOW TEST:40.937 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:56:21.255: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 16:56:21.472: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8819753b-af53-4cb2-82f2-fdc7d4c6e118" in namespace "projected-1643" to be "success or failure"
Feb 17 16:56:21.484: INFO: Pod "downwardapi-volume-8819753b-af53-4cb2-82f2-fdc7d4c6e118": Phase="Pending", Reason="", readiness=false. Elapsed: 11.290557ms
Feb 17 16:56:23.492: INFO: Pod "downwardapi-volume-8819753b-af53-4cb2-82f2-fdc7d4c6e118": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019105918s
Feb 17 16:56:25.500: INFO: Pod "downwardapi-volume-8819753b-af53-4cb2-82f2-fdc7d4c6e118": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027656932s
Feb 17 16:56:27.508: INFO: Pod "downwardapi-volume-8819753b-af53-4cb2-82f2-fdc7d4c6e118": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035224505s
Feb 17 16:56:29.515: INFO: Pod "downwardapi-volume-8819753b-af53-4cb2-82f2-fdc7d4c6e118": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.042428884s
STEP: Saw pod success
Feb 17 16:56:29.515: INFO: Pod "downwardapi-volume-8819753b-af53-4cb2-82f2-fdc7d4c6e118" satisfied condition "success or failure"
Feb 17 16:56:29.522: INFO: Trying to get logs from node 10.45.66.189 pod downwardapi-volume-8819753b-af53-4cb2-82f2-fdc7d4c6e118 container client-container: <nil>
STEP: delete the pod
Feb 17 16:56:29.577: INFO: Waiting for pod downwardapi-volume-8819753b-af53-4cb2-82f2-fdc7d4c6e118 to disappear
Feb 17 16:56:29.584: INFO: Pod downwardapi-volume-8819753b-af53-4cb2-82f2-fdc7d4c6e118 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:56:29.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1643" for this suite.
Feb 17 16:56:37.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:56:39.710: INFO: namespace projected-1643 deletion completed in 10.108526106s

• [SLOW TEST:18.456 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:56:39.710: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 17 16:56:39.895: INFO: Waiting up to 5m0s for pod "pod-7e943d18-bb69-40e3-b858-3786312b2b6b" in namespace "emptydir-8235" to be "success or failure"
Feb 17 16:56:39.902: INFO: Pod "pod-7e943d18-bb69-40e3-b858-3786312b2b6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.767541ms
Feb 17 16:56:41.909: INFO: Pod "pod-7e943d18-bb69-40e3-b858-3786312b2b6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013882113s
Feb 17 16:56:43.917: INFO: Pod "pod-7e943d18-bb69-40e3-b858-3786312b2b6b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02131616s
Feb 17 16:56:45.924: INFO: Pod "pod-7e943d18-bb69-40e3-b858-3786312b2b6b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028764619s
Feb 17 16:56:47.931: INFO: Pod "pod-7e943d18-bb69-40e3-b858-3786312b2b6b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.035516082s
Feb 17 16:56:49.938: INFO: Pod "pod-7e943d18-bb69-40e3-b858-3786312b2b6b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.042820862s
Feb 17 16:56:51.945: INFO: Pod "pod-7e943d18-bb69-40e3-b858-3786312b2b6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.04977629s
STEP: Saw pod success
Feb 17 16:56:51.945: INFO: Pod "pod-7e943d18-bb69-40e3-b858-3786312b2b6b" satisfied condition "success or failure"
Feb 17 16:56:51.953: INFO: Trying to get logs from node 10.45.66.189 pod pod-7e943d18-bb69-40e3-b858-3786312b2b6b container test-container: <nil>
STEP: delete the pod
Feb 17 16:56:52.001: INFO: Waiting for pod pod-7e943d18-bb69-40e3-b858-3786312b2b6b to disappear
Feb 17 16:56:52.008: INFO: Pod pod-7e943d18-bb69-40e3-b858-3786312b2b6b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:56:52.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8235" for this suite.
Feb 17 16:57:00.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:57:02.131: INFO: namespace emptydir-8235 deletion completed in 10.104746783s

• [SLOW TEST:22.421 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:57:02.132: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 17 16:57:10.387: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:57:10.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2423" for this suite.
Feb 17 16:57:18.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:57:20.530: INFO: namespace container-runtime-2423 deletion completed in 10.09811349s

• [SLOW TEST:18.398 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:57:20.531: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 17 16:57:28.817: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:57:28.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6223" for this suite.
Feb 17 16:57:36.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:57:38.962: INFO: namespace container-runtime-6223 deletion completed in 10.097737956s

• [SLOW TEST:18.431 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:57:38.963: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Feb 17 16:57:39.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 create -f - --namespace=kubectl-374'
Feb 17 16:57:39.786: INFO: stderr: ""
Feb 17 16:57:39.786: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 17 16:57:40.793: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 16:57:40.793: INFO: Found 0 / 1
Feb 17 16:57:41.795: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 16:57:41.795: INFO: Found 0 / 1
Feb 17 16:57:42.793: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 16:57:42.793: INFO: Found 0 / 1
Feb 17 16:57:43.794: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 16:57:43.794: INFO: Found 0 / 1
Feb 17 16:57:44.793: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 16:57:44.793: INFO: Found 0 / 1
Feb 17 16:57:45.794: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 16:57:45.794: INFO: Found 0 / 1
Feb 17 16:57:46.793: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 16:57:46.793: INFO: Found 0 / 1
Feb 17 16:57:47.796: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 16:57:47.796: INFO: Found 0 / 1
Feb 17 16:57:48.798: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 16:57:48.798: INFO: Found 0 / 1
Feb 17 16:57:49.793: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 16:57:49.793: INFO: Found 0 / 1
Feb 17 16:57:50.793: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 16:57:50.793: INFO: Found 0 / 1
Feb 17 16:57:51.795: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 16:57:51.795: INFO: Found 0 / 1
Feb 17 16:57:52.794: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 16:57:52.794: INFO: Found 0 / 1
Feb 17 16:57:53.794: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 16:57:53.794: INFO: Found 1 / 1
Feb 17 16:57:53.794: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Feb 17 16:57:53.802: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 16:57:53.802: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 17 16:57:53.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 patch pod redis-master-rss75 --namespace=kubectl-374 -p {"metadata":{"annotations":{"x":"y"}}}'
Feb 17 16:57:53.971: INFO: stderr: ""
Feb 17 16:57:53.971: INFO: stdout: "pod/redis-master-rss75 patched\n"
STEP: checking annotations
Feb 17 16:57:53.977: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 16:57:53.978: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:57:53.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-374" for this suite.
Feb 17 16:58:26.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:58:28.088: INFO: namespace kubectl-374 deletion completed in 34.096372804s

• [SLOW TEST:49.125 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:58:28.088: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Feb 17 16:58:28.253: INFO: Pod name pod-release: Found 0 pods out of 1
Feb 17 16:58:33.262: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:58:34.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6726" for this suite.
Feb 17 16:58:42.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:58:44.438: INFO: namespace replication-controller-6726 deletion completed in 10.093292152s

• [SLOW TEST:16.350 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:58:44.439: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Feb 17 16:59:02.765: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 16:59:02.775: INFO: Pod pod-with-prestop-http-hook still exists
Feb 17 16:59:04.775: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 16:59:04.784: INFO: Pod pod-with-prestop-http-hook still exists
Feb 17 16:59:06.776: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 16:59:06.784: INFO: Pod pod-with-prestop-http-hook still exists
Feb 17 16:59:08.776: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 16:59:08.795: INFO: Pod pod-with-prestop-http-hook still exists
Feb 17 16:59:10.776: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 16:59:10.784: INFO: Pod pod-with-prestop-http-hook still exists
Feb 17 16:59:12.776: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 16:59:12.783: INFO: Pod pod-with-prestop-http-hook still exists
Feb 17 16:59:14.776: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 16:59:14.783: INFO: Pod pod-with-prestop-http-hook still exists
Feb 17 16:59:16.776: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 16:59:16.787: INFO: Pod pod-with-prestop-http-hook still exists
Feb 17 16:59:18.776: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Feb 17 16:59:18.783: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:59:18.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-638" for this suite.
Feb 17 16:59:32.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:59:34.954: INFO: namespace container-lifecycle-hook-638 deletion completed in 16.100021053s

• [SLOW TEST:50.515 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:59:34.954: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-3e387122-5055-4eca-bdf6-cf993b14b3f4
STEP: Creating a pod to test consume secrets
Feb 17 16:59:35.161: INFO: Waiting up to 5m0s for pod "pod-secrets-06947915-eabf-438e-87b4-ce4145b60a02" in namespace "secrets-4793" to be "success or failure"
Feb 17 16:59:35.168: INFO: Pod "pod-secrets-06947915-eabf-438e-87b4-ce4145b60a02": Phase="Pending", Reason="", readiness=false. Elapsed: 6.482065ms
Feb 17 16:59:37.175: INFO: Pod "pod-secrets-06947915-eabf-438e-87b4-ce4145b60a02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013158966s
Feb 17 16:59:39.183: INFO: Pod "pod-secrets-06947915-eabf-438e-87b4-ce4145b60a02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021208782s
Feb 17 16:59:41.190: INFO: Pod "pod-secrets-06947915-eabf-438e-87b4-ce4145b60a02": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028232621s
Feb 17 16:59:43.198: INFO: Pod "pod-secrets-06947915-eabf-438e-87b4-ce4145b60a02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.036491832s
STEP: Saw pod success
Feb 17 16:59:43.198: INFO: Pod "pod-secrets-06947915-eabf-438e-87b4-ce4145b60a02" satisfied condition "success or failure"
Feb 17 16:59:43.204: INFO: Trying to get logs from node 10.45.66.189 pod pod-secrets-06947915-eabf-438e-87b4-ce4145b60a02 container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 16:59:43.277: INFO: Waiting for pod pod-secrets-06947915-eabf-438e-87b4-ce4145b60a02 to disappear
Feb 17 16:59:43.284: INFO: Pod pod-secrets-06947915-eabf-438e-87b4-ce4145b60a02 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 16:59:43.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4793" for this suite.
Feb 17 16:59:51.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 16:59:53.413: INFO: namespace secrets-4793 deletion completed in 10.109607156s

• [SLOW TEST:18.459 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 16:59:53.416: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 17 16:59:53.602: INFO: Waiting up to 5m0s for pod "pod-e3d20af4-0a1d-4c95-8dba-fbdf7dbaf634" in namespace "emptydir-1749" to be "success or failure"
Feb 17 16:59:53.608: INFO: Pod "pod-e3d20af4-0a1d-4c95-8dba-fbdf7dbaf634": Phase="Pending", Reason="", readiness=false. Elapsed: 6.458465ms
Feb 17 16:59:55.615: INFO: Pod "pod-e3d20af4-0a1d-4c95-8dba-fbdf7dbaf634": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013343538s
Feb 17 16:59:57.624: INFO: Pod "pod-e3d20af4-0a1d-4c95-8dba-fbdf7dbaf634": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022229608s
Feb 17 16:59:59.633: INFO: Pod "pod-e3d20af4-0a1d-4c95-8dba-fbdf7dbaf634": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031343368s
Feb 17 17:00:01.640: INFO: Pod "pod-e3d20af4-0a1d-4c95-8dba-fbdf7dbaf634": Phase="Pending", Reason="", readiness=false. Elapsed: 8.038713426s
Feb 17 17:00:03.649: INFO: Pod "pod-e3d20af4-0a1d-4c95-8dba-fbdf7dbaf634": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047198021s
Feb 17 17:00:05.657: INFO: Pod "pod-e3d20af4-0a1d-4c95-8dba-fbdf7dbaf634": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.055559364s
STEP: Saw pod success
Feb 17 17:00:05.657: INFO: Pod "pod-e3d20af4-0a1d-4c95-8dba-fbdf7dbaf634" satisfied condition "success or failure"
Feb 17 17:00:05.665: INFO: Trying to get logs from node 10.45.66.178 pod pod-e3d20af4-0a1d-4c95-8dba-fbdf7dbaf634 container test-container: <nil>
STEP: delete the pod
Feb 17 17:00:05.716: INFO: Waiting for pod pod-e3d20af4-0a1d-4c95-8dba-fbdf7dbaf634 to disappear
Feb 17 17:00:05.722: INFO: Pod pod-e3d20af4-0a1d-4c95-8dba-fbdf7dbaf634 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:00:05.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1749" for this suite.
Feb 17 17:00:13.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:00:15.838: INFO: namespace emptydir-1749 deletion completed in 10.093303852s

• [SLOW TEST:22.423 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:00:15.839: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Feb 17 17:00:15.969: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-559420064 proxy --unix-socket=/tmp/kubectl-proxy-unix116097315/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:00:16.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4688" for this suite.
Feb 17 17:00:24.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:00:26.195: INFO: namespace kubectl-4688 deletion completed in 10.10939183s

• [SLOW TEST:10.356 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:00:26.195: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:00:26.324: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:00:26.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-435" for this suite.
Feb 17 17:00:34.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:00:37.055: INFO: namespace custom-resource-definition-435 deletion completed in 10.109509504s

• [SLOW TEST:10.860 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:00:37.055: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:00:37.222: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Feb 17 17:00:44.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-7709 create -f -'
Feb 17 17:00:45.079: INFO: stderr: ""
Feb 17 17:00:45.080: INFO: stdout: "e2e-test-crd-publish-openapi-983-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 17 17:00:45.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-7709 delete e2e-test-crd-publish-openapi-983-crds test-foo'
Feb 17 17:00:45.320: INFO: stderr: ""
Feb 17 17:00:45.320: INFO: stdout: "e2e-test-crd-publish-openapi-983-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Feb 17 17:00:45.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-7709 apply -f -'
Feb 17 17:00:45.723: INFO: stderr: ""
Feb 17 17:00:45.723: INFO: stdout: "e2e-test-crd-publish-openapi-983-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Feb 17 17:00:45.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-7709 delete e2e-test-crd-publish-openapi-983-crds test-foo'
Feb 17 17:00:45.972: INFO: stderr: ""
Feb 17 17:00:45.972: INFO: stdout: "e2e-test-crd-publish-openapi-983-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Feb 17 17:00:45.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-7709 create -f -'
Feb 17 17:00:46.311: INFO: rc: 1
Feb 17 17:00:46.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-7709 apply -f -'
Feb 17 17:00:46.654: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Feb 17 17:00:46.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-7709 create -f -'
Feb 17 17:00:47.212: INFO: rc: 1
Feb 17 17:00:47.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-7709 apply -f -'
Feb 17 17:00:47.548: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Feb 17 17:00:47.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 explain e2e-test-crd-publish-openapi-983-crds'
Feb 17 17:00:48.063: INFO: stderr: ""
Feb 17 17:00:48.063: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-983-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Feb 17 17:00:48.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 explain e2e-test-crd-publish-openapi-983-crds.metadata'
Feb 17 17:00:48.426: INFO: stderr: ""
Feb 17 17:00:48.427: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-983-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Feb 17 17:00:48.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 explain e2e-test-crd-publish-openapi-983-crds.spec'
Feb 17 17:00:48.782: INFO: stderr: ""
Feb 17 17:00:48.782: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-983-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Feb 17 17:00:48.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 explain e2e-test-crd-publish-openapi-983-crds.spec.bars'
Feb 17 17:00:49.146: INFO: stderr: ""
Feb 17 17:00:49.146: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-983-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Feb 17 17:00:49.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 explain e2e-test-crd-publish-openapi-983-crds.spec.bars2'
Feb 17 17:00:49.502: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:00:58.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7709" for this suite.
Feb 17 17:01:08.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:01:10.180: INFO: namespace crd-publish-openapi-7709 deletion completed in 12.098237987s

• [SLOW TEST:33.125 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:01:10.181: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-d4693e36-2c60-4c4f-aa9f-a4ecab1b1747
STEP: Creating a pod to test consume secrets
Feb 17 17:01:10.395: INFO: Waiting up to 5m0s for pod "pod-secrets-b969355f-1e7d-48d8-936b-c59af0f8f707" in namespace "secrets-7352" to be "success or failure"
Feb 17 17:01:10.407: INFO: Pod "pod-secrets-b969355f-1e7d-48d8-936b-c59af0f8f707": Phase="Pending", Reason="", readiness=false. Elapsed: 12.314519ms
Feb 17 17:01:12.419: INFO: Pod "pod-secrets-b969355f-1e7d-48d8-936b-c59af0f8f707": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024249867s
Feb 17 17:01:14.431: INFO: Pod "pod-secrets-b969355f-1e7d-48d8-936b-c59af0f8f707": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036049938s
Feb 17 17:01:16.445: INFO: Pod "pod-secrets-b969355f-1e7d-48d8-936b-c59af0f8f707": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049913798s
Feb 17 17:01:18.460: INFO: Pod "pod-secrets-b969355f-1e7d-48d8-936b-c59af0f8f707": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.06533898s
STEP: Saw pod success
Feb 17 17:01:18.461: INFO: Pod "pod-secrets-b969355f-1e7d-48d8-936b-c59af0f8f707" satisfied condition "success or failure"
Feb 17 17:01:18.475: INFO: Trying to get logs from node 10.45.66.189 pod pod-secrets-b969355f-1e7d-48d8-936b-c59af0f8f707 container secret-env-test: <nil>
STEP: delete the pod
Feb 17 17:01:18.582: INFO: Waiting for pod pod-secrets-b969355f-1e7d-48d8-936b-c59af0f8f707 to disappear
Feb 17 17:01:18.594: INFO: Pod pod-secrets-b969355f-1e7d-48d8-936b-c59af0f8f707 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:01:18.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7352" for this suite.
Feb 17 17:01:26.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:01:28.718: INFO: namespace secrets-7352 deletion completed in 10.108382841s

• [SLOW TEST:18.537 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:01:28.718: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:01:39.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-869" for this suite.
Feb 17 17:02:01.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:02:03.150: INFO: namespace containers-869 deletion completed in 24.116029262s

• [SLOW TEST:34.432 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:02:03.151: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:02:03.405: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Feb 17 17:02:03.468: INFO: Number of nodes with available pods: 0
Feb 17 17:02:03.468: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:02:04.495: INFO: Number of nodes with available pods: 0
Feb 17 17:02:04.495: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:02:05.496: INFO: Number of nodes with available pods: 0
Feb 17 17:02:05.496: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:02:06.496: INFO: Number of nodes with available pods: 0
Feb 17 17:02:06.496: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:02:07.500: INFO: Number of nodes with available pods: 0
Feb 17 17:02:07.500: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:02:08.503: INFO: Number of nodes with available pods: 0
Feb 17 17:02:08.503: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:02:09.497: INFO: Number of nodes with available pods: 0
Feb 17 17:02:09.498: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:02:10.505: INFO: Number of nodes with available pods: 0
Feb 17 17:02:10.505: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:02:11.495: INFO: Number of nodes with available pods: 3
Feb 17 17:02:11.495: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Feb 17 17:02:11.594: INFO: Wrong image for pod: daemon-set-brg28. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:11.594: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:11.594: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:12.622: INFO: Wrong image for pod: daemon-set-brg28. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:12.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:12.622: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:13.622: INFO: Wrong image for pod: daemon-set-brg28. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:13.623: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:13.623: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:14.625: INFO: Wrong image for pod: daemon-set-brg28. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:14.625: INFO: Pod daemon-set-brg28 is not available
Feb 17 17:02:14.625: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:14.625: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:15.628: INFO: Wrong image for pod: daemon-set-brg28. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:15.628: INFO: Pod daemon-set-brg28 is not available
Feb 17 17:02:15.628: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:15.628: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:16.622: INFO: Wrong image for pod: daemon-set-brg28. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:16.622: INFO: Pod daemon-set-brg28 is not available
Feb 17 17:02:16.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:16.622: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:17.623: INFO: Wrong image for pod: daemon-set-brg28. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:17.623: INFO: Pod daemon-set-brg28 is not available
Feb 17 17:02:17.623: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:17.623: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:18.623: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:18.623: INFO: Pod daemon-set-lcqqd is not available
Feb 17 17:02:18.623: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:19.621: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:19.621: INFO: Pod daemon-set-lcqqd is not available
Feb 17 17:02:19.621: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:20.630: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:20.630: INFO: Pod daemon-set-lcqqd is not available
Feb 17 17:02:20.630: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:21.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:21.622: INFO: Pod daemon-set-lcqqd is not available
Feb 17 17:02:21.622: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:22.627: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:22.627: INFO: Pod daemon-set-lcqqd is not available
Feb 17 17:02:22.627: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:23.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:23.622: INFO: Pod daemon-set-lcqqd is not available
Feb 17 17:02:23.622: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:24.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:24.622: INFO: Pod daemon-set-lcqqd is not available
Feb 17 17:02:24.622: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:25.624: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:25.624: INFO: Pod daemon-set-lcqqd is not available
Feb 17 17:02:25.624: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:26.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:26.622: INFO: Pod daemon-set-lcqqd is not available
Feb 17 17:02:26.622: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:27.624: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:27.624: INFO: Pod daemon-set-lcqqd is not available
Feb 17 17:02:27.624: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:28.623: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:28.623: INFO: Pod daemon-set-lcqqd is not available
Feb 17 17:02:28.623: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:29.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:29.622: INFO: Pod daemon-set-lcqqd is not available
Feb 17 17:02:29.622: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:30.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:30.622: INFO: Pod daemon-set-lcqqd is not available
Feb 17 17:02:30.622: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:31.623: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:31.623: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:32.657: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:32.657: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:33.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:33.622: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:33.622: INFO: Pod daemon-set-p26xl is not available
Feb 17 17:02:34.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:34.622: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:34.622: INFO: Pod daemon-set-p26xl is not available
Feb 17 17:02:35.629: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:35.629: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:35.629: INFO: Pod daemon-set-p26xl is not available
Feb 17 17:02:36.623: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:36.623: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:36.623: INFO: Pod daemon-set-p26xl is not available
Feb 17 17:02:37.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:37.622: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:37.622: INFO: Pod daemon-set-p26xl is not available
Feb 17 17:02:38.623: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:38.623: INFO: Wrong image for pod: daemon-set-p26xl. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:38.623: INFO: Pod daemon-set-p26xl is not available
Feb 17 17:02:39.622: INFO: Pod daemon-set-bdjgx is not available
Feb 17 17:02:39.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:40.623: INFO: Pod daemon-set-bdjgx is not available
Feb 17 17:02:40.623: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:41.622: INFO: Pod daemon-set-bdjgx is not available
Feb 17 17:02:41.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:42.622: INFO: Pod daemon-set-bdjgx is not available
Feb 17 17:02:42.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:43.622: INFO: Pod daemon-set-bdjgx is not available
Feb 17 17:02:43.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:44.622: INFO: Pod daemon-set-bdjgx is not available
Feb 17 17:02:44.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:45.623: INFO: Pod daemon-set-bdjgx is not available
Feb 17 17:02:45.623: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:46.623: INFO: Pod daemon-set-bdjgx is not available
Feb 17 17:02:46.623: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:47.622: INFO: Pod daemon-set-bdjgx is not available
Feb 17 17:02:47.623: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:48.622: INFO: Pod daemon-set-bdjgx is not available
Feb 17 17:02:48.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:49.626: INFO: Pod daemon-set-bdjgx is not available
Feb 17 17:02:49.626: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:50.622: INFO: Pod daemon-set-bdjgx is not available
Feb 17 17:02:50.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:51.623: INFO: Pod daemon-set-bdjgx is not available
Feb 17 17:02:51.623: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:52.623: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:53.622: INFO: Wrong image for pod: daemon-set-h6864. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Feb 17 17:02:53.622: INFO: Pod daemon-set-h6864 is not available
Feb 17 17:02:55.625: INFO: Pod daemon-set-mtwch is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Feb 17 17:02:55.669: INFO: Number of nodes with available pods: 2
Feb 17 17:02:55.669: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:02:56.702: INFO: Number of nodes with available pods: 2
Feb 17 17:02:56.702: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:02:57.697: INFO: Number of nodes with available pods: 2
Feb 17 17:02:57.697: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:02:58.701: INFO: Number of nodes with available pods: 2
Feb 17 17:02:58.701: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:02:59.701: INFO: Number of nodes with available pods: 2
Feb 17 17:02:59.701: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:03:00.701: INFO: Number of nodes with available pods: 2
Feb 17 17:03:00.701: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:03:01.698: INFO: Number of nodes with available pods: 2
Feb 17 17:03:01.698: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:03:02.699: INFO: Number of nodes with available pods: 3
Feb 17 17:03:02.699: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2571, will wait for the garbage collector to delete the pods
Feb 17 17:03:02.859: INFO: Deleting DaemonSet.extensions daemon-set took: 31.736418ms
Feb 17 17:03:03.360: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.265997ms
Feb 17 17:03:13.772: INFO: Number of nodes with available pods: 0
Feb 17 17:03:13.772: INFO: Number of running nodes: 0, number of available pods: 0
Feb 17 17:03:13.785: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2571/daemonsets","resourceVersion":"58499"},"items":null}

Feb 17 17:03:13.799: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2571/pods","resourceVersion":"58499"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:03:13.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2571" for this suite.
Feb 17 17:03:21.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:03:23.994: INFO: namespace daemonsets-2571 deletion completed in 10.096892385s

• [SLOW TEST:80.843 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:03:23.994: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Feb 17 17:03:24.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 create -f - --namespace=kubectl-6816'
Feb 17 17:03:24.664: INFO: stderr: ""
Feb 17 17:03:24.664: INFO: stdout: "pod/pause created\n"
Feb 17 17:03:24.664: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Feb 17 17:03:24.664: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6816" to be "running and ready"
Feb 17 17:03:24.682: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.28766ms
Feb 17 17:03:26.694: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029886059s
Feb 17 17:03:28.706: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042158434s
Feb 17 17:03:30.718: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053978248s
Feb 17 17:03:32.729: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 8.065282159s
Feb 17 17:03:32.729: INFO: Pod "pause" satisfied condition "running and ready"
Feb 17 17:03:32.729: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Feb 17 17:03:32.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 label pods pause testing-label=testing-label-value --namespace=kubectl-6816'
Feb 17 17:03:32.906: INFO: stderr: ""
Feb 17 17:03:32.906: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Feb 17 17:03:32.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pod pause -L testing-label --namespace=kubectl-6816'
Feb 17 17:03:33.031: INFO: stderr: ""
Feb 17 17:03:33.031: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          9s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Feb 17 17:03:33.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 label pods pause testing-label- --namespace=kubectl-6816'
Feb 17 17:03:33.181: INFO: stderr: ""
Feb 17 17:03:33.181: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Feb 17 17:03:33.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pod pause -L testing-label --namespace=kubectl-6816'
Feb 17 17:03:33.313: INFO: stderr: ""
Feb 17 17:03:33.313: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          9s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Feb 17 17:03:33.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete --grace-period=0 --force -f - --namespace=kubectl-6816'
Feb 17 17:03:33.500: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 17:03:33.500: INFO: stdout: "pod \"pause\" force deleted\n"
Feb 17 17:03:33.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get rc,svc -l name=pause --no-headers --namespace=kubectl-6816'
Feb 17 17:03:33.661: INFO: stderr: "No resources found in kubectl-6816 namespace.\n"
Feb 17 17:03:33.661: INFO: stdout: ""
Feb 17 17:03:33.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -l name=pause --namespace=kubectl-6816 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 17 17:03:33.815: INFO: stderr: ""
Feb 17 17:03:33.815: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:03:33.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6816" for this suite.
Feb 17 17:03:41.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:03:43.939: INFO: namespace kubectl-6816 deletion completed in 10.099614897s

• [SLOW TEST:19.944 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:03:43.939: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:03:44.118: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-f6fa15b1-afe6-4065-bc3e-95a63bbd7cc6" in namespace "security-context-test-20" to be "success or failure"
Feb 17 17:03:44.130: INFO: Pod "alpine-nnp-false-f6fa15b1-afe6-4065-bc3e-95a63bbd7cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.266519ms
Feb 17 17:03:46.143: INFO: Pod "alpine-nnp-false-f6fa15b1-afe6-4065-bc3e-95a63bbd7cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024699285s
Feb 17 17:03:48.154: INFO: Pod "alpine-nnp-false-f6fa15b1-afe6-4065-bc3e-95a63bbd7cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036265784s
Feb 17 17:03:50.166: INFO: Pod "alpine-nnp-false-f6fa15b1-afe6-4065-bc3e-95a63bbd7cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048323539s
Feb 17 17:03:52.179: INFO: Pod "alpine-nnp-false-f6fa15b1-afe6-4065-bc3e-95a63bbd7cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.060704695s
Feb 17 17:03:54.191: INFO: Pod "alpine-nnp-false-f6fa15b1-afe6-4065-bc3e-95a63bbd7cc6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.072473537s
Feb 17 17:03:56.202: INFO: Pod "alpine-nnp-false-f6fa15b1-afe6-4065-bc3e-95a63bbd7cc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.084188643s
Feb 17 17:03:56.202: INFO: Pod "alpine-nnp-false-f6fa15b1-afe6-4065-bc3e-95a63bbd7cc6" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:03:56.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-20" for this suite.
Feb 17 17:04:04.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:04:06.396: INFO: namespace security-context-test-20 deletion completed in 10.106651279s

• [SLOW TEST:22.457 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:04:06.396: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 17:04:07.489: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 17:04:09.526: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555847, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555847, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555847, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555847, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:04:11.538: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555847, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555847, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555847, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555847, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:04:13.537: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555847, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555847, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555847, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555847, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:04:15.538: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555847, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555847, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555847, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717555847, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 17:04:18.571: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:04:18.587: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-265-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:04:19.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1983" for this suite.
Feb 17 17:04:27.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:04:30.011: INFO: namespace webhook-1983 deletion completed in 10.132577808s
STEP: Destroying namespace "webhook-1983-markers" for this suite.
Feb 17 17:04:38.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:04:40.109: INFO: namespace webhook-1983-markers deletion completed in 10.097483126s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:33.773 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:04:40.170: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Feb 17 17:04:54.431: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-559420064 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 17 17:05:09.595: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:05:09.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7482" for this suite.
Feb 17 17:05:17.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:05:19.723: INFO: namespace pods-7482 deletion completed in 10.098461538s

• [SLOW TEST:39.554 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:05:19.728: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:05:28.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5327" for this suite.
Feb 17 17:05:36.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:05:38.194: INFO: namespace emptydir-wrapper-5327 deletion completed in 10.097824522s

• [SLOW TEST:18.466 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:05:38.194: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 17 17:05:38.379: INFO: Waiting up to 5m0s for pod "pod-721b5e2e-c6a0-4019-b60c-3d8c1be940b2" in namespace "emptydir-7372" to be "success or failure"
Feb 17 17:05:38.391: INFO: Pod "pod-721b5e2e-c6a0-4019-b60c-3d8c1be940b2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.034466ms
Feb 17 17:05:40.404: INFO: Pod "pod-721b5e2e-c6a0-4019-b60c-3d8c1be940b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025482944s
Feb 17 17:05:42.416: INFO: Pod "pod-721b5e2e-c6a0-4019-b60c-3d8c1be940b2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037789111s
Feb 17 17:05:44.432: INFO: Pod "pod-721b5e2e-c6a0-4019-b60c-3d8c1be940b2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053185166s
Feb 17 17:05:46.445: INFO: Pod "pod-721b5e2e-c6a0-4019-b60c-3d8c1be940b2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.066230498s
Feb 17 17:05:48.457: INFO: Pod "pod-721b5e2e-c6a0-4019-b60c-3d8c1be940b2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.077857981s
Feb 17 17:05:50.469: INFO: Pod "pod-721b5e2e-c6a0-4019-b60c-3d8c1be940b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.090255445s
STEP: Saw pod success
Feb 17 17:05:50.469: INFO: Pod "pod-721b5e2e-c6a0-4019-b60c-3d8c1be940b2" satisfied condition "success or failure"
Feb 17 17:05:50.481: INFO: Trying to get logs from node 10.45.66.177 pod pod-721b5e2e-c6a0-4019-b60c-3d8c1be940b2 container test-container: <nil>
STEP: delete the pod
Feb 17 17:05:50.597: INFO: Waiting for pod pod-721b5e2e-c6a0-4019-b60c-3d8c1be940b2 to disappear
Feb 17 17:05:50.608: INFO: Pod pod-721b5e2e-c6a0-4019-b60c-3d8c1be940b2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:05:50.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7372" for this suite.
Feb 17 17:05:58.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:06:00.725: INFO: namespace emptydir-7372 deletion completed in 10.098774686s

• [SLOW TEST:22.531 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:06:00.728: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 17:06:00.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-5218'
Feb 17 17:06:01.086: INFO: stderr: ""
Feb 17 17:06:01.086: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Feb 17 17:06:11.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pod e2e-test-httpd-pod --namespace=kubectl-5218 -o json'
Feb 17 17:06:11.263: INFO: stderr: ""
Feb 17 17:06:11.263: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.30.178.248/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.178.248/32\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.178.248\\\"\\n    ],\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2020-02-17T17:06:01Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5218\",\n        \"resourceVersion\": \"59844\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5218/pods/e2e-test-httpd-pod\",\n        \"uid\": \"40e84718-922a-4ab9-b5ea-0c99aeecf40d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-8sf4b\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-rqhj4\"\n            }\n        ],\n        \"nodeName\": \"10.45.66.177\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c36,c25\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-8sf4b\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-8sf4b\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-17T17:06:01Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-17T17:06:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-17T17:06:08Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-02-17T17:06:01Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://b1e882a96cea5df27fc125e0182b297877ba2f590c7b2bc0f8785f9c5ce2b8e3\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-02-17T17:06:08Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.45.66.177\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.178.248\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.178.248\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-02-17T17:06:01Z\"\n    }\n}\n"
STEP: replace the image in the pod
Feb 17 17:06:11.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 replace -f - --namespace=kubectl-5218'
Feb 17 17:06:11.774: INFO: stderr: ""
Feb 17 17:06:11.774: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Feb 17 17:06:11.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete pods e2e-test-httpd-pod --namespace=kubectl-5218'
Feb 17 17:06:14.364: INFO: stderr: ""
Feb 17 17:06:14.364: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:06:14.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5218" for this suite.
Feb 17 17:06:22.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:06:24.486: INFO: namespace kubectl-5218 deletion completed in 10.097047365s

• [SLOW TEST:23.758 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:06:24.487: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-8197/configmap-test-7b4c050a-23d2-4a8a-99d3-7de1ea9ac27b
STEP: Creating a pod to test consume configMaps
Feb 17 17:06:24.711: INFO: Waiting up to 5m0s for pod "pod-configmaps-59c83699-e42b-41b2-a20e-46511c4c501b" in namespace "configmap-8197" to be "success or failure"
Feb 17 17:06:24.722: INFO: Pod "pod-configmaps-59c83699-e42b-41b2-a20e-46511c4c501b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.071743ms
Feb 17 17:06:26.737: INFO: Pod "pod-configmaps-59c83699-e42b-41b2-a20e-46511c4c501b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025537594s
Feb 17 17:06:28.749: INFO: Pod "pod-configmaps-59c83699-e42b-41b2-a20e-46511c4c501b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03810749s
Feb 17 17:06:30.762: INFO: Pod "pod-configmaps-59c83699-e42b-41b2-a20e-46511c4c501b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050649655s
Feb 17 17:06:32.774: INFO: Pod "pod-configmaps-59c83699-e42b-41b2-a20e-46511c4c501b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.062614249s
STEP: Saw pod success
Feb 17 17:06:32.774: INFO: Pod "pod-configmaps-59c83699-e42b-41b2-a20e-46511c4c501b" satisfied condition "success or failure"
Feb 17 17:06:32.785: INFO: Trying to get logs from node 10.45.66.178 pod pod-configmaps-59c83699-e42b-41b2-a20e-46511c4c501b container env-test: <nil>
STEP: delete the pod
Feb 17 17:06:32.892: INFO: Waiting for pod pod-configmaps-59c83699-e42b-41b2-a20e-46511c4c501b to disappear
Feb 17 17:06:32.906: INFO: Pod pod-configmaps-59c83699-e42b-41b2-a20e-46511c4c501b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:06:32.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8197" for this suite.
Feb 17 17:06:40.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:06:43.022: INFO: namespace configmap-8197 deletion completed in 10.099755567s

• [SLOW TEST:18.536 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:06:43.023: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 17 17:06:51.843: INFO: Successfully updated pod "pod-update-100d5265-7a76-4ccc-9943-a81cb04e1fdb"
STEP: verifying the updated pod is in kubernetes
Feb 17 17:06:51.874: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:06:51.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7533" for this suite.
Feb 17 17:07:05.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:07:07.996: INFO: namespace pods-7533 deletion completed in 16.103016602s

• [SLOW TEST:24.974 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:07:07.999: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:07:08.222: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Feb 17 17:07:08.264: INFO: Number of nodes with available pods: 0
Feb 17 17:07:08.264: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Feb 17 17:07:08.329: INFO: Number of nodes with available pods: 0
Feb 17 17:07:08.329: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:09.342: INFO: Number of nodes with available pods: 0
Feb 17 17:07:09.342: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:10.341: INFO: Number of nodes with available pods: 0
Feb 17 17:07:10.341: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:11.342: INFO: Number of nodes with available pods: 0
Feb 17 17:07:11.342: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:12.341: INFO: Number of nodes with available pods: 0
Feb 17 17:07:12.341: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:13.340: INFO: Number of nodes with available pods: 0
Feb 17 17:07:13.340: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:14.340: INFO: Number of nodes with available pods: 0
Feb 17 17:07:14.340: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:15.340: INFO: Number of nodes with available pods: 0
Feb 17 17:07:15.340: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:16.340: INFO: Number of nodes with available pods: 1
Feb 17 17:07:16.341: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Feb 17 17:07:16.395: INFO: Number of nodes with available pods: 1
Feb 17 17:07:16.395: INFO: Number of running nodes: 0, number of available pods: 1
Feb 17 17:07:17.407: INFO: Number of nodes with available pods: 0
Feb 17 17:07:17.407: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Feb 17 17:07:17.435: INFO: Number of nodes with available pods: 0
Feb 17 17:07:17.435: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:18.447: INFO: Number of nodes with available pods: 0
Feb 17 17:07:18.447: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:19.447: INFO: Number of nodes with available pods: 0
Feb 17 17:07:19.447: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:20.447: INFO: Number of nodes with available pods: 0
Feb 17 17:07:20.447: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:21.447: INFO: Number of nodes with available pods: 0
Feb 17 17:07:21.448: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:22.447: INFO: Number of nodes with available pods: 0
Feb 17 17:07:22.447: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:23.447: INFO: Number of nodes with available pods: 0
Feb 17 17:07:23.447: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:24.447: INFO: Number of nodes with available pods: 0
Feb 17 17:07:24.447: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:25.448: INFO: Number of nodes with available pods: 0
Feb 17 17:07:25.448: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:26.447: INFO: Number of nodes with available pods: 0
Feb 17 17:07:26.447: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:27.449: INFO: Number of nodes with available pods: 0
Feb 17 17:07:27.450: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:28.448: INFO: Number of nodes with available pods: 0
Feb 17 17:07:28.449: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:29.448: INFO: Number of nodes with available pods: 0
Feb 17 17:07:29.448: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:30.447: INFO: Number of nodes with available pods: 0
Feb 17 17:07:30.447: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:31.447: INFO: Number of nodes with available pods: 0
Feb 17 17:07:31.448: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:07:32.449: INFO: Number of nodes with available pods: 1
Feb 17 17:07:32.450: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-351, will wait for the garbage collector to delete the pods
Feb 17 17:07:32.572: INFO: Deleting DaemonSet.extensions daemon-set took: 32.297782ms
Feb 17 17:07:33.073: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.300516ms
Feb 17 17:07:43.785: INFO: Number of nodes with available pods: 0
Feb 17 17:07:43.785: INFO: Number of running nodes: 0, number of available pods: 0
Feb 17 17:07:43.798: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-351/daemonsets","resourceVersion":"60568"},"items":null}

Feb 17 17:07:43.812: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-351/pods","resourceVersion":"60569"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:07:43.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-351" for this suite.
Feb 17 17:07:51.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:07:54.034: INFO: namespace daemonsets-351 deletion completed in 10.124268229s

• [SLOW TEST:46.035 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:07:54.035: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:07:54.209: INFO: Pod name rollover-pod: Found 0 pods out of 1
Feb 17 17:07:59.222: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 17 17:08:03.245: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Feb 17 17:08:05.259: INFO: Creating deployment "test-rollover-deployment"
Feb 17 17:08:05.285: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Feb 17 17:08:07.311: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Feb 17 17:08:07.341: INFO: Ensure that both replica sets have 1 created replica
Feb 17 17:08:07.373: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Feb 17 17:08:07.399: INFO: Updating deployment test-rollover-deployment
Feb 17 17:08:07.399: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Feb 17 17:08:09.422: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Feb 17 17:08:09.449: INFO: Make sure deployment "test-rollover-deployment" is complete
Feb 17 17:08:09.476: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 17:08:09.476: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556087, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:08:11.502: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 17:08:11.502: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556087, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:08:13.503: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 17:08:13.504: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556087, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:08:15.502: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 17:08:15.502: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556095, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:08:17.502: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 17:08:17.502: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556095, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:08:19.503: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 17:08:19.503: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556095, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:08:21.514: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 17:08:21.514: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556095, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:08:23.502: INFO: all replica sets need to contain the pod-template-hash label
Feb 17 17:08:23.502: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556095, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556085, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:08:25.504: INFO: 
Feb 17 17:08:25.504: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Feb 17 17:08:25.564: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-191 /apis/apps/v1/namespaces/deployment-191/deployments/test-rollover-deployment becd6b3e-721e-49f1-a177-4a642eb4c698 60873 2 2020-02-17 17:08:05 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001ac0d38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-17 17:08:05 +0000 UTC,LastTransitionTime:2020-02-17 17:08:05 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-02-17 17:08:25 +0000 UTC,LastTransitionTime:2020-02-17 17:08:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 17 17:08:25.578: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-191 /apis/apps/v1/namespaces/deployment-191/replicasets/test-rollover-deployment-7d7dc6548c 3efdfc15-856f-4ee1-8880-5abd56b86f15 60863 2 2020-02-17 17:08:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment becd6b3e-721e-49f1-a177-4a642eb4c698 0xc001ac11f7 0xc001ac11f8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001ac1258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 17 17:08:25.578: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Feb 17 17:08:25.578: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-191 /apis/apps/v1/namespaces/deployment-191/replicasets/test-rollover-controller da3b1000-aa6d-4c4d-ac22-ec0e44418cc5 60872 2 2020-02-17 17:07:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment becd6b3e-721e-49f1-a177-4a642eb4c698 0xc001ac1127 0xc001ac1128}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc001ac1188 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 17:08:25.578: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-191 /apis/apps/v1/namespaces/deployment-191/replicasets/test-rollover-deployment-f6c94f66c 67c022ed-bfbb-40c9-bdce-71825b4937e6 60768 2 2020-02-17 17:08:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment becd6b3e-721e-49f1-a177-4a642eb4c698 0xc001ac12c0 0xc001ac12c1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001ac1338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 17:08:25.589: INFO: Pod "test-rollover-deployment-7d7dc6548c-w8m2s" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-w8m2s test-rollover-deployment-7d7dc6548c- deployment-191 /api/v1/namespaces/deployment-191/pods/test-rollover-deployment-7d7dc6548c-w8m2s c4f232c3-f02c-4f39-a6fb-8894c7233936 60819 0 2020-02-17 17:08:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:172.30.50.149/32 cni.projectcalico.org/podIPs:172.30.50.149/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.50.149"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 3efdfc15-856f-4ee1-8880-5abd56b86f15 0xc001ac1907 0xc001ac1908}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hz5f4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hz5f4,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hz5f4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-tds2j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:08:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:08:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:08:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.189,PodIP:172.30.50.149,StartTime:2020-02-17 17:08:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 17:08:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://a12df28b2fba7fc5eadac0346a986c1010a1294d60a25a3c217f192991ec5b97,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.50.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:08:25.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-191" for this suite.
Feb 17 17:08:33.664: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:08:35.716: INFO: namespace deployment-191 deletion completed in 10.105058306s

• [SLOW TEST:41.681 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:08:35.716: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:08:42.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5788" for this suite.
Feb 17 17:08:50.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:08:53.033: INFO: namespace resourcequota-5788 deletion completed in 10.111289061s

• [SLOW TEST:17.317 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:08:53.033: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7751
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7751
STEP: Creating statefulset with conflicting port in namespace statefulset-7751
STEP: Waiting until pod test-pod will start running in namespace statefulset-7751
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7751
Feb 17 17:09:01.335: INFO: Observed stateful pod in namespace: statefulset-7751, name: ss-0, uid: 5d646cc5-268b-4ad7-b2d5-89a7fc481f24, status phase: Pending. Waiting for statefulset controller to delete.
Feb 17 17:09:01.410: INFO: Observed stateful pod in namespace: statefulset-7751, name: ss-0, uid: 5d646cc5-268b-4ad7-b2d5-89a7fc481f24, status phase: Failed. Waiting for statefulset controller to delete.
Feb 17 17:09:01.435: INFO: Observed stateful pod in namespace: statefulset-7751, name: ss-0, uid: 5d646cc5-268b-4ad7-b2d5-89a7fc481f24, status phase: Failed. Waiting for statefulset controller to delete.
Feb 17 17:09:01.442: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7751
STEP: Removing pod with conflicting port in namespace statefulset-7751
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7751 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 17 17:09:09.537: INFO: Deleting all statefulset in ns statefulset-7751
Feb 17 17:09:09.548: INFO: Scaling statefulset ss to 0
Feb 17 17:09:29.597: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 17:09:29.609: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:09:29.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7751" for this suite.
Feb 17 17:09:39.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:09:41.812: INFO: namespace statefulset-7751 deletion completed in 12.126965054s

• [SLOW TEST:48.779 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:09:41.812: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:09:41.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6676" for this suite.
Feb 17 17:09:50.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:09:52.102: INFO: namespace tables-6676 deletion completed in 10.111200966s

• [SLOW TEST:10.290 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:09:52.102: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4843
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4843
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4843
Feb 17 17:09:52.300: INFO: Found 0 stateful pods, waiting for 1
Feb 17 17:10:02.314: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Feb 17 17:10:02.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-4843 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 17:10:02.670: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 17:10:02.670: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 17:10:02.670: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 17:10:02.683: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 17 17:10:12.696: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 17:10:12.696: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 17:10:12.743: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998026s
Feb 17 17:10:13.756: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.987875995s
Feb 17 17:10:14.769: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.974806325s
Feb 17 17:10:15.781: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.961451777s
Feb 17 17:10:16.794: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.949398974s
Feb 17 17:10:17.806: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.936828505s
Feb 17 17:10:18.818: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.924374848s
Feb 17 17:10:19.831: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.912265203s
Feb 17 17:10:20.844: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.899935307s
Feb 17 17:10:21.858: INFO: Verifying statefulset ss doesn't scale past 1 for another 886.813219ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4843
Feb 17 17:10:22.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-4843 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:10:23.231: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 17:10:23.231: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 17:10:23.231: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 17:10:23.244: INFO: Found 1 stateful pods, waiting for 3
Feb 17 17:10:33.257: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:10:33.257: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:10:33.257: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 17 17:10:43.256: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:10:43.256: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:10:43.256: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Feb 17 17:10:43.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-4843 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 17:10:43.637: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 17:10:43.637: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 17:10:43.637: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 17:10:43.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-4843 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 17:10:43.992: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 17:10:43.992: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 17:10:43.992: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 17:10:43.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-4843 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 17:10:44.359: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 17:10:44.359: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 17:10:44.359: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 17:10:44.359: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 17:10:44.370: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Feb 17 17:10:54.396: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 17:10:54.396: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 17:10:54.396: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 17:10:54.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998006s
Feb 17 17:10:55.450: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986902376s
Feb 17 17:10:56.466: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.974541424s
Feb 17 17:10:57.479: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.959006607s
Feb 17 17:10:58.506: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.945129374s
Feb 17 17:10:59.519: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.918946917s
Feb 17 17:11:00.532: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.905598522s
Feb 17 17:11:01.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.892540158s
Feb 17 17:11:02.558: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.879501801s
Feb 17 17:11:03.571: INFO: Verifying statefulset ss doesn't scale past 3 for another 866.37505ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4843
Feb 17 17:11:04.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-4843 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:11:05.153: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 17:11:05.153: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 17:11:05.153: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 17:11:05.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-4843 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:11:05.512: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 17:11:05.512: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 17:11:05.512: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 17:11:05.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-4843 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:11:05.914: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 17:11:05.914: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 17:11:05.914: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 17:11:05.914: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 17 17:11:35.967: INFO: Deleting all statefulset in ns statefulset-4843
Feb 17 17:11:35.977: INFO: Scaling statefulset ss to 0
Feb 17 17:11:36.020: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 17:11:36.031: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:11:36.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4843" for this suite.
Feb 17 17:11:44.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:11:46.201: INFO: namespace statefulset-4843 deletion completed in 10.098495061s

• [SLOW TEST:114.099 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:11:46.201: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:11:46.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3214" for this suite.
Feb 17 17:11:54.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:11:56.575: INFO: namespace resourcequota-3214 deletion completed in 10.104023112s

• [SLOW TEST:10.374 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:11:56.576: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-875de67c-af40-4665-922e-8d0c1c8226f1
STEP: Creating a pod to test consume configMaps
Feb 17 17:11:56.792: INFO: Waiting up to 5m0s for pod "pod-configmaps-6d3dde8b-65cd-486d-84d4-8b2796c634cc" in namespace "configmap-6513" to be "success or failure"
Feb 17 17:11:56.808: INFO: Pod "pod-configmaps-6d3dde8b-65cd-486d-84d4-8b2796c634cc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.026044ms
Feb 17 17:11:58.825: INFO: Pod "pod-configmaps-6d3dde8b-65cd-486d-84d4-8b2796c634cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033331878s
Feb 17 17:12:00.837: INFO: Pod "pod-configmaps-6d3dde8b-65cd-486d-84d4-8b2796c634cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045363583s
Feb 17 17:12:02.849: INFO: Pod "pod-configmaps-6d3dde8b-65cd-486d-84d4-8b2796c634cc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056693496s
Feb 17 17:12:04.859: INFO: Pod "pod-configmaps-6d3dde8b-65cd-486d-84d4-8b2796c634cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.067574012s
STEP: Saw pod success
Feb 17 17:12:04.859: INFO: Pod "pod-configmaps-6d3dde8b-65cd-486d-84d4-8b2796c634cc" satisfied condition "success or failure"
Feb 17 17:12:04.871: INFO: Trying to get logs from node 10.45.66.189 pod pod-configmaps-6d3dde8b-65cd-486d-84d4-8b2796c634cc container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 17:12:04.980: INFO: Waiting for pod pod-configmaps-6d3dde8b-65cd-486d-84d4-8b2796c634cc to disappear
Feb 17 17:12:04.993: INFO: Pod pod-configmaps-6d3dde8b-65cd-486d-84d4-8b2796c634cc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:12:04.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6513" for this suite.
Feb 17 17:12:13.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:12:15.114: INFO: namespace configmap-6513 deletion completed in 10.100717288s

• [SLOW TEST:18.539 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:12:15.114: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:12:23.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8436" for this suite.
Feb 17 17:13:05.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:13:07.517: INFO: namespace kubelet-test-8436 deletion completed in 44.100477212s

• [SLOW TEST:52.403 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:13:07.519: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 17:13:08.665: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 17:13:10.708: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556388, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556388, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556388, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556388, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:13:12.720: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556388, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556388, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556388, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556388, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:13:14.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556388, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556388, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556388, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556388, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:13:16.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556388, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556388, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556388, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717556388, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 17:13:19.755: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:13:32.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5654" for this suite.
Feb 17 17:13:40.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:13:42.392: INFO: namespace webhook-5654 deletion completed in 10.097390136s
STEP: Destroying namespace "webhook-5654-markers" for this suite.
Feb 17 17:13:50.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:13:52.498: INFO: namespace webhook-5654-markers deletion completed in 10.105682125s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:45.039 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:13:52.558: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Feb 17 17:13:52.696: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 17:13:59.877: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:14:41.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9248" for this suite.
Feb 17 17:14:49.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:14:51.418: INFO: namespace crd-publish-openapi-9248 deletion completed in 10.112799987s

• [SLOW TEST:58.860 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:14:51.418: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:14:51.698: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"dedb7887-81da-4c76-ad92-7476d42736c1", Controller:(*bool)(0xc006a3dc12), BlockOwnerDeletion:(*bool)(0xc006a3dc13)}}
Feb 17 17:14:51.723: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b444768b-9f87-4429-aef4-445fbada45a8", Controller:(*bool)(0xc0063be0f2), BlockOwnerDeletion:(*bool)(0xc0063be0f3)}}
Feb 17 17:14:51.738: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"9421b313-187c-4c8e-bc9a-9c4edf9c30a2", Controller:(*bool)(0xc006a15766), BlockOwnerDeletion:(*bool)(0xc006a15767)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:14:56.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2658" for this suite.
Feb 17 17:15:04.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:15:06.931: INFO: namespace gc-2658 deletion completed in 10.126061044s

• [SLOW TEST:15.513 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:15:06.932: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 17 17:15:25.302: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 17 17:15:25.315: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 17 17:15:27.315: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 17 17:15:27.328: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 17 17:15:29.315: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 17 17:15:29.328: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 17 17:15:31.315: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 17 17:15:31.328: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 17 17:15:33.315: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 17 17:15:33.328: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 17 17:15:35.315: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 17 17:15:35.328: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 17 17:15:37.315: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 17 17:15:37.329: INFO: Pod pod-with-poststart-exec-hook still exists
Feb 17 17:15:39.315: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Feb 17 17:15:39.328: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:15:39.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2993" for this suite.
Feb 17 17:16:11.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:16:13.549: INFO: namespace container-lifecycle-hook-2993 deletion completed in 34.193731s

• [SLOW TEST:66.617 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:16:13.549: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 17 17:16:13.735: INFO: Waiting up to 5m0s for pod "pod-c051783f-ca62-4151-bca9-ebbaa102e652" in namespace "emptydir-9484" to be "success or failure"
Feb 17 17:16:13.750: INFO: Pod "pod-c051783f-ca62-4151-bca9-ebbaa102e652": Phase="Pending", Reason="", readiness=false. Elapsed: 15.017708ms
Feb 17 17:16:15.764: INFO: Pod "pod-c051783f-ca62-4151-bca9-ebbaa102e652": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028752303s
Feb 17 17:16:17.776: INFO: Pod "pod-c051783f-ca62-4151-bca9-ebbaa102e652": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041274392s
Feb 17 17:16:19.790: INFO: Pod "pod-c051783f-ca62-4151-bca9-ebbaa102e652": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054853541s
Feb 17 17:16:21.805: INFO: Pod "pod-c051783f-ca62-4151-bca9-ebbaa102e652": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.069721467s
STEP: Saw pod success
Feb 17 17:16:21.805: INFO: Pod "pod-c051783f-ca62-4151-bca9-ebbaa102e652" satisfied condition "success or failure"
Feb 17 17:16:21.817: INFO: Trying to get logs from node 10.45.66.178 pod pod-c051783f-ca62-4151-bca9-ebbaa102e652 container test-container: <nil>
STEP: delete the pod
Feb 17 17:16:21.915: INFO: Waiting for pod pod-c051783f-ca62-4151-bca9-ebbaa102e652 to disappear
Feb 17 17:16:21.926: INFO: Pod pod-c051783f-ca62-4151-bca9-ebbaa102e652 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:16:21.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9484" for this suite.
Feb 17 17:16:29.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:16:32.101: INFO: namespace emptydir-9484 deletion completed in 10.148365875s

• [SLOW TEST:18.552 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:16:32.101: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 17 17:16:32.235: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 17 17:16:32.307: INFO: Waiting for terminating namespaces to be deleted...
Feb 17 17:16:32.330: INFO: 
Logging pods the kubelet thinks is on node 10.45.66.177 before test
Feb 17 17:16:32.442: INFO: packageserver-749668c8dd-fpjbg from openshift-operator-lifecycle-manager started at 2020-02-17 15:40:00 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container packageserver ready: true, restart count 0
Feb 17 17:16:32.443: INFO: openshift-service-catalog-apiserver-operator-577c6ffb4f-r7vw8 from openshift-service-catalog-apiserver-operator started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container operator ready: true, restart count 1
Feb 17 17:16:32.443: INFO: cluster-image-registry-operator-79bfcb686-62774 from openshift-image-registry started at 2020-02-17 14:36:04 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Feb 17 17:16:32.443: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Feb 17 17:16:32.443: INFO: router-default-6c67c67864-hlm85 from openshift-ingress started at 2020-02-17 14:42:12 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container router ready: true, restart count 0
Feb 17 17:16:32.443: INFO: downloads-65dd498cf8-cqpss from openshift-console started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container download-server ready: true, restart count 0
Feb 17 17:16:32.443: INFO: dns-default-xw2km from openshift-dns started at 2020-02-17 14:37:34 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container dns ready: true, restart count 0
Feb 17 17:16:32.443: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 17 17:16:32.443: INFO: sonobuoy from sonobuoy started at 2020-02-17 16:29:56 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 17 17:16:32.443: INFO: vpn-5d9c8dbf8-fwt5m from kube-system started at 2020-02-17 14:45:06 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container vpn ready: true, restart count 0
Feb 17 17:16:32.443: INFO: sonobuoy-e2e-job-a662486eb1ae4ce5 from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container e2e ready: true, restart count 0
Feb 17 17:16:32.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 17:16:32.443: INFO: calico-node-mjlf4 from kube-system started at 2020-02-17 14:34:53 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 17:16:32.443: INFO: catalog-operator-7fccd6877f-xzz4r from openshift-operator-lifecycle-manager started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container catalog-operator ready: true, restart count 0
Feb 17 17:16:32.443: INFO: service-ca-operator-5f9bd445b8-k65l4 from openshift-service-ca-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container operator ready: true, restart count 0
Feb 17 17:16:32.443: INFO: sonobuoy-systemd-logs-daemon-set-67976b35ad084332-bvf8d from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 17:16:32.443: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 17:16:32.443: INFO: configmap-cabundle-injector-c749c6984-hlkf5 from openshift-service-ca started at 2020-02-17 14:36:50 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Feb 17 17:16:32.443: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-02-17 14:41:40 +0000 UTC (3 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container alertmanager ready: true, restart count 0
Feb 17 17:16:32.443: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 17 17:16:32.443: INFO: 	Container config-reloader ready: true, restart count 0
Feb 17 17:16:32.443: INFO: marketplace-operator-664f66c947-jckz5 from openshift-marketplace started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container marketplace-operator ready: true, restart count 0
Feb 17 17:16:32.443: INFO: olm-operator-5f5f7bbd7d-8k49r from openshift-operator-lifecycle-manager started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container olm-operator ready: true, restart count 0
Feb 17 17:16:32.443: INFO: ibm-storage-watcher-5cf8f8b4cb-pnqvx from kube-system started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Feb 17 17:16:32.443: INFO: network-operator-86974b9cf-spclm from openshift-network-operator started at 2020-02-17 14:34:58 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container network-operator ready: true, restart count 0
Feb 17 17:16:32.443: INFO: ibmcloud-block-storage-plugin-75cf87c786-cc9kd from kube-system started at 2020-02-17 14:34:58 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Feb 17 17:16:32.443: INFO: calico-typha-76c597bc7d-4tfhl from kube-system started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container calico-typha ready: true, restart count 0
Feb 17 17:16:32.443: INFO: kube-state-metrics-5745cc99f5-vd9j6 from openshift-monitoring started at 2020-02-17 14:36:27 +0000 UTC (3 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 17 17:16:32.443: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 17 17:16:32.443: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 17 17:16:32.443: INFO: node-exporter-rfpkb from openshift-monitoring started at 2020-02-17 14:36:28 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 17:16:32.443: INFO: 	Container node-exporter ready: true, restart count 0
Feb 17 17:16:32.443: INFO: cluster-node-tuning-operator-5cbf9c8db5-mr2tk from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Feb 17 17:16:32.443: INFO: cloud-credential-operator-6fc5bb58fc-npgb8 from openshift-cloud-credential-operator started at 2020-02-17 14:36:06 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container manager ready: true, restart count 0
Feb 17 17:16:32.443: INFO: openshift-state-metrics-86bbf546f-gglb8 from openshift-monitoring started at 2020-02-17 14:36:26 +0000 UTC (3 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 17 17:16:32.443: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 17 17:16:32.443: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Feb 17 17:16:32.443: INFO: multus-admission-controller-x8vxh from openshift-multus started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 17 17:16:32.443: INFO: downloads-65dd498cf8-vvlwb from openshift-console started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container download-server ready: true, restart count 0
Feb 17 17:16:32.443: INFO: service-serving-cert-signer-6669ffbb98-s7bf6 from openshift-service-ca started at 2020-02-17 14:36:49 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Feb 17 17:16:32.443: INFO: node-ca-49d68 from openshift-image-registry started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container node-ca ready: true, restart count 0
Feb 17 17:16:32.443: INFO: multus-z6qv7 from openshift-multus started at 2020-02-17 14:35:36 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container kube-multus ready: true, restart count 0
Feb 17 17:16:32.443: INFO: openshift-service-catalog-controller-manager-operator-6857997dg from openshift-service-catalog-controller-manager-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container operator ready: true, restart count 1
Feb 17 17:16:32.443: INFO: dns-operator-586c9c56ff-nxk8c from openshift-dns-operator started at 2020-02-17 14:36:04 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container dns-operator ready: true, restart count 0
Feb 17 17:16:32.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 17:16:32.443: INFO: cluster-monitoring-operator-55d99d5ffc-qlxs4 from openshift-monitoring started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Feb 17 17:16:32.443: INFO: apiservice-cabundle-injector-745c6fc9d8-h2znf from openshift-service-ca started at 2020-02-17 14:36:50 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Feb 17 17:16:32.443: INFO: ibmcloud-block-storage-driver-vnql8 from kube-system started at 2020-02-17 14:34:55 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 17 17:16:32.443: INFO: ibm-file-plugin-788cbbd987-7hzlx from kube-system started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Feb 17 17:16:32.443: INFO: ingress-operator-855959d8d9-xhrmn from openshift-ingress-operator started at 2020-02-17 14:36:04 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container ingress-operator ready: true, restart count 0
Feb 17 17:16:32.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 17:16:32.443: INFO: cluster-storage-operator-646c48994f-4nvbp from openshift-cluster-storage-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Feb 17 17:16:32.443: INFO: console-operator-7897bcfbd8-m6vt2 from openshift-console-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container console-operator ready: true, restart count 1
Feb 17 17:16:32.443: INFO: calico-kube-controllers-5df9fd5998-tgjvk from kube-system started at 2020-02-17 14:36:06 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 17 17:16:32.443: INFO: tuned-f4zpl from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:28 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container tuned ready: true, restart count 0
Feb 17 17:16:32.443: INFO: ibm-keepalived-watcher-684qj from kube-system started at 2020-02-17 14:34:53 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 17:16:32.443: INFO: ibm-master-proxy-static-10.45.66.177 from kube-system started at 2020-02-17 14:34:51 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 17:16:32.443: INFO: 	Container pause ready: true, restart count 0
Feb 17 17:16:32.443: INFO: openshift-kube-proxy-sz6st from openshift-kube-proxy started at 2020-02-17 14:35:42 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.443: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 17:16:32.443: INFO: 
Logging pods the kubelet thinks is on node 10.45.66.178 before test
Feb 17 17:16:32.521: INFO: prometheus-operator-7d5f885558-bv55g from openshift-monitoring started at 2020-02-17 14:41:26 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container prometheus-operator ready: true, restart count 0
Feb 17 17:16:32.521: INFO: node-ca-n4v6k from openshift-image-registry started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container node-ca ready: true, restart count 0
Feb 17 17:16:32.521: INFO: registry-pvc-permissions-jlk4p from openshift-image-registry started at 2020-02-17 14:40:17 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container pvc-permissions ready: false, restart count 0
Feb 17 17:16:32.521: INFO: thanos-querier-b949c5db-ns4qs from openshift-monitoring started at 2020-02-17 14:42:53 +0000 UTC (4 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 17:16:32.521: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 17 17:16:32.521: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 17:16:32.521: INFO: 	Container thanos-querier ready: true, restart count 0
Feb 17 17:16:32.521: INFO: router-default-6c67c67864-f8hvb from openshift-ingress started at 2020-02-17 14:42:29 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container router ready: true, restart count 0
Feb 17 17:16:32.521: INFO: ibm-master-proxy-static-10.45.66.178 from kube-system started at 2020-02-17 14:37:31 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 17:16:32.521: INFO: 	Container pause ready: true, restart count 0
Feb 17 17:16:32.521: INFO: packageserver-749668c8dd-slgc8 from openshift-operator-lifecycle-manager started at 2020-02-17 15:40:09 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container packageserver ready: true, restart count 0
Feb 17 17:16:32.521: INFO: certified-operators-77688559b9-692q9 from openshift-marketplace started at 2020-02-17 14:38:05 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container certified-operators ready: true, restart count 0
Feb 17 17:16:32.521: INFO: sonobuoy-systemd-logs-daemon-set-67976b35ad084332-thldq from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 17:16:32.521: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 17:16:32.521: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-02-17 14:41:51 +0000 UTC (3 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container alertmanager ready: true, restart count 0
Feb 17 17:16:32.521: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 17 17:16:32.521: INFO: 	Container config-reloader ready: true, restart count 0
Feb 17 17:16:32.521: INFO: node-exporter-wmxf5 from openshift-monitoring started at 2020-02-17 14:36:43 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 17:16:32.521: INFO: 	Container node-exporter ready: true, restart count 0
Feb 17 17:16:32.521: INFO: dns-default-9fgff from openshift-dns started at 2020-02-17 14:38:00 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container dns ready: true, restart count 0
Feb 17 17:16:32.521: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 17 17:16:32.521: INFO: cluster-samples-operator-946f4d4c5-66g9k from openshift-cluster-samples-operator started at 2020-02-17 14:38:52 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Feb 17 17:16:32.521: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Feb 17 17:16:32.521: INFO: image-registry-6d4857c99d-q2dn4 from openshift-image-registry started at 2020-02-17 14:40:17 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container registry ready: true, restart count 0
Feb 17 17:16:32.521: INFO: console-7547c7bc6c-wschb from openshift-console started at 2020-02-17 14:39:26 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container console ready: true, restart count 0
Feb 17 17:16:32.521: INFO: calico-node-zkvdn from kube-system started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 17:16:32.521: INFO: ibm-keepalived-watcher-4nt2b from kube-system started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 17:16:32.521: INFO: multus-wbdmp from openshift-multus started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container kube-multus ready: true, restart count 0
Feb 17 17:16:32.521: INFO: ibm-cloud-provider-ip-158-175-93-66-7b5b7d9cb-jbd6r from ibm-system started at 2020-02-17 14:41:22 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container ibm-cloud-provider-ip-158-175-93-66 ready: true, restart count 0
Feb 17 17:16:32.521: INFO: grafana-66dff46d7c-l5jsk from openshift-monitoring started at 2020-02-17 14:41:46 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container grafana ready: true, restart count 0
Feb 17 17:16:32.521: INFO: 	Container grafana-proxy ready: true, restart count 0
Feb 17 17:16:32.521: INFO: tuned-2n22g from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.521: INFO: 	Container tuned ready: true, restart count 0
Feb 17 17:16:32.521: INFO: openshift-kube-proxy-kchsl from openshift-kube-proxy started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.522: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 17:16:32.522: INFO: calico-typha-76c597bc7d-72pj2 from kube-system started at 2020-02-17 14:38:05 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.522: INFO: 	Container calico-typha ready: true, restart count 0
Feb 17 17:16:32.522: INFO: prometheus-adapter-55674d9685-vvdz2 from openshift-monitoring started at 2020-02-17 14:41:40 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.522: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 17 17:16:32.522: INFO: ibmcloud-block-storage-driver-d7lnw from kube-system started at 2020-02-17 14:36:45 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.522: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 17 17:16:32.522: INFO: multus-admission-controller-s6fg5 from openshift-multus started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.522: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 17 17:16:32.522: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-02-17 14:42:48 +0000 UTC (7 container statuses recorded)
Feb 17 17:16:32.522: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 17:16:32.522: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 17:16:32.522: INFO: 	Container prometheus ready: true, restart count 1
Feb 17 17:16:32.522: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Feb 17 17:16:32.522: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 17 17:16:32.522: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Feb 17 17:16:32.522: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 17 17:16:32.522: INFO: 
Logging pods the kubelet thinks is on node 10.45.66.189 before test
Feb 17 17:16:32.627: INFO: openshift-kube-proxy-gwxkw from openshift-kube-proxy started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 17:16:32.627: INFO: node-ca-56bzx from openshift-image-registry started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container node-ca ready: true, restart count 0
Feb 17 17:16:32.627: INFO: prometheus-adapter-55674d9685-g5b96 from openshift-monitoring started at 2020-02-17 14:41:40 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 17 17:16:32.627: INFO: node-exporter-4fhhh from openshift-monitoring started at 2020-02-17 14:36:28 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 17:16:32.627: INFO: 	Container node-exporter ready: true, restart count 0
Feb 17 17:16:32.627: INFO: community-operators-6c7f5d4977-fxhls from openshift-marketplace started at 2020-02-17 14:38:06 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container community-operators ready: true, restart count 0
Feb 17 17:16:32.627: INFO: ibm-keepalived-watcher-5hn6l from kube-system started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 17:16:32.627: INFO: calico-node-cctn2 from kube-system started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 17:16:32.627: INFO: ibmcloud-block-storage-driver-jnxhv from kube-system started at 2020-02-17 14:36:32 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 17 17:16:32.627: INFO: telemeter-client-56f4c98664-jhjns from openshift-monitoring started at 2020-02-17 14:41:37 +0000 UTC (3 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 17:16:32.627: INFO: 	Container reload ready: true, restart count 0
Feb 17 17:16:32.627: INFO: 	Container telemeter-client ready: true, restart count 0
Feb 17 17:16:32.627: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-02-17 14:42:08 +0000 UTC (3 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container alertmanager ready: true, restart count 0
Feb 17 17:16:32.627: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 17 17:16:32.627: INFO: 	Container config-reloader ready: true, restart count 0
Feb 17 17:16:32.627: INFO: tuned-kg4dp from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:28 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container tuned ready: true, restart count 0
Feb 17 17:16:32.627: INFO: thanos-querier-b949c5db-5nhn5 from openshift-monitoring started at 2020-02-17 14:42:35 +0000 UTC (4 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 17:16:32.627: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 17 17:16:32.627: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 17:16:32.627: INFO: 	Container thanos-querier ready: true, restart count 0
Feb 17 17:16:32.627: INFO: redhat-operators-7cc9d7897-nsjpx from openshift-marketplace started at 2020-02-17 14:38:05 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container redhat-operators ready: true, restart count 0
Feb 17 17:16:32.627: INFO: console-7547c7bc6c-rjbss from openshift-console started at 2020-02-17 14:39:12 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container console ready: true, restart count 0
Feb 17 17:16:32.627: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-02-17 14:39:35 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Feb 17 17:16:32.627: INFO: calico-typha-76c597bc7d-h6b67 from kube-system started at 2020-02-17 14:37:29 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container calico-typha ready: true, restart count 0
Feb 17 17:16:32.627: INFO: dns-default-wkxng from openshift-dns started at 2020-02-17 14:37:34 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container dns ready: true, restart count 0
Feb 17 17:16:32.627: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 17 17:16:32.627: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-02-17 14:43:08 +0000 UTC (7 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 17:16:32.627: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 17:16:32.627: INFO: 	Container prometheus ready: true, restart count 1
Feb 17 17:16:32.627: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Feb 17 17:16:32.627: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 17 17:16:32.627: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Feb 17 17:16:32.627: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 17 17:16:32.627: INFO: multus-admission-controller-lpf76 from openshift-multus started at 2020-02-17 14:37:28 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 17 17:16:32.627: INFO: sonobuoy-systemd-logs-daemon-set-67976b35ad084332-4jdsp from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 17:16:32.627: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 17:16:32.627: INFO: ibm-master-proxy-static-10.45.66.189 from kube-system started at 2020-02-17 14:37:14 +0000 UTC (2 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 17:16:32.627: INFO: 	Container pause ready: true, restart count 0
Feb 17 17:16:32.627: INFO: multus-df87p from openshift-multus started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container kube-multus ready: true, restart count 0
Feb 17 17:16:32.627: INFO: ibm-cloud-provider-ip-158-175-93-66-7b5b7d9cb-mhbr7 from ibm-system started at 2020-02-17 14:41:22 +0000 UTC (1 container statuses recorded)
Feb 17 17:16:32.627: INFO: 	Container ibm-cloud-provider-ip-158-175-93-66 ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-a351a2c9-1964-4ed0-a880-03cc9f956df1 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-a351a2c9-1964-4ed0-a880-03cc9f956df1 off the node 10.45.66.189
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a351a2c9-1964-4ed0-a880-03cc9f956df1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:21:48.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6889" for this suite.
Feb 17 17:22:01.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:22:03.138: INFO: namespace sched-pred-6889 deletion completed in 14.147020416s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:331.037 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:22:03.139: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:22:03.313: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Feb 17 17:22:08.325: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 17 17:22:12.354: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Feb 17 17:22:20.453: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5983 /apis/apps/v1/namespaces/deployment-5983/deployments/test-cleanup-deployment 6b6ea66f-e211-4596-9b26-62c57c83d863 66157 1 2020-02-17 17:22:12 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004a68c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-17 17:22:12 +0000 UTC,LastTransitionTime:2020-02-17 17:22:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-65db99849b" has successfully progressed.,LastUpdateTime:2020-02-17 17:22:19 +0000 UTC,LastTransitionTime:2020-02-17 17:22:12 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 17 17:22:20.467: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-5983 /apis/apps/v1/namespaces/deployment-5983/replicasets/test-cleanup-deployment-65db99849b 4c55d946-45d5-4ef2-a359-3d70db2059bd 66146 1 2020-02-17 17:22:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 6b6ea66f-e211-4596-9b26-62c57c83d863 0xc004a69047 0xc004a69048}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004a690a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 17 17:22:20.479: INFO: Pod "test-cleanup-deployment-65db99849b-9fqsm" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-9fqsm test-cleanup-deployment-65db99849b- deployment-5983 /api/v1/namespaces/deployment-5983/pods/test-cleanup-deployment-65db99849b-9fqsm 46680907-7480-465c-a878-16b26e451fb0 66145 0 2020-02-17 17:22:12 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[cni.projectcalico.org/podIP:172.30.80.23/32 cni.projectcalico.org/podIPs:172.30.80.23/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.80.23"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b 4c55d946-45d5-4ef2-a359-3d70db2059bd 0xc004a694b7 0xc004a694b8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zjq6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zjq6k,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zjq6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-2fz5j,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:22:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:22:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:22:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:22:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.178,PodIP:172.30.80.23,StartTime:2020-02-17 17:22:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 17:22:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://dcb6ff56be42a034c087e26a4f0cd34586947f4cb15767b5df7d402356087146,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.80.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:22:20.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5983" for this suite.
Feb 17 17:22:28.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:22:30.638: INFO: namespace deployment-5983 deletion completed in 10.126891597s

• [SLOW TEST:27.499 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:22:30.639: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:22:30.825: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a9b8229-fea3-43f5-9510-e34394b55ce2" in namespace "projected-1266" to be "success or failure"
Feb 17 17:22:30.840: INFO: Pod "downwardapi-volume-9a9b8229-fea3-43f5-9510-e34394b55ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.545819ms
Feb 17 17:22:32.852: INFO: Pod "downwardapi-volume-9a9b8229-fea3-43f5-9510-e34394b55ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027220606s
Feb 17 17:22:34.866: INFO: Pod "downwardapi-volume-9a9b8229-fea3-43f5-9510-e34394b55ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041247975s
Feb 17 17:22:36.879: INFO: Pod "downwardapi-volume-9a9b8229-fea3-43f5-9510-e34394b55ce2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053933598s
Feb 17 17:22:38.892: INFO: Pod "downwardapi-volume-9a9b8229-fea3-43f5-9510-e34394b55ce2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.066785749s
STEP: Saw pod success
Feb 17 17:22:38.892: INFO: Pod "downwardapi-volume-9a9b8229-fea3-43f5-9510-e34394b55ce2" satisfied condition "success or failure"
Feb 17 17:22:38.904: INFO: Trying to get logs from node 10.45.66.177 pod downwardapi-volume-9a9b8229-fea3-43f5-9510-e34394b55ce2 container client-container: <nil>
STEP: delete the pod
Feb 17 17:22:39.009: INFO: Waiting for pod downwardapi-volume-9a9b8229-fea3-43f5-9510-e34394b55ce2 to disappear
Feb 17 17:22:39.021: INFO: Pod downwardapi-volume-9a9b8229-fea3-43f5-9510-e34394b55ce2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:22:39.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1266" for this suite.
Feb 17 17:22:47.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:22:49.231: INFO: namespace projected-1266 deletion completed in 10.181669105s

• [SLOW TEST:18.592 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:22:49.231: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Feb 17 17:22:49.432: INFO: Waiting up to 5m0s for pod "var-expansion-a24a352c-e697-44e5-9d6e-5ace1ee92ad6" in namespace "var-expansion-8991" to be "success or failure"
Feb 17 17:22:49.451: INFO: Pod "var-expansion-a24a352c-e697-44e5-9d6e-5ace1ee92ad6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.179201ms
Feb 17 17:22:51.463: INFO: Pod "var-expansion-a24a352c-e697-44e5-9d6e-5ace1ee92ad6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031071925s
Feb 17 17:22:53.477: INFO: Pod "var-expansion-a24a352c-e697-44e5-9d6e-5ace1ee92ad6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044826635s
Feb 17 17:22:55.491: INFO: Pod "var-expansion-a24a352c-e697-44e5-9d6e-5ace1ee92ad6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058609636s
Feb 17 17:22:57.505: INFO: Pod "var-expansion-a24a352c-e697-44e5-9d6e-5ace1ee92ad6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.072865216s
STEP: Saw pod success
Feb 17 17:22:57.506: INFO: Pod "var-expansion-a24a352c-e697-44e5-9d6e-5ace1ee92ad6" satisfied condition "success or failure"
Feb 17 17:22:57.518: INFO: Trying to get logs from node 10.45.66.178 pod var-expansion-a24a352c-e697-44e5-9d6e-5ace1ee92ad6 container dapi-container: <nil>
STEP: delete the pod
Feb 17 17:22:57.634: INFO: Waiting for pod var-expansion-a24a352c-e697-44e5-9d6e-5ace1ee92ad6 to disappear
Feb 17 17:22:57.647: INFO: Pod var-expansion-a24a352c-e697-44e5-9d6e-5ace1ee92ad6 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:22:57.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8991" for this suite.
Feb 17 17:23:05.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:23:07.790: INFO: namespace var-expansion-8991 deletion completed in 10.118578706s

• [SLOW TEST:18.559 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:23:07.790: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Feb 17 17:23:07.922: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 17:23:15.005: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:23:43.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2301" for this suite.
Feb 17 17:23:51.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:23:53.350: INFO: namespace crd-publish-openapi-2301 deletion completed in 10.128897795s

• [SLOW TEST:45.560 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:23:53.351: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:23:53.494: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:23:54.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-867" for this suite.
Feb 17 17:24:02.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:24:04.782: INFO: namespace custom-resource-definition-867 deletion completed in 10.185185779s

• [SLOW TEST:11.432 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:24:04.783: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-k26zr in namespace proxy-2082
I0217 17:24:04.974271      26 runners.go:184] Created replication controller with name: proxy-service-k26zr, namespace: proxy-2082, replica count: 1
I0217 17:24:06.024628      26 runners.go:184] proxy-service-k26zr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 17:24:07.024999      26 runners.go:184] proxy-service-k26zr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 17:24:08.025353      26 runners.go:184] proxy-service-k26zr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 17:24:09.025713      26 runners.go:184] proxy-service-k26zr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 17:24:10.025959      26 runners.go:184] proxy-service-k26zr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 17:24:11.026221      26 runners.go:184] proxy-service-k26zr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 17:24:12.026430      26 runners.go:184] proxy-service-k26zr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 17:24:13.026758      26 runners.go:184] proxy-service-k26zr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0217 17:24:14.027066      26 runners.go:184] proxy-service-k26zr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0217 17:24:15.027290      26 runners.go:184] proxy-service-k26zr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 17:24:15.046: INFO: setup took 10.123992793s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Feb 17 17:24:15.083: INFO: (0) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 36.21063ms)
Feb 17 17:24:15.083: INFO: (0) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 36.23332ms)
Feb 17 17:24:15.083: INFO: (0) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 37.02306ms)
Feb 17 17:24:15.087: INFO: (0) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 41.296904ms)
Feb 17 17:24:15.087: INFO: (0) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 41.236484ms)
Feb 17 17:24:15.088: INFO: (0) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 40.990653ms)
Feb 17 17:24:15.091: INFO: (0) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 44.35654ms)
Feb 17 17:24:15.093: INFO: (0) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 46.365656ms)
Feb 17 17:24:15.093: INFO: (0) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 46.970191ms)
Feb 17 17:24:15.093: INFO: (0) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 47.323113ms)
Feb 17 17:24:15.096: INFO: (0) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 49.181768ms)
Feb 17 17:24:15.127: INFO: (0) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 81.321905ms)
Feb 17 17:24:15.128: INFO: (0) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 81.258482ms)
Feb 17 17:24:15.128: INFO: (0) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 81.326843ms)
Feb 17 17:24:15.128: INFO: (0) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 81.762602ms)
Feb 17 17:24:15.128: INFO: (0) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 82.008503ms)
Feb 17 17:24:15.148: INFO: (1) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 19.571836ms)
Feb 17 17:24:15.152: INFO: (1) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 22.75827ms)
Feb 17 17:24:15.153: INFO: (1) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 23.370289ms)
Feb 17 17:24:15.157: INFO: (1) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 27.951248ms)
Feb 17 17:24:15.157: INFO: (1) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 27.913121ms)
Feb 17 17:24:15.157: INFO: (1) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 28.16911ms)
Feb 17 17:24:15.157: INFO: (1) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 28.606229ms)
Feb 17 17:24:15.158: INFO: (1) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 28.526024ms)
Feb 17 17:24:15.158: INFO: (1) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 28.403399ms)
Feb 17 17:24:15.158: INFO: (1) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 28.747173ms)
Feb 17 17:24:15.161: INFO: (1) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 32.361906ms)
Feb 17 17:24:15.163: INFO: (1) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 33.96814ms)
Feb 17 17:24:15.170: INFO: (1) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 40.635511ms)
Feb 17 17:24:15.170: INFO: (1) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 40.688305ms)
Feb 17 17:24:15.170: INFO: (1) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 41.036026ms)
Feb 17 17:24:15.170: INFO: (1) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 40.939162ms)
Feb 17 17:24:15.191: INFO: (2) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 20.656083ms)
Feb 17 17:24:15.196: INFO: (2) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 23.040901ms)
Feb 17 17:24:15.197: INFO: (2) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 24.420483ms)
Feb 17 17:24:15.202: INFO: (2) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 30.18328ms)
Feb 17 17:24:15.202: INFO: (2) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 29.581121ms)
Feb 17 17:24:15.206: INFO: (2) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 32.916396ms)
Feb 17 17:24:15.206: INFO: (2) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 32.820876ms)
Feb 17 17:24:15.206: INFO: (2) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 33.900587ms)
Feb 17 17:24:15.206: INFO: (2) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 34.574924ms)
Feb 17 17:24:15.206: INFO: (2) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 32.572005ms)
Feb 17 17:24:15.206: INFO: (2) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 35.592957ms)
Feb 17 17:24:15.212: INFO: (2) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 38.773619ms)
Feb 17 17:24:15.212: INFO: (2) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 38.11132ms)
Feb 17 17:24:15.212: INFO: (2) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 39.090566ms)
Feb 17 17:24:15.212: INFO: (2) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 40.009317ms)
Feb 17 17:24:15.212: INFO: (2) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 40.828879ms)
Feb 17 17:24:15.233: INFO: (3) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 21.27395ms)
Feb 17 17:24:15.239: INFO: (3) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 26.42861ms)
Feb 17 17:24:15.239: INFO: (3) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 27.005742ms)
Feb 17 17:24:15.239: INFO: (3) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 26.828701ms)
Feb 17 17:24:15.239: INFO: (3) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 26.787838ms)
Feb 17 17:24:15.240: INFO: (3) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 27.100443ms)
Feb 17 17:24:15.244: INFO: (3) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 30.998531ms)
Feb 17 17:24:15.244: INFO: (3) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 31.405884ms)
Feb 17 17:24:15.244: INFO: (3) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 31.725457ms)
Feb 17 17:24:15.244: INFO: (3) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 31.573011ms)
Feb 17 17:24:15.246: INFO: (3) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 34.049351ms)
Feb 17 17:24:15.248: INFO: (3) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 35.890097ms)
Feb 17 17:24:15.253: INFO: (3) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 40.4583ms)
Feb 17 17:24:15.253: INFO: (3) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 40.814372ms)
Feb 17 17:24:15.253: INFO: (3) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 40.474796ms)
Feb 17 17:24:15.253: INFO: (3) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 40.790185ms)
Feb 17 17:24:15.273: INFO: (4) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 19.533753ms)
Feb 17 17:24:15.278: INFO: (4) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 24.516858ms)
Feb 17 17:24:15.280: INFO: (4) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 26.542694ms)
Feb 17 17:24:15.280: INFO: (4) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 26.378757ms)
Feb 17 17:24:15.284: INFO: (4) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 30.06306ms)
Feb 17 17:24:15.284: INFO: (4) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 30.233007ms)
Feb 17 17:24:15.284: INFO: (4) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 30.136953ms)
Feb 17 17:24:15.284: INFO: (4) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 30.321391ms)
Feb 17 17:24:15.284: INFO: (4) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 30.463481ms)
Feb 17 17:24:15.284: INFO: (4) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 30.352209ms)
Feb 17 17:24:15.290: INFO: (4) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 36.312181ms)
Feb 17 17:24:15.290: INFO: (4) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 36.158575ms)
Feb 17 17:24:15.294: INFO: (4) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 39.569627ms)
Feb 17 17:24:15.294: INFO: (4) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 39.713921ms)
Feb 17 17:24:15.294: INFO: (4) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 40.330381ms)
Feb 17 17:24:15.297: INFO: (4) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 42.966956ms)
Feb 17 17:24:15.317: INFO: (5) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 20.24943ms)
Feb 17 17:24:15.324: INFO: (5) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 26.89975ms)
Feb 17 17:24:15.324: INFO: (5) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 27.087849ms)
Feb 17 17:24:15.329: INFO: (5) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 31.543924ms)
Feb 17 17:24:15.329: INFO: (5) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 31.575982ms)
Feb 17 17:24:15.329: INFO: (5) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 31.668111ms)
Feb 17 17:24:15.329: INFO: (5) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 31.670437ms)
Feb 17 17:24:15.329: INFO: (5) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 31.639928ms)
Feb 17 17:24:15.329: INFO: (5) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 31.809638ms)
Feb 17 17:24:15.329: INFO: (5) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 32.037566ms)
Feb 17 17:24:15.332: INFO: (5) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 35.090363ms)
Feb 17 17:24:15.333: INFO: (5) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 35.327282ms)
Feb 17 17:24:15.336: INFO: (5) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 38.90399ms)
Feb 17 17:24:15.337: INFO: (5) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 39.636272ms)
Feb 17 17:24:15.337: INFO: (5) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 39.794347ms)
Feb 17 17:24:15.337: INFO: (5) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 39.681532ms)
Feb 17 17:24:15.357: INFO: (6) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 19.529032ms)
Feb 17 17:24:15.363: INFO: (6) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 25.178188ms)
Feb 17 17:24:15.363: INFO: (6) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 24.979326ms)
Feb 17 17:24:15.363: INFO: (6) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 25.644036ms)
Feb 17 17:24:15.367: INFO: (6) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 29.436372ms)
Feb 17 17:24:15.367: INFO: (6) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 29.565319ms)
Feb 17 17:24:15.368: INFO: (6) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 29.728773ms)
Feb 17 17:24:15.368: INFO: (6) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 30.021108ms)
Feb 17 17:24:15.368: INFO: (6) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 30.103887ms)
Feb 17 17:24:15.368: INFO: (6) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 29.956114ms)
Feb 17 17:24:15.371: INFO: (6) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 33.525408ms)
Feb 17 17:24:15.372: INFO: (6) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 34.419484ms)
Feb 17 17:24:15.379: INFO: (6) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 41.000651ms)
Feb 17 17:24:15.379: INFO: (6) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 41.063225ms)
Feb 17 17:24:15.379: INFO: (6) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 41.235793ms)
Feb 17 17:24:15.383: INFO: (6) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 45.615533ms)
Feb 17 17:24:15.404: INFO: (7) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 20.799218ms)
Feb 17 17:24:15.409: INFO: (7) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 23.332581ms)
Feb 17 17:24:15.409: INFO: (7) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 23.977226ms)
Feb 17 17:24:15.409: INFO: (7) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 24.750514ms)
Feb 17 17:24:15.417: INFO: (7) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 32.752749ms)
Feb 17 17:24:15.417: INFO: (7) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 33.323631ms)
Feb 17 17:24:15.417: INFO: (7) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 30.905129ms)
Feb 17 17:24:15.417: INFO: (7) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 31.114314ms)
Feb 17 17:24:15.417: INFO: (7) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 32.527241ms)
Feb 17 17:24:15.417: INFO: (7) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 32.193758ms)
Feb 17 17:24:15.417: INFO: (7) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 32.747197ms)
Feb 17 17:24:15.421: INFO: (7) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 35.150822ms)
Feb 17 17:24:15.429: INFO: (7) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 44.00076ms)
Feb 17 17:24:15.429: INFO: (7) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 43.632268ms)
Feb 17 17:24:15.429: INFO: (7) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 44.064271ms)
Feb 17 17:24:15.429: INFO: (7) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 43.14327ms)
Feb 17 17:24:15.449: INFO: (8) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 19.850172ms)
Feb 17 17:24:15.454: INFO: (8) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 22.510374ms)
Feb 17 17:24:15.454: INFO: (8) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 23.777697ms)
Feb 17 17:24:15.454: INFO: (8) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 24.473926ms)
Feb 17 17:24:15.455: INFO: (8) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 23.987867ms)
Feb 17 17:24:15.455: INFO: (8) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 23.259901ms)
Feb 17 17:24:15.459: INFO: (8) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 29.374295ms)
Feb 17 17:24:15.459: INFO: (8) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 28.473773ms)
Feb 17 17:24:15.459: INFO: (8) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 27.76354ms)
Feb 17 17:24:15.459: INFO: (8) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 29.301897ms)
Feb 17 17:24:15.459: INFO: (8) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 27.777756ms)
Feb 17 17:24:15.463: INFO: (8) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 33.482662ms)
Feb 17 17:24:15.469: INFO: (8) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 39.160732ms)
Feb 17 17:24:15.469: INFO: (8) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 38.429062ms)
Feb 17 17:24:15.470: INFO: (8) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 39.254891ms)
Feb 17 17:24:15.474: INFO: (8) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 42.623679ms)
Feb 17 17:24:15.494: INFO: (9) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 19.677624ms)
Feb 17 17:24:15.499: INFO: (9) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 23.929637ms)
Feb 17 17:24:15.499: INFO: (9) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 24.319935ms)
Feb 17 17:24:15.500: INFO: (9) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 25.189688ms)
Feb 17 17:24:15.503: INFO: (9) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 28.622473ms)
Feb 17 17:24:15.503: INFO: (9) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 28.535044ms)
Feb 17 17:24:15.503: INFO: (9) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 28.526533ms)
Feb 17 17:24:15.503: INFO: (9) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 28.87455ms)
Feb 17 17:24:15.503: INFO: (9) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 28.669743ms)
Feb 17 17:24:15.503: INFO: (9) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 28.80193ms)
Feb 17 17:24:15.509: INFO: (9) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 34.373221ms)
Feb 17 17:24:15.515: INFO: (9) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 40.080805ms)
Feb 17 17:24:15.515: INFO: (9) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 40.034322ms)
Feb 17 17:24:15.515: INFO: (9) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 40.110754ms)
Feb 17 17:24:15.519: INFO: (9) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 44.3568ms)
Feb 17 17:24:15.519: INFO: (9) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 44.432123ms)
Feb 17 17:24:15.541: INFO: (10) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 21.535145ms)
Feb 17 17:24:15.545: INFO: (10) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 25.002014ms)
Feb 17 17:24:15.546: INFO: (10) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 25.951208ms)
Feb 17 17:24:15.546: INFO: (10) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 26.170372ms)
Feb 17 17:24:15.546: INFO: (10) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 26.017238ms)
Feb 17 17:24:15.546: INFO: (10) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 26.637335ms)
Feb 17 17:24:15.550: INFO: (10) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 29.941691ms)
Feb 17 17:24:15.551: INFO: (10) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 31.412365ms)
Feb 17 17:24:15.551: INFO: (10) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 31.259235ms)
Feb 17 17:24:15.551: INFO: (10) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 31.436648ms)
Feb 17 17:24:15.554: INFO: (10) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 34.140328ms)
Feb 17 17:24:15.556: INFO: (10) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 35.872286ms)
Feb 17 17:24:15.558: INFO: (10) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 38.609451ms)
Feb 17 17:24:15.560: INFO: (10) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 40.954121ms)
Feb 17 17:24:15.561: INFO: (10) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 40.966515ms)
Feb 17 17:24:15.561: INFO: (10) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 40.853396ms)
Feb 17 17:24:15.581: INFO: (11) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 20.560438ms)
Feb 17 17:24:15.585: INFO: (11) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 23.917724ms)
Feb 17 17:24:15.586: INFO: (11) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 25.135828ms)
Feb 17 17:24:15.586: INFO: (11) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 25.276958ms)
Feb 17 17:24:15.586: INFO: (11) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 25.231625ms)
Feb 17 17:24:15.587: INFO: (11) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 25.321937ms)
Feb 17 17:24:15.590: INFO: (11) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 29.1635ms)
Feb 17 17:24:15.591: INFO: (11) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 29.475391ms)
Feb 17 17:24:15.591: INFO: (11) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 29.600303ms)
Feb 17 17:24:15.591: INFO: (11) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 29.66208ms)
Feb 17 17:24:15.593: INFO: (11) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 32.126904ms)
Feb 17 17:24:15.596: INFO: (11) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 35.230211ms)
Feb 17 17:24:15.601: INFO: (11) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 39.708479ms)
Feb 17 17:24:15.601: INFO: (11) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 39.855211ms)
Feb 17 17:24:15.606: INFO: (11) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 44.363495ms)
Feb 17 17:24:15.606: INFO: (11) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 44.407027ms)
Feb 17 17:24:15.629: INFO: (12) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 22.74335ms)
Feb 17 17:24:15.629: INFO: (12) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 22.917838ms)
Feb 17 17:24:15.634: INFO: (12) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 27.584987ms)
Feb 17 17:24:15.634: INFO: (12) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 27.933004ms)
Feb 17 17:24:15.634: INFO: (12) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 27.553259ms)
Feb 17 17:24:15.634: INFO: (12) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 27.678464ms)
Feb 17 17:24:15.634: INFO: (12) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 27.833607ms)
Feb 17 17:24:15.634: INFO: (12) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 28.009828ms)
Feb 17 17:24:15.634: INFO: (12) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 28.041599ms)
Feb 17 17:24:15.634: INFO: (12) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 28.360933ms)
Feb 17 17:24:15.634: INFO: (12) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 28.370559ms)
Feb 17 17:24:15.638: INFO: (12) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 31.937694ms)
Feb 17 17:24:15.647: INFO: (12) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 40.730687ms)
Feb 17 17:24:15.647: INFO: (12) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 41.077172ms)
Feb 17 17:24:15.647: INFO: (12) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 40.992768ms)
Feb 17 17:24:15.651: INFO: (12) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 45.261838ms)
Feb 17 17:24:15.687: INFO: (13) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 34.991179ms)
Feb 17 17:24:15.687: INFO: (13) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 34.665043ms)
Feb 17 17:24:15.687: INFO: (13) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 34.49044ms)
Feb 17 17:24:15.687: INFO: (13) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 34.109313ms)
Feb 17 17:24:15.688: INFO: (13) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 35.988554ms)
Feb 17 17:24:15.688: INFO: (13) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 35.514237ms)
Feb 17 17:24:15.687: INFO: (13) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 34.564663ms)
Feb 17 17:24:15.691: INFO: (13) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 38.993723ms)
Feb 17 17:24:15.691: INFO: (13) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 38.80301ms)
Feb 17 17:24:15.691: INFO: (13) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 39.60657ms)
Feb 17 17:24:15.692: INFO: (13) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 40.090077ms)
Feb 17 17:24:15.701: INFO: (13) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 48.522819ms)
Feb 17 17:24:15.701: INFO: (13) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 49.337194ms)
Feb 17 17:24:15.701: INFO: (13) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 48.549972ms)
Feb 17 17:24:15.707: INFO: (13) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 54.201621ms)
Feb 17 17:24:15.707: INFO: (13) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 55.36102ms)
Feb 17 17:24:15.732: INFO: (14) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 24.707748ms)
Feb 17 17:24:15.739: INFO: (14) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 31.411429ms)
Feb 17 17:24:15.739: INFO: (14) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 31.397353ms)
Feb 17 17:24:15.739: INFO: (14) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 31.449041ms)
Feb 17 17:24:15.740: INFO: (14) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 31.755047ms)
Feb 17 17:24:15.740: INFO: (14) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 32.060852ms)
Feb 17 17:24:15.743: INFO: (14) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 35.379629ms)
Feb 17 17:24:15.743: INFO: (14) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 35.347539ms)
Feb 17 17:24:15.743: INFO: (14) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 35.521137ms)
Feb 17 17:24:15.744: INFO: (14) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 35.505831ms)
Feb 17 17:24:15.744: INFO: (14) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 35.980128ms)
Feb 17 17:24:15.744: INFO: (14) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 36.264603ms)
Feb 17 17:24:15.745: INFO: (14) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 37.168369ms)
Feb 17 17:24:15.747: INFO: (14) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 38.699156ms)
Feb 17 17:24:15.749: INFO: (14) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 41.961317ms)
Feb 17 17:24:15.749: INFO: (14) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 41.423103ms)
Feb 17 17:24:15.772: INFO: (15) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 21.978186ms)
Feb 17 17:24:15.777: INFO: (15) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 26.723465ms)
Feb 17 17:24:15.777: INFO: (15) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 27.150108ms)
Feb 17 17:24:15.779: INFO: (15) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 28.592769ms)
Feb 17 17:24:15.781: INFO: (15) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 31.308838ms)
Feb 17 17:24:15.782: INFO: (15) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 31.630312ms)
Feb 17 17:24:15.782: INFO: (15) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 31.918706ms)
Feb 17 17:24:15.782: INFO: (15) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 31.722329ms)
Feb 17 17:24:15.782: INFO: (15) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 31.918777ms)
Feb 17 17:24:15.782: INFO: (15) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 32.110247ms)
Feb 17 17:24:15.786: INFO: (15) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 35.651012ms)
Feb 17 17:24:15.786: INFO: (15) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 36.603562ms)
Feb 17 17:24:15.791: INFO: (15) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 40.763894ms)
Feb 17 17:24:15.792: INFO: (15) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 42.405057ms)
Feb 17 17:24:15.793: INFO: (15) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 42.919461ms)
Feb 17 17:24:15.793: INFO: (15) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 42.920932ms)
Feb 17 17:24:15.825: INFO: (16) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 29.712111ms)
Feb 17 17:24:15.825: INFO: (16) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 30.909162ms)
Feb 17 17:24:15.825: INFO: (16) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 32.145653ms)
Feb 17 17:24:15.825: INFO: (16) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 30.333989ms)
Feb 17 17:24:15.826: INFO: (16) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 30.108692ms)
Feb 17 17:24:15.826: INFO: (16) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 31.468494ms)
Feb 17 17:24:15.826: INFO: (16) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 30.459636ms)
Feb 17 17:24:15.830: INFO: (16) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 36.791522ms)
Feb 17 17:24:15.830: INFO: (16) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 36.539663ms)
Feb 17 17:24:15.830: INFO: (16) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 36.832111ms)
Feb 17 17:24:15.831: INFO: (16) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 37.640396ms)
Feb 17 17:24:15.831: INFO: (16) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 35.833118ms)
Feb 17 17:24:15.831: INFO: (16) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 36.213785ms)
Feb 17 17:24:15.835: INFO: (16) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 39.078865ms)
Feb 17 17:24:15.835: INFO: (16) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 39.107055ms)
Feb 17 17:24:15.839: INFO: (16) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 43.970024ms)
Feb 17 17:24:15.861: INFO: (17) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 21.317218ms)
Feb 17 17:24:15.867: INFO: (17) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 27.008048ms)
Feb 17 17:24:15.868: INFO: (17) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 26.864623ms)
Feb 17 17:24:15.868: INFO: (17) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 27.260609ms)
Feb 17 17:24:15.872: INFO: (17) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 31.422571ms)
Feb 17 17:24:15.872: INFO: (17) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 32.91434ms)
Feb 17 17:24:15.872: INFO: (17) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 32.259976ms)
Feb 17 17:24:15.872: INFO: (17) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 32.434541ms)
Feb 17 17:24:15.873: INFO: (17) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 32.851512ms)
Feb 17 17:24:15.873: INFO: (17) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 32.76083ms)
Feb 17 17:24:15.884: INFO: (17) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 43.91199ms)
Feb 17 17:24:15.884: INFO: (17) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 43.440495ms)
Feb 17 17:24:15.886: INFO: (17) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 45.882159ms)
Feb 17 17:24:15.886: INFO: (17) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 45.629962ms)
Feb 17 17:24:15.886: INFO: (17) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 46.792045ms)
Feb 17 17:24:15.886: INFO: (17) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 46.535106ms)
Feb 17 17:24:15.906: INFO: (18) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 19.82721ms)
Feb 17 17:24:15.912: INFO: (18) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 23.79936ms)
Feb 17 17:24:15.912: INFO: (18) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 24.522045ms)
Feb 17 17:24:15.912: INFO: (18) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 24.425804ms)
Feb 17 17:24:15.916: INFO: (18) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 29.528937ms)
Feb 17 17:24:15.916: INFO: (18) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 28.111838ms)
Feb 17 17:24:15.916: INFO: (18) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 29.373083ms)
Feb 17 17:24:15.917: INFO: (18) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 28.542895ms)
Feb 17 17:24:15.917: INFO: (18) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 29.20677ms)
Feb 17 17:24:15.917: INFO: (18) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 29.907947ms)
Feb 17 17:24:15.917: INFO: (18) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 29.02784ms)
Feb 17 17:24:15.922: INFO: (18) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 33.532111ms)
Feb 17 17:24:15.926: INFO: (18) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 39.549405ms)
Feb 17 17:24:15.927: INFO: (18) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 38.792934ms)
Feb 17 17:24:15.931: INFO: (18) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 43.645389ms)
Feb 17 17:24:15.931: INFO: (18) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 43.336026ms)
Feb 17 17:24:15.957: INFO: (19) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">... (200; 25.065253ms)
Feb 17 17:24:15.957: INFO: (19) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 25.207943ms)
Feb 17 17:24:15.957: INFO: (19) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9/proxy/rewriteme">test</a> (200; 25.362838ms)
Feb 17 17:24:15.961: INFO: (19) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:462/proxy/: tls qux (200; 30.246149ms)
Feb 17 17:24:15.962: INFO: (19) /api/v1/namespaces/proxy-2082/pods/http:proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 30.028131ms)
Feb 17 17:24:15.962: INFO: (19) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:162/proxy/: bar (200; 30.211519ms)
Feb 17 17:24:15.962: INFO: (19) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:1080/proxy/rewriteme">test<... (200; 29.987987ms)
Feb 17 17:24:15.962: INFO: (19) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/: <a href="/api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:443/proxy/tlsrewritem... (200; 30.127764ms)
Feb 17 17:24:15.962: INFO: (19) /api/v1/namespaces/proxy-2082/pods/proxy-service-k26zr-xxnt9:160/proxy/: foo (200; 30.02099ms)
Feb 17 17:24:15.962: INFO: (19) /api/v1/namespaces/proxy-2082/pods/https:proxy-service-k26zr-xxnt9:460/proxy/: tls baz (200; 30.227634ms)
Feb 17 17:24:15.964: INFO: (19) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname1/proxy/: foo (200; 32.935438ms)
Feb 17 17:24:15.970: INFO: (19) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname2/proxy/: tls qux (200; 38.267266ms)
Feb 17 17:24:15.970: INFO: (19) /api/v1/namespaces/proxy-2082/services/proxy-service-k26zr:portname2/proxy/: bar (200; 38.369073ms)
Feb 17 17:24:15.970: INFO: (19) /api/v1/namespaces/proxy-2082/services/https:proxy-service-k26zr:tlsportname1/proxy/: tls baz (200; 38.546155ms)
Feb 17 17:24:15.974: INFO: (19) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname1/proxy/: foo (200; 42.720511ms)
Feb 17 17:24:15.974: INFO: (19) /api/v1/namespaces/proxy-2082/services/http:proxy-service-k26zr:portname2/proxy/: bar (200; 42.681907ms)
STEP: deleting ReplicationController proxy-service-k26zr in namespace proxy-2082, will wait for the garbage collector to delete the pods
Feb 17 17:24:16.077: INFO: Deleting ReplicationController proxy-service-k26zr took: 37.420799ms
Feb 17 17:24:16.577: INFO: Terminating ReplicationController proxy-service-k26zr pods took: 500.43975ms
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:24:23.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2082" for this suite.
Feb 17 17:24:31.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:24:32.917: INFO: namespace proxy-2082 deletion completed in 9.106181504s

• [SLOW TEST:28.134 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:24:32.917: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 17:24:33.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-5228'
Feb 17 17:24:33.449: INFO: stderr: ""
Feb 17 17:24:33.450: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Feb 17 17:24:33.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete pods e2e-test-httpd-pod --namespace=kubectl-5228'
Feb 17 17:24:43.699: INFO: stderr: ""
Feb 17 17:24:43.699: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:24:43.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5228" for this suite.
Feb 17 17:24:51.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:24:53.872: INFO: namespace kubectl-5228 deletion completed in 10.143325735s

• [SLOW TEST:20.955 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:24:53.872: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:24:54.054: INFO: Waiting up to 5m0s for pod "downwardapi-volume-941650f7-2d3f-473f-b7f9-6e65e2790d63" in namespace "downward-api-9542" to be "success or failure"
Feb 17 17:24:54.067: INFO: Pod "downwardapi-volume-941650f7-2d3f-473f-b7f9-6e65e2790d63": Phase="Pending", Reason="", readiness=false. Elapsed: 13.001496ms
Feb 17 17:24:56.080: INFO: Pod "downwardapi-volume-941650f7-2d3f-473f-b7f9-6e65e2790d63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026110539s
Feb 17 17:24:58.093: INFO: Pod "downwardapi-volume-941650f7-2d3f-473f-b7f9-6e65e2790d63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039140852s
Feb 17 17:25:00.106: INFO: Pod "downwardapi-volume-941650f7-2d3f-473f-b7f9-6e65e2790d63": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052081881s
Feb 17 17:25:02.119: INFO: Pod "downwardapi-volume-941650f7-2d3f-473f-b7f9-6e65e2790d63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.065533624s
STEP: Saw pod success
Feb 17 17:25:02.119: INFO: Pod "downwardapi-volume-941650f7-2d3f-473f-b7f9-6e65e2790d63" satisfied condition "success or failure"
Feb 17 17:25:02.132: INFO: Trying to get logs from node 10.45.66.178 pod downwardapi-volume-941650f7-2d3f-473f-b7f9-6e65e2790d63 container client-container: <nil>
STEP: delete the pod
Feb 17 17:25:02.229: INFO: Waiting for pod downwardapi-volume-941650f7-2d3f-473f-b7f9-6e65e2790d63 to disappear
Feb 17 17:25:02.240: INFO: Pod downwardapi-volume-941650f7-2d3f-473f-b7f9-6e65e2790d63 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:25:02.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9542" for this suite.
Feb 17 17:25:10.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:25:12.400: INFO: namespace downward-api-9542 deletion completed in 10.136184917s

• [SLOW TEST:18.528 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:25:12.400: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-45b602be-0553-4e2a-ae9b-e362bd2ecc1d
STEP: Creating a pod to test consume secrets
Feb 17 17:25:12.598: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-38ee44ef-c2f5-4aac-b362-ab9c27863861" in namespace "projected-8266" to be "success or failure"
Feb 17 17:25:12.611: INFO: Pod "pod-projected-secrets-38ee44ef-c2f5-4aac-b362-ab9c27863861": Phase="Pending", Reason="", readiness=false. Elapsed: 12.552665ms
Feb 17 17:25:14.624: INFO: Pod "pod-projected-secrets-38ee44ef-c2f5-4aac-b362-ab9c27863861": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025859478s
Feb 17 17:25:16.638: INFO: Pod "pod-projected-secrets-38ee44ef-c2f5-4aac-b362-ab9c27863861": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040190654s
Feb 17 17:25:18.652: INFO: Pod "pod-projected-secrets-38ee44ef-c2f5-4aac-b362-ab9c27863861": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053948034s
Feb 17 17:25:20.665: INFO: Pod "pod-projected-secrets-38ee44ef-c2f5-4aac-b362-ab9c27863861": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.067415237s
STEP: Saw pod success
Feb 17 17:25:20.665: INFO: Pod "pod-projected-secrets-38ee44ef-c2f5-4aac-b362-ab9c27863861" satisfied condition "success or failure"
Feb 17 17:25:20.678: INFO: Trying to get logs from node 10.45.66.177 pod pod-projected-secrets-38ee44ef-c2f5-4aac-b362-ab9c27863861 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 17 17:25:20.774: INFO: Waiting for pod pod-projected-secrets-38ee44ef-c2f5-4aac-b362-ab9c27863861 to disappear
Feb 17 17:25:20.786: INFO: Pod pod-projected-secrets-38ee44ef-c2f5-4aac-b362-ab9c27863861 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:25:20.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8266" for this suite.
Feb 17 17:25:28.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:25:30.927: INFO: namespace projected-8266 deletion completed in 10.111470302s

• [SLOW TEST:18.527 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:25:30.931: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-1589
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-1589
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1589
Feb 17 17:25:31.123: INFO: Found 0 stateful pods, waiting for 1
Feb 17 17:25:41.138: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Feb 17 17:25:41.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-1589 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 17:25:41.502: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 17:25:41.503: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 17:25:41.503: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 17:25:41.517: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Feb 17 17:25:51.531: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 17:25:51.531: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 17:25:51.582: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 17 17:25:51.582: INFO: ss-0  10.45.66.177  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:41 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:31 +0000 UTC  }]
Feb 17 17:25:51.582: INFO: 
Feb 17 17:25:51.582: INFO: StatefulSet ss has not reached scale 3, at 1
Feb 17 17:25:52.596: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986372649s
Feb 17 17:25:53.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.972920506s
Feb 17 17:25:54.625: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.95780035s
Feb 17 17:25:55.641: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.943245158s
Feb 17 17:25:56.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.927505392s
Feb 17 17:25:57.671: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.913689862s
Feb 17 17:25:58.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.89774957s
Feb 17 17:25:59.697: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.884566122s
Feb 17 17:26:00.711: INFO: Verifying statefulset ss doesn't scale past 3 for another 871.228927ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1589
Feb 17 17:26:01.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-1589 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:26:02.081: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Feb 17 17:26:02.081: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 17:26:02.081: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 17:26:02.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-1589 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:26:02.448: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 17 17:26:02.448: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 17:26:02.448: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 17:26:02.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-1589 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Feb 17 17:26:02.829: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Feb 17 17:26:02.829: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Feb 17 17:26:02.829: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Feb 17 17:26:02.842: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Feb 17 17:26:12.860: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:26:12.860: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:26:12.860: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Feb 17 17:26:12.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-1589 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 17:26:13.229: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 17:26:13.229: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 17:26:13.229: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 17:26:13.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-1589 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 17:26:13.666: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 17:26:13.666: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 17:26:13.666: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 17:26:13.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=statefulset-1589 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Feb 17 17:26:14.055: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Feb 17 17:26:14.055: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Feb 17 17:26:14.055: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Feb 17 17:26:14.055: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 17:26:14.069: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Feb 17 17:26:24.094: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 17:26:24.094: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 17:26:24.094: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Feb 17 17:26:24.136: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 17 17:26:24.136: INFO: ss-0  10.45.66.177  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:31 +0000 UTC  }]
Feb 17 17:26:24.136: INFO: ss-1  10.45.66.189  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:51 +0000 UTC  }]
Feb 17 17:26:24.136: INFO: ss-2  10.45.66.178  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:51 +0000 UTC  }]
Feb 17 17:26:24.136: INFO: 
Feb 17 17:26:24.136: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 17 17:26:25.173: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 17 17:26:25.173: INFO: ss-0  10.45.66.177  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:31 +0000 UTC  }]
Feb 17 17:26:25.173: INFO: ss-1  10.45.66.189  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:51 +0000 UTC  }]
Feb 17 17:26:25.173: INFO: ss-2  10.45.66.178  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:51 +0000 UTC  }]
Feb 17 17:26:25.174: INFO: 
Feb 17 17:26:25.174: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 17 17:26:26.187: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 17 17:26:26.187: INFO: ss-0  10.45.66.177  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:13 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:31 +0000 UTC  }]
Feb 17 17:26:26.187: INFO: ss-1  10.45.66.189  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:51 +0000 UTC  }]
Feb 17 17:26:26.187: INFO: ss-2  10.45.66.178  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:51 +0000 UTC  }]
Feb 17 17:26:26.187: INFO: 
Feb 17 17:26:26.187: INFO: StatefulSet ss has not reached scale 0, at 3
Feb 17 17:26:27.201: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 17 17:26:27.201: INFO: ss-2  10.45.66.178  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:51 +0000 UTC  }]
Feb 17 17:26:27.201: INFO: 
Feb 17 17:26:27.201: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 17 17:26:28.214: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Feb 17 17:26:28.214: INFO: ss-2  10.45.66.178  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:26:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-02-17 17:25:51 +0000 UTC  }]
Feb 17 17:26:28.214: INFO: 
Feb 17 17:26:28.214: INFO: StatefulSet ss has not reached scale 0, at 1
Feb 17 17:26:29.228: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.903961881s
Feb 17 17:26:30.240: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.890903432s
Feb 17 17:26:31.254: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.878097603s
Feb 17 17:26:32.268: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.863977815s
Feb 17 17:26:33.282: INFO: Verifying statefulset ss doesn't scale past 0 for another 849.897843ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1589
Feb 17 17:26:34.295: INFO: Scaling statefulset ss to 0
Feb 17 17:26:34.331: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 17 17:26:34.342: INFO: Deleting all statefulset in ns statefulset-1589
Feb 17 17:26:34.353: INFO: Scaling statefulset ss to 0
Feb 17 17:26:34.392: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 17:26:34.403: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:26:34.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1589" for this suite.
Feb 17 17:26:42.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:26:44.610: INFO: namespace statefulset-1589 deletion completed in 10.131885396s

• [SLOW TEST:73.679 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:26:44.616: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Feb 17 17:26:44.765: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Feb 17 17:26:44.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 create -f - --namespace=kubectl-2775'
Feb 17 17:26:45.344: INFO: stderr: ""
Feb 17 17:26:45.344: INFO: stdout: "service/redis-slave created\n"
Feb 17 17:26:45.344: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Feb 17 17:26:45.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 create -f - --namespace=kubectl-2775'
Feb 17 17:26:45.911: INFO: stderr: ""
Feb 17 17:26:45.911: INFO: stdout: "service/redis-master created\n"
Feb 17 17:26:45.911: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Feb 17 17:26:45.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 create -f - --namespace=kubectl-2775'
Feb 17 17:26:46.482: INFO: stderr: ""
Feb 17 17:26:46.482: INFO: stdout: "service/frontend created\n"
Feb 17 17:26:46.483: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Feb 17 17:26:46.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 create -f - --namespace=kubectl-2775'
Feb 17 17:26:47.017: INFO: stderr: ""
Feb 17 17:26:47.017: INFO: stdout: "deployment.apps/frontend created\n"
Feb 17 17:26:47.018: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Feb 17 17:26:47.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 create -f - --namespace=kubectl-2775'
Feb 17 17:26:47.651: INFO: stderr: ""
Feb 17 17:26:47.651: INFO: stdout: "deployment.apps/redis-master created\n"
Feb 17 17:26:47.653: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Feb 17 17:26:47.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 create -f - --namespace=kubectl-2775'
Feb 17 17:26:48.243: INFO: stderr: ""
Feb 17 17:26:48.243: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Feb 17 17:26:48.243: INFO: Waiting for all frontend pods to be Running.
Feb 17 17:27:13.296: INFO: Waiting for frontend to serve content.
Feb 17 17:27:13.347: INFO: Trying to add a new entry to the guestbook.
Feb 17 17:27:13.393: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Feb 17 17:27:13.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete --grace-period=0 --force -f - --namespace=kubectl-2775'
Feb 17 17:27:13.679: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 17:27:13.679: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Feb 17 17:27:13.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete --grace-period=0 --force -f - --namespace=kubectl-2775'
Feb 17 17:27:13.886: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 17:27:13.886: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 17 17:27:13.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete --grace-period=0 --force -f - --namespace=kubectl-2775'
Feb 17 17:27:14.115: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 17:27:14.115: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 17 17:27:14.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete --grace-period=0 --force -f - --namespace=kubectl-2775'
Feb 17 17:27:14.295: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 17:27:14.295: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Feb 17 17:27:14.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete --grace-period=0 --force -f - --namespace=kubectl-2775'
Feb 17 17:27:14.462: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 17:27:14.463: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Feb 17 17:27:14.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete --grace-period=0 --force -f - --namespace=kubectl-2775'
Feb 17 17:27:14.614: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 17:27:14.614: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:27:14.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2775" for this suite.
Feb 17 17:27:36.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:27:38.745: INFO: namespace kubectl-2775 deletion completed in 24.100774421s

• [SLOW TEST:54.129 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:27:38.745: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 17:27:39.757: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 17:27:41.795: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557259, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557259, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557259, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557259, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:27:43.808: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557259, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557259, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557259, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557259, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:27:45.809: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557259, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557259, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557259, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557259, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:27:47.807: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557259, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557259, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557259, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557259, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 17:27:50.834: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:27:50.849: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:27:52.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7591" for this suite.
Feb 17 17:28:00.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:28:02.498: INFO: namespace webhook-7591 deletion completed in 10.164687115s
STEP: Destroying namespace "webhook-7591-markers" for this suite.
Feb 17 17:28:10.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:28:12.657: INFO: namespace webhook-7591-markers deletion completed in 10.158938092s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:33.982 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:28:12.728: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Feb 17 17:28:13.926: INFO: Waiting up to 5m0s for pod "pod-76dbd0f5-b4e3-4a49-859b-6abd9cf3bf8e" in namespace "emptydir-7863" to be "success or failure"
Feb 17 17:28:13.939: INFO: Pod "pod-76dbd0f5-b4e3-4a49-859b-6abd9cf3bf8e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.246337ms
Feb 17 17:28:15.952: INFO: Pod "pod-76dbd0f5-b4e3-4a49-859b-6abd9cf3bf8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025966842s
Feb 17 17:28:17.966: INFO: Pod "pod-76dbd0f5-b4e3-4a49-859b-6abd9cf3bf8e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039500972s
Feb 17 17:28:19.979: INFO: Pod "pod-76dbd0f5-b4e3-4a49-859b-6abd9cf3bf8e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052627121s
Feb 17 17:28:21.992: INFO: Pod "pod-76dbd0f5-b4e3-4a49-859b-6abd9cf3bf8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.065815739s
STEP: Saw pod success
Feb 17 17:28:21.992: INFO: Pod "pod-76dbd0f5-b4e3-4a49-859b-6abd9cf3bf8e" satisfied condition "success or failure"
Feb 17 17:28:22.004: INFO: Trying to get logs from node 10.45.66.178 pod pod-76dbd0f5-b4e3-4a49-859b-6abd9cf3bf8e container test-container: <nil>
STEP: delete the pod
Feb 17 17:28:22.104: INFO: Waiting for pod pod-76dbd0f5-b4e3-4a49-859b-6abd9cf3bf8e to disappear
Feb 17 17:28:22.117: INFO: Pod pod-76dbd0f5-b4e3-4a49-859b-6abd9cf3bf8e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:28:22.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7863" for this suite.
Feb 17 17:28:30.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:28:32.257: INFO: namespace emptydir-7863 deletion completed in 10.111437003s

• [SLOW TEST:19.529 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:28:32.257: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-dca12bcb-91e0-4b0d-af8e-307e4bfca082
STEP: Creating a pod to test consume configMaps
Feb 17 17:28:32.445: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-72b0d46a-4a8f-4e02-9fd1-4a557e547118" in namespace "projected-3653" to be "success or failure"
Feb 17 17:28:32.461: INFO: Pod "pod-projected-configmaps-72b0d46a-4a8f-4e02-9fd1-4a557e547118": Phase="Pending", Reason="", readiness=false. Elapsed: 15.714242ms
Feb 17 17:28:34.474: INFO: Pod "pod-projected-configmaps-72b0d46a-4a8f-4e02-9fd1-4a557e547118": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028875583s
Feb 17 17:28:36.487: INFO: Pod "pod-projected-configmaps-72b0d46a-4a8f-4e02-9fd1-4a557e547118": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041939702s
Feb 17 17:28:38.500: INFO: Pod "pod-projected-configmaps-72b0d46a-4a8f-4e02-9fd1-4a557e547118": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054895023s
Feb 17 17:28:40.515: INFO: Pod "pod-projected-configmaps-72b0d46a-4a8f-4e02-9fd1-4a557e547118": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.069738184s
STEP: Saw pod success
Feb 17 17:28:40.515: INFO: Pod "pod-projected-configmaps-72b0d46a-4a8f-4e02-9fd1-4a557e547118" satisfied condition "success or failure"
Feb 17 17:28:40.527: INFO: Trying to get logs from node 10.45.66.189 pod pod-projected-configmaps-72b0d46a-4a8f-4e02-9fd1-4a557e547118 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 17:28:40.637: INFO: Waiting for pod pod-projected-configmaps-72b0d46a-4a8f-4e02-9fd1-4a557e547118 to disappear
Feb 17 17:28:40.649: INFO: Pod pod-projected-configmaps-72b0d46a-4a8f-4e02-9fd1-4a557e547118 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:28:40.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3653" for this suite.
Feb 17 17:28:48.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:28:50.833: INFO: namespace projected-3653 deletion completed in 10.157371148s

• [SLOW TEST:18.575 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:28:50.833: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Feb 17 17:28:51.031: INFO: Waiting up to 5m0s for pod "downward-api-ea25644b-9606-4cb3-890f-26f9acdd944a" in namespace "downward-api-1283" to be "success or failure"
Feb 17 17:28:51.045: INFO: Pod "downward-api-ea25644b-9606-4cb3-890f-26f9acdd944a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.313596ms
Feb 17 17:28:53.060: INFO: Pod "downward-api-ea25644b-9606-4cb3-890f-26f9acdd944a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029341581s
Feb 17 17:28:55.077: INFO: Pod "downward-api-ea25644b-9606-4cb3-890f-26f9acdd944a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046008268s
Feb 17 17:28:57.090: INFO: Pod "downward-api-ea25644b-9606-4cb3-890f-26f9acdd944a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05946347s
Feb 17 17:28:59.105: INFO: Pod "downward-api-ea25644b-9606-4cb3-890f-26f9acdd944a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.073896494s
STEP: Saw pod success
Feb 17 17:28:59.105: INFO: Pod "downward-api-ea25644b-9606-4cb3-890f-26f9acdd944a" satisfied condition "success or failure"
Feb 17 17:28:59.117: INFO: Trying to get logs from node 10.45.66.177 pod downward-api-ea25644b-9606-4cb3-890f-26f9acdd944a container dapi-container: <nil>
STEP: delete the pod
Feb 17 17:28:59.222: INFO: Waiting for pod downward-api-ea25644b-9606-4cb3-890f-26f9acdd944a to disappear
Feb 17 17:28:59.235: INFO: Pod downward-api-ea25644b-9606-4cb3-890f-26f9acdd944a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:28:59.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1283" for this suite.
Feb 17 17:29:07.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:29:09.430: INFO: namespace downward-api-1283 deletion completed in 10.155639504s

• [SLOW TEST:18.597 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:29:09.432: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Feb 17 17:29:09.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 create -f - --namespace=kubectl-7455'
Feb 17 17:29:10.171: INFO: stderr: ""
Feb 17 17:29:10.171: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 17:29:10.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7455'
Feb 17 17:29:10.348: INFO: stderr: ""
Feb 17 17:29:10.348: INFO: stdout: "update-demo-nautilus-s287q update-demo-nautilus-zdp4c "
Feb 17 17:29:10.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-s287q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7455'
Feb 17 17:29:10.539: INFO: stderr: ""
Feb 17 17:29:10.539: INFO: stdout: ""
Feb 17 17:29:10.539: INFO: update-demo-nautilus-s287q is created but not running
Feb 17 17:29:15.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7455'
Feb 17 17:29:15.683: INFO: stderr: ""
Feb 17 17:29:15.683: INFO: stdout: "update-demo-nautilus-s287q update-demo-nautilus-zdp4c "
Feb 17 17:29:15.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-s287q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7455'
Feb 17 17:29:15.835: INFO: stderr: ""
Feb 17 17:29:15.835: INFO: stdout: ""
Feb 17 17:29:15.835: INFO: update-demo-nautilus-s287q is created but not running
Feb 17 17:29:20.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7455'
Feb 17 17:29:21.006: INFO: stderr: ""
Feb 17 17:29:21.006: INFO: stdout: "update-demo-nautilus-s287q update-demo-nautilus-zdp4c "
Feb 17 17:29:21.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-s287q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7455'
Feb 17 17:29:21.133: INFO: stderr: ""
Feb 17 17:29:21.133: INFO: stdout: "true"
Feb 17 17:29:21.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-s287q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7455'
Feb 17 17:29:21.273: INFO: stderr: ""
Feb 17 17:29:21.273: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 17:29:21.273: INFO: validating pod update-demo-nautilus-s287q
Feb 17 17:29:21.318: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 17:29:21.318: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 17:29:21.318: INFO: update-demo-nautilus-s287q is verified up and running
Feb 17 17:29:21.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-zdp4c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7455'
Feb 17 17:29:21.490: INFO: stderr: ""
Feb 17 17:29:21.490: INFO: stdout: "true"
Feb 17 17:29:21.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-zdp4c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7455'
Feb 17 17:29:21.639: INFO: stderr: ""
Feb 17 17:29:21.639: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 17:29:21.639: INFO: validating pod update-demo-nautilus-zdp4c
Feb 17 17:29:21.673: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 17:29:21.673: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 17:29:21.673: INFO: update-demo-nautilus-zdp4c is verified up and running
STEP: scaling down the replication controller
Feb 17 17:29:21.678: INFO: scanned /root for discovery docs: <nil>
Feb 17 17:29:21.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7455'
Feb 17 17:29:22.924: INFO: stderr: ""
Feb 17 17:29:22.924: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 17:29:22.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7455'
Feb 17 17:29:23.083: INFO: stderr: ""
Feb 17 17:29:23.083: INFO: stdout: "update-demo-nautilus-s287q update-demo-nautilus-zdp4c "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 17 17:29:28.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7455'
Feb 17 17:29:28.217: INFO: stderr: ""
Feb 17 17:29:28.217: INFO: stdout: "update-demo-nautilus-s287q update-demo-nautilus-zdp4c "
STEP: Replicas for name=update-demo: expected=1 actual=2
Feb 17 17:29:33.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7455'
Feb 17 17:29:33.352: INFO: stderr: ""
Feb 17 17:29:33.352: INFO: stdout: "update-demo-nautilus-s287q "
Feb 17 17:29:33.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-s287q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7455'
Feb 17 17:29:33.489: INFO: stderr: ""
Feb 17 17:29:33.489: INFO: stdout: "true"
Feb 17 17:29:33.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-s287q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7455'
Feb 17 17:29:33.637: INFO: stderr: ""
Feb 17 17:29:33.637: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 17:29:33.637: INFO: validating pod update-demo-nautilus-s287q
Feb 17 17:29:33.662: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 17:29:33.662: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 17:29:33.662: INFO: update-demo-nautilus-s287q is verified up and running
STEP: scaling up the replication controller
Feb 17 17:29:33.666: INFO: scanned /root for discovery docs: <nil>
Feb 17 17:29:33.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7455'
Feb 17 17:29:34.874: INFO: stderr: ""
Feb 17 17:29:34.874: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 17:29:34.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7455'
Feb 17 17:29:35.017: INFO: stderr: ""
Feb 17 17:29:35.017: INFO: stdout: "update-demo-nautilus-ns765 update-demo-nautilus-s287q "
Feb 17 17:29:35.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-ns765 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7455'
Feb 17 17:29:35.186: INFO: stderr: ""
Feb 17 17:29:35.186: INFO: stdout: ""
Feb 17 17:29:35.186: INFO: update-demo-nautilus-ns765 is created but not running
Feb 17 17:29:40.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7455'
Feb 17 17:29:40.354: INFO: stderr: ""
Feb 17 17:29:40.354: INFO: stdout: "update-demo-nautilus-ns765 update-demo-nautilus-s287q "
Feb 17 17:29:40.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-ns765 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7455'
Feb 17 17:29:40.533: INFO: stderr: ""
Feb 17 17:29:40.533: INFO: stdout: ""
Feb 17 17:29:40.533: INFO: update-demo-nautilus-ns765 is created but not running
Feb 17 17:29:45.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7455'
Feb 17 17:29:45.680: INFO: stderr: ""
Feb 17 17:29:45.680: INFO: stdout: "update-demo-nautilus-ns765 update-demo-nautilus-s287q "
Feb 17 17:29:45.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-ns765 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7455'
Feb 17 17:29:45.832: INFO: stderr: ""
Feb 17 17:29:45.832: INFO: stdout: "true"
Feb 17 17:29:45.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-ns765 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7455'
Feb 17 17:29:45.956: INFO: stderr: ""
Feb 17 17:29:45.956: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 17:29:45.956: INFO: validating pod update-demo-nautilus-ns765
Feb 17 17:29:45.995: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 17:29:45.995: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 17:29:45.995: INFO: update-demo-nautilus-ns765 is verified up and running
Feb 17 17:29:45.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-s287q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7455'
Feb 17 17:29:46.124: INFO: stderr: ""
Feb 17 17:29:46.124: INFO: stdout: "true"
Feb 17 17:29:46.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-s287q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7455'
Feb 17 17:29:46.258: INFO: stderr: ""
Feb 17 17:29:46.258: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 17:29:46.258: INFO: validating pod update-demo-nautilus-s287q
Feb 17 17:29:46.279: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 17:29:46.279: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 17:29:46.279: INFO: update-demo-nautilus-s287q is verified up and running
STEP: using delete to clean up resources
Feb 17 17:29:46.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete --grace-period=0 --force -f - --namespace=kubectl-7455'
Feb 17 17:29:46.440: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 17:29:46.440: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 17 17:29:46.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7455'
Feb 17 17:29:46.590: INFO: stderr: "No resources found in kubectl-7455 namespace.\n"
Feb 17 17:29:46.590: INFO: stdout: ""
Feb 17 17:29:46.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -l name=update-demo --namespace=kubectl-7455 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 17 17:29:46.760: INFO: stderr: ""
Feb 17 17:29:46.760: INFO: stdout: "update-demo-nautilus-ns765\nupdate-demo-nautilus-s287q\n"
Feb 17 17:29:47.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7455'
Feb 17 17:29:47.470: INFO: stderr: "No resources found in kubectl-7455 namespace.\n"
Feb 17 17:29:47.470: INFO: stdout: ""
Feb 17 17:29:47.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -l name=update-demo --namespace=kubectl-7455 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 17 17:29:47.689: INFO: stderr: ""
Feb 17 17:29:47.689: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:29:47.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7455" for this suite.
Feb 17 17:30:01.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:30:03.920: INFO: namespace kubectl-7455 deletion completed in 16.176959686s

• [SLOW TEST:54.488 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:30:03.920: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-54dc8d7f-fdbe-4651-993d-1890e28a780d
STEP: Creating a pod to test consume secrets
Feb 17 17:30:04.121: INFO: Waiting up to 5m0s for pod "pod-secrets-c034c2ad-0466-49c0-bb72-6fac290f1adf" in namespace "secrets-3049" to be "success or failure"
Feb 17 17:30:04.133: INFO: Pod "pod-secrets-c034c2ad-0466-49c0-bb72-6fac290f1adf": Phase="Pending", Reason="", readiness=false. Elapsed: 12.105555ms
Feb 17 17:30:06.147: INFO: Pod "pod-secrets-c034c2ad-0466-49c0-bb72-6fac290f1adf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026175682s
Feb 17 17:30:08.161: INFO: Pod "pod-secrets-c034c2ad-0466-49c0-bb72-6fac290f1adf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040117873s
Feb 17 17:30:10.174: INFO: Pod "pod-secrets-c034c2ad-0466-49c0-bb72-6fac290f1adf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053042953s
Feb 17 17:30:12.189: INFO: Pod "pod-secrets-c034c2ad-0466-49c0-bb72-6fac290f1adf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068027193s
STEP: Saw pod success
Feb 17 17:30:12.189: INFO: Pod "pod-secrets-c034c2ad-0466-49c0-bb72-6fac290f1adf" satisfied condition "success or failure"
Feb 17 17:30:12.201: INFO: Trying to get logs from node 10.45.66.189 pod pod-secrets-c034c2ad-0466-49c0-bb72-6fac290f1adf container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 17:30:12.299: INFO: Waiting for pod pod-secrets-c034c2ad-0466-49c0-bb72-6fac290f1adf to disappear
Feb 17 17:30:12.312: INFO: Pod pod-secrets-c034c2ad-0466-49c0-bb72-6fac290f1adf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:30:12.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3049" for this suite.
Feb 17 17:30:20.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:30:22.558: INFO: namespace secrets-3049 deletion completed in 10.202365153s

• [SLOW TEST:18.638 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:30:22.559: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Feb 17 17:30:22.750: INFO: Waiting up to 5m0s for pod "pod-34ca88ce-8d45-42a0-9a31-66d462e6e4d3" in namespace "emptydir-9507" to be "success or failure"
Feb 17 17:30:22.765: INFO: Pod "pod-34ca88ce-8d45-42a0-9a31-66d462e6e4d3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.557342ms
Feb 17 17:30:24.779: INFO: Pod "pod-34ca88ce-8d45-42a0-9a31-66d462e6e4d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029272717s
Feb 17 17:30:26.793: INFO: Pod "pod-34ca88ce-8d45-42a0-9a31-66d462e6e4d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042835903s
Feb 17 17:30:28.806: INFO: Pod "pod-34ca88ce-8d45-42a0-9a31-66d462e6e4d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055831964s
Feb 17 17:30:30.819: INFO: Pod "pod-34ca88ce-8d45-42a0-9a31-66d462e6e4d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068821937s
STEP: Saw pod success
Feb 17 17:30:30.819: INFO: Pod "pod-34ca88ce-8d45-42a0-9a31-66d462e6e4d3" satisfied condition "success or failure"
Feb 17 17:30:30.832: INFO: Trying to get logs from node 10.45.66.178 pod pod-34ca88ce-8d45-42a0-9a31-66d462e6e4d3 container test-container: <nil>
STEP: delete the pod
Feb 17 17:30:30.936: INFO: Waiting for pod pod-34ca88ce-8d45-42a0-9a31-66d462e6e4d3 to disappear
Feb 17 17:30:30.949: INFO: Pod pod-34ca88ce-8d45-42a0-9a31-66d462e6e4d3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:30:30.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9507" for this suite.
Feb 17 17:30:39.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:30:41.145: INFO: namespace emptydir-9507 deletion completed in 10.162237108s

• [SLOW TEST:18.586 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:30:41.145: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Feb 17 17:30:42.341: INFO: Waiting up to 5m0s for pod "pod-b33bad13-426e-4169-aee4-14184996d03a" in namespace "emptydir-6437" to be "success or failure"
Feb 17 17:30:42.357: INFO: Pod "pod-b33bad13-426e-4169-aee4-14184996d03a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.597494ms
Feb 17 17:30:44.370: INFO: Pod "pod-b33bad13-426e-4169-aee4-14184996d03a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028496867s
Feb 17 17:30:46.383: INFO: Pod "pod-b33bad13-426e-4169-aee4-14184996d03a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041827881s
Feb 17 17:30:48.396: INFO: Pod "pod-b33bad13-426e-4169-aee4-14184996d03a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054965816s
Feb 17 17:30:50.410: INFO: Pod "pod-b33bad13-426e-4169-aee4-14184996d03a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068652049s
STEP: Saw pod success
Feb 17 17:30:50.410: INFO: Pod "pod-b33bad13-426e-4169-aee4-14184996d03a" satisfied condition "success or failure"
Feb 17 17:30:50.422: INFO: Trying to get logs from node 10.45.66.177 pod pod-b33bad13-426e-4169-aee4-14184996d03a container test-container: <nil>
STEP: delete the pod
Feb 17 17:30:50.519: INFO: Waiting for pod pod-b33bad13-426e-4169-aee4-14184996d03a to disappear
Feb 17 17:30:50.531: INFO: Pod pod-b33bad13-426e-4169-aee4-14184996d03a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:30:50.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6437" for this suite.
Feb 17 17:30:58.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:31:00.707: INFO: namespace emptydir-6437 deletion completed in 10.148068417s

• [SLOW TEST:19.562 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:31:00.708: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-de8c5504-c79a-4c44-9a46-c1f7130ee532
STEP: Creating a pod to test consume configMaps
Feb 17 17:31:00.907: INFO: Waiting up to 5m0s for pod "pod-configmaps-f08da646-6361-43dc-9eaa-2a66ba8f2eff" in namespace "configmap-967" to be "success or failure"
Feb 17 17:31:00.921: INFO: Pod "pod-configmaps-f08da646-6361-43dc-9eaa-2a66ba8f2eff": Phase="Pending", Reason="", readiness=false. Elapsed: 13.832384ms
Feb 17 17:31:02.934: INFO: Pod "pod-configmaps-f08da646-6361-43dc-9eaa-2a66ba8f2eff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026708821s
Feb 17 17:31:04.947: INFO: Pod "pod-configmaps-f08da646-6361-43dc-9eaa-2a66ba8f2eff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040017571s
Feb 17 17:31:06.961: INFO: Pod "pod-configmaps-f08da646-6361-43dc-9eaa-2a66ba8f2eff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054093301s
Feb 17 17:31:08.974: INFO: Pod "pod-configmaps-f08da646-6361-43dc-9eaa-2a66ba8f2eff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.067223209s
STEP: Saw pod success
Feb 17 17:31:08.975: INFO: Pod "pod-configmaps-f08da646-6361-43dc-9eaa-2a66ba8f2eff" satisfied condition "success or failure"
Feb 17 17:31:08.989: INFO: Trying to get logs from node 10.45.66.178 pod pod-configmaps-f08da646-6361-43dc-9eaa-2a66ba8f2eff container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 17:31:09.059: INFO: Waiting for pod pod-configmaps-f08da646-6361-43dc-9eaa-2a66ba8f2eff to disappear
Feb 17 17:31:09.072: INFO: Pod pod-configmaps-f08da646-6361-43dc-9eaa-2a66ba8f2eff no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:31:09.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-967" for this suite.
Feb 17 17:31:17.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:31:19.214: INFO: namespace configmap-967 deletion completed in 10.117911145s

• [SLOW TEST:18.506 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:31:19.215: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Feb 17 17:31:19.513: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6865 /api/v1/namespaces/watch-6865/configmaps/e2e-watch-test-resource-version 0078a319-528f-4ab0-b4cd-86ea48375cfc 70395 0 2020-02-17 17:31:19 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 17 17:31:19.514: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6865 /api/v1/namespaces/watch-6865/configmaps/e2e-watch-test-resource-version 0078a319-528f-4ab0-b4cd-86ea48375cfc 70397 0 2020-02-17 17:31:19 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:31:19.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6865" for this suite.
Feb 17 17:31:27.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:31:29.671: INFO: namespace watch-6865 deletion completed in 10.134229369s

• [SLOW TEST:10.456 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:31:29.671: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Feb 17 17:32:09.966: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:32:09.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0217 17:32:09.965994      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-1234" for this suite.
Feb 17 17:32:20.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:32:22.112: INFO: namespace gc-1234 deletion completed in 12.121991545s

• [SLOW TEST:52.441 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:32:22.114: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-21d54b04-de8e-4cb3-b546-3909077287f8
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:32:22.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6533" for this suite.
Feb 17 17:32:30.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:32:32.458: INFO: namespace secrets-6533 deletion completed in 10.147907558s

• [SLOW TEST:10.344 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:32:32.459: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 17 17:32:32.756: INFO: Number of nodes with available pods: 0
Feb 17 17:32:32.756: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:33.794: INFO: Number of nodes with available pods: 0
Feb 17 17:32:33.794: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:34.793: INFO: Number of nodes with available pods: 0
Feb 17 17:32:34.793: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:35.793: INFO: Number of nodes with available pods: 0
Feb 17 17:32:35.793: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:36.798: INFO: Number of nodes with available pods: 0
Feb 17 17:32:36.798: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:37.795: INFO: Number of nodes with available pods: 0
Feb 17 17:32:37.795: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:38.793: INFO: Number of nodes with available pods: 0
Feb 17 17:32:38.793: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:39.792: INFO: Number of nodes with available pods: 0
Feb 17 17:32:39.792: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:40.790: INFO: Number of nodes with available pods: 3
Feb 17 17:32:40.791: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Feb 17 17:32:40.871: INFO: Number of nodes with available pods: 2
Feb 17 17:32:40.871: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:41.905: INFO: Number of nodes with available pods: 2
Feb 17 17:32:41.906: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:42.907: INFO: Number of nodes with available pods: 2
Feb 17 17:32:42.908: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:43.903: INFO: Number of nodes with available pods: 2
Feb 17 17:32:43.903: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:44.904: INFO: Number of nodes with available pods: 2
Feb 17 17:32:44.904: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:45.906: INFO: Number of nodes with available pods: 2
Feb 17 17:32:45.906: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:46.912: INFO: Number of nodes with available pods: 2
Feb 17 17:32:46.912: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:47.906: INFO: Number of nodes with available pods: 2
Feb 17 17:32:47.906: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:48.905: INFO: Number of nodes with available pods: 2
Feb 17 17:32:48.906: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:49.905: INFO: Number of nodes with available pods: 2
Feb 17 17:32:49.905: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:50.907: INFO: Number of nodes with available pods: 2
Feb 17 17:32:50.908: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:51.904: INFO: Number of nodes with available pods: 2
Feb 17 17:32:51.904: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 17:32:52.904: INFO: Number of nodes with available pods: 3
Feb 17 17:32:52.904: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2315, will wait for the garbage collector to delete the pods
Feb 17 17:32:53.017: INFO: Deleting DaemonSet.extensions daemon-set took: 27.976843ms
Feb 17 17:32:53.517: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.344072ms
Feb 17 17:32:58.730: INFO: Number of nodes with available pods: 0
Feb 17 17:32:58.730: INFO: Number of running nodes: 0, number of available pods: 0
Feb 17 17:32:58.742: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2315/daemonsets","resourceVersion":"71343"},"items":null}

Feb 17 17:32:58.756: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2315/pods","resourceVersion":"71343"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:32:58.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2315" for this suite.
Feb 17 17:33:06.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:33:08.977: INFO: namespace daemonsets-2315 deletion completed in 10.144176443s

• [SLOW TEST:36.517 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:33:08.977: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-d434982d-1691-4ce4-8cea-35ba9951c358
STEP: Creating a pod to test consume configMaps
Feb 17 17:33:09.239: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4e8e48e6-9cdb-4b55-bc11-172011d7fa9d" in namespace "projected-2757" to be "success or failure"
Feb 17 17:33:09.258: INFO: Pod "pod-projected-configmaps-4e8e48e6-9cdb-4b55-bc11-172011d7fa9d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.642946ms
Feb 17 17:33:11.271: INFO: Pod "pod-projected-configmaps-4e8e48e6-9cdb-4b55-bc11-172011d7fa9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031669622s
Feb 17 17:33:13.283: INFO: Pod "pod-projected-configmaps-4e8e48e6-9cdb-4b55-bc11-172011d7fa9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044167536s
Feb 17 17:33:15.297: INFO: Pod "pod-projected-configmaps-4e8e48e6-9cdb-4b55-bc11-172011d7fa9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058487093s
Feb 17 17:33:17.311: INFO: Pod "pod-projected-configmaps-4e8e48e6-9cdb-4b55-bc11-172011d7fa9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.071821288s
STEP: Saw pod success
Feb 17 17:33:17.311: INFO: Pod "pod-projected-configmaps-4e8e48e6-9cdb-4b55-bc11-172011d7fa9d" satisfied condition "success or failure"
Feb 17 17:33:17.323: INFO: Trying to get logs from node 10.45.66.178 pod pod-projected-configmaps-4e8e48e6-9cdb-4b55-bc11-172011d7fa9d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 17:33:17.434: INFO: Waiting for pod pod-projected-configmaps-4e8e48e6-9cdb-4b55-bc11-172011d7fa9d to disappear
Feb 17 17:33:17.447: INFO: Pod pod-projected-configmaps-4e8e48e6-9cdb-4b55-bc11-172011d7fa9d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:33:17.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2757" for this suite.
Feb 17 17:33:25.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:33:27.598: INFO: namespace projected-2757 deletion completed in 10.126601921s

• [SLOW TEST:18.622 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:33:27.602: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5291
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-5291
I0217 17:33:27.845299      26 runners.go:184] Created replication controller with name: externalname-service, namespace: services-5291, replica count: 2
I0217 17:33:30.895950      26 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 17:33:33.897045      26 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 17:33:36.897: INFO: Creating new exec pod
I0217 17:33:36.897317      26 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 17:33:45.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=services-5291 execpodqzclb -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 17 17:33:46.372: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 17 17:33:46.372: INFO: stdout: ""
Feb 17 17:33:46.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=services-5291 execpodqzclb -- /bin/sh -x -c nc -zv -t -w 2 172.21.26.194 80'
Feb 17 17:33:46.723: INFO: stderr: "+ nc -zv -t -w 2 172.21.26.194 80\nConnection to 172.21.26.194 80 port [tcp/http] succeeded!\n"
Feb 17 17:33:46.723: INFO: stdout: ""
Feb 17 17:33:46.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=services-5291 execpodqzclb -- /bin/sh -x -c nc -zv -t -w 2 10.45.66.177 31191'
Feb 17 17:33:47.118: INFO: stderr: "+ nc -zv -t -w 2 10.45.66.177 31191\nConnection to 10.45.66.177 31191 port [tcp/31191] succeeded!\n"
Feb 17 17:33:47.118: INFO: stdout: ""
Feb 17 17:33:47.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=services-5291 execpodqzclb -- /bin/sh -x -c nc -zv -t -w 2 10.45.66.178 31191'
Feb 17 17:33:47.471: INFO: stderr: "+ nc -zv -t -w 2 10.45.66.178 31191\nConnection to 10.45.66.178 31191 port [tcp/31191] succeeded!\n"
Feb 17 17:33:47.471: INFO: stdout: ""
Feb 17 17:33:47.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=services-5291 execpodqzclb -- /bin/sh -x -c nc -zv -t -w 2 158.175.157.140 31191'
Feb 17 17:33:47.867: INFO: stderr: "+ nc -zv -t -w 2 158.175.157.140 31191\nConnection to 158.175.157.140 31191 port [tcp/31191] succeeded!\n"
Feb 17 17:33:47.867: INFO: stdout: ""
Feb 17 17:33:47.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=services-5291 execpodqzclb -- /bin/sh -x -c nc -zv -t -w 2 158.175.157.133 31191'
Feb 17 17:33:48.228: INFO: stderr: "+ nc -zv -t -w 2 158.175.157.133 31191\nConnection to 158.175.157.133 31191 port [tcp/31191] succeeded!\n"
Feb 17 17:33:48.228: INFO: stdout: ""
Feb 17 17:33:48.228: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:33:48.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5291" for this suite.
Feb 17 17:33:56.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:33:58.465: INFO: namespace services-5291 deletion completed in 10.111808245s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:30.864 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:33:58.467: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:33:58.667: INFO: Waiting up to 5m0s for pod "busybox-user-65534-7877dcbe-c2a1-49ca-81c3-e1b438b11a2f" in namespace "security-context-test-2543" to be "success or failure"
Feb 17 17:33:58.681: INFO: Pod "busybox-user-65534-7877dcbe-c2a1-49ca-81c3-e1b438b11a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.726495ms
Feb 17 17:34:00.697: INFO: Pod "busybox-user-65534-7877dcbe-c2a1-49ca-81c3-e1b438b11a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029038989s
Feb 17 17:34:02.712: INFO: Pod "busybox-user-65534-7877dcbe-c2a1-49ca-81c3-e1b438b11a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044580868s
Feb 17 17:34:04.725: INFO: Pod "busybox-user-65534-7877dcbe-c2a1-49ca-81c3-e1b438b11a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.057695616s
Feb 17 17:34:06.738: INFO: Pod "busybox-user-65534-7877dcbe-c2a1-49ca-81c3-e1b438b11a2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.07073145s
Feb 17 17:34:06.738: INFO: Pod "busybox-user-65534-7877dcbe-c2a1-49ca-81c3-e1b438b11a2f" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:34:06.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2543" for this suite.
Feb 17 17:34:14.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:34:16.876: INFO: namespace security-context-test-2543 deletion completed in 10.113374056s

• [SLOW TEST:18.409 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:34:16.877: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:34:17.052: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-accb1003-5c67-4f89-9045-1909ebe1ac6d
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-accb1003-5c67-4f89-9045-1909ebe1ac6d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:34:27.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7164" for this suite.
Feb 17 17:34:41.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:34:43.400: INFO: namespace configmap-7164 deletion completed in 16.120579536s

• [SLOW TEST:26.524 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:34:43.401: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Feb 17 17:34:52.116: INFO: Successfully updated pod "adopt-release-576cf"
STEP: Checking that the Job readopts the Pod
Feb 17 17:34:52.116: INFO: Waiting up to 15m0s for pod "adopt-release-576cf" in namespace "job-7692" to be "adopted"
Feb 17 17:34:52.128: INFO: Pod "adopt-release-576cf": Phase="Running", Reason="", readiness=true. Elapsed: 11.866809ms
Feb 17 17:34:54.141: INFO: Pod "adopt-release-576cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.025441563s
Feb 17 17:34:54.142: INFO: Pod "adopt-release-576cf" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Feb 17 17:34:54.695: INFO: Successfully updated pod "adopt-release-576cf"
STEP: Checking that the Job releases the Pod
Feb 17 17:34:54.695: INFO: Waiting up to 15m0s for pod "adopt-release-576cf" in namespace "job-7692" to be "released"
Feb 17 17:34:54.712: INFO: Pod "adopt-release-576cf": Phase="Running", Reason="", readiness=true. Elapsed: 17.025466ms
Feb 17 17:34:56.726: INFO: Pod "adopt-release-576cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.030861166s
Feb 17 17:34:56.726: INFO: Pod "adopt-release-576cf" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:34:56.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7692" for this suite.
Feb 17 17:35:46.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:35:48.892: INFO: namespace job-7692 deletion completed in 52.142786292s

• [SLOW TEST:65.492 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:35:48.893: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Feb 17 17:35:49.020: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Feb 17 17:35:49.682: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Feb 17 17:35:51.829: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:35:53.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:35:55.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:35:57.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:35:59.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:36:01.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:36:03.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:36:05.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:36:07.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:36:09.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:36:11.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717557749, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:36:14.050: INFO: Waited 187.348598ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:36:16.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1773" for this suite.
Feb 17 17:36:24.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:36:26.508: INFO: namespace aggregator-1773 deletion completed in 10.207736358s

• [SLOW TEST:37.615 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:36:26.511: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Feb 17 17:36:26.650: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:36:36.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7746" for this suite.
Feb 17 17:36:44.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:36:46.531: INFO: namespace init-container-7746 deletion completed in 10.153837683s

• [SLOW TEST:20.020 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:36:46.531: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Feb 17 17:36:47.695: INFO: Pod name wrapped-volume-race-e528a385-7d1d-4a5f-bb5f-93646f7e33f2: Found 0 pods out of 5
Feb 17 17:36:52.719: INFO: Pod name wrapped-volume-race-e528a385-7d1d-4a5f-bb5f-93646f7e33f2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e528a385-7d1d-4a5f-bb5f-93646f7e33f2 in namespace emptydir-wrapper-4351, will wait for the garbage collector to delete the pods
Feb 17 17:36:58.897: INFO: Deleting ReplicationController wrapped-volume-race-e528a385-7d1d-4a5f-bb5f-93646f7e33f2 took: 34.591987ms
Feb 17 17:36:59.498: INFO: Terminating ReplicationController wrapped-volume-race-e528a385-7d1d-4a5f-bb5f-93646f7e33f2 pods took: 600.422016ms
STEP: Creating RC which spawns configmap-volume pods
Feb 17 17:37:32.565: INFO: Pod name wrapped-volume-race-d35bd4ce-458e-4ad9-a782-5c337677b5a5: Found 0 pods out of 5
Feb 17 17:37:37.590: INFO: Pod name wrapped-volume-race-d35bd4ce-458e-4ad9-a782-5c337677b5a5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d35bd4ce-458e-4ad9-a782-5c337677b5a5 in namespace emptydir-wrapper-4351, will wait for the garbage collector to delete the pods
Feb 17 17:37:43.771: INFO: Deleting ReplicationController wrapped-volume-race-d35bd4ce-458e-4ad9-a782-5c337677b5a5 took: 33.122468ms
Feb 17 17:37:44.271: INFO: Terminating ReplicationController wrapped-volume-race-d35bd4ce-458e-4ad9-a782-5c337677b5a5 pods took: 500.235963ms
STEP: Creating RC which spawns configmap-volume pods
Feb 17 17:38:24.139: INFO: Pod name wrapped-volume-race-67f7b1a7-4a1d-4f5a-a45f-cecad7b75980: Found 0 pods out of 5
Feb 17 17:38:29.164: INFO: Pod name wrapped-volume-race-67f7b1a7-4a1d-4f5a-a45f-cecad7b75980: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-67f7b1a7-4a1d-4f5a-a45f-cecad7b75980 in namespace emptydir-wrapper-4351, will wait for the garbage collector to delete the pods
Feb 17 17:38:35.362: INFO: Deleting ReplicationController wrapped-volume-race-67f7b1a7-4a1d-4f5a-a45f-cecad7b75980 took: 33.36448ms
Feb 17 17:38:35.867: INFO: Terminating ReplicationController wrapped-volume-race-67f7b1a7-4a1d-4f5a-a45f-cecad7b75980 pods took: 504.476684ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:39:15.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4351" for this suite.
Feb 17 17:39:25.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:39:27.884: INFO: namespace emptydir-wrapper-4351 deletion completed in 12.138876934s

• [SLOW TEST:161.353 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:39:27.884: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:39:28.034: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 17 17:39:35.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-2180 create -f -'
Feb 17 17:39:36.335: INFO: stderr: ""
Feb 17 17:39:36.335: INFO: stdout: "e2e-test-crd-publish-openapi-5957-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 17 17:39:36.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-2180 delete e2e-test-crd-publish-openapi-5957-crds test-cr'
Feb 17 17:39:36.557: INFO: stderr: ""
Feb 17 17:39:36.557: INFO: stdout: "e2e-test-crd-publish-openapi-5957-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Feb 17 17:39:36.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-2180 apply -f -'
Feb 17 17:39:36.954: INFO: stderr: ""
Feb 17 17:39:36.954: INFO: stdout: "e2e-test-crd-publish-openapi-5957-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Feb 17 17:39:36.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-2180 delete e2e-test-crd-publish-openapi-5957-crds test-cr'
Feb 17 17:39:37.181: INFO: stderr: ""
Feb 17 17:39:37.181: INFO: stdout: "e2e-test-crd-publish-openapi-5957-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Feb 17 17:39:37.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 explain e2e-test-crd-publish-openapi-5957-crds'
Feb 17 17:39:37.684: INFO: stderr: ""
Feb 17 17:39:37.685: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5957-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:39:47.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2180" for this suite.
Feb 17 17:39:55.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:39:57.942: INFO: namespace crd-publish-openapi-2180 deletion completed in 10.138779178s

• [SLOW TEST:30.058 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:39:57.942: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:40:58.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-167" for this suite.
Feb 17 17:41:12.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:41:14.284: INFO: namespace container-probe-167 deletion completed in 16.13192106s

• [SLOW TEST:76.342 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:41:14.284: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-9208
STEP: creating replication controller nodeport-test in namespace services-9208
I0217 17:41:14.489426      26 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-9208, replica count: 2
I0217 17:41:17.540699      26 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 17:41:20.546137      26 runners.go:184] nodeport-test Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 17:41:23.546: INFO: Creating new exec pod
I0217 17:41:23.546421      26 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 17:41:32.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=services-9208 execpodgxx9k -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Feb 17 17:41:32.997: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Feb 17 17:41:32.997: INFO: stdout: ""
Feb 17 17:41:32.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=services-9208 execpodgxx9k -- /bin/sh -x -c nc -zv -t -w 2 172.21.80.60 80'
Feb 17 17:41:33.369: INFO: stderr: "+ nc -zv -t -w 2 172.21.80.60 80\nConnection to 172.21.80.60 80 port [tcp/http] succeeded!\n"
Feb 17 17:41:33.369: INFO: stdout: ""
Feb 17 17:41:33.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=services-9208 execpodgxx9k -- /bin/sh -x -c nc -zv -t -w 2 10.45.66.177 32656'
Feb 17 17:41:33.729: INFO: stderr: "+ nc -zv -t -w 2 10.45.66.177 32656\nConnection to 10.45.66.177 32656 port [tcp/32656] succeeded!\n"
Feb 17 17:41:33.729: INFO: stdout: ""
Feb 17 17:41:33.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=services-9208 execpodgxx9k -- /bin/sh -x -c nc -zv -t -w 2 10.45.66.178 32656'
Feb 17 17:41:34.077: INFO: stderr: "+ nc -zv -t -w 2 10.45.66.178 32656\nConnection to 10.45.66.178 32656 port [tcp/32656] succeeded!\n"
Feb 17 17:41:34.077: INFO: stdout: ""
Feb 17 17:41:34.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=services-9208 execpodgxx9k -- /bin/sh -x -c nc -zv -t -w 2 158.175.157.140 32656'
Feb 17 17:41:34.469: INFO: stderr: "+ nc -zv -t -w 2 158.175.157.140 32656\nConnection to 158.175.157.140 32656 port [tcp/32656] succeeded!\n"
Feb 17 17:41:34.469: INFO: stdout: ""
Feb 17 17:41:34.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=services-9208 execpodgxx9k -- /bin/sh -x -c nc -zv -t -w 2 158.175.157.133 32656'
Feb 17 17:41:34.833: INFO: stderr: "+ nc -zv -t -w 2 158.175.157.133 32656\nConnection to 158.175.157.133 32656 port [tcp/32656] succeeded!\n"
Feb 17 17:41:34.833: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:41:34.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9208" for this suite.
Feb 17 17:41:42.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:41:44.974: INFO: namespace services-9208 deletion completed in 10.120946653s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:30.690 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:41:44.974: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 17:41:45.145: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0c0dd1fb-cbfa-4b68-b96c-c32ae88fd098" in namespace "downward-api-7677" to be "success or failure"
Feb 17 17:41:45.161: INFO: Pod "downwardapi-volume-0c0dd1fb-cbfa-4b68-b96c-c32ae88fd098": Phase="Pending", Reason="", readiness=false. Elapsed: 15.257425ms
Feb 17 17:41:47.175: INFO: Pod "downwardapi-volume-0c0dd1fb-cbfa-4b68-b96c-c32ae88fd098": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029733242s
Feb 17 17:41:49.190: INFO: Pod "downwardapi-volume-0c0dd1fb-cbfa-4b68-b96c-c32ae88fd098": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045069266s
Feb 17 17:41:51.204: INFO: Pod "downwardapi-volume-0c0dd1fb-cbfa-4b68-b96c-c32ae88fd098": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058199669s
Feb 17 17:41:53.227: INFO: Pod "downwardapi-volume-0c0dd1fb-cbfa-4b68-b96c-c32ae88fd098": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.081194519s
STEP: Saw pod success
Feb 17 17:41:53.227: INFO: Pod "downwardapi-volume-0c0dd1fb-cbfa-4b68-b96c-c32ae88fd098" satisfied condition "success or failure"
Feb 17 17:41:53.243: INFO: Trying to get logs from node 10.45.66.177 pod downwardapi-volume-0c0dd1fb-cbfa-4b68-b96c-c32ae88fd098 container client-container: <nil>
STEP: delete the pod
Feb 17 17:41:53.339: INFO: Waiting for pod downwardapi-volume-0c0dd1fb-cbfa-4b68-b96c-c32ae88fd098 to disappear
Feb 17 17:41:53.351: INFO: Pod downwardapi-volume-0c0dd1fb-cbfa-4b68-b96c-c32ae88fd098 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:41:53.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7677" for this suite.
Feb 17 17:42:01.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:42:03.490: INFO: namespace downward-api-7677 deletion completed in 10.120536643s

• [SLOW TEST:18.516 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:42:03.490: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Feb 17 17:42:03.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 create -f - --namespace=kubectl-632'
Feb 17 17:42:04.178: INFO: stderr: ""
Feb 17 17:42:04.178: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 17:42:04.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-632'
Feb 17 17:42:04.350: INFO: stderr: ""
Feb 17 17:42:04.350: INFO: stdout: "update-demo-nautilus-dz68b update-demo-nautilus-jwtzz "
Feb 17 17:42:04.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-dz68b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-632'
Feb 17 17:42:04.482: INFO: stderr: ""
Feb 17 17:42:04.482: INFO: stdout: ""
Feb 17 17:42:04.482: INFO: update-demo-nautilus-dz68b is created but not running
Feb 17 17:42:09.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-632'
Feb 17 17:42:09.651: INFO: stderr: ""
Feb 17 17:42:09.651: INFO: stdout: "update-demo-nautilus-dz68b update-demo-nautilus-jwtzz "
Feb 17 17:42:09.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-dz68b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-632'
Feb 17 17:42:09.830: INFO: stderr: ""
Feb 17 17:42:09.830: INFO: stdout: ""
Feb 17 17:42:09.830: INFO: update-demo-nautilus-dz68b is created but not running
Feb 17 17:42:14.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-632'
Feb 17 17:42:14.973: INFO: stderr: ""
Feb 17 17:42:14.973: INFO: stdout: "update-demo-nautilus-dz68b update-demo-nautilus-jwtzz "
Feb 17 17:42:14.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-dz68b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-632'
Feb 17 17:42:15.155: INFO: stderr: ""
Feb 17 17:42:15.155: INFO: stdout: "true"
Feb 17 17:42:15.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-dz68b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-632'
Feb 17 17:42:15.300: INFO: stderr: ""
Feb 17 17:42:15.300: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 17:42:15.300: INFO: validating pod update-demo-nautilus-dz68b
Feb 17 17:42:15.330: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 17:42:15.330: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 17:42:15.330: INFO: update-demo-nautilus-dz68b is verified up and running
Feb 17 17:42:15.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-jwtzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-632'
Feb 17 17:42:15.468: INFO: stderr: ""
Feb 17 17:42:15.468: INFO: stdout: "true"
Feb 17 17:42:15.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-jwtzz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-632'
Feb 17 17:42:15.603: INFO: stderr: ""
Feb 17 17:42:15.603: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 17:42:15.603: INFO: validating pod update-demo-nautilus-jwtzz
Feb 17 17:42:15.640: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 17:42:15.640: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 17:42:15.640: INFO: update-demo-nautilus-jwtzz is verified up and running
STEP: using delete to clean up resources
Feb 17 17:42:15.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete --grace-period=0 --force -f - --namespace=kubectl-632'
Feb 17 17:42:15.828: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Feb 17 17:42:15.828: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Feb 17 17:42:15.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-632'
Feb 17 17:42:15.997: INFO: stderr: "No resources found in kubectl-632 namespace.\n"
Feb 17 17:42:15.997: INFO: stdout: ""
Feb 17 17:42:15.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -l name=update-demo --namespace=kubectl-632 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 17 17:42:16.144: INFO: stderr: ""
Feb 17 17:42:16.144: INFO: stdout: "update-demo-nautilus-dz68b\nupdate-demo-nautilus-jwtzz\n"
Feb 17 17:42:16.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-632'
Feb 17 17:42:16.803: INFO: stderr: "No resources found in kubectl-632 namespace.\n"
Feb 17 17:42:16.803: INFO: stdout: ""
Feb 17 17:42:16.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -l name=update-demo --namespace=kubectl-632 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Feb 17 17:42:16.958: INFO: stderr: ""
Feb 17 17:42:16.958: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:42:16.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-632" for this suite.
Feb 17 17:42:33.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:42:35.114: INFO: namespace kubectl-632 deletion completed in 18.136503245s

• [SLOW TEST:31.624 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:42:35.115: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 17:42:35.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-2749'
Feb 17 17:42:35.424: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 17 17:42:35.424: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Feb 17 17:42:35.445: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Feb 17 17:42:35.462: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Feb 17 17:42:35.481: INFO: scanned /root for discovery docs: <nil>
Feb 17 17:42:35.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-2749'
Feb 17 17:42:54.676: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 17 17:42:54.676: INFO: stdout: "Created e2e-test-httpd-rc-6d14d35eeb98b271689ff85e8b25d257\nScaling up e2e-test-httpd-rc-6d14d35eeb98b271689ff85e8b25d257 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-6d14d35eeb98b271689ff85e8b25d257 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-6d14d35eeb98b271689ff85e8b25d257 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Feb 17 17:42:54.676: INFO: stdout: "Created e2e-test-httpd-rc-6d14d35eeb98b271689ff85e8b25d257\nScaling up e2e-test-httpd-rc-6d14d35eeb98b271689ff85e8b25d257 from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-6d14d35eeb98b271689ff85e8b25d257 up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-6d14d35eeb98b271689ff85e8b25d257 to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Feb 17 17:42:54.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-2749'
Feb 17 17:42:54.819: INFO: stderr: ""
Feb 17 17:42:54.819: INFO: stdout: "e2e-test-httpd-rc-6d14d35eeb98b271689ff85e8b25d257-8hgth "
Feb 17 17:42:54.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods e2e-test-httpd-rc-6d14d35eeb98b271689ff85e8b25d257-8hgth -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2749'
Feb 17 17:42:54.955: INFO: stderr: ""
Feb 17 17:42:54.955: INFO: stdout: "true"
Feb 17 17:42:54.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods e2e-test-httpd-rc-6d14d35eeb98b271689ff85e8b25d257-8hgth -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2749'
Feb 17 17:42:55.097: INFO: stderr: ""
Feb 17 17:42:55.097: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Feb 17 17:42:55.097: INFO: e2e-test-httpd-rc-6d14d35eeb98b271689ff85e8b25d257-8hgth is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Feb 17 17:42:55.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete rc e2e-test-httpd-rc --namespace=kubectl-2749'
Feb 17 17:42:55.261: INFO: stderr: ""
Feb 17 17:42:55.261: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:42:55.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2749" for this suite.
Feb 17 17:43:03.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:43:05.403: INFO: namespace kubectl-2749 deletion completed in 10.121856153s

• [SLOW TEST:30.289 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:43:05.403: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 17:43:06.342: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 17:43:08.380: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558186, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558186, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558186, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558186, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:43:10.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558186, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558186, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558186, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558186, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:43:12.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558186, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558186, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558186, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558186, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 17:43:15.421: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Feb 17 17:43:23.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 attach --namespace=webhook-718 to-be-attached-pod -i -c=container1'
Feb 17 17:43:23.753: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:43:23.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-718" for this suite.
Feb 17 17:43:55.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:43:57.943: INFO: namespace webhook-718 deletion completed in 34.143438489s
STEP: Destroying namespace "webhook-718-markers" for this suite.
Feb 17 17:44:05.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:44:08.089: INFO: namespace webhook-718-markers deletion completed in 10.146447287s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:62.758 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:44:08.161: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:44:08.336: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-dc5c7410-8ea7-4576-ba36-45dc112d7c4b
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-dc5c7410-8ea7-4576-ba36-45dc112d7c4b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:44:18.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5305" for this suite.
Feb 17 17:44:32.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:44:34.723: INFO: namespace projected-5305 deletion completed in 16.11450918s

• [SLOW TEST:26.562 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:44:34.723: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:44:34.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-367" for this suite.
Feb 17 17:44:43.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:44:45.140: INFO: namespace kubelet-test-367 deletion completed in 10.136581996s

• [SLOW TEST:10.417 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:44:45.141: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:44:45.276: INFO: Creating deployment "webserver-deployment"
Feb 17 17:44:45.293: INFO: Waiting for observed generation 1
Feb 17 17:44:47.326: INFO: Waiting for all required pods to come up
Feb 17 17:44:47.344: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Feb 17 17:44:55.383: INFO: Waiting for deployment "webserver-deployment" to complete
Feb 17 17:44:55.406: INFO: Updating deployment "webserver-deployment" with a non-existent image
Feb 17 17:44:55.477: INFO: Updating deployment webserver-deployment
Feb 17 17:44:55.477: INFO: Waiting for observed generation 2
Feb 17 17:44:57.503: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Feb 17 17:44:57.516: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Feb 17 17:44:57.528: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 17 17:44:57.564: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Feb 17 17:44:57.565: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Feb 17 17:44:57.576: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Feb 17 17:44:57.600: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Feb 17 17:44:57.600: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Feb 17 17:44:57.626: INFO: Updating deployment webserver-deployment
Feb 17 17:44:57.626: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Feb 17 17:44:57.651: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Feb 17 17:44:57.664: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Feb 17 17:44:57.698: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8959 /apis/apps/v1/namespaces/deployment-8959/deployments/webserver-deployment 722831e3-31f8-4755-ad1e-f91b2d10126d 77011 3 2020-02-17 17:44:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005a020f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-02-17 17:44:55 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-02-17 17:44:57 +0000 UTC,LastTransitionTime:2020-02-17 17:44:57 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Feb 17 17:44:57.714: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-8959 /apis/apps/v1/namespaces/deployment-8959/replicasets/webserver-deployment-c7997dcc8 00d53ad2-ce30-44b8-b3bb-dfccbe935e56 77007 3 2020-02-17 17:44:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 722831e3-31f8-4755-ad1e-f91b2d10126d 0xc005a02627 0xc005a02628}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005a02698 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 17:44:57.714: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Feb 17 17:44:57.714: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-8959 /apis/apps/v1/namespaces/deployment-8959/replicasets/webserver-deployment-595b5b9587 1dc0a1ac-ea32-4f18-8bdf-3e82c0c07645 77005 3 2020-02-17 17:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 722831e3-31f8-4755-ad1e-f91b2d10126d 0xc005a02567 0xc005a02568}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005a025c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Feb 17 17:44:57.737: INFO: Pod "webserver-deployment-595b5b9587-2dvfv" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2dvfv webserver-deployment-595b5b9587- deployment-8959 /api/v1/namespaces/deployment-8959/pods/webserver-deployment-595b5b9587-2dvfv 34d5f2f3-3213-42d5-b979-a17da16078e9 76920 0 2020-02-17 17:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.50.169/32 cni.projectcalico.org/podIPs:172.30.50.169/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.50.169"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1dc0a1ac-ea32-4f18-8bdf-3e82c0c07645 0xc004202077 0xc004202078}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mgdt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mgdt7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mgdt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dr8dx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.189,PodIP:172.30.50.169,StartTime:2020-02-17 17:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 17:44:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://dd20f33277f5ca191cae43f4808148517690696b085f41c99397571d16d763f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.50.169,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 17:44:57.737: INFO: Pod "webserver-deployment-595b5b9587-4586j" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4586j webserver-deployment-595b5b9587- deployment-8959 /api/v1/namespaces/deployment-8959/pods/webserver-deployment-595b5b9587-4586j 25cb8b45-0a05-4c79-81e8-ba1774aab29d 76903 0 2020-02-17 17:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.178.233/32 cni.projectcalico.org/podIPs:172.30.178.233/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.178.233"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1dc0a1ac-ea32-4f18-8bdf-3e82c0c07645 0xc004202217 0xc004202218}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mgdt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mgdt7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mgdt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dr8dx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.177,PodIP:172.30.178.233,StartTime:2020-02-17 17:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 17:44:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://3839513ab30d260298a7ac894b1a493873766bb4910d31fbf84d2463141cea5d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.178.233,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 17:44:57.737: INFO: Pod "webserver-deployment-595b5b9587-67txh" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-67txh webserver-deployment-595b5b9587- deployment-8959 /api/v1/namespaces/deployment-8959/pods/webserver-deployment-595b5b9587-67txh 43f882a5-df79-44fd-9c95-d71cc2abb530 76904 0 2020-02-17 17:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.80.53/32 cni.projectcalico.org/podIPs:172.30.80.53/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.80.53"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1dc0a1ac-ea32-4f18-8bdf-3e82c0c07645 0xc0042023d7 0xc0042023d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mgdt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mgdt7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mgdt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dr8dx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.178,PodIP:172.30.80.53,StartTime:2020-02-17 17:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 17:44:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://c38c16e525b760512b6be3abf831ca534965cc1808c4515d77ba52f926c9dcfa,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.80.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 17:44:57.738: INFO: Pod "webserver-deployment-595b5b9587-f2qwv" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-f2qwv webserver-deployment-595b5b9587- deployment-8959 /api/v1/namespaces/deployment-8959/pods/webserver-deployment-595b5b9587-f2qwv 766912cf-07d6-4805-a225-07b5203923c4 77017 0 2020-02-17 17:44:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1dc0a1ac-ea32-4f18-8bdf-3e82c0c07645 0xc004202577 0xc004202578}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mgdt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mgdt7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mgdt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dr8dx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:57 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.178,PodIP:,StartTime:2020-02-17 17:44:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 17:44:57.738: INFO: Pod "webserver-deployment-595b5b9587-k97bt" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-k97bt webserver-deployment-595b5b9587- deployment-8959 /api/v1/namespaces/deployment-8959/pods/webserver-deployment-595b5b9587-k97bt a6f6ef12-f1e4-4771-874c-2e5de0ee35c2 77018 0 2020-02-17 17:44:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1dc0a1ac-ea32-4f18-8bdf-3e82c0c07645 0xc004202707 0xc004202708}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mgdt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mgdt7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mgdt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dr8dx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 17:44:57.739: INFO: Pod "webserver-deployment-595b5b9587-km866" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-km866 webserver-deployment-595b5b9587- deployment-8959 /api/v1/namespaces/deployment-8959/pods/webserver-deployment-595b5b9587-km866 0552fd05-2ddd-4069-9075-76236778daa5 76878 0 2020-02-17 17:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.50.171/32 cni.projectcalico.org/podIPs:172.30.50.171/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.50.171"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1dc0a1ac-ea32-4f18-8bdf-3e82c0c07645 0xc004202860 0xc004202861}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mgdt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mgdt7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mgdt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dr8dx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.189,PodIP:172.30.50.171,StartTime:2020-02-17 17:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 17:44:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://dddf20440c271b696f5e85dacbe8d03d9be21480dac622639d1687a72191dfd1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.50.171,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 17:44:57.739: INFO: Pod "webserver-deployment-595b5b9587-mbtd9" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-mbtd9 webserver-deployment-595b5b9587- deployment-8959 /api/v1/namespaces/deployment-8959/pods/webserver-deployment-595b5b9587-mbtd9 4447b5fd-8e8e-4cb0-b1a2-903afa890cfd 76907 0 2020-02-17 17:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.178.202/32 cni.projectcalico.org/podIPs:172.30.178.202/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.178.202"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1dc0a1ac-ea32-4f18-8bdf-3e82c0c07645 0xc0042029f7 0xc0042029f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mgdt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mgdt7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mgdt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dr8dx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.177,PodIP:172.30.178.202,StartTime:2020-02-17 17:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 17:44:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://46b44c1d8b7c9efdf73d38370c1e346ee7af9eb67af6c7995a45c4fa68e5aca7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.178.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 17:44:57.739: INFO: Pod "webserver-deployment-595b5b9587-p2r4s" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-p2r4s webserver-deployment-595b5b9587- deployment-8959 /api/v1/namespaces/deployment-8959/pods/webserver-deployment-595b5b9587-p2r4s 6985941a-0812-45c6-99da-8698ba345258 76913 0 2020-02-17 17:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.80.52/32 cni.projectcalico.org/podIPs:172.30.80.52/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.80.52"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1dc0a1ac-ea32-4f18-8bdf-3e82c0c07645 0xc004202bb7 0xc004202bb8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mgdt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mgdt7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mgdt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dr8dx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.178,PodIP:172.30.80.52,StartTime:2020-02-17 17:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 17:44:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://d659c48dbdc2f9d594ca913f5441dbb3134c0be853c1f59d433d5c3d14d314d4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.80.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 17:44:57.740: INFO: Pod "webserver-deployment-595b5b9587-qzmvl" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qzmvl webserver-deployment-595b5b9587- deployment-8959 /api/v1/namespaces/deployment-8959/pods/webserver-deployment-595b5b9587-qzmvl c84071ab-0762-4e33-8c86-e17dbaf6de08 76916 0 2020-02-17 17:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.178.234/32 cni.projectcalico.org/podIPs:172.30.178.234/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.178.234"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1dc0a1ac-ea32-4f18-8bdf-3e82c0c07645 0xc004202d57 0xc004202d58}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mgdt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mgdt7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mgdt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dr8dx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.177,PodIP:172.30.178.234,StartTime:2020-02-17 17:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 17:44:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://0ffacd2093172708a608a433dbfd8d498ae8bbf10c7111b6057b4aed421e0e48,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.178.234,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 17:44:57.741: INFO: Pod "webserver-deployment-595b5b9587-xj4xm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xj4xm webserver-deployment-595b5b9587- deployment-8959 /api/v1/namespaces/deployment-8959/pods/webserver-deployment-595b5b9587-xj4xm 6273a7f9-34fc-4d6a-a7ae-32b19daf95d1 77016 0 2020-02-17 17:44:57 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1dc0a1ac-ea32-4f18-8bdf-3e82c0c07645 0xc004202ef7 0xc004202ef8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mgdt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mgdt7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mgdt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dr8dx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 17:44:57.742: INFO: Pod "webserver-deployment-595b5b9587-xncv8" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-xncv8 webserver-deployment-595b5b9587- deployment-8959 /api/v1/namespaces/deployment-8959/pods/webserver-deployment-595b5b9587-xncv8 7b2c77f7-6cf2-4da5-a0e4-a81a925c05ff 76924 0 2020-02-17 17:44:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.50.172/32 cni.projectcalico.org/podIPs:172.30.50.172/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.50.172"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 1dc0a1ac-ea32-4f18-8bdf-3e82c0c07645 0xc004203027 0xc004203028}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mgdt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mgdt7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mgdt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dr8dx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.189,PodIP:172.30.50.172,StartTime:2020-02-17 17:44:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 17:44:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://216bff2bb38860bb66a20a3d51fe7592c35a6246391cc1bffee041fa25842f97,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.50.172,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 17:44:57.742: INFO: Pod "webserver-deployment-c7997dcc8-8ht55" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8ht55 webserver-deployment-c7997dcc8- deployment-8959 /api/v1/namespaces/deployment-8959/pods/webserver-deployment-c7997dcc8-8ht55 8e07f3ee-4e53-4423-a5e4-fe1f0c7235d3 76991 0 2020-02-17 17:44:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 00d53ad2-ce30-44b8-b3bb-dfccbe935e56 0xc0042031c7 0xc0042031c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mgdt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mgdt7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mgdt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dr8dx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.189,PodIP:,StartTime:2020-02-17 17:44:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 17:44:57.743: INFO: Pod "webserver-deployment-c7997dcc8-9s79f" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-9s79f webserver-deployment-c7997dcc8- deployment-8959 /api/v1/namespaces/deployment-8959/pods/webserver-deployment-c7997dcc8-9s79f 6f5f9111-632e-42a3-a451-a77672a0fed6 76974 0 2020-02-17 17:44:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 00d53ad2-ce30-44b8-b3bb-dfccbe935e56 0xc004203367 0xc004203368}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mgdt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mgdt7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mgdt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.177,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dr8dx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.177,PodIP:,StartTime:2020-02-17 17:44:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 17:44:57.743: INFO: Pod "webserver-deployment-c7997dcc8-cdqb7" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cdqb7 webserver-deployment-c7997dcc8- deployment-8959 /api/v1/namespaces/deployment-8959/pods/webserver-deployment-c7997dcc8-cdqb7 748ce6d5-ecf9-4532-916c-d46936d0634a 76982 0 2020-02-17 17:44:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 00d53ad2-ce30-44b8-b3bb-dfccbe935e56 0xc004203507 0xc004203508}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mgdt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mgdt7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mgdt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dr8dx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.178,PodIP:,StartTime:2020-02-17 17:44:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 17:44:57.743: INFO: Pod "webserver-deployment-c7997dcc8-xqwbn" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xqwbn webserver-deployment-c7997dcc8- deployment-8959 /api/v1/namespaces/deployment-8959/pods/webserver-deployment-c7997dcc8-xqwbn ba6235bb-93e1-446d-93e0-c8d9ce5fb29a 76975 0 2020-02-17 17:44:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 00d53ad2-ce30-44b8-b3bb-dfccbe935e56 0xc0042036a7 0xc0042036a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mgdt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mgdt7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mgdt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dr8dx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.189,PodIP:,StartTime:2020-02-17 17:44:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Feb 17 17:44:57.743: INFO: Pod "webserver-deployment-c7997dcc8-zfxs5" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-zfxs5 webserver-deployment-c7997dcc8- deployment-8959 /api/v1/namespaces/deployment-8959/pods/webserver-deployment-c7997dcc8-zfxs5 b4734aa5-a199-476d-b188-8b2d07996782 76964 0 2020-02-17 17:44:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 00d53ad2-ce30-44b8-b3bb-dfccbe935e56 0xc004203847 0xc004203848}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mgdt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mgdt7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mgdt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-dr8dx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:44:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.178,PodIP:,StartTime:2020-02-17 17:44:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:44:57.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8959" for this suite.
Feb 17 17:45:07.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:45:09.918: INFO: namespace deployment-8959 deletion completed in 12.150655717s

• [SLOW TEST:24.778 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:45:09.923: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-fc4763d7-6e90-44c4-ac67-02b00beedaef
STEP: Creating a pod to test consume configMaps
Feb 17 17:45:10.110: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b62de76c-d04a-4ba3-b271-236403f0acd9" in namespace "projected-9613" to be "success or failure"
Feb 17 17:45:10.122: INFO: Pod "pod-projected-configmaps-b62de76c-d04a-4ba3-b271-236403f0acd9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.223732ms
Feb 17 17:45:12.135: INFO: Pod "pod-projected-configmaps-b62de76c-d04a-4ba3-b271-236403f0acd9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024910244s
Feb 17 17:45:14.148: INFO: Pod "pod-projected-configmaps-b62de76c-d04a-4ba3-b271-236403f0acd9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038086741s
Feb 17 17:45:16.161: INFO: Pod "pod-projected-configmaps-b62de76c-d04a-4ba3-b271-236403f0acd9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051419499s
Feb 17 17:45:18.174: INFO: Pod "pod-projected-configmaps-b62de76c-d04a-4ba3-b271-236403f0acd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.064235015s
STEP: Saw pod success
Feb 17 17:45:18.174: INFO: Pod "pod-projected-configmaps-b62de76c-d04a-4ba3-b271-236403f0acd9" satisfied condition "success or failure"
Feb 17 17:45:18.186: INFO: Trying to get logs from node 10.45.66.189 pod pod-projected-configmaps-b62de76c-d04a-4ba3-b271-236403f0acd9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 17:45:18.286: INFO: Waiting for pod pod-projected-configmaps-b62de76c-d04a-4ba3-b271-236403f0acd9 to disappear
Feb 17 17:45:18.298: INFO: Pod pod-projected-configmaps-b62de76c-d04a-4ba3-b271-236403f0acd9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:45:18.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9613" for this suite.
Feb 17 17:45:26.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:45:28.422: INFO: namespace projected-9613 deletion completed in 10.100899083s

• [SLOW TEST:18.500 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:45:28.422: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:45:28.650: INFO: (0) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 58.107203ms)
Feb 17 17:45:28.673: INFO: (1) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.817533ms)
Feb 17 17:45:28.695: INFO: (2) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.328225ms)
Feb 17 17:45:28.716: INFO: (3) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.205066ms)
Feb 17 17:45:28.736: INFO: (4) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.404073ms)
Feb 17 17:45:28.758: INFO: (5) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.596905ms)
Feb 17 17:45:28.780: INFO: (6) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.441026ms)
Feb 17 17:45:28.802: INFO: (7) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.013408ms)
Feb 17 17:45:28.824: INFO: (8) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.736573ms)
Feb 17 17:45:28.845: INFO: (9) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.521319ms)
Feb 17 17:45:28.866: INFO: (10) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.266852ms)
Feb 17 17:45:28.888: INFO: (11) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.351252ms)
Feb 17 17:45:28.910: INFO: (12) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.904924ms)
Feb 17 17:45:28.932: INFO: (13) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.944067ms)
Feb 17 17:45:28.955: INFO: (14) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.228408ms)
Feb 17 17:45:28.975: INFO: (15) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.676474ms)
Feb 17 17:45:28.997: INFO: (16) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.874358ms)
Feb 17 17:45:29.018: INFO: (17) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.178887ms)
Feb 17 17:45:29.057: INFO: (18) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 39.222348ms)
Feb 17 17:45:29.078: INFO: (19) /api/v1/nodes/10.45.66.177:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.766824ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:45:29.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2911" for this suite.
Feb 17 17:45:37.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:45:38.178: INFO: namespace proxy-2911 deletion completed in 9.072720187s

• [SLOW TEST:9.756 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:45:38.179: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Feb 17 17:45:46.915: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1214 pod-service-account-c903fc24-15db-4a7b-8972-d7a213a1e8d2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Feb 17 17:45:47.273: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1214 pod-service-account-c903fc24-15db-4a7b-8972-d7a213a1e8d2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Feb 17 17:45:47.677: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1214 pod-service-account-c903fc24-15db-4a7b-8972-d7a213a1e8d2 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:45:48.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1214" for this suite.
Feb 17 17:45:56.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:45:58.206: INFO: namespace svcaccounts-1214 deletion completed in 10.138332169s

• [SLOW TEST:20.028 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:45:58.208: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-ae5bc0b9-2811-455e-a8be-22f50d2b13bb
STEP: Creating a pod to test consume secrets
Feb 17 17:45:58.569: INFO: Waiting up to 5m0s for pod "pod-secrets-c0638cf9-2c23-4824-8298-2805d46308f2" in namespace "secrets-9251" to be "success or failure"
Feb 17 17:45:58.590: INFO: Pod "pod-secrets-c0638cf9-2c23-4824-8298-2805d46308f2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.94431ms
Feb 17 17:46:00.603: INFO: Pod "pod-secrets-c0638cf9-2c23-4824-8298-2805d46308f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034043644s
Feb 17 17:46:02.616: INFO: Pod "pod-secrets-c0638cf9-2c23-4824-8298-2805d46308f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047056453s
Feb 17 17:46:04.630: INFO: Pod "pod-secrets-c0638cf9-2c23-4824-8298-2805d46308f2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060993938s
Feb 17 17:46:06.643: INFO: Pod "pod-secrets-c0638cf9-2c23-4824-8298-2805d46308f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.07414086s
STEP: Saw pod success
Feb 17 17:46:06.644: INFO: Pod "pod-secrets-c0638cf9-2c23-4824-8298-2805d46308f2" satisfied condition "success or failure"
Feb 17 17:46:06.656: INFO: Trying to get logs from node 10.45.66.177 pod pod-secrets-c0638cf9-2c23-4824-8298-2805d46308f2 container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 17:46:06.718: INFO: Waiting for pod pod-secrets-c0638cf9-2c23-4824-8298-2805d46308f2 to disappear
Feb 17 17:46:06.730: INFO: Pod pod-secrets-c0638cf9-2c23-4824-8298-2805d46308f2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:46:06.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9251" for this suite.
Feb 17 17:46:14.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:46:16.914: INFO: namespace secrets-9251 deletion completed in 10.155605411s
STEP: Destroying namespace "secret-namespace-5940" for this suite.
Feb 17 17:46:24.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:46:27.037: INFO: namespace secret-namespace-5940 deletion completed in 10.123609975s

• [SLOW TEST:28.829 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:46:27.037: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Feb 17 17:46:27.239: INFO: Waiting up to 5m0s for pod "downward-api-738e5264-9e97-4cea-a242-22aa23f7d3c7" in namespace "downward-api-7143" to be "success or failure"
Feb 17 17:46:27.253: INFO: Pod "downward-api-738e5264-9e97-4cea-a242-22aa23f7d3c7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.460313ms
Feb 17 17:46:29.271: INFO: Pod "downward-api-738e5264-9e97-4cea-a242-22aa23f7d3c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031470112s
Feb 17 17:46:31.283: INFO: Pod "downward-api-738e5264-9e97-4cea-a242-22aa23f7d3c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043695301s
Feb 17 17:46:33.295: INFO: Pod "downward-api-738e5264-9e97-4cea-a242-22aa23f7d3c7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055941297s
Feb 17 17:46:35.317: INFO: Pod "downward-api-738e5264-9e97-4cea-a242-22aa23f7d3c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.077826236s
STEP: Saw pod success
Feb 17 17:46:35.317: INFO: Pod "downward-api-738e5264-9e97-4cea-a242-22aa23f7d3c7" satisfied condition "success or failure"
Feb 17 17:46:35.329: INFO: Trying to get logs from node 10.45.66.177 pod downward-api-738e5264-9e97-4cea-a242-22aa23f7d3c7 container dapi-container: <nil>
STEP: delete the pod
Feb 17 17:46:35.414: INFO: Waiting for pod downward-api-738e5264-9e97-4cea-a242-22aa23f7d3c7 to disappear
Feb 17 17:46:35.428: INFO: Pod downward-api-738e5264-9e97-4cea-a242-22aa23f7d3c7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:46:35.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7143" for this suite.
Feb 17 17:46:43.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:46:45.654: INFO: namespace downward-api-7143 deletion completed in 10.191500131s

• [SLOW TEST:18.616 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:46:45.654: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 17:46:45.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-7183'
Feb 17 17:46:45.974: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 17 17:46:45.974: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Feb 17 17:46:46.007: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-mwl7h]
Feb 17 17:46:46.007: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-mwl7h" in namespace "kubectl-7183" to be "running and ready"
Feb 17 17:46:46.023: INFO: Pod "e2e-test-httpd-rc-mwl7h": Phase="Pending", Reason="", readiness=false. Elapsed: 16.54436ms
Feb 17 17:46:48.037: INFO: Pod "e2e-test-httpd-rc-mwl7h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030529331s
Feb 17 17:46:50.057: INFO: Pod "e2e-test-httpd-rc-mwl7h": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050244343s
Feb 17 17:46:52.076: INFO: Pod "e2e-test-httpd-rc-mwl7h": Phase="Pending", Reason="", readiness=false. Elapsed: 6.069575046s
Feb 17 17:46:54.091: INFO: Pod "e2e-test-httpd-rc-mwl7h": Phase="Running", Reason="", readiness=true. Elapsed: 8.084102951s
Feb 17 17:46:54.091: INFO: Pod "e2e-test-httpd-rc-mwl7h" satisfied condition "running and ready"
Feb 17 17:46:54.091: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-mwl7h]
Feb 17 17:46:54.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 logs rc/e2e-test-httpd-rc --namespace=kubectl-7183'
Feb 17 17:46:54.304: INFO: stderr: ""
Feb 17 17:46:54.304: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.50.175. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.50.175. Set the 'ServerName' directive globally to suppress this message\n[Mon Feb 17 17:46:52.986548 2020] [mpm_event:notice] [pid 1:tid 140536326261608] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Mon Feb 17 17:46:52.986617 2020] [core:notice] [pid 1:tid 140536326261608] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Feb 17 17:46:54.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete rc e2e-test-httpd-rc --namespace=kubectl-7183'
Feb 17 17:46:54.453: INFO: stderr: ""
Feb 17 17:46:54.453: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:46:54.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7183" for this suite.
Feb 17 17:47:26.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:47:28.628: INFO: namespace kubectl-7183 deletion completed in 34.124612855s

• [SLOW TEST:42.974 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:47:28.628: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Feb 17 17:47:28.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-9875 -- logs-generator --log-lines-total 100 --run-duration 20s'
Feb 17 17:47:28.967: INFO: stderr: ""
Feb 17 17:47:28.967: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Feb 17 17:47:28.967: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Feb 17 17:47:28.968: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9875" to be "running and ready, or succeeded"
Feb 17 17:47:28.980: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.125413ms
Feb 17 17:47:30.993: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025127376s
Feb 17 17:47:33.006: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038706966s
Feb 17 17:47:35.020: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052363658s
Feb 17 17:47:37.037: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 8.069493545s
Feb 17 17:47:37.037: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Feb 17 17:47:37.037: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Feb 17 17:47:37.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 logs logs-generator logs-generator --namespace=kubectl-9875'
Feb 17 17:47:37.213: INFO: stderr: ""
Feb 17 17:47:37.213: INFO: stdout: "I0217 17:47:36.006535       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/rkp 402\nI0217 17:47:36.206780       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/n858 454\nI0217 17:47:36.406709       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/bz6 356\nI0217 17:47:36.606727       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/khs 426\nI0217 17:47:36.806723       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/bb4 410\nI0217 17:47:37.006687       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/clk 462\n"
STEP: limiting log lines
Feb 17 17:47:37.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 logs logs-generator logs-generator --namespace=kubectl-9875 --tail=1'
Feb 17 17:47:37.422: INFO: stderr: ""
Feb 17 17:47:37.422: INFO: stdout: "I0217 17:47:37.406697       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/klg 212\n"
STEP: limiting log bytes
Feb 17 17:47:37.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 logs logs-generator logs-generator --namespace=kubectl-9875 --limit-bytes=1'
Feb 17 17:47:37.593: INFO: stderr: ""
Feb 17 17:47:37.593: INFO: stdout: "I"
STEP: exposing timestamps
Feb 17 17:47:37.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 logs logs-generator logs-generator --namespace=kubectl-9875 --tail=1 --timestamps'
Feb 17 17:47:37.788: INFO: stderr: ""
Feb 17 17:47:37.788: INFO: stdout: "2020-02-17T11:47:37.60672467-06:00 I0217 17:47:37.606677       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/dm2 202\n"
STEP: restricting to a time range
Feb 17 17:47:40.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 logs logs-generator logs-generator --namespace=kubectl-9875 --since=1s'
Feb 17 17:47:40.536: INFO: stderr: ""
Feb 17 17:47:40.536: INFO: stdout: "I0217 17:47:39.606720       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/8zz 248\nI0217 17:47:39.806685       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/csv 210\nI0217 17:47:40.006695       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/kwt 258\nI0217 17:47:40.206784       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/vqpz 279\nI0217 17:47:40.414512       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/kf8 501\n"
Feb 17 17:47:40.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 logs logs-generator logs-generator --namespace=kubectl-9875 --since=24h'
Feb 17 17:47:40.739: INFO: stderr: ""
Feb 17 17:47:40.739: INFO: stdout: "I0217 17:47:36.006535       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/rkp 402\nI0217 17:47:36.206780       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/n858 454\nI0217 17:47:36.406709       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/bz6 356\nI0217 17:47:36.606727       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/khs 426\nI0217 17:47:36.806723       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/bb4 410\nI0217 17:47:37.006687       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/clk 462\nI0217 17:47:37.208242       1 logs_generator.go:76] 6 POST /api/v1/namespaces/kube-system/pods/h9g 536\nI0217 17:47:37.406697       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/klg 212\nI0217 17:47:37.606677       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/dm2 202\nI0217 17:47:37.806745       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/ltk2 512\nI0217 17:47:38.006740       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/8wk 373\nI0217 17:47:38.206725       1 logs_generator.go:76] 11 GET /api/v1/namespaces/default/pods/pcjp 358\nI0217 17:47:38.406673       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/2kl 233\nI0217 17:47:38.606712       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/tr8r 391\nI0217 17:47:38.806677       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/fljw 596\nI0217 17:47:39.006717       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/nhfl 451\nI0217 17:47:39.206726       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/jdsg 453\nI0217 17:47:39.406693       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/jnr7 256\nI0217 17:47:39.606720       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/8zz 248\nI0217 17:47:39.806685       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/csv 210\nI0217 17:47:40.006695       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/kwt 258\nI0217 17:47:40.206784       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/vqpz 279\nI0217 17:47:40.414512       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/kf8 501\nI0217 17:47:40.611779       1 logs_generator.go:76] 23 PUT /api/v1/namespaces/kube-system/pods/8q9 534\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Feb 17 17:47:40.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete pod logs-generator --namespace=kubectl-9875'
Feb 17 17:47:53.721: INFO: stderr: ""
Feb 17 17:47:53.721: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:47:53.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9875" for this suite.
Feb 17 17:48:01.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:48:03.879: INFO: namespace kubectl-9875 deletion completed in 10.125404118s

• [SLOW TEST:35.252 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:48:03.880: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-4375/configmap-test-652fdb16-4d4d-41b6-9ef5-37e6790d773f
STEP: Creating a pod to test consume configMaps
Feb 17 17:48:04.080: INFO: Waiting up to 5m0s for pod "pod-configmaps-062747a2-5aa4-4225-9d38-81050b5af426" in namespace "configmap-4375" to be "success or failure"
Feb 17 17:48:04.094: INFO: Pod "pod-configmaps-062747a2-5aa4-4225-9d38-81050b5af426": Phase="Pending", Reason="", readiness=false. Elapsed: 13.803128ms
Feb 17 17:48:06.108: INFO: Pod "pod-configmaps-062747a2-5aa4-4225-9d38-81050b5af426": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027983028s
Feb 17 17:48:08.122: INFO: Pod "pod-configmaps-062747a2-5aa4-4225-9d38-81050b5af426": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041227208s
Feb 17 17:48:10.135: INFO: Pod "pod-configmaps-062747a2-5aa4-4225-9d38-81050b5af426": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054678621s
Feb 17 17:48:12.149: INFO: Pod "pod-configmaps-062747a2-5aa4-4225-9d38-81050b5af426": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068510671s
STEP: Saw pod success
Feb 17 17:48:12.149: INFO: Pod "pod-configmaps-062747a2-5aa4-4225-9d38-81050b5af426" satisfied condition "success or failure"
Feb 17 17:48:12.162: INFO: Trying to get logs from node 10.45.66.189 pod pod-configmaps-062747a2-5aa4-4225-9d38-81050b5af426 container env-test: <nil>
STEP: delete the pod
Feb 17 17:48:12.257: INFO: Waiting for pod pod-configmaps-062747a2-5aa4-4225-9d38-81050b5af426 to disappear
Feb 17 17:48:12.269: INFO: Pod pod-configmaps-062747a2-5aa4-4225-9d38-81050b5af426 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:48:12.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4375" for this suite.
Feb 17 17:48:20.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:48:22.445: INFO: namespace configmap-4375 deletion completed in 10.141735662s

• [SLOW TEST:18.565 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:48:22.445: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:48:30.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2266" for this suite.
Feb 17 17:48:38.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:48:41.109: INFO: namespace namespaces-2266 deletion completed in 10.167732875s
STEP: Destroying namespace "nsdeletetest-9900" for this suite.
Feb 17 17:48:41.121: INFO: Namespace nsdeletetest-9900 was already deleted
STEP: Destroying namespace "nsdeletetest-4373" for this suite.
Feb 17 17:48:49.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:48:51.288: INFO: namespace nsdeletetest-4373 deletion completed in 10.167211293s

• [SLOW TEST:28.843 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:48:51.289: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 17:48:51.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-2641'
Feb 17 17:48:51.633: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 17 17:48:51.633: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Feb 17 17:48:51.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete jobs e2e-test-httpd-job --namespace=kubectl-2641'
Feb 17 17:48:51.833: INFO: stderr: ""
Feb 17 17:48:51.833: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:48:51.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2641" for this suite.
Feb 17 17:49:05.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:49:07.995: INFO: namespace kubectl-2641 deletion completed in 16.127151212s

• [SLOW TEST:16.706 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:49:07.996: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6817.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6817.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6817.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6817.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 17:49:18.266: INFO: DNS probes using dns-test-47814b48-6712-4bd8-b576-2ce99a88d774 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6817.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6817.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6817.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6817.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 17:49:28.457: INFO: DNS probes using dns-test-f3aee0bb-a135-44ab-953e-b8ba32518da8 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6817.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6817.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6817.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6817.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 17:49:36.732: INFO: DNS probes using dns-test-d611a65e-c5f7-4dcb-bf0f-94a50da3062e succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:49:36.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6817" for this suite.
Feb 17 17:49:44.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:49:46.978: INFO: namespace dns-6817 deletion completed in 10.125890415s

• [SLOW TEST:38.982 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:49:46.978: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Feb 17 17:49:55.760: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c134d420-75b5-4dcd-ab96-e54b1ab66ad9"
Feb 17 17:49:55.760: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c134d420-75b5-4dcd-ab96-e54b1ab66ad9" in namespace "pods-7641" to be "terminated due to deadline exceeded"
Feb 17 17:49:55.772: INFO: Pod "pod-update-activedeadlineseconds-c134d420-75b5-4dcd-ab96-e54b1ab66ad9": Phase="Running", Reason="", readiness=true. Elapsed: 12.680363ms
Feb 17 17:49:57.787: INFO: Pod "pod-update-activedeadlineseconds-c134d420-75b5-4dcd-ab96-e54b1ab66ad9": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.026724765s
Feb 17 17:49:57.787: INFO: Pod "pod-update-activedeadlineseconds-c134d420-75b5-4dcd-ab96-e54b1ab66ad9" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:49:57.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7641" for this suite.
Feb 17 17:50:05.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:50:07.971: INFO: namespace pods-7641 deletion completed in 10.138175021s

• [SLOW TEST:20.993 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:50:07.972: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-2663
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 17 17:50:08.112: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 17 17:50:44.533: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.50.179:8080/dial?request=hostName&protocol=udp&host=172.30.50.178&port=8081&tries=1'] Namespace:pod-network-test-2663 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:50:44.533: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 17:50:44.802: INFO: Waiting for endpoints: map[]
Feb 17 17:50:44.815: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.50.179:8080/dial?request=hostName&protocol=udp&host=172.30.80.59&port=8081&tries=1'] Namespace:pod-network-test-2663 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:50:44.815: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 17:50:45.033: INFO: Waiting for endpoints: map[]
Feb 17 17:50:45.046: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.50.179:8080/dial?request=hostName&protocol=udp&host=172.30.178.238&port=8081&tries=1'] Namespace:pod-network-test-2663 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 17:50:45.046: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 17:50:45.310: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:50:45.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2663" for this suite.
Feb 17 17:50:53.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:50:55.604: INFO: namespace pod-network-test-2663 deletion completed in 10.265411088s

• [SLOW TEST:47.633 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:50:55.605: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Feb 17 17:50:55.781: INFO: Waiting up to 5m0s for pod "pod-26c10582-d38f-459b-92a4-f95b02a5ee60" in namespace "emptydir-7080" to be "success or failure"
Feb 17 17:50:55.795: INFO: Pod "pod-26c10582-d38f-459b-92a4-f95b02a5ee60": Phase="Pending", Reason="", readiness=false. Elapsed: 13.749648ms
Feb 17 17:50:57.809: INFO: Pod "pod-26c10582-d38f-459b-92a4-f95b02a5ee60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027752326s
Feb 17 17:50:59.822: INFO: Pod "pod-26c10582-d38f-459b-92a4-f95b02a5ee60": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040592934s
Feb 17 17:51:01.836: INFO: Pod "pod-26c10582-d38f-459b-92a4-f95b02a5ee60": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05501852s
Feb 17 17:51:03.850: INFO: Pod "pod-26c10582-d38f-459b-92a4-f95b02a5ee60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.06854638s
STEP: Saw pod success
Feb 17 17:51:03.850: INFO: Pod "pod-26c10582-d38f-459b-92a4-f95b02a5ee60" satisfied condition "success or failure"
Feb 17 17:51:03.862: INFO: Trying to get logs from node 10.45.66.189 pod pod-26c10582-d38f-459b-92a4-f95b02a5ee60 container test-container: <nil>
STEP: delete the pod
Feb 17 17:51:03.961: INFO: Waiting for pod pod-26c10582-d38f-459b-92a4-f95b02a5ee60 to disappear
Feb 17 17:51:03.973: INFO: Pod pod-26c10582-d38f-459b-92a4-f95b02a5ee60 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:51:03.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7080" for this suite.
Feb 17 17:51:12.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:51:14.139: INFO: namespace emptydir-7080 deletion completed in 10.128466177s

• [SLOW TEST:18.534 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:51:14.139: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-239d149d-95ec-435b-b9bf-a748df249a76
STEP: Creating a pod to test consume configMaps
Feb 17 17:51:14.361: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-caf2750d-ac3b-4cb4-95b0-e7d18d998f6c" in namespace "projected-26" to be "success or failure"
Feb 17 17:51:14.381: INFO: Pod "pod-projected-configmaps-caf2750d-ac3b-4cb4-95b0-e7d18d998f6c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.348589ms
Feb 17 17:51:16.394: INFO: Pod "pod-projected-configmaps-caf2750d-ac3b-4cb4-95b0-e7d18d998f6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033845969s
Feb 17 17:51:18.408: INFO: Pod "pod-projected-configmaps-caf2750d-ac3b-4cb4-95b0-e7d18d998f6c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047244993s
Feb 17 17:51:20.420: INFO: Pod "pod-projected-configmaps-caf2750d-ac3b-4cb4-95b0-e7d18d998f6c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.059828368s
Feb 17 17:51:22.434: INFO: Pod "pod-projected-configmaps-caf2750d-ac3b-4cb4-95b0-e7d18d998f6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.073241112s
STEP: Saw pod success
Feb 17 17:51:22.434: INFO: Pod "pod-projected-configmaps-caf2750d-ac3b-4cb4-95b0-e7d18d998f6c" satisfied condition "success or failure"
Feb 17 17:51:22.447: INFO: Trying to get logs from node 10.45.66.177 pod pod-projected-configmaps-caf2750d-ac3b-4cb4-95b0-e7d18d998f6c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 17:51:22.537: INFO: Waiting for pod pod-projected-configmaps-caf2750d-ac3b-4cb4-95b0-e7d18d998f6c to disappear
Feb 17 17:51:22.550: INFO: Pod pod-projected-configmaps-caf2750d-ac3b-4cb4-95b0-e7d18d998f6c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:51:22.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-26" for this suite.
Feb 17 17:51:30.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:51:32.725: INFO: namespace projected-26 deletion completed in 10.146312815s

• [SLOW TEST:18.586 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:51:32.727: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 17:51:33.784: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 17:51:35.822: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558693, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558693, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558693, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558693, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:51:37.836: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558693, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558693, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558693, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558693, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:51:39.835: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558693, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558693, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558693, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558693, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 17:51:42.865: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:51:43.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6097" for this suite.
Feb 17 17:51:51.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:51:53.413: INFO: namespace webhook-6097 deletion completed in 10.372497294s
STEP: Destroying namespace "webhook-6097-markers" for this suite.
Feb 17 17:52:01.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:52:03.552: INFO: namespace webhook-6097-markers deletion completed in 10.138744904s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:30.916 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:52:03.642: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:52:11.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7930" for this suite.
Feb 17 17:52:19.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:52:22.047: INFO: namespace kubelet-test-7930 deletion completed in 10.133586339s

• [SLOW TEST:18.405 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:52:22.047: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 17:52:23.183: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 17:52:25.221: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558743, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558743, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558743, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558743, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:52:27.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558743, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558743, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558743, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558743, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:52:29.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558743, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558743, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558743, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558743, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 17:52:32.264: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:52:32.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8988" for this suite.
Feb 17 17:52:40.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:52:42.497: INFO: namespace webhook-8988 deletion completed in 10.142805496s
STEP: Destroying namespace "webhook-8988-markers" for this suite.
Feb 17 17:52:50.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:52:52.647: INFO: namespace webhook-8988-markers deletion completed in 10.149817835s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:30.679 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:52:52.727: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:52:52.891: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Feb 17 17:53:00.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-3033 create -f -'
Feb 17 17:53:01.259: INFO: stderr: ""
Feb 17 17:53:01.259: INFO: stdout: "e2e-test-crd-publish-openapi-7404-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 17 17:53:01.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-3033 delete e2e-test-crd-publish-openapi-7404-crds test-cr'
Feb 17 17:53:01.473: INFO: stderr: ""
Feb 17 17:53:01.473: INFO: stdout: "e2e-test-crd-publish-openapi-7404-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Feb 17 17:53:01.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-3033 apply -f -'
Feb 17 17:53:01.998: INFO: stderr: ""
Feb 17 17:53:01.999: INFO: stdout: "e2e-test-crd-publish-openapi-7404-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Feb 17 17:53:01.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=crd-publish-openapi-3033 delete e2e-test-crd-publish-openapi-7404-crds test-cr'
Feb 17 17:53:02.160: INFO: stderr: ""
Feb 17 17:53:02.160: INFO: stdout: "e2e-test-crd-publish-openapi-7404-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Feb 17 17:53:02.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 explain e2e-test-crd-publish-openapi-7404-crds'
Feb 17 17:53:02.508: INFO: stderr: ""
Feb 17 17:53:02.508: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7404-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:53:09.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3033" for this suite.
Feb 17 17:53:17.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:53:19.723: INFO: namespace crd-publish-openapi-3033 deletion completed in 10.124646324s

• [SLOW TEST:26.996 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:53:19.724: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-69669188-7a22-4044-afe8-2bd5f3936b1c
STEP: Creating a pod to test consume secrets
Feb 17 17:53:19.933: INFO: Waiting up to 5m0s for pod "pod-secrets-d8a2326d-f135-411f-97d0-9fe2cc939507" in namespace "secrets-6552" to be "success or failure"
Feb 17 17:53:19.946: INFO: Pod "pod-secrets-d8a2326d-f135-411f-97d0-9fe2cc939507": Phase="Pending", Reason="", readiness=false. Elapsed: 12.849465ms
Feb 17 17:53:21.960: INFO: Pod "pod-secrets-d8a2326d-f135-411f-97d0-9fe2cc939507": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026508639s
Feb 17 17:53:23.973: INFO: Pod "pod-secrets-d8a2326d-f135-411f-97d0-9fe2cc939507": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039343486s
Feb 17 17:53:25.985: INFO: Pod "pod-secrets-d8a2326d-f135-411f-97d0-9fe2cc939507": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051896634s
Feb 17 17:53:28.012: INFO: Pod "pod-secrets-d8a2326d-f135-411f-97d0-9fe2cc939507": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.078263074s
STEP: Saw pod success
Feb 17 17:53:28.012: INFO: Pod "pod-secrets-d8a2326d-f135-411f-97d0-9fe2cc939507" satisfied condition "success or failure"
Feb 17 17:53:28.026: INFO: Trying to get logs from node 10.45.66.189 pod pod-secrets-d8a2326d-f135-411f-97d0-9fe2cc939507 container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 17:53:28.130: INFO: Waiting for pod pod-secrets-d8a2326d-f135-411f-97d0-9fe2cc939507 to disappear
Feb 17 17:53:28.142: INFO: Pod pod-secrets-d8a2326d-f135-411f-97d0-9fe2cc939507 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:53:28.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6552" for this suite.
Feb 17 17:53:36.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:53:38.306: INFO: namespace secrets-6552 deletion completed in 10.145097921s

• [SLOW TEST:18.583 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:53:38.307: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-08cf2632-0204-4e6e-b89d-6411480b6a89
Feb 17 17:53:38.502: INFO: Pod name my-hostname-basic-08cf2632-0204-4e6e-b89d-6411480b6a89: Found 0 pods out of 1
Feb 17 17:53:43.516: INFO: Pod name my-hostname-basic-08cf2632-0204-4e6e-b89d-6411480b6a89: Found 1 pods out of 1
Feb 17 17:53:43.516: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-08cf2632-0204-4e6e-b89d-6411480b6a89" are running
Feb 17 17:53:47.542: INFO: Pod "my-hostname-basic-08cf2632-0204-4e6e-b89d-6411480b6a89-tf2rl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 17:53:38 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 17:53:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-08cf2632-0204-4e6e-b89d-6411480b6a89]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 17:53:38 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-08cf2632-0204-4e6e-b89d-6411480b6a89]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 17:53:38 +0000 UTC Reason: Message:}])
Feb 17 17:53:47.542: INFO: Trying to dial the pod
Feb 17 17:53:52.599: INFO: Controller my-hostname-basic-08cf2632-0204-4e6e-b89d-6411480b6a89: Got expected result from replica 1 [my-hostname-basic-08cf2632-0204-4e6e-b89d-6411480b6a89-tf2rl]: "my-hostname-basic-08cf2632-0204-4e6e-b89d-6411480b6a89-tf2rl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:53:52.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-763" for this suite.
Feb 17 17:54:00.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:54:02.780: INFO: namespace replication-controller-763 deletion completed in 10.161566765s

• [SLOW TEST:24.473 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:54:02.780: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Feb 17 17:54:11.010: INFO: &Pod{ObjectMeta:{send-events-43a1c5f8-116f-4b6d-9746-9fadd77d8454  events-6051 /api/v1/namespaces/events-6051/pods/send-events-43a1c5f8-116f-4b6d-9746-9fadd77d8454 d0f66692-451d-465b-9cb2-4487f99fd15c 81657 0 2020-02-17 17:54:02 +0000 UTC <nil> <nil> map[name:foo time:912719910] map[cni.projectcalico.org/podIP:172.30.80.63/32 cni.projectcalico.org/podIPs:172.30.80.63/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.80.63"
    ],
    "dns": {}
}] openshift.io/scc:anyuid] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-9bgnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-9bgnk,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-9bgnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.178,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c47,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:54:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:54:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:54:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 17:54:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.178,PodIP:172.30.80.63,StartTime:2020-02-17 17:54:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 17:54:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:cri-o://460a74ca64d19380c61a492f6ce58ae7f0f14f5d5d433c68de9411bc2fa87acd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.80.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Feb 17 17:54:13.023: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Feb 17 17:54:15.036: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:54:15.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6051" for this suite.
Feb 17 17:55:01.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:55:03.242: INFO: namespace events-6051 deletion completed in 48.157490273s

• [SLOW TEST:60.462 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:55:03.243: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Feb 17 17:55:03.379: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-559420064 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:55:03.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5048" for this suite.
Feb 17 17:55:11.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:55:13.662: INFO: namespace kubectl-5048 deletion completed in 10.135450691s

• [SLOW TEST:10.419 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:55:13.663: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 17:55:14.252: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 17:55:16.299: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558914, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558914, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558914, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558914, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:55:18.312: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558914, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558914, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558914, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558914, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 17:55:20.312: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558914, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558914, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558914, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717558914, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 17:55:23.346: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:55:23.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6004" for this suite.
Feb 17 17:55:31.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:55:34.065: INFO: namespace webhook-6004 deletion completed in 10.154277416s
STEP: Destroying namespace "webhook-6004-markers" for this suite.
Feb 17 17:55:42.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:55:44.215: INFO: namespace webhook-6004-markers deletion completed in 10.149322415s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:30.622 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:55:44.286: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:56:00.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1407" for this suite.
Feb 17 17:56:08.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:56:10.952: INFO: namespace resourcequota-1407 deletion completed in 10.192207862s

• [SLOW TEST:26.666 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:56:10.952: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2517
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Feb 17 17:56:11.169: INFO: Found 0 stateful pods, waiting for 3
Feb 17 17:56:21.182: INFO: Found 2 stateful pods, waiting for 3
Feb 17 17:56:31.190: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:56:31.190: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:56:31.190: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 17 17:56:41.187: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:56:41.187: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:56:41.187: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Feb 17 17:56:41.270: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Feb 17 17:56:51.375: INFO: Updating stateful set ss2
Feb 17 17:56:51.405: INFO: Waiting for Pod statefulset-2517/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Feb 17 17:57:01.532: INFO: Found 2 stateful pods, waiting for 3
Feb 17 17:57:11.551: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:57:11.551: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:57:11.551: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Feb 17 17:57:21.551: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:57:21.551: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Feb 17 17:57:21.551: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Feb 17 17:57:21.614: INFO: Updating stateful set ss2
Feb 17 17:57:21.642: INFO: Waiting for Pod statefulset-2517/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 17:57:31.710: INFO: Updating stateful set ss2
Feb 17 17:57:31.739: INFO: Waiting for StatefulSet statefulset-2517/ss2 to complete update
Feb 17 17:57:31.739: INFO: Waiting for Pod statefulset-2517/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 17:57:41.772: INFO: Waiting for StatefulSet statefulset-2517/ss2 to complete update
Feb 17 17:57:41.772: INFO: Waiting for Pod statefulset-2517/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Feb 17 17:57:51.769: INFO: Waiting for StatefulSet statefulset-2517/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 17 17:58:01.771: INFO: Deleting all statefulset in ns statefulset-2517
Feb 17 17:58:01.784: INFO: Scaling statefulset ss2 to 0
Feb 17 17:58:21.855: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 17:58:21.867: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:58:21.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2517" for this suite.
Feb 17 17:58:29.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:58:32.052: INFO: namespace statefulset-2517 deletion completed in 10.098256508s

• [SLOW TEST:141.100 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:58:32.052: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 17:58:32.233: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-8318a6ea-5bed-4b8e-8dd6-443beea151f1" in namespace "security-context-test-9224" to be "success or failure"
Feb 17 17:58:32.253: INFO: Pod "busybox-readonly-false-8318a6ea-5bed-4b8e-8dd6-443beea151f1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.200469ms
Feb 17 17:58:34.267: INFO: Pod "busybox-readonly-false-8318a6ea-5bed-4b8e-8dd6-443beea151f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034347428s
Feb 17 17:58:36.281: INFO: Pod "busybox-readonly-false-8318a6ea-5bed-4b8e-8dd6-443beea151f1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047999913s
Feb 17 17:58:38.294: INFO: Pod "busybox-readonly-false-8318a6ea-5bed-4b8e-8dd6-443beea151f1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061252771s
Feb 17 17:58:40.309: INFO: Pod "busybox-readonly-false-8318a6ea-5bed-4b8e-8dd6-443beea151f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.076236917s
Feb 17 17:58:40.309: INFO: Pod "busybox-readonly-false-8318a6ea-5bed-4b8e-8dd6-443beea151f1" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:58:40.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9224" for this suite.
Feb 17 17:58:48.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:58:50.492: INFO: namespace security-context-test-9224 deletion completed in 10.15457418s

• [SLOW TEST:18.440 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:58:50.492: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-996
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-996
Feb 17 17:58:50.669: INFO: Found 0 stateful pods, waiting for 1
Feb 17 17:59:00.683: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Feb 17 17:59:00.751: INFO: Deleting all statefulset in ns statefulset-996
Feb 17 17:59:00.764: INFO: Scaling statefulset ss to 0
Feb 17 17:59:20.844: INFO: Waiting for statefulset status.replicas updated to 0
Feb 17 17:59:20.856: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:59:20.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-996" for this suite.
Feb 17 17:59:28.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:59:31.060: INFO: namespace statefulset-996 deletion completed in 10.128926569s

• [SLOW TEST:40.568 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:59:31.060: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Feb 17 17:59:31.236: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Feb 17 17:59:44.460: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 17:59:44.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9823" for this suite.
Feb 17 17:59:52.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 17:59:54.639: INFO: namespace pods-9823 deletion completed in 10.140956144s

• [SLOW TEST:23.579 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 17:59:54.639: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Feb 17 18:00:04.893: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:00:04.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0217 18:00:04.893028      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-125" for this suite.
Feb 17 18:00:12.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:00:15.049: INFO: namespace gc-125 deletion completed in 10.133285118s

• [SLOW TEST:20.410 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:00:15.050: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:00:16.304: INFO: Waiting up to 5m0s for pod "downwardapi-volume-717af303-81d5-483e-a568-8fb0a9e710f6" in namespace "projected-4768" to be "success or failure"
Feb 17 18:00:16.317: INFO: Pod "downwardapi-volume-717af303-81d5-483e-a568-8fb0a9e710f6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.702958ms
Feb 17 18:00:18.330: INFO: Pod "downwardapi-volume-717af303-81d5-483e-a568-8fb0a9e710f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025791287s
Feb 17 18:00:20.343: INFO: Pod "downwardapi-volume-717af303-81d5-483e-a568-8fb0a9e710f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038430828s
Feb 17 18:00:22.356: INFO: Pod "downwardapi-volume-717af303-81d5-483e-a568-8fb0a9e710f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051433431s
Feb 17 18:00:24.369: INFO: Pod "downwardapi-volume-717af303-81d5-483e-a568-8fb0a9e710f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.064704057s
STEP: Saw pod success
Feb 17 18:00:24.369: INFO: Pod "downwardapi-volume-717af303-81d5-483e-a568-8fb0a9e710f6" satisfied condition "success or failure"
Feb 17 18:00:24.381: INFO: Trying to get logs from node 10.45.66.189 pod downwardapi-volume-717af303-81d5-483e-a568-8fb0a9e710f6 container client-container: <nil>
STEP: delete the pod
Feb 17 18:00:24.444: INFO: Waiting for pod downwardapi-volume-717af303-81d5-483e-a568-8fb0a9e710f6 to disappear
Feb 17 18:00:24.458: INFO: Pod downwardapi-volume-717af303-81d5-483e-a568-8fb0a9e710f6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:00:24.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4768" for this suite.
Feb 17 18:00:32.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:00:34.584: INFO: namespace projected-4768 deletion completed in 10.10224692s

• [SLOW TEST:19.535 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:00:34.585: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 17 18:00:42.895: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:00:42.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2062" for this suite.
Feb 17 18:00:51.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:00:53.308: INFO: namespace container-runtime-2062 deletion completed in 10.333568885s

• [SLOW TEST:18.724 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:00:53.309: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:01:11.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1584" for this suite.
Feb 17 18:01:19.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:01:21.460: INFO: namespace resourcequota-1584 deletion completed in 10.232555923s

• [SLOW TEST:28.151 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:01:21.461: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-2307
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2307 to expose endpoints map[]
Feb 17 18:01:21.636: INFO: successfully validated that service endpoint-test2 in namespace services-2307 exposes endpoints map[] (16.236621ms elapsed)
STEP: Creating pod pod1 in namespace services-2307
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2307 to expose endpoints map[pod1:[80]]
Feb 17 18:01:25.843: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.166032004s elapsed, will retry)
Feb 17 18:01:29.964: INFO: successfully validated that service endpoint-test2 in namespace services-2307 exposes endpoints map[pod1:[80]] (8.287241238s elapsed)
STEP: Creating pod pod2 in namespace services-2307
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2307 to expose endpoints map[pod1:[80] pod2:[80]]
Feb 17 18:01:34.216: INFO: Unexpected endpoints: found map[f2ebebe8-4dfe-4d7b-9a7c-643f66866dd1:[80]], expected map[pod1:[80] pod2:[80]] (4.222232679s elapsed, will retry)
Feb 17 18:01:37.356: INFO: successfully validated that service endpoint-test2 in namespace services-2307 exposes endpoints map[pod1:[80] pod2:[80]] (7.362815s elapsed)
STEP: Deleting pod pod1 in namespace services-2307
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2307 to expose endpoints map[pod2:[80]]
Feb 17 18:01:37.415: INFO: successfully validated that service endpoint-test2 in namespace services-2307 exposes endpoints map[pod2:[80]] (36.199349ms elapsed)
STEP: Deleting pod pod2 in namespace services-2307
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2307 to expose endpoints map[]
Feb 17 18:01:37.460: INFO: successfully validated that service endpoint-test2 in namespace services-2307 exposes endpoints map[] (16.72518ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:01:37.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2307" for this suite.
Feb 17 18:01:51.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:01:53.936: INFO: namespace services-2307 deletion completed in 16.370992922s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:32.475 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:01:53.937: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:02:10.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1566" for this suite.
Feb 17 18:02:18.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:02:20.484: INFO: namespace resourcequota-1566 deletion completed in 10.177726865s

• [SLOW TEST:26.547 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:02:20.486: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-4126775c-d4f1-4512-9a07-72f4bdb0ed44
STEP: Creating a pod to test consume configMaps
Feb 17 18:02:20.693: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-61f0cb90-bb5e-4d36-9771-b580ea5e49a5" in namespace "projected-858" to be "success or failure"
Feb 17 18:02:20.708: INFO: Pod "pod-projected-configmaps-61f0cb90-bb5e-4d36-9771-b580ea5e49a5": Phase="Pending", Reason="", readiness=false. Elapsed: 15.104182ms
Feb 17 18:02:22.722: INFO: Pod "pod-projected-configmaps-61f0cb90-bb5e-4d36-9771-b580ea5e49a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028765543s
Feb 17 18:02:24.735: INFO: Pod "pod-projected-configmaps-61f0cb90-bb5e-4d36-9771-b580ea5e49a5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04168488s
Feb 17 18:02:26.749: INFO: Pod "pod-projected-configmaps-61f0cb90-bb5e-4d36-9771-b580ea5e49a5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055363909s
Feb 17 18:02:28.762: INFO: Pod "pod-projected-configmaps-61f0cb90-bb5e-4d36-9771-b580ea5e49a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068329265s
STEP: Saw pod success
Feb 17 18:02:28.762: INFO: Pod "pod-projected-configmaps-61f0cb90-bb5e-4d36-9771-b580ea5e49a5" satisfied condition "success or failure"
Feb 17 18:02:28.774: INFO: Trying to get logs from node 10.45.66.189 pod pod-projected-configmaps-61f0cb90-bb5e-4d36-9771-b580ea5e49a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:02:28.890: INFO: Waiting for pod pod-projected-configmaps-61f0cb90-bb5e-4d36-9771-b580ea5e49a5 to disappear
Feb 17 18:02:28.903: INFO: Pod pod-projected-configmaps-61f0cb90-bb5e-4d36-9771-b580ea5e49a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:02:28.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-858" for this suite.
Feb 17 18:02:36.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:02:39.130: INFO: namespace projected-858 deletion completed in 10.198851865s

• [SLOW TEST:18.644 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:02:39.132: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:03:21.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3019" for this suite.
Feb 17 18:03:29.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:03:31.560: INFO: namespace container-runtime-3019 deletion completed in 10.17372035s

• [SLOW TEST:52.428 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:03:31.561: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 18:03:31.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 version'
Feb 17 18:03:31.825: INFO: stderr: ""
Feb 17 18:03:31.825: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.2\", GitCommit:\"c97fe5036ef3df2967d086711e6c0c405941e14b\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T19:18:23Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16+\", GitVersion:\"v1.16.2\", GitCommit:\"94e669a\", GitTreeState:\"clean\", BuildDate:\"2020-02-03T23:11:39Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:03:31.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5145" for this suite.
Feb 17 18:03:39.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:03:42.012: INFO: namespace kubectl-5145 deletion completed in 10.147732039s

• [SLOW TEST:10.450 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:03:42.012: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:03:50.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3404" for this suite.
Feb 17 18:04:42.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:04:44.493: INFO: namespace kubelet-test-3404 deletion completed in 54.131216733s

• [SLOW TEST:62.481 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:04:44.494: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-3164
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 17 18:04:44.632: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 17 18:05:19.011: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.50.135 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3164 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:05:19.011: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 18:05:20.257: INFO: Found all expected endpoints: [netserver-0]
Feb 17 18:05:20.270: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.178.253 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3164 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:05:20.270: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 18:05:21.515: INFO: Found all expected endpoints: [netserver-1]
Feb 17 18:05:21.528: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.80.18 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3164 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:05:21.528: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 18:05:22.748: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:05:22.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3164" for this suite.
Feb 17 18:05:30.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:05:32.919: INFO: namespace pod-network-test-3164 deletion completed in 10.145772021s

• [SLOW TEST:48.425 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:05:32.919: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:05:33.090: INFO: Waiting up to 5m0s for pod "downwardapi-volume-91b8a5a8-bdab-4b01-9ff1-db055fee1121" in namespace "downward-api-6136" to be "success or failure"
Feb 17 18:05:33.104: INFO: Pod "downwardapi-volume-91b8a5a8-bdab-4b01-9ff1-db055fee1121": Phase="Pending", Reason="", readiness=false. Elapsed: 13.592185ms
Feb 17 18:05:35.117: INFO: Pod "downwardapi-volume-91b8a5a8-bdab-4b01-9ff1-db055fee1121": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026548989s
Feb 17 18:05:37.130: INFO: Pod "downwardapi-volume-91b8a5a8-bdab-4b01-9ff1-db055fee1121": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039918334s
Feb 17 18:05:39.144: INFO: Pod "downwardapi-volume-91b8a5a8-bdab-4b01-9ff1-db055fee1121": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054035339s
Feb 17 18:05:41.158: INFO: Pod "downwardapi-volume-91b8a5a8-bdab-4b01-9ff1-db055fee1121": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068318236s
STEP: Saw pod success
Feb 17 18:05:41.158: INFO: Pod "downwardapi-volume-91b8a5a8-bdab-4b01-9ff1-db055fee1121" satisfied condition "success or failure"
Feb 17 18:05:41.171: INFO: Trying to get logs from node 10.45.66.177 pod downwardapi-volume-91b8a5a8-bdab-4b01-9ff1-db055fee1121 container client-container: <nil>
STEP: delete the pod
Feb 17 18:05:41.284: INFO: Waiting for pod downwardapi-volume-91b8a5a8-bdab-4b01-9ff1-db055fee1121 to disappear
Feb 17 18:05:41.299: INFO: Pod downwardapi-volume-91b8a5a8-bdab-4b01-9ff1-db055fee1121 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:05:41.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6136" for this suite.
Feb 17 18:05:49.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:05:51.438: INFO: namespace downward-api-6136 deletion completed in 10.115499949s

• [SLOW TEST:18.519 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:05:51.438: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Feb 17 18:06:00.294: INFO: Successfully updated pod "annotationupdatefc2c0d5c-39e1-4855-91ad-2794089a69c6"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:06:02.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6468" for this suite.
Feb 17 18:06:34.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:06:36.522: INFO: namespace projected-6468 deletion completed in 34.147048569s

• [SLOW TEST:45.085 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:06:36.525: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Feb 17 18:06:36.683: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Feb 17 18:07:05.485: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 18:07:12.537: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:07:41.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8083" for this suite.
Feb 17 18:07:49.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:07:51.922: INFO: namespace crd-publish-openapi-8083 deletion completed in 10.197790546s

• [SLOW TEST:75.398 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:07:51.922: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Feb 17 18:07:58.218: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0217 18:07:58.218359      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:07:58.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-0" for this suite.
Feb 17 18:08:06.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:08:08.396: INFO: namespace gc-0 deletion completed in 10.155826766s

• [SLOW TEST:16.474 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:08:08.396: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:08:09.370: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:08:11.409: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559689, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559689, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559689, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559689, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:08:13.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559689, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559689, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559689, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559689, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:08:15.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559689, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559689, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559689, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559689, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:08:17.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559689, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559689, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559689, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717559689, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:08:20.452: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:08:20.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6675" for this suite.
Feb 17 18:08:28.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:08:30.928: INFO: namespace webhook-6675 deletion completed in 10.172430752s
STEP: Destroying namespace "webhook-6675-markers" for this suite.
Feb 17 18:08:38.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:08:41.068: INFO: namespace webhook-6675-markers deletion completed in 10.139823489s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:32.742 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:08:41.138: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4335
I0217 18:08:41.299173      26 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4335, replica count: 1
I0217 18:08:42.349709      26 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 18:08:43.349953      26 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 18:08:44.350220      26 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 18:08:45.350465      26 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 18:08:46.350664      26 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 18:08:47.350880      26 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 18:08:48.351078      26 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 18:08:49.351323      26 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 18:08:49.479: INFO: Created: latency-svc-w8ntc
Feb 17 18:08:49.500: INFO: Got endpoints: latency-svc-w8ntc [48.911314ms]
Feb 17 18:08:49.528: INFO: Created: latency-svc-8flq6
Feb 17 18:08:49.543: INFO: Got endpoints: latency-svc-8flq6 [42.905801ms]
Feb 17 18:08:49.544: INFO: Created: latency-svc-8nwhk
Feb 17 18:08:49.567: INFO: Got endpoints: latency-svc-8nwhk [65.938655ms]
Feb 17 18:08:49.569: INFO: Created: latency-svc-4rrl9
Feb 17 18:08:49.583: INFO: Got endpoints: latency-svc-4rrl9 [82.216997ms]
Feb 17 18:08:49.583: INFO: Created: latency-svc-rgdz2
Feb 17 18:08:49.593: INFO: Created: latency-svc-l6d9p
Feb 17 18:08:49.597: INFO: Got endpoints: latency-svc-rgdz2 [96.591208ms]
Feb 17 18:08:49.604: INFO: Created: latency-svc-rq8fc
Feb 17 18:08:49.608: INFO: Got endpoints: latency-svc-l6d9p [107.044471ms]
Feb 17 18:08:49.617: INFO: Created: latency-svc-7qgfv
Feb 17 18:08:49.618: INFO: Got endpoints: latency-svc-rq8fc [117.078087ms]
Feb 17 18:08:49.627: INFO: Created: latency-svc-z4htq
Feb 17 18:08:49.631: INFO: Got endpoints: latency-svc-7qgfv [130.749066ms]
Feb 17 18:08:49.654: INFO: Got endpoints: latency-svc-z4htq [152.671776ms]
Feb 17 18:08:49.655: INFO: Created: latency-svc-mtwhs
Feb 17 18:08:49.655: INFO: Created: latency-svc-qj69m
Feb 17 18:08:49.659: INFO: Got endpoints: latency-svc-mtwhs [158.196239ms]
Feb 17 18:08:49.667: INFO: Created: latency-svc-bpwj8
Feb 17 18:08:49.669: INFO: Got endpoints: latency-svc-qj69m [168.263444ms]
Feb 17 18:08:49.679: INFO: Created: latency-svc-6hbcx
Feb 17 18:08:49.680: INFO: Got endpoints: latency-svc-bpwj8 [179.090925ms]
Feb 17 18:08:49.690: INFO: Created: latency-svc-bnhqn
Feb 17 18:08:49.692: INFO: Got endpoints: latency-svc-6hbcx [191.139657ms]
Feb 17 18:08:49.703: INFO: Created: latency-svc-69b98
Feb 17 18:08:49.703: INFO: Got endpoints: latency-svc-bnhqn [202.183107ms]
Feb 17 18:08:49.714: INFO: Created: latency-svc-58p8n
Feb 17 18:08:49.716: INFO: Got endpoints: latency-svc-69b98 [214.955583ms]
Feb 17 18:08:49.726: INFO: Created: latency-svc-xvk4b
Feb 17 18:08:49.731: INFO: Got endpoints: latency-svc-58p8n [229.522027ms]
Feb 17 18:08:49.739: INFO: Created: latency-svc-s5n2j
Feb 17 18:08:49.739: INFO: Got endpoints: latency-svc-xvk4b [195.543027ms]
Feb 17 18:08:49.753: INFO: Created: latency-svc-sqdc6
Feb 17 18:08:49.753: INFO: Got endpoints: latency-svc-s5n2j [185.504169ms]
Feb 17 18:08:49.763: INFO: Created: latency-svc-hl5z8
Feb 17 18:08:49.768: INFO: Got endpoints: latency-svc-sqdc6 [185.328664ms]
Feb 17 18:08:49.776: INFO: Created: latency-svc-rrxb2
Feb 17 18:08:49.779: INFO: Got endpoints: latency-svc-hl5z8 [181.896024ms]
Feb 17 18:08:49.793: INFO: Got endpoints: latency-svc-rrxb2 [185.170585ms]
Feb 17 18:08:49.795: INFO: Created: latency-svc-qts5c
Feb 17 18:08:49.806: INFO: Created: latency-svc-vdctg
Feb 17 18:08:49.809: INFO: Got endpoints: latency-svc-qts5c [190.597873ms]
Feb 17 18:08:49.821: INFO: Got endpoints: latency-svc-vdctg [189.138697ms]
Feb 17 18:08:49.821: INFO: Created: latency-svc-j52xx
Feb 17 18:08:49.829: INFO: Created: latency-svc-q577f
Feb 17 18:08:49.839: INFO: Got endpoints: latency-svc-j52xx [185.487896ms]
Feb 17 18:08:49.844: INFO: Created: latency-svc-bjk2f
Feb 17 18:08:49.847: INFO: Got endpoints: latency-svc-q577f [188.084586ms]
Feb 17 18:08:49.859: INFO: Got endpoints: latency-svc-bjk2f [190.121681ms]
Feb 17 18:08:49.861: INFO: Created: latency-svc-lm769
Feb 17 18:08:49.880: INFO: Created: latency-svc-z776c
Feb 17 18:08:49.882: INFO: Got endpoints: latency-svc-lm769 [202.305515ms]
Feb 17 18:08:49.892: INFO: Created: latency-svc-6wtqc
Feb 17 18:08:49.898: INFO: Got endpoints: latency-svc-z776c [50.521553ms]
Feb 17 18:08:49.905: INFO: Created: latency-svc-2lbcn
Feb 17 18:08:49.907: INFO: Got endpoints: latency-svc-6wtqc [214.854633ms]
Feb 17 18:08:49.920: INFO: Created: latency-svc-9z65g
Feb 17 18:08:49.920: INFO: Got endpoints: latency-svc-2lbcn [216.492098ms]
Feb 17 18:08:49.932: INFO: Created: latency-svc-qw2cv
Feb 17 18:08:49.938: INFO: Got endpoints: latency-svc-9z65g [221.378483ms]
Feb 17 18:08:49.944: INFO: Created: latency-svc-sx9zg
Feb 17 18:08:49.945: INFO: Got endpoints: latency-svc-qw2cv [213.913375ms]
Feb 17 18:08:49.958: INFO: Got endpoints: latency-svc-sx9zg [218.669172ms]
Feb 17 18:08:49.959: INFO: Created: latency-svc-kl52c
Feb 17 18:08:49.970: INFO: Created: latency-svc-nhwjr
Feb 17 18:08:49.972: INFO: Got endpoints: latency-svc-kl52c [218.93987ms]
Feb 17 18:08:49.981: INFO: Created: latency-svc-k5zqx
Feb 17 18:08:49.986: INFO: Got endpoints: latency-svc-nhwjr [218.034712ms]
Feb 17 18:08:49.994: INFO: Created: latency-svc-ndbrf
Feb 17 18:08:49.997: INFO: Got endpoints: latency-svc-k5zqx [218.0136ms]
Feb 17 18:08:50.013: INFO: Created: latency-svc-52kxg
Feb 17 18:08:50.013: INFO: Got endpoints: latency-svc-ndbrf [220.169768ms]
Feb 17 18:08:50.023: INFO: Created: latency-svc-fmfq4
Feb 17 18:08:50.023: INFO: Got endpoints: latency-svc-52kxg [214.006429ms]
Feb 17 18:08:50.035: INFO: Created: latency-svc-vq4l7
Feb 17 18:08:50.043: INFO: Got endpoints: latency-svc-fmfq4 [222.161954ms]
Feb 17 18:08:50.048: INFO: Created: latency-svc-w5hsw
Feb 17 18:08:50.051: INFO: Got endpoints: latency-svc-vq4l7 [211.505964ms]
Feb 17 18:08:50.070: INFO: Created: latency-svc-d42mw
Feb 17 18:08:50.070: INFO: Got endpoints: latency-svc-w5hsw [210.160837ms]
Feb 17 18:08:50.084: INFO: Got endpoints: latency-svc-d42mw [202.095732ms]
Feb 17 18:08:50.086: INFO: Created: latency-svc-n8q65
Feb 17 18:08:50.099: INFO: Created: latency-svc-p4kt7
Feb 17 18:08:50.103: INFO: Got endpoints: latency-svc-n8q65 [204.950917ms]
Feb 17 18:08:50.113: INFO: Created: latency-svc-sz922
Feb 17 18:08:50.114: INFO: Got endpoints: latency-svc-p4kt7 [207.041168ms]
Feb 17 18:08:50.128: INFO: Created: latency-svc-nlpfl
Feb 17 18:08:50.130: INFO: Got endpoints: latency-svc-sz922 [209.777748ms]
Feb 17 18:08:50.145: INFO: Got endpoints: latency-svc-nlpfl [207.689613ms]
Feb 17 18:08:50.146: INFO: Created: latency-svc-lfwfx
Feb 17 18:08:50.159: INFO: Created: latency-svc-cqj7n
Feb 17 18:08:50.161: INFO: Got endpoints: latency-svc-lfwfx [216.376407ms]
Feb 17 18:08:50.171: INFO: Created: latency-svc-jq6v6
Feb 17 18:08:50.176: INFO: Got endpoints: latency-svc-cqj7n [217.836742ms]
Feb 17 18:08:50.185: INFO: Created: latency-svc-hb8rw
Feb 17 18:08:50.189: INFO: Got endpoints: latency-svc-jq6v6 [216.762641ms]
Feb 17 18:08:50.197: INFO: Created: latency-svc-lnnv2
Feb 17 18:08:50.203: INFO: Got endpoints: latency-svc-hb8rw [217.037467ms]
Feb 17 18:08:50.214: INFO: Created: latency-svc-dp254
Feb 17 18:08:50.216: INFO: Got endpoints: latency-svc-lnnv2 [218.643771ms]
Feb 17 18:08:50.223: INFO: Created: latency-svc-4td8m
Feb 17 18:08:50.225: INFO: Got endpoints: latency-svc-dp254 [211.264454ms]
Feb 17 18:08:50.237: INFO: Created: latency-svc-pwpcn
Feb 17 18:08:50.238: INFO: Got endpoints: latency-svc-4td8m [215.210955ms]
Feb 17 18:08:50.254: INFO: Got endpoints: latency-svc-pwpcn [210.843601ms]
Feb 17 18:08:50.256: INFO: Created: latency-svc-zlhkc
Feb 17 18:08:50.272: INFO: Created: latency-svc-tj7mn
Feb 17 18:08:50.272: INFO: Got endpoints: latency-svc-zlhkc [221.331917ms]
Feb 17 18:08:50.285: INFO: Created: latency-svc-m9wp8
Feb 17 18:08:50.285: INFO: Got endpoints: latency-svc-tj7mn [215.395702ms]
Feb 17 18:08:50.302: INFO: Got endpoints: latency-svc-m9wp8 [217.356029ms]
Feb 17 18:08:50.304: INFO: Created: latency-svc-qg972
Feb 17 18:08:50.310: INFO: Created: latency-svc-c8nbs
Feb 17 18:08:50.318: INFO: Got endpoints: latency-svc-qg972 [215.530893ms]
Feb 17 18:08:50.323: INFO: Created: latency-svc-8xk7f
Feb 17 18:08:50.324: INFO: Got endpoints: latency-svc-c8nbs [209.974638ms]
Feb 17 18:08:50.335: INFO: Created: latency-svc-lqbpv
Feb 17 18:08:50.336: INFO: Got endpoints: latency-svc-8xk7f [206.201211ms]
Feb 17 18:08:50.353: INFO: Got endpoints: latency-svc-lqbpv [207.62976ms]
Feb 17 18:08:50.355: INFO: Created: latency-svc-lqgkx
Feb 17 18:08:50.370: INFO: Created: latency-svc-g9t2v
Feb 17 18:08:50.374: INFO: Got endpoints: latency-svc-lqgkx [212.339647ms]
Feb 17 18:08:50.379: INFO: Created: latency-svc-9qg2w
Feb 17 18:08:50.382: INFO: Got endpoints: latency-svc-g9t2v [205.464102ms]
Feb 17 18:08:50.392: INFO: Created: latency-svc-97njx
Feb 17 18:08:50.397: INFO: Got endpoints: latency-svc-9qg2w [207.947143ms]
Feb 17 18:08:50.405: INFO: Created: latency-svc-649bm
Feb 17 18:08:50.406: INFO: Got endpoints: latency-svc-97njx [202.207007ms]
Feb 17 18:08:50.417: INFO: Got endpoints: latency-svc-649bm [201.685167ms]
Feb 17 18:08:50.419: INFO: Created: latency-svc-wmkcx
Feb 17 18:08:50.431: INFO: Got endpoints: latency-svc-wmkcx [206.294652ms]
Feb 17 18:08:50.431: INFO: Created: latency-svc-bcz7x
Feb 17 18:08:50.441: INFO: Created: latency-svc-7gdjh
Feb 17 18:08:50.447: INFO: Got endpoints: latency-svc-bcz7x [209.434891ms]
Feb 17 18:08:50.455: INFO: Created: latency-svc-56blt
Feb 17 18:08:50.456: INFO: Got endpoints: latency-svc-7gdjh [202.292254ms]
Feb 17 18:08:50.477: INFO: Got endpoints: latency-svc-56blt [204.625817ms]
Feb 17 18:08:50.477: INFO: Created: latency-svc-mtrz6
Feb 17 18:08:50.484: INFO: Created: latency-svc-9trsp
Feb 17 18:08:50.485: INFO: Got endpoints: latency-svc-mtrz6 [199.264739ms]
Feb 17 18:08:50.497: INFO: Created: latency-svc-brjhj
Feb 17 18:08:50.498: INFO: Got endpoints: latency-svc-9trsp [196.260429ms]
Feb 17 18:08:50.519: INFO: Created: latency-svc-4q8nv
Feb 17 18:08:50.523: INFO: Got endpoints: latency-svc-brjhj [203.97751ms]
Feb 17 18:08:50.531: INFO: Created: latency-svc-rcrxq
Feb 17 18:08:50.542: INFO: Got endpoints: latency-svc-4q8nv [217.832389ms]
Feb 17 18:08:50.546: INFO: Got endpoints: latency-svc-rcrxq [209.381896ms]
Feb 17 18:08:50.550: INFO: Created: latency-svc-wrjj4
Feb 17 18:08:50.565: INFO: Created: latency-svc-v5nzr
Feb 17 18:08:50.572: INFO: Got endpoints: latency-svc-wrjj4 [218.899887ms]
Feb 17 18:08:50.580: INFO: Created: latency-svc-qzf5m
Feb 17 18:08:50.581: INFO: Got endpoints: latency-svc-v5nzr [207.033575ms]
Feb 17 18:08:50.593: INFO: Created: latency-svc-b889p
Feb 17 18:08:50.596: INFO: Got endpoints: latency-svc-qzf5m [214.192722ms]
Feb 17 18:08:50.608: INFO: Got endpoints: latency-svc-b889p [211.275893ms]
Feb 17 18:08:50.613: INFO: Created: latency-svc-qjdvk
Feb 17 18:08:50.627: INFO: Got endpoints: latency-svc-qjdvk [221.25161ms]
Feb 17 18:08:50.627: INFO: Created: latency-svc-lt5rh
Feb 17 18:08:50.649: INFO: Got endpoints: latency-svc-lt5rh [231.497708ms]
Feb 17 18:08:50.653: INFO: Created: latency-svc-qfrrx
Feb 17 18:08:50.666: INFO: Got endpoints: latency-svc-qfrrx [234.632886ms]
Feb 17 18:08:50.669: INFO: Created: latency-svc-mn7qz
Feb 17 18:08:50.681: INFO: Created: latency-svc-blwhq
Feb 17 18:08:50.687: INFO: Got endpoints: latency-svc-mn7qz [239.164173ms]
Feb 17 18:08:50.698: INFO: Created: latency-svc-swh8q
Feb 17 18:08:50.705: INFO: Got endpoints: latency-svc-blwhq [248.206914ms]
Feb 17 18:08:50.711: INFO: Created: latency-svc-b7ddq
Feb 17 18:08:50.712: INFO: Got endpoints: latency-svc-swh8q [234.745178ms]
Feb 17 18:08:50.723: INFO: Created: latency-svc-67cwf
Feb 17 18:08:50.725: INFO: Got endpoints: latency-svc-b7ddq [240.449643ms]
Feb 17 18:08:50.735: INFO: Created: latency-svc-8hqrz
Feb 17 18:08:50.740: INFO: Got endpoints: latency-svc-67cwf [241.914814ms]
Feb 17 18:08:50.750: INFO: Got endpoints: latency-svc-8hqrz [227.559349ms]
Feb 17 18:08:50.753: INFO: Created: latency-svc-pqpkf
Feb 17 18:08:50.770: INFO: Got endpoints: latency-svc-pqpkf [228.472284ms]
Feb 17 18:08:50.779: INFO: Created: latency-svc-gmmqz
Feb 17 18:08:50.790: INFO: Created: latency-svc-9rbmt
Feb 17 18:08:50.804: INFO: Created: latency-svc-c8wtr
Feb 17 18:08:50.806: INFO: Got endpoints: latency-svc-gmmqz [259.975326ms]
Feb 17 18:08:50.808: INFO: Got endpoints: latency-svc-9rbmt [235.788469ms]
Feb 17 18:08:50.817: INFO: Created: latency-svc-vgz8n
Feb 17 18:08:50.818: INFO: Got endpoints: latency-svc-c8wtr [237.151662ms]
Feb 17 18:08:50.831: INFO: Created: latency-svc-p95gn
Feb 17 18:08:50.832: INFO: Got endpoints: latency-svc-vgz8n [236.410597ms]
Feb 17 18:08:50.843: INFO: Created: latency-svc-rkv7p
Feb 17 18:08:50.845: INFO: Got endpoints: latency-svc-p95gn [236.571254ms]
Feb 17 18:08:50.856: INFO: Created: latency-svc-bkn9w
Feb 17 18:08:50.857: INFO: Got endpoints: latency-svc-rkv7p [229.610661ms]
Feb 17 18:08:50.871: INFO: Created: latency-svc-bkqj6
Feb 17 18:08:50.875: INFO: Got endpoints: latency-svc-bkn9w [225.213937ms]
Feb 17 18:08:50.888: INFO: Created: latency-svc-62hnk
Feb 17 18:08:50.888: INFO: Got endpoints: latency-svc-bkqj6 [222.482749ms]
Feb 17 18:08:50.898: INFO: Created: latency-svc-wrvpm
Feb 17 18:08:50.898: INFO: Got endpoints: latency-svc-62hnk [211.577992ms]
Feb 17 18:08:50.910: INFO: Created: latency-svc-tlldz
Feb 17 18:08:50.911: INFO: Got endpoints: latency-svc-wrvpm [206.43138ms]
Feb 17 18:08:50.922: INFO: Created: latency-svc-6rb55
Feb 17 18:08:50.924: INFO: Got endpoints: latency-svc-tlldz [212.057615ms]
Feb 17 18:08:50.936: INFO: Created: latency-svc-x8ds9
Feb 17 18:08:50.939: INFO: Got endpoints: latency-svc-6rb55 [213.253791ms]
Feb 17 18:08:50.948: INFO: Created: latency-svc-ssxwv
Feb 17 18:08:50.950: INFO: Got endpoints: latency-svc-x8ds9 [209.424726ms]
Feb 17 18:08:50.961: INFO: Got endpoints: latency-svc-ssxwv [210.940634ms]
Feb 17 18:08:50.962: INFO: Created: latency-svc-j8kxx
Feb 17 18:08:50.975: INFO: Created: latency-svc-f8wh7
Feb 17 18:08:50.976: INFO: Got endpoints: latency-svc-j8kxx [205.822702ms]
Feb 17 18:08:50.987: INFO: Created: latency-svc-nvmn6
Feb 17 18:08:50.988: INFO: Got endpoints: latency-svc-f8wh7 [182.00509ms]
Feb 17 18:08:51.002: INFO: Created: latency-svc-j7wcs
Feb 17 18:08:51.004: INFO: Got endpoints: latency-svc-nvmn6 [195.986092ms]
Feb 17 18:08:51.015: INFO: Created: latency-svc-pf988
Feb 17 18:08:51.016: INFO: Got endpoints: latency-svc-j7wcs [197.985316ms]
Feb 17 18:08:51.037: INFO: Got endpoints: latency-svc-pf988 [204.781478ms]
Feb 17 18:08:51.038: INFO: Created: latency-svc-spxb2
Feb 17 18:08:51.041: INFO: Created: latency-svc-jsm5w
Feb 17 18:08:51.047: INFO: Got endpoints: latency-svc-spxb2 [202.012984ms]
Feb 17 18:08:51.053: INFO: Got endpoints: latency-svc-jsm5w [195.928885ms]
Feb 17 18:08:51.054: INFO: Created: latency-svc-4n5q7
Feb 17 18:08:51.068: INFO: Created: latency-svc-hdmhc
Feb 17 18:08:51.071: INFO: Got endpoints: latency-svc-4n5q7 [195.878974ms]
Feb 17 18:08:51.082: INFO: Got endpoints: latency-svc-hdmhc [193.741486ms]
Feb 17 18:08:51.083: INFO: Created: latency-svc-bp6z5
Feb 17 18:08:51.097: INFO: Created: latency-svc-2crnt
Feb 17 18:08:51.098: INFO: Got endpoints: latency-svc-bp6z5 [199.912139ms]
Feb 17 18:08:51.108: INFO: Created: latency-svc-q9wwc
Feb 17 18:08:51.111: INFO: Got endpoints: latency-svc-2crnt [200.143468ms]
Feb 17 18:08:51.118: INFO: Created: latency-svc-rvmwc
Feb 17 18:08:51.121: INFO: Got endpoints: latency-svc-q9wwc [196.722525ms]
Feb 17 18:08:51.133: INFO: Created: latency-svc-xfpz9
Feb 17 18:08:51.136: INFO: Got endpoints: latency-svc-rvmwc [196.907632ms]
Feb 17 18:08:51.143: INFO: Created: latency-svc-kngh9
Feb 17 18:08:51.147: INFO: Got endpoints: latency-svc-xfpz9 [197.526046ms]
Feb 17 18:08:51.158: INFO: Got endpoints: latency-svc-kngh9 [196.709852ms]
Feb 17 18:08:51.158: INFO: Created: latency-svc-5n8bq
Feb 17 18:08:51.176: INFO: Created: latency-svc-s4sjf
Feb 17 18:08:51.176: INFO: Got endpoints: latency-svc-5n8bq [199.962774ms]
Feb 17 18:08:51.188: INFO: Created: latency-svc-bmf5x
Feb 17 18:08:51.191: INFO: Got endpoints: latency-svc-s4sjf [203.20502ms]
Feb 17 18:08:51.201: INFO: Created: latency-svc-svk7h
Feb 17 18:08:51.204: INFO: Got endpoints: latency-svc-bmf5x [199.771036ms]
Feb 17 18:08:51.235: INFO: Created: latency-svc-j7v86
Feb 17 18:08:51.235: INFO: Got endpoints: latency-svc-svk7h [218.692793ms]
Feb 17 18:08:51.236: INFO: Created: latency-svc-hkrt4
Feb 17 18:08:51.240: INFO: Created: latency-svc-kpr9p
Feb 17 18:08:51.240: INFO: Got endpoints: latency-svc-hkrt4 [202.322226ms]
Feb 17 18:08:51.244: INFO: Got endpoints: latency-svc-j7v86 [196.970699ms]
Feb 17 18:08:51.254: INFO: Created: latency-svc-h22rl
Feb 17 18:08:51.255: INFO: Got endpoints: latency-svc-kpr9p [201.506916ms]
Feb 17 18:08:51.268: INFO: Created: latency-svc-4z72d
Feb 17 18:08:51.268: INFO: Got endpoints: latency-svc-h22rl [197.493713ms]
Feb 17 18:08:51.281: INFO: Created: latency-svc-nzph5
Feb 17 18:08:51.285: INFO: Got endpoints: latency-svc-4z72d [203.073182ms]
Feb 17 18:08:51.293: INFO: Created: latency-svc-p87gb
Feb 17 18:08:51.296: INFO: Got endpoints: latency-svc-nzph5 [197.245619ms]
Feb 17 18:08:51.305: INFO: Created: latency-svc-8kkf4
Feb 17 18:08:51.309: INFO: Got endpoints: latency-svc-p87gb [197.977554ms]
Feb 17 18:08:51.317: INFO: Created: latency-svc-9vwkv
Feb 17 18:08:51.319: INFO: Got endpoints: latency-svc-8kkf4 [198.128875ms]
Feb 17 18:08:51.328: INFO: Created: latency-svc-g5dcf
Feb 17 18:08:51.330: INFO: Got endpoints: latency-svc-9vwkv [193.904207ms]
Feb 17 18:08:51.344: INFO: Created: latency-svc-fk7qf
Feb 17 18:08:51.344: INFO: Got endpoints: latency-svc-g5dcf [196.25435ms]
Feb 17 18:08:51.356: INFO: Created: latency-svc-vd56d
Feb 17 18:08:51.367: INFO: Got endpoints: latency-svc-fk7qf [208.873037ms]
Feb 17 18:08:51.368: INFO: Created: latency-svc-fwhlk
Feb 17 18:08:51.373: INFO: Got endpoints: latency-svc-vd56d [197.132363ms]
Feb 17 18:08:51.380: INFO: Created: latency-svc-g76fw
Feb 17 18:08:51.381: INFO: Got endpoints: latency-svc-fwhlk [190.062599ms]
Feb 17 18:08:51.392: INFO: Created: latency-svc-nbqwg
Feb 17 18:08:51.394: INFO: Got endpoints: latency-svc-g76fw [189.432494ms]
Feb 17 18:08:51.411: INFO: Got endpoints: latency-svc-nbqwg [176.066554ms]
Feb 17 18:08:51.412: INFO: Created: latency-svc-czfpk
Feb 17 18:08:51.420: INFO: Created: latency-svc-scx4d
Feb 17 18:08:51.425: INFO: Got endpoints: latency-svc-czfpk [185.064624ms]
Feb 17 18:08:51.445: INFO: Got endpoints: latency-svc-scx4d [200.604611ms]
Feb 17 18:08:51.445: INFO: Created: latency-svc-gcmtb
Feb 17 18:08:51.447: INFO: Got endpoints: latency-svc-gcmtb [192.472797ms]
Feb 17 18:08:51.447: INFO: Created: latency-svc-pdt6n
Feb 17 18:08:51.460: INFO: Created: latency-svc-vlz62
Feb 17 18:08:51.460: INFO: Got endpoints: latency-svc-pdt6n [192.127085ms]
Feb 17 18:08:51.473: INFO: Created: latency-svc-mpffj
Feb 17 18:08:51.474: INFO: Got endpoints: latency-svc-vlz62 [188.859874ms]
Feb 17 18:08:51.486: INFO: Created: latency-svc-7bhkr
Feb 17 18:08:51.488: INFO: Got endpoints: latency-svc-mpffj [192.436284ms]
Feb 17 18:08:51.499: INFO: Created: latency-svc-6z6n5
Feb 17 18:08:51.502: INFO: Got endpoints: latency-svc-7bhkr [192.354229ms]
Feb 17 18:08:51.513: INFO: Got endpoints: latency-svc-6z6n5 [193.895984ms]
Feb 17 18:08:51.514: INFO: Created: latency-svc-2w4wk
Feb 17 18:08:51.527: INFO: Created: latency-svc-d4qss
Feb 17 18:08:51.528: INFO: Got endpoints: latency-svc-2w4wk [197.98951ms]
Feb 17 18:08:51.539: INFO: Created: latency-svc-bpc7x
Feb 17 18:08:51.540: INFO: Got endpoints: latency-svc-d4qss [196.10104ms]
Feb 17 18:08:51.568: INFO: Got endpoints: latency-svc-bpc7x [201.01301ms]
Feb 17 18:08:51.569: INFO: Created: latency-svc-7vrdh
Feb 17 18:08:51.569: INFO: Got endpoints: latency-svc-7vrdh [195.236959ms]
Feb 17 18:08:51.571: INFO: Created: latency-svc-lprzv
Feb 17 18:08:51.581: INFO: Created: latency-svc-zh4jx
Feb 17 18:08:51.581: INFO: Got endpoints: latency-svc-lprzv [200.063605ms]
Feb 17 18:08:51.594: INFO: Got endpoints: latency-svc-zh4jx [199.936417ms]
Feb 17 18:08:51.594: INFO: Created: latency-svc-trzwk
Feb 17 18:08:51.604: INFO: Created: latency-svc-9wqjf
Feb 17 18:08:51.606: INFO: Got endpoints: latency-svc-trzwk [194.808146ms]
Feb 17 18:08:51.618: INFO: Created: latency-svc-gzp6p
Feb 17 18:08:51.631: INFO: Created: latency-svc-p7lgd
Feb 17 18:08:51.632: INFO: Got endpoints: latency-svc-9wqjf [186.356004ms]
Feb 17 18:08:51.632: INFO: Got endpoints: latency-svc-gzp6p [187.143239ms]
Feb 17 18:08:51.643: INFO: Created: latency-svc-zz9kd
Feb 17 18:08:51.645: INFO: Got endpoints: latency-svc-p7lgd [197.191438ms]
Feb 17 18:08:51.662: INFO: Created: latency-svc-mw4b2
Feb 17 18:08:51.666: INFO: Got endpoints: latency-svc-zz9kd [205.59765ms]
Feb 17 18:08:51.677: INFO: Created: latency-svc-lr2nb
Feb 17 18:08:51.682: INFO: Got endpoints: latency-svc-mw4b2 [207.985978ms]
Feb 17 18:08:51.687: INFO: Created: latency-svc-5n445
Feb 17 18:08:51.693: INFO: Got endpoints: latency-svc-lr2nb [204.810077ms]
Feb 17 18:08:51.701: INFO: Created: latency-svc-xhrtj
Feb 17 18:08:51.702: INFO: Got endpoints: latency-svc-5n445 [200.386522ms]
Feb 17 18:08:51.716: INFO: Got endpoints: latency-svc-xhrtj [203.164436ms]
Feb 17 18:08:51.717: INFO: Created: latency-svc-n6mwz
Feb 17 18:08:51.727: INFO: Got endpoints: latency-svc-n6mwz [198.936671ms]
Feb 17 18:08:51.728: INFO: Created: latency-svc-rd7f6
Feb 17 18:08:51.739: INFO: Created: latency-svc-5vttd
Feb 17 18:08:51.745: INFO: Got endpoints: latency-svc-rd7f6 [204.732692ms]
Feb 17 18:08:51.754: INFO: Created: latency-svc-4fgt9
Feb 17 18:08:51.756: INFO: Got endpoints: latency-svc-5vttd [188.250188ms]
Feb 17 18:08:51.768: INFO: Created: latency-svc-sg2c9
Feb 17 18:08:51.769: INFO: Got endpoints: latency-svc-4fgt9 [200.259629ms]
Feb 17 18:08:51.784: INFO: Created: latency-svc-5pgnl
Feb 17 18:08:51.784: INFO: Got endpoints: latency-svc-sg2c9 [202.778263ms]
Feb 17 18:08:51.796: INFO: Created: latency-svc-8qkpx
Feb 17 18:08:51.797: INFO: Got endpoints: latency-svc-5pgnl [203.185747ms]
Feb 17 18:08:51.810: INFO: Created: latency-svc-rmzd5
Feb 17 18:08:51.810: INFO: Got endpoints: latency-svc-8qkpx [204.040697ms]
Feb 17 18:08:51.822: INFO: Created: latency-svc-z2z58
Feb 17 18:08:51.824: INFO: Got endpoints: latency-svc-rmzd5 [192.167755ms]
Feb 17 18:08:51.836: INFO: Got endpoints: latency-svc-z2z58 [203.122715ms]
Feb 17 18:08:51.836: INFO: Created: latency-svc-qfrb7
Feb 17 18:08:51.848: INFO: Created: latency-svc-6njpj
Feb 17 18:08:51.851: INFO: Got endpoints: latency-svc-qfrb7 [206.476471ms]
Feb 17 18:08:51.860: INFO: Created: latency-svc-tctvh
Feb 17 18:08:51.862: INFO: Got endpoints: latency-svc-6njpj [194.705633ms]
Feb 17 18:08:51.874: INFO: Got endpoints: latency-svc-tctvh [191.43589ms]
Feb 17 18:08:51.875: INFO: Created: latency-svc-2qxq4
Feb 17 18:08:51.889: INFO: Got endpoints: latency-svc-2qxq4 [195.879193ms]
Feb 17 18:08:51.889: INFO: Created: latency-svc-tzcm7
Feb 17 18:08:51.901: INFO: Created: latency-svc-blfjx
Feb 17 18:08:51.905: INFO: Got endpoints: latency-svc-tzcm7 [202.532003ms]
Feb 17 18:08:51.912: INFO: Created: latency-svc-vmnmq
Feb 17 18:08:51.918: INFO: Got endpoints: latency-svc-blfjx [201.0983ms]
Feb 17 18:08:51.924: INFO: Created: latency-svc-m68rm
Feb 17 18:08:51.925: INFO: Got endpoints: latency-svc-vmnmq [197.817453ms]
Feb 17 18:08:51.938: INFO: Created: latency-svc-vkjbd
Feb 17 18:08:51.938: INFO: Got endpoints: latency-svc-m68rm [193.106661ms]
Feb 17 18:08:51.952: INFO: Created: latency-svc-cdbpw
Feb 17 18:08:51.960: INFO: Created: latency-svc-xdfjj
Feb 17 18:08:51.960: INFO: Got endpoints: latency-svc-vkjbd [203.523508ms]
Feb 17 18:08:51.968: INFO: Got endpoints: latency-svc-cdbpw [198.530728ms]
Feb 17 18:08:51.973: INFO: Created: latency-svc-p49fs
Feb 17 18:08:51.981: INFO: Got endpoints: latency-svc-xdfjj [196.71934ms]
Feb 17 18:08:51.985: INFO: Created: latency-svc-lzhfb
Feb 17 18:08:51.994: INFO: Got endpoints: latency-svc-p49fs [196.881269ms]
Feb 17 18:08:51.998: INFO: Created: latency-svc-lzj84
Feb 17 18:08:52.003: INFO: Got endpoints: latency-svc-lzhfb [192.168433ms]
Feb 17 18:08:52.011: INFO: Created: latency-svc-5qrgt
Feb 17 18:08:52.012: INFO: Got endpoints: latency-svc-lzj84 [187.611022ms]
Feb 17 18:08:52.024: INFO: Created: latency-svc-2vtd8
Feb 17 18:08:52.025: INFO: Got endpoints: latency-svc-5qrgt [188.721352ms]
Feb 17 18:08:52.037: INFO: Created: latency-svc-56w2q
Feb 17 18:08:52.042: INFO: Got endpoints: latency-svc-2vtd8 [190.575664ms]
Feb 17 18:08:52.049: INFO: Created: latency-svc-4pxjz
Feb 17 18:08:52.077: INFO: Created: latency-svc-2vmlb
Feb 17 18:08:52.078: INFO: Got endpoints: latency-svc-4pxjz [203.513347ms]
Feb 17 18:08:52.078: INFO: Got endpoints: latency-svc-56w2q [216.079288ms]
Feb 17 18:08:52.081: INFO: Created: latency-svc-w867v
Feb 17 18:08:52.081: INFO: Got endpoints: latency-svc-2vmlb [191.765015ms]
Feb 17 18:08:52.100: INFO: Created: latency-svc-xbmfl
Feb 17 18:08:52.100: INFO: Got endpoints: latency-svc-w867v [195.142535ms]
Feb 17 18:08:52.111: INFO: Created: latency-svc-t45cf
Feb 17 18:08:52.114: INFO: Got endpoints: latency-svc-xbmfl [195.641949ms]
Feb 17 18:08:52.123: INFO: Created: latency-svc-479ms
Feb 17 18:08:52.126: INFO: Got endpoints: latency-svc-t45cf [200.857358ms]
Feb 17 18:08:52.138: INFO: Created: latency-svc-zssdn
Feb 17 18:08:52.138: INFO: Got endpoints: latency-svc-479ms [199.200918ms]
Feb 17 18:08:52.154: INFO: Created: latency-svc-tbx8w
Feb 17 18:08:52.160: INFO: Got endpoints: latency-svc-zssdn [199.488998ms]
Feb 17 18:08:52.167: INFO: Created: latency-svc-mqfcp
Feb 17 18:08:52.169: INFO: Got endpoints: latency-svc-tbx8w [200.958935ms]
Feb 17 18:08:52.178: INFO: Created: latency-svc-vtghz
Feb 17 18:08:52.183: INFO: Got endpoints: latency-svc-mqfcp [201.525309ms]
Feb 17 18:08:52.192: INFO: Created: latency-svc-xcj4m
Feb 17 18:08:52.192: INFO: Got endpoints: latency-svc-vtghz [197.886646ms]
Feb 17 18:08:52.206: INFO: Got endpoints: latency-svc-xcj4m [203.37325ms]
Feb 17 18:08:52.208: INFO: Created: latency-svc-skh7n
Feb 17 18:08:52.221: INFO: Created: latency-svc-hd9jp
Feb 17 18:08:52.224: INFO: Got endpoints: latency-svc-skh7n [211.971221ms]
Feb 17 18:08:52.231: INFO: Created: latency-svc-tsm7n
Feb 17 18:08:52.239: INFO: Got endpoints: latency-svc-hd9jp [214.009332ms]
Feb 17 18:08:52.245: INFO: Created: latency-svc-d6fcq
Feb 17 18:08:52.246: INFO: Got endpoints: latency-svc-tsm7n [203.420351ms]
Feb 17 18:08:52.257: INFO: Got endpoints: latency-svc-d6fcq [179.513795ms]
Feb 17 18:08:52.257: INFO: Latencies: [42.905801ms 50.521553ms 65.938655ms 82.216997ms 96.591208ms 107.044471ms 117.078087ms 130.749066ms 152.671776ms 158.196239ms 168.263444ms 176.066554ms 179.090925ms 179.513795ms 181.896024ms 182.00509ms 185.064624ms 185.170585ms 185.328664ms 185.487896ms 185.504169ms 186.356004ms 187.143239ms 187.611022ms 188.084586ms 188.250188ms 188.721352ms 188.859874ms 189.138697ms 189.432494ms 190.062599ms 190.121681ms 190.575664ms 190.597873ms 191.139657ms 191.43589ms 191.765015ms 192.127085ms 192.167755ms 192.168433ms 192.354229ms 192.436284ms 192.472797ms 193.106661ms 193.741486ms 193.895984ms 193.904207ms 194.705633ms 194.808146ms 195.142535ms 195.236959ms 195.543027ms 195.641949ms 195.878974ms 195.879193ms 195.928885ms 195.986092ms 196.10104ms 196.25435ms 196.260429ms 196.709852ms 196.71934ms 196.722525ms 196.881269ms 196.907632ms 196.970699ms 197.132363ms 197.191438ms 197.245619ms 197.493713ms 197.526046ms 197.817453ms 197.886646ms 197.977554ms 197.985316ms 197.98951ms 198.128875ms 198.530728ms 198.936671ms 199.200918ms 199.264739ms 199.488998ms 199.771036ms 199.912139ms 199.936417ms 199.962774ms 200.063605ms 200.143468ms 200.259629ms 200.386522ms 200.604611ms 200.857358ms 200.958935ms 201.01301ms 201.0983ms 201.506916ms 201.525309ms 201.685167ms 202.012984ms 202.095732ms 202.183107ms 202.207007ms 202.292254ms 202.305515ms 202.322226ms 202.532003ms 202.778263ms 203.073182ms 203.122715ms 203.164436ms 203.185747ms 203.20502ms 203.37325ms 203.420351ms 203.513347ms 203.523508ms 203.97751ms 204.040697ms 204.625817ms 204.732692ms 204.781478ms 204.810077ms 204.950917ms 205.464102ms 205.59765ms 205.822702ms 206.201211ms 206.294652ms 206.43138ms 206.476471ms 207.033575ms 207.041168ms 207.62976ms 207.689613ms 207.947143ms 207.985978ms 208.873037ms 209.381896ms 209.424726ms 209.434891ms 209.777748ms 209.974638ms 210.160837ms 210.843601ms 210.940634ms 211.264454ms 211.275893ms 211.505964ms 211.577992ms 211.971221ms 212.057615ms 212.339647ms 213.253791ms 213.913375ms 214.006429ms 214.009332ms 214.192722ms 214.854633ms 214.955583ms 215.210955ms 215.395702ms 215.530893ms 216.079288ms 216.376407ms 216.492098ms 216.762641ms 217.037467ms 217.356029ms 217.832389ms 217.836742ms 218.0136ms 218.034712ms 218.643771ms 218.669172ms 218.692793ms 218.899887ms 218.93987ms 220.169768ms 221.25161ms 221.331917ms 221.378483ms 222.161954ms 222.482749ms 225.213937ms 227.559349ms 228.472284ms 229.522027ms 229.610661ms 231.497708ms 234.632886ms 234.745178ms 235.788469ms 236.410597ms 236.571254ms 237.151662ms 239.164173ms 240.449643ms 241.914814ms 248.206914ms 259.975326ms]
Feb 17 18:08:52.258: INFO: 50 %ile: 202.183107ms
Feb 17 18:08:52.258: INFO: 90 %ile: 221.378483ms
Feb 17 18:08:52.258: INFO: 99 %ile: 248.206914ms
Feb 17 18:08:52.258: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:08:52.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4335" for this suite.
Feb 17 18:09:20.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:09:22.463: INFO: namespace svc-latency-4335 deletion completed in 30.171972442s

• [SLOW TEST:41.324 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:09:22.465: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Feb 17 18:09:22.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 create -f - --namespace=kubectl-2014'
Feb 17 18:09:23.362: INFO: stderr: ""
Feb 17 18:09:23.362: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 18:09:23.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2014'
Feb 17 18:09:23.520: INFO: stderr: ""
Feb 17 18:09:23.520: INFO: stdout: "update-demo-nautilus-2zxh8 update-demo-nautilus-z727w "
Feb 17 18:09:23.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-2zxh8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2014'
Feb 17 18:09:23.655: INFO: stderr: ""
Feb 17 18:09:23.655: INFO: stdout: ""
Feb 17 18:09:23.655: INFO: update-demo-nautilus-2zxh8 is created but not running
Feb 17 18:09:28.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2014'
Feb 17 18:09:28.811: INFO: stderr: ""
Feb 17 18:09:28.811: INFO: stdout: "update-demo-nautilus-2zxh8 update-demo-nautilus-z727w "
Feb 17 18:09:28.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-2zxh8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2014'
Feb 17 18:09:28.964: INFO: stderr: ""
Feb 17 18:09:28.964: INFO: stdout: ""
Feb 17 18:09:28.964: INFO: update-demo-nautilus-2zxh8 is created but not running
Feb 17 18:09:33.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2014'
Feb 17 18:09:34.116: INFO: stderr: ""
Feb 17 18:09:34.116: INFO: stdout: "update-demo-nautilus-2zxh8 update-demo-nautilus-z727w "
Feb 17 18:09:34.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-2zxh8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2014'
Feb 17 18:09:34.268: INFO: stderr: ""
Feb 17 18:09:34.268: INFO: stdout: "true"
Feb 17 18:09:34.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-2zxh8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2014'
Feb 17 18:09:34.400: INFO: stderr: ""
Feb 17 18:09:34.400: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 18:09:34.400: INFO: validating pod update-demo-nautilus-2zxh8
Feb 17 18:09:34.431: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 18:09:34.431: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 18:09:34.431: INFO: update-demo-nautilus-2zxh8 is verified up and running
Feb 17 18:09:34.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-z727w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2014'
Feb 17 18:09:34.582: INFO: stderr: ""
Feb 17 18:09:34.582: INFO: stdout: "true"
Feb 17 18:09:34.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-nautilus-z727w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2014'
Feb 17 18:09:34.712: INFO: stderr: ""
Feb 17 18:09:34.712: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Feb 17 18:09:34.712: INFO: validating pod update-demo-nautilus-z727w
Feb 17 18:09:34.744: INFO: got data: {
  "image": "nautilus.jpg"
}

Feb 17 18:09:34.744: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Feb 17 18:09:34.744: INFO: update-demo-nautilus-z727w is verified up and running
STEP: rolling-update to new replication controller
Feb 17 18:09:34.749: INFO: scanned /root for discovery docs: <nil>
Feb 17 18:09:34.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2014'
Feb 17 18:10:12.112: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Feb 17 18:10:12.112: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Feb 17 18:10:12.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2014'
Feb 17 18:10:12.255: INFO: stderr: ""
Feb 17 18:10:12.255: INFO: stdout: "update-demo-kitten-4rtvq update-demo-kitten-kpgst "
Feb 17 18:10:12.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-kitten-4rtvq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2014'
Feb 17 18:10:12.391: INFO: stderr: ""
Feb 17 18:10:12.391: INFO: stdout: "true"
Feb 17 18:10:12.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-kitten-4rtvq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2014'
Feb 17 18:10:12.543: INFO: stderr: ""
Feb 17 18:10:12.543: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 17 18:10:12.543: INFO: validating pod update-demo-kitten-4rtvq
Feb 17 18:10:12.575: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 17 18:10:12.575: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 17 18:10:12.575: INFO: update-demo-kitten-4rtvq is verified up and running
Feb 17 18:10:12.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-kitten-kpgst -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2014'
Feb 17 18:10:12.687: INFO: stderr: ""
Feb 17 18:10:12.688: INFO: stdout: "true"
Feb 17 18:10:12.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 get pods update-demo-kitten-kpgst -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2014'
Feb 17 18:10:12.826: INFO: stderr: ""
Feb 17 18:10:12.826: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Feb 17 18:10:12.826: INFO: validating pod update-demo-kitten-kpgst
Feb 17 18:10:12.857: INFO: got data: {
  "image": "kitten.jpg"
}

Feb 17 18:10:12.857: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Feb 17 18:10:12.857: INFO: update-demo-kitten-kpgst is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:10:12.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2014" for this suite.
Feb 17 18:10:26.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:10:29.112: INFO: namespace kubectl-2014 deletion completed in 16.202432531s

• [SLOW TEST:66.647 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:10:29.113: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Feb 17 18:10:38.010: INFO: Successfully updated pod "labelsupdate2af09c16-264c-4637-9dbd-3d6e1b34584a"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:10:40.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9155" for this suite.
Feb 17 18:11:12.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:11:14.253: INFO: namespace projected-9155 deletion completed in 34.126578966s

• [SLOW TEST:45.140 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:11:14.254: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-44785131-a936-418a-9066-6f3b1d3a58db
STEP: Creating a pod to test consume configMaps
Feb 17 18:11:14.448: INFO: Waiting up to 5m0s for pod "pod-configmaps-d533f721-ea88-49de-8b58-ed71190f4c5a" in namespace "configmap-6342" to be "success or failure"
Feb 17 18:11:14.460: INFO: Pod "pod-configmaps-d533f721-ea88-49de-8b58-ed71190f4c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.091566ms
Feb 17 18:11:16.472: INFO: Pod "pod-configmaps-d533f721-ea88-49de-8b58-ed71190f4c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024158714s
Feb 17 18:11:18.486: INFO: Pod "pod-configmaps-d533f721-ea88-49de-8b58-ed71190f4c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037551088s
Feb 17 18:11:20.499: INFO: Pod "pod-configmaps-d533f721-ea88-49de-8b58-ed71190f4c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050720641s
Feb 17 18:11:22.517: INFO: Pod "pod-configmaps-d533f721-ea88-49de-8b58-ed71190f4c5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068381224s
STEP: Saw pod success
Feb 17 18:11:22.517: INFO: Pod "pod-configmaps-d533f721-ea88-49de-8b58-ed71190f4c5a" satisfied condition "success or failure"
Feb 17 18:11:22.529: INFO: Trying to get logs from node 10.45.66.178 pod pod-configmaps-d533f721-ea88-49de-8b58-ed71190f4c5a container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:11:22.649: INFO: Waiting for pod pod-configmaps-d533f721-ea88-49de-8b58-ed71190f4c5a to disappear
Feb 17 18:11:22.661: INFO: Pod pod-configmaps-d533f721-ea88-49de-8b58-ed71190f4c5a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:11:22.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6342" for this suite.
Feb 17 18:11:30.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:11:32.833: INFO: namespace configmap-6342 deletion completed in 10.127182861s

• [SLOW TEST:18.579 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:11:32.835: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Feb 17 18:11:33.035: INFO: Waiting up to 5m0s for pod "downward-api-61e860ac-fffc-441d-a8cf-26c665e7ef04" in namespace "downward-api-1071" to be "success or failure"
Feb 17 18:11:33.052: INFO: Pod "downward-api-61e860ac-fffc-441d-a8cf-26c665e7ef04": Phase="Pending", Reason="", readiness=false. Elapsed: 16.431526ms
Feb 17 18:11:35.064: INFO: Pod "downward-api-61e860ac-fffc-441d-a8cf-26c665e7ef04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02920109s
Feb 17 18:11:37.081: INFO: Pod "downward-api-61e860ac-fffc-441d-a8cf-26c665e7ef04": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045695276s
Feb 17 18:11:39.095: INFO: Pod "downward-api-61e860ac-fffc-441d-a8cf-26c665e7ef04": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060063314s
Feb 17 18:11:41.110: INFO: Pod "downward-api-61e860ac-fffc-441d-a8cf-26c665e7ef04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.074864316s
STEP: Saw pod success
Feb 17 18:11:41.110: INFO: Pod "downward-api-61e860ac-fffc-441d-a8cf-26c665e7ef04" satisfied condition "success or failure"
Feb 17 18:11:41.122: INFO: Trying to get logs from node 10.45.66.189 pod downward-api-61e860ac-fffc-441d-a8cf-26c665e7ef04 container dapi-container: <nil>
STEP: delete the pod
Feb 17 18:11:41.229: INFO: Waiting for pod downward-api-61e860ac-fffc-441d-a8cf-26c665e7ef04 to disappear
Feb 17 18:11:41.241: INFO: Pod downward-api-61e860ac-fffc-441d-a8cf-26c665e7ef04 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:11:41.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1071" for this suite.
Feb 17 18:11:49.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:11:51.408: INFO: namespace downward-api-1071 deletion completed in 10.13908757s

• [SLOW TEST:18.573 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:11:51.409: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Feb 17 18:11:51.598: INFO: Waiting up to 5m0s for pod "var-expansion-712edf56-a5aa-4d21-b5c6-d508dea3f75b" in namespace "var-expansion-3962" to be "success or failure"
Feb 17 18:11:51.611: INFO: Pod "var-expansion-712edf56-a5aa-4d21-b5c6-d508dea3f75b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.119975ms
Feb 17 18:11:53.625: INFO: Pod "var-expansion-712edf56-a5aa-4d21-b5c6-d508dea3f75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026529447s
Feb 17 18:11:55.638: INFO: Pod "var-expansion-712edf56-a5aa-4d21-b5c6-d508dea3f75b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039898078s
Feb 17 18:11:57.652: INFO: Pod "var-expansion-712edf56-a5aa-4d21-b5c6-d508dea3f75b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053353335s
Feb 17 18:11:59.672: INFO: Pod "var-expansion-712edf56-a5aa-4d21-b5c6-d508dea3f75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.073171834s
STEP: Saw pod success
Feb 17 18:11:59.672: INFO: Pod "var-expansion-712edf56-a5aa-4d21-b5c6-d508dea3f75b" satisfied condition "success or failure"
Feb 17 18:11:59.685: INFO: Trying to get logs from node 10.45.66.189 pod var-expansion-712edf56-a5aa-4d21-b5c6-d508dea3f75b container dapi-container: <nil>
STEP: delete the pod
Feb 17 18:11:59.747: INFO: Waiting for pod var-expansion-712edf56-a5aa-4d21-b5c6-d508dea3f75b to disappear
Feb 17 18:11:59.759: INFO: Pod var-expansion-712edf56-a5aa-4d21-b5c6-d508dea3f75b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:11:59.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3962" for this suite.
Feb 17 18:12:07.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:12:09.964: INFO: namespace var-expansion-3962 deletion completed in 10.166537345s

• [SLOW TEST:18.556 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:12:09.965: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-e7e85194-75a3-407e-98fc-93d0c9ad4f43
STEP: Creating a pod to test consume secrets
Feb 17 18:12:10.218: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-97e1c79e-0bab-4080-9d83-1e152c4841f4" in namespace "projected-1654" to be "success or failure"
Feb 17 18:12:10.245: INFO: Pod "pod-projected-secrets-97e1c79e-0bab-4080-9d83-1e152c4841f4": Phase="Pending", Reason="", readiness=false. Elapsed: 26.610667ms
Feb 17 18:12:12.258: INFO: Pod "pod-projected-secrets-97e1c79e-0bab-4080-9d83-1e152c4841f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039854178s
Feb 17 18:12:14.272: INFO: Pod "pod-projected-secrets-97e1c79e-0bab-4080-9d83-1e152c4841f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053380318s
Feb 17 18:12:16.285: INFO: Pod "pod-projected-secrets-97e1c79e-0bab-4080-9d83-1e152c4841f4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066965221s
Feb 17 18:12:18.298: INFO: Pod "pod-projected-secrets-97e1c79e-0bab-4080-9d83-1e152c4841f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.079487005s
STEP: Saw pod success
Feb 17 18:12:18.298: INFO: Pod "pod-projected-secrets-97e1c79e-0bab-4080-9d83-1e152c4841f4" satisfied condition "success or failure"
Feb 17 18:12:18.310: INFO: Trying to get logs from node 10.45.66.177 pod pod-projected-secrets-97e1c79e-0bab-4080-9d83-1e152c4841f4 container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 18:12:18.408: INFO: Waiting for pod pod-projected-secrets-97e1c79e-0bab-4080-9d83-1e152c4841f4 to disappear
Feb 17 18:12:18.420: INFO: Pod pod-projected-secrets-97e1c79e-0bab-4080-9d83-1e152c4841f4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:12:18.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1654" for this suite.
Feb 17 18:12:26.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:12:28.596: INFO: namespace projected-1654 deletion completed in 10.134897993s

• [SLOW TEST:18.632 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:12:28.597: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Feb 17 18:12:28.729: INFO: namespace kubectl-2548
Feb 17 18:12:28.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 create -f - --namespace=kubectl-2548'
Feb 17 18:12:29.234: INFO: stderr: ""
Feb 17 18:12:29.234: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 17 18:12:30.248: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:12:30.248: INFO: Found 0 / 1
Feb 17 18:12:31.248: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:12:31.248: INFO: Found 0 / 1
Feb 17 18:12:32.247: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:12:32.247: INFO: Found 0 / 1
Feb 17 18:12:33.249: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:12:33.249: INFO: Found 0 / 1
Feb 17 18:12:34.248: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:12:34.248: INFO: Found 0 / 1
Feb 17 18:12:35.248: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:12:35.248: INFO: Found 0 / 1
Feb 17 18:12:36.248: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:12:36.248: INFO: Found 0 / 1
Feb 17 18:12:37.248: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:12:37.248: INFO: Found 1 / 1
Feb 17 18:12:37.248: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 17 18:12:37.264: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:12:37.265: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 17 18:12:37.265: INFO: wait on redis-master startup in kubectl-2548 
Feb 17 18:12:37.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 logs redis-master-l8wt5 redis-master --namespace=kubectl-2548'
Feb 17 18:12:37.472: INFO: stderr: ""
Feb 17 18:12:37.472: INFO: stdout: "1:C 17 Feb 2020 18:12:36.273 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 17 Feb 2020 18:12:36.273 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 17 Feb 2020 18:12:36.273 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 17 Feb 2020 18:12:36.275 * Running mode=standalone, port=6379.\n1:M 17 Feb 2020 18:12:36.275 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 17 Feb 2020 18:12:36.275 # Server initialized\n1:M 17 Feb 2020 18:12:36.275 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 17 Feb 2020 18:12:36.275 * Ready to accept connections\n"
STEP: exposing RC
Feb 17 18:12:37.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-2548'
Feb 17 18:12:37.660: INFO: stderr: ""
Feb 17 18:12:37.660: INFO: stdout: "service/rm2 exposed\n"
Feb 17 18:12:37.674: INFO: Service rm2 in namespace kubectl-2548 found.
STEP: exposing service
Feb 17 18:12:39.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-2548'
Feb 17 18:12:39.873: INFO: stderr: ""
Feb 17 18:12:39.873: INFO: stdout: "service/rm3 exposed\n"
Feb 17 18:12:39.885: INFO: Service rm3 in namespace kubectl-2548 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:12:41.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2548" for this suite.
Feb 17 18:12:55.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:12:58.108: INFO: namespace kubectl-2548 deletion completed in 16.164339974s

• [SLOW TEST:29.511 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:12:58.109: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 18:12:58.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 create -f - --namespace=kubectl-4091'
Feb 17 18:12:58.837: INFO: stderr: ""
Feb 17 18:12:58.837: INFO: stdout: "replicationcontroller/redis-master created\n"
Feb 17 18:12:58.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 create -f - --namespace=kubectl-4091'
Feb 17 18:12:59.225: INFO: stderr: ""
Feb 17 18:12:59.225: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Feb 17 18:13:00.239: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:13:00.239: INFO: Found 0 / 1
Feb 17 18:13:01.238: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:13:01.238: INFO: Found 0 / 1
Feb 17 18:13:02.238: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:13:02.238: INFO: Found 0 / 1
Feb 17 18:13:03.239: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:13:03.239: INFO: Found 0 / 1
Feb 17 18:13:04.239: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:13:04.239: INFO: Found 0 / 1
Feb 17 18:13:05.238: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:13:05.238: INFO: Found 0 / 1
Feb 17 18:13:06.242: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:13:06.242: INFO: Found 1 / 1
Feb 17 18:13:06.242: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Feb 17 18:13:06.255: INFO: Selector matched 1 pods for map[app:redis]
Feb 17 18:13:06.255: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Feb 17 18:13:06.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 describe pod redis-master-p66cd --namespace=kubectl-4091'
Feb 17 18:13:06.438: INFO: stderr: ""
Feb 17 18:13:06.438: INFO: stdout: "Name:         redis-master-p66cd\nNamespace:    kubectl-4091\nPriority:     0\nNode:         10.45.66.189/10.45.66.189\nStart Time:   Mon, 17 Feb 2020 18:12:58 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 172.30.50.152/32\n              cni.projectcalico.org/podIPs: 172.30.50.152/32\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.30.50.152\"\n                    ],\n                    \"dns\": {}\n                }]\n              openshift.io/scc: privileged\nStatus:       Running\nIP:           172.30.50.152\nIPs:\n  IP:           172.30.50.152\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   cri-o://51f838bf94a42791ad5dfde165762a68e699dd241f8a8e3ac1bcff28f4bffca6\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 17 Feb 2020 18:13:05 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-fsq9q (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-fsq9q:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-fsq9q\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                   Message\n  ----    ------     ----       ----                   -------\n  Normal  Scheduled  <unknown>  default-scheduler      Successfully assigned kubectl-4091/redis-master-p66cd to 10.45.66.189\n  Normal  Pulled     1s         kubelet, 10.45.66.189  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    1s         kubelet, 10.45.66.189  Created container redis-master\n  Normal  Started    1s         kubelet, 10.45.66.189  Started container redis-master\n"
Feb 17 18:13:06.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 describe rc redis-master --namespace=kubectl-4091'
Feb 17 18:13:06.645: INFO: stderr: ""
Feb 17 18:13:06.645: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4091\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  8s    replication-controller  Created pod: redis-master-p66cd\n"
Feb 17 18:13:06.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 describe service redis-master --namespace=kubectl-4091'
Feb 17 18:13:06.826: INFO: stderr: ""
Feb 17 18:13:06.826: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4091\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.21.182.64\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.30.50.152:6379\nSession Affinity:  None\nEvents:            <none>\n"
Feb 17 18:13:06.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 describe node 10.45.66.177'
Feb 17 18:13:07.128: INFO: stderr: ""
Feb 17 18:13:07.128: INFO: stdout: "Name:               10.45.66.177\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-gb\n                    failure-domain.beta.kubernetes.io/zone=lon04\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=158.175.157.140\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.45.66.177\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_7_64\n                    ibm-cloud.kubernetes.io/region=eu-gb\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-bp5a2h3l0finhn06568g-kubee2epvgx-default-00000342\n                    ibm-cloud.kubernetes.io/worker-pool-id=bp5a2h3l0finhn06568g-88ff146\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.3.0_1507_openshift\n                    ibm-cloud.kubernetes.io/zone=lon04\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.45.66.177\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=2722966\n                    publicVLAN=2722964\n                    topology.kubernetes.io/region=eu-gb\n                    topology.kubernetes.io/zone=lon04\nAnnotations:        projectcalico.org/IPv4Address: 10.45.66.177/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.178.192\nCreationTimestamp:  Mon, 17 Feb 2020 14:34:52 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 17 Feb 2020 14:36:00 +0000   Mon, 17 Feb 2020 14:36:00 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 17 Feb 2020 18:12:27 +0000   Mon, 17 Feb 2020 14:34:52 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 17 Feb 2020 18:12:27 +0000   Mon, 17 Feb 2020 14:34:52 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 17 Feb 2020 18:12:27 +0000   Mon, 17 Feb 2020 14:34:52 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 17 Feb 2020 18:12:27 +0000   Mon, 17 Feb 2020 14:36:03 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.45.66.177\n  ExternalIP:  158.175.157.140\n  Hostname:    10.45.66.177\nCapacity:\n cpu:                4\n ephemeral-storage:  103078840Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16260924Ki\n pods:               110\nAllocatable:\n cpu:                3910m\n ephemeral-storage:  100275095474\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             13484860Ki\n pods:               110\nSystem Info:\n Machine ID:                                             639182efdc514c91b82b2dabe8bf047a\n System UUID:                                            A93E6B1A-C0C2-DE3C-E003-24CED401AB40\n Boot ID:                                                08a5fe24-a950-49a9-b507-468407ccb725\n Kernel Version:                                         3.10.0-1062.9.1.el7.x86_64\n OS Image:                                               Red Hat\n Operating System:                                       linux\n Architecture:                                           amd64\n Container Runtime Version:                              cri-o://1.16.2-6.dev.rhaos4.3.git9e3db66.el7\n Kubelet Version:                                        v1.16.2\n Kube-Proxy Version:                                     v1.16.2\nProviderID:                                              ibm://fee034388aa6435883a1f720010ab3a2///bp5a2h3l0finhn06568g/kube-bp5a2h3l0finhn06568g-kubee2epvgx-default-00000342\nNon-terminated Pods:                                     (45 in total)\n  Namespace                                              Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                                              ----                                                               ------------  ----------  ---------------  -------------  ---\n  kube-system                                            calico-kube-controllers-5df9fd5998-tgjvk                           10m (0%)      0 (0%)      25Mi (0%)        3Gi (23%)      3h45m\n  kube-system                                            calico-node-mjlf4                                                  250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h38m\n  kube-system                                            calico-typha-76c597bc7d-4tfhl                                      250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h45m\n  kube-system                                            ibm-file-plugin-788cbbd987-7hzlx                                   50m (1%)      200m (5%)   100Mi (0%)       0 (0%)         3h45m\n  kube-system                                            ibm-keepalived-watcher-684qj                                       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         3h38m\n  kube-system                                            ibm-master-proxy-static-10.45.66.177                               25m (0%)      300m (7%)   32M (0%)         512M (3%)      3h38m\n  kube-system                                            ibm-storage-watcher-5cf8f8b4cb-pnqvx                               50m (1%)      200m (5%)   100Mi (0%)       0 (0%)         3h45m\n  kube-system                                            ibmcloud-block-storage-driver-vnql8                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h38m\n  kube-system                                            ibmcloud-block-storage-plugin-75cf87c786-cc9kd                     50m (1%)      200m (5%)   100Mi (0%)       0 (0%)         3h44m\n  kube-system                                            vpn-5d9c8dbf8-fwt5m                                                5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         3h28m\n  openshift-cloud-credential-operator                    cloud-credential-operator-6fc5bb58fc-npgb8                         10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         3h45m\n  openshift-cluster-node-tuning-operator                 cluster-node-tuning-operator-5cbf9c8db5-mr2tk                      10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         3h45m\n  openshift-cluster-node-tuning-operator                 tuned-f4zpl                                                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h36m\n  openshift-cluster-storage-operator                     cluster-storage-operator-646c48994f-4nvbp                          10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         3h45m\n  openshift-console-operator                             console-operator-7897bcfbd8-m6vt2                                  10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         3h45m\n  openshift-console                                      downloads-65dd498cf8-cqpss                                         10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h45m\n  openshift-console                                      downloads-65dd498cf8-vvlwb                                         10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h45m\n  openshift-dns-operator                                 dns-operator-586c9c56ff-nxk8c                                      20m (0%)      0 (0%)      40Mi (0%)        0 (0%)         3h45m\n  openshift-dns                                          dns-default-xw2km                                                  110m (2%)     0 (0%)      70Mi (0%)        512Mi (3%)     3h35m\n  openshift-image-registry                               cluster-image-registry-operator-79bfcb686-62774                    20m (0%)      0 (0%)      0 (0%)           0 (0%)         3h45m\n  openshift-image-registry                               node-ca-49d68                                                      10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h35m\n  openshift-ingress-operator                             ingress-operator-855959d8d9-xhrmn                                  20m (0%)      0 (0%)      40Mi (0%)        0 (0%)         3h45m\n  openshift-ingress                                      router-default-6c67c67864-hlm85                                    100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         3h30m\n  openshift-kube-proxy                                   openshift-kube-proxy-sz6st                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h37m\n  openshift-marketplace                                  marketplace-operator-664f66c947-jckz5                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h45m\n  openshift-monitoring                                   alertmanager-main-2                                                100m (2%)     100m (2%)   225Mi (1%)       25Mi (0%)      3h31m\n  openshift-monitoring                                   cluster-monitoring-operator-55d99d5ffc-qlxs4                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h45m\n  openshift-monitoring                                   kube-state-metrics-5745cc99f5-vd9j6                                30m (0%)      0 (0%)      120Mi (0%)       0 (0%)         3h36m\n  openshift-monitoring                                   node-exporter-rfpkb                                                10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         3h36m\n  openshift-monitoring                                   openshift-state-metrics-86bbf546f-gglb8                            120m (3%)     0 (0%)      190Mi (1%)       0 (0%)         3h36m\n  openshift-multus                                       multus-admission-controller-x8vxh                                  10m (0%)      0 (0%)      0 (0%)           0 (0%)         3h37m\n  openshift-multus                                       multus-z6qv7                                                       10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         3h37m\n  openshift-network-operator                             network-operator-86974b9cf-spclm                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h45m\n  openshift-operator-lifecycle-manager                   catalog-operator-7fccd6877f-xzz4r                                  10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         3h45m\n  openshift-operator-lifecycle-manager                   olm-operator-5f5f7bbd7d-8k49r                                      10m (0%)      0 (0%)      160Mi (1%)       0 (0%)         3h45m\n  openshift-operator-lifecycle-manager                   packageserver-749668c8dd-fpjbg                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         153m\n  openshift-service-ca-operator                          service-ca-operator-5f9bd445b8-k65l4                               10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         3h45m\n  openshift-service-ca                                   apiservice-cabundle-injector-745c6fc9d8-h2znf                      10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h36m\n  openshift-service-ca                                   configmap-cabundle-injector-c749c6984-hlkf5                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h36m\n  openshift-service-ca                                   service-serving-cert-signer-6669ffbb98-s7bf6                       10m (0%)      0 (0%)      120Mi (0%)       0 (0%)         3h36m\n  openshift-service-catalog-apiserver-operator           openshift-service-catalog-apiserver-operator-577c6ffb4f-r7vw8      0 (0%)        0 (0%)      50Mi (0%)        0 (0%)         3h45m\n  openshift-service-catalog-controller-manager-operator  openshift-service-catalog-controller-manager-operator-6857997dg    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h45m\n  sonobuoy                                               sonobuoy                                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         103m\n  sonobuoy                                               sonobuoy-e2e-job-a662486eb1ae4ce5                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         102m\n  sonobuoy                                               sonobuoy-systemd-logs-daemon-set-67976b35ad084332-bvf8d            0 (0%)        0 (0%)      0 (0%)           0 (0%)         102m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1435m (36%)      1 (25%)\n  memory             3001874Ki (22%)  4195616Ki (31%)\n  ephemeral-storage  0 (0%)           0 (0%)\nEvents:              <none>\n"
Feb 17 18:13:07.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 describe namespace kubectl-4091'
Feb 17 18:13:07.302: INFO: stderr: ""
Feb 17 18:13:07.302: INFO: stdout: "Name:         kubectl-4091\nLabels:       e2e-framework=kubectl\n              e2e-run=30c90592-a32f-44bd-9829-57f58a215716\nAnnotations:  openshift.io/sa.scc.mcs: s0:c50,c45\n              openshift.io/sa.scc.supplemental-groups: 1002540000/10000\n              openshift.io/sa.scc.uid-range: 1002540000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:13:07.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4091" for this suite.
Feb 17 18:13:39.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:13:41.461: INFO: namespace kubectl-4091 deletion completed in 34.125837755s

• [SLOW TEST:43.352 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:13:41.461: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 18:13:41.622: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-a7333515-8de5-47d9-b052-44b366691d71
STEP: Creating configMap with name cm-test-opt-upd-1a84f769-9a1b-4cde-8b37-38e27eaf26f7
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-a7333515-8de5-47d9-b052-44b366691d71
STEP: Updating configmap cm-test-opt-upd-1a84f769-9a1b-4cde-8b37-38e27eaf26f7
STEP: Creating configMap with name cm-test-opt-create-1fcc1a47-137a-429f-9181-74004933a22a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:15:23.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-702" for this suite.
Feb 17 18:15:37.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:15:39.567: INFO: namespace configmap-702 deletion completed in 16.146507075s

• [SLOW TEST:118.106 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:15:39.570: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Feb 17 18:15:39.728: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:15:49.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2417" for this suite.
Feb 17 18:16:21.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:16:23.905: INFO: namespace init-container-2417 deletion completed in 34.151197509s

• [SLOW TEST:44.335 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:16:23.906: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 17 18:16:24.043: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 17 18:16:24.126: INFO: Waiting for terminating namespaces to be deleted...
Feb 17 18:16:24.147: INFO: 
Logging pods the kubelet thinks is on node 10.45.66.177 before test
Feb 17 18:16:24.298: INFO: ibmcloud-block-storage-plugin-75cf87c786-cc9kd from kube-system started at 2020-02-17 14:34:58 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.298: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Feb 17 18:16:24.298: INFO: cluster-node-tuning-operator-5cbf9c8db5-mr2tk from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.298: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Feb 17 18:16:24.298: INFO: cloud-credential-operator-6fc5bb58fc-npgb8 from openshift-cloud-credential-operator started at 2020-02-17 14:36:06 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.298: INFO: 	Container manager ready: true, restart count 0
Feb 17 18:16:24.298: INFO: kube-state-metrics-5745cc99f5-vd9j6 from openshift-monitoring started at 2020-02-17 14:36:27 +0000 UTC (3 container statuses recorded)
Feb 17 18:16:24.298: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 17 18:16:24.298: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 17 18:16:24.298: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 17 18:16:24.298: INFO: multus-admission-controller-x8vxh from openshift-multus started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.298: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 17 18:16:24.298: INFO: downloads-65dd498cf8-vvlwb from openshift-console started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.298: INFO: 	Container download-server ready: true, restart count 0
Feb 17 18:16:24.298: INFO: service-serving-cert-signer-6669ffbb98-s7bf6 from openshift-service-ca started at 2020-02-17 14:36:49 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.298: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Feb 17 18:16:24.298: INFO: node-ca-49d68 from openshift-image-registry started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.298: INFO: 	Container node-ca ready: true, restart count 0
Feb 17 18:16:24.298: INFO: ibmcloud-block-storage-driver-vnql8 from kube-system started at 2020-02-17 14:34:55 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.298: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 17 18:16:24.298: INFO: apiservice-cabundle-injector-745c6fc9d8-h2znf from openshift-service-ca started at 2020-02-17 14:36:50 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Feb 17 18:16:24.299: INFO: ibm-keepalived-watcher-684qj from kube-system started at 2020-02-17 14:34:53 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 18:16:24.299: INFO: console-operator-7897bcfbd8-m6vt2 from openshift-console-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container console-operator ready: true, restart count 1
Feb 17 18:16:24.299: INFO: calico-node-mjlf4 from kube-system started at 2020-02-17 14:34:53 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:16:24.299: INFO: catalog-operator-7fccd6877f-xzz4r from openshift-operator-lifecycle-manager started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container catalog-operator ready: true, restart count 0
Feb 17 18:16:24.299: INFO: downloads-65dd498cf8-cqpss from openshift-console started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container download-server ready: true, restart count 0
Feb 17 18:16:24.299: INFO: sonobuoy from sonobuoy started at 2020-02-17 16:29:56 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 17 18:16:24.299: INFO: marketplace-operator-664f66c947-jckz5 from openshift-marketplace started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container marketplace-operator ready: true, restart count 0
Feb 17 18:16:24.299: INFO: configmap-cabundle-injector-c749c6984-hlkf5 from openshift-service-ca started at 2020-02-17 14:36:50 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Feb 17 18:16:24.299: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-02-17 14:41:40 +0000 UTC (3 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container alertmanager ready: true, restart count 0
Feb 17 18:16:24.299: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 17 18:16:24.299: INFO: 	Container config-reloader ready: true, restart count 0
Feb 17 18:16:24.299: INFO: network-operator-86974b9cf-spclm from openshift-network-operator started at 2020-02-17 14:34:58 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container network-operator ready: true, restart count 0
Feb 17 18:16:24.299: INFO: calico-typha-76c597bc7d-4tfhl from kube-system started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container calico-typha ready: true, restart count 0
Feb 17 18:16:24.299: INFO: ibm-storage-watcher-5cf8f8b4cb-pnqvx from kube-system started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Feb 17 18:16:24.299: INFO: openshift-state-metrics-86bbf546f-gglb8 from openshift-monitoring started at 2020-02-17 14:36:26 +0000 UTC (3 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 17 18:16:24.299: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 17 18:16:24.299: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Feb 17 18:16:24.299: INFO: node-exporter-rfpkb from openshift-monitoring started at 2020-02-17 14:36:28 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:16:24.299: INFO: 	Container node-exporter ready: true, restart count 0
Feb 17 18:16:24.299: INFO: multus-z6qv7 from openshift-multus started at 2020-02-17 14:35:36 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container kube-multus ready: true, restart count 0
Feb 17 18:16:24.299: INFO: openshift-service-catalog-controller-manager-operator-6857997dg from openshift-service-catalog-controller-manager-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container operator ready: true, restart count 1
Feb 17 18:16:24.299: INFO: dns-operator-586c9c56ff-nxk8c from openshift-dns-operator started at 2020-02-17 14:36:04 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container dns-operator ready: true, restart count 0
Feb 17 18:16:24.299: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:16:24.299: INFO: ibm-file-plugin-788cbbd987-7hzlx from kube-system started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Feb 17 18:16:24.299: INFO: cluster-monitoring-operator-55d99d5ffc-qlxs4 from openshift-monitoring started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Feb 17 18:16:24.299: INFO: tuned-f4zpl from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:28 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container tuned ready: true, restart count 0
Feb 17 18:16:24.299: INFO: ibm-master-proxy-static-10.45.66.177 from kube-system started at 2020-02-17 14:34:51 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 18:16:24.299: INFO: 	Container pause ready: true, restart count 0
Feb 17 18:16:24.299: INFO: openshift-kube-proxy-sz6st from openshift-kube-proxy started at 2020-02-17 14:35:42 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:16:24.299: INFO: ingress-operator-855959d8d9-xhrmn from openshift-ingress-operator started at 2020-02-17 14:36:04 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container ingress-operator ready: true, restart count 0
Feb 17 18:16:24.299: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:16:24.299: INFO: cluster-storage-operator-646c48994f-4nvbp from openshift-cluster-storage-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.299: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Feb 17 18:16:24.299: INFO: calico-kube-controllers-5df9fd5998-tgjvk from kube-system started at 2020-02-17 14:36:06 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.300: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 17 18:16:24.300: INFO: openshift-service-catalog-apiserver-operator-577c6ffb4f-r7vw8 from openshift-service-catalog-apiserver-operator started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.300: INFO: 	Container operator ready: true, restart count 1
Feb 17 18:16:24.300: INFO: cluster-image-registry-operator-79bfcb686-62774 from openshift-image-registry started at 2020-02-17 14:36:04 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.300: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Feb 17 18:16:24.300: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Feb 17 18:16:24.300: INFO: router-default-6c67c67864-hlm85 from openshift-ingress started at 2020-02-17 14:42:12 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.300: INFO: 	Container router ready: true, restart count 0
Feb 17 18:16:24.300: INFO: packageserver-749668c8dd-fpjbg from openshift-operator-lifecycle-manager started at 2020-02-17 15:40:00 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.300: INFO: 	Container packageserver ready: true, restart count 0
Feb 17 18:16:24.300: INFO: service-ca-operator-5f9bd445b8-k65l4 from openshift-service-ca-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.300: INFO: 	Container operator ready: true, restart count 0
Feb 17 18:16:24.300: INFO: dns-default-xw2km from openshift-dns started at 2020-02-17 14:37:34 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.300: INFO: 	Container dns ready: true, restart count 0
Feb 17 18:16:24.300: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 17 18:16:24.300: INFO: vpn-5d9c8dbf8-fwt5m from kube-system started at 2020-02-17 14:45:06 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.300: INFO: 	Container vpn ready: true, restart count 0
Feb 17 18:16:24.300: INFO: sonobuoy-e2e-job-a662486eb1ae4ce5 from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.300: INFO: 	Container e2e ready: true, restart count 0
Feb 17 18:16:24.300: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:16:24.300: INFO: sonobuoy-systemd-logs-daemon-set-67976b35ad084332-bvf8d from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.300: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 17 18:16:24.300: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 18:16:24.300: INFO: olm-operator-5f5f7bbd7d-8k49r from openshift-operator-lifecycle-manager started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.300: INFO: 	Container olm-operator ready: true, restart count 0
Feb 17 18:16:24.300: INFO: 
Logging pods the kubelet thinks is on node 10.45.66.178 before test
Feb 17 18:16:24.423: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-02-17 14:42:48 +0000 UTC (7 container statuses recorded)
Feb 17 18:16:24.423: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:16:24.423: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 18:16:24.423: INFO: 	Container prometheus ready: true, restart count 1
Feb 17 18:16:24.423: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Feb 17 18:16:24.423: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 17 18:16:24.423: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Feb 17 18:16:24.423: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 17 18:16:24.423: INFO: prometheus-operator-7d5f885558-bv55g from openshift-monitoring started at 2020-02-17 14:41:26 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.423: INFO: 	Container prometheus-operator ready: true, restart count 0
Feb 17 18:16:24.423: INFO: router-default-6c67c67864-f8hvb from openshift-ingress started at 2020-02-17 14:42:29 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.423: INFO: 	Container router ready: true, restart count 0
Feb 17 18:16:24.424: INFO: ibm-master-proxy-static-10.45.66.178 from kube-system started at 2020-02-17 14:37:31 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.424: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 18:16:24.424: INFO: 	Container pause ready: true, restart count 0
Feb 17 18:16:24.424: INFO: node-ca-n4v6k from openshift-image-registry started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.424: INFO: 	Container node-ca ready: true, restart count 0
Feb 17 18:16:24.424: INFO: registry-pvc-permissions-jlk4p from openshift-image-registry started at 2020-02-17 14:40:17 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.424: INFO: 	Container pvc-permissions ready: false, restart count 0
Feb 17 18:16:24.424: INFO: thanos-querier-b949c5db-ns4qs from openshift-monitoring started at 2020-02-17 14:42:53 +0000 UTC (4 container statuses recorded)
Feb 17 18:16:24.424: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:16:24.424: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 17 18:16:24.424: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 18:16:24.424: INFO: 	Container thanos-querier ready: true, restart count 0
Feb 17 18:16:24.424: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-02-17 14:41:51 +0000 UTC (3 container statuses recorded)
Feb 17 18:16:24.424: INFO: 	Container alertmanager ready: true, restart count 0
Feb 17 18:16:24.424: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 17 18:16:24.424: INFO: 	Container config-reloader ready: true, restart count 0
Feb 17 18:16:24.424: INFO: certified-operators-8577555dd-n9z77 from openshift-marketplace started at 2020-02-17 17:37:51 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.424: INFO: 	Container certified-operators ready: true, restart count 0
Feb 17 18:16:24.424: INFO: node-exporter-wmxf5 from openshift-monitoring started at 2020-02-17 14:36:43 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.424: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:16:24.424: INFO: 	Container node-exporter ready: true, restart count 0
Feb 17 18:16:24.424: INFO: dns-default-9fgff from openshift-dns started at 2020-02-17 14:38:00 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.425: INFO: 	Container dns ready: true, restart count 0
Feb 17 18:16:24.425: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 17 18:16:24.425: INFO: packageserver-749668c8dd-slgc8 from openshift-operator-lifecycle-manager started at 2020-02-17 15:40:09 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.425: INFO: 	Container packageserver ready: true, restart count 0
Feb 17 18:16:24.425: INFO: sonobuoy-systemd-logs-daemon-set-67976b35ad084332-thldq from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.425: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 17 18:16:24.425: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 18:16:24.425: INFO: calico-node-zkvdn from kube-system started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.425: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:16:24.425: INFO: ibm-keepalived-watcher-4nt2b from kube-system started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.425: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 18:16:24.425: INFO: cluster-samples-operator-946f4d4c5-66g9k from openshift-cluster-samples-operator started at 2020-02-17 14:38:52 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.425: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Feb 17 18:16:24.425: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Feb 17 18:16:24.425: INFO: image-registry-6d4857c99d-q2dn4 from openshift-image-registry started at 2020-02-17 14:40:17 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.425: INFO: 	Container registry ready: true, restart count 0
Feb 17 18:16:24.425: INFO: console-7547c7bc6c-wschb from openshift-console started at 2020-02-17 14:39:26 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.425: INFO: 	Container console ready: true, restart count 0
Feb 17 18:16:24.425: INFO: multus-wbdmp from openshift-multus started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.425: INFO: 	Container kube-multus ready: true, restart count 0
Feb 17 18:16:24.425: INFO: ibm-cloud-provider-ip-158-175-93-66-7b5b7d9cb-jbd6r from ibm-system started at 2020-02-17 14:41:22 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.425: INFO: 	Container ibm-cloud-provider-ip-158-175-93-66 ready: true, restart count 0
Feb 17 18:16:24.425: INFO: tuned-2n22g from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.426: INFO: 	Container tuned ready: true, restart count 0
Feb 17 18:16:24.426: INFO: openshift-kube-proxy-kchsl from openshift-kube-proxy started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.426: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:16:24.426: INFO: grafana-66dff46d7c-l5jsk from openshift-monitoring started at 2020-02-17 14:41:46 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.426: INFO: 	Container grafana ready: true, restart count 0
Feb 17 18:16:24.426: INFO: 	Container grafana-proxy ready: true, restart count 0
Feb 17 18:16:24.426: INFO: ibmcloud-block-storage-driver-d7lnw from kube-system started at 2020-02-17 14:36:45 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.426: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 17 18:16:24.426: INFO: multus-admission-controller-s6fg5 from openshift-multus started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.426: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 17 18:16:24.426: INFO: calico-typha-76c597bc7d-72pj2 from kube-system started at 2020-02-17 14:38:05 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.426: INFO: 	Container calico-typha ready: true, restart count 0
Feb 17 18:16:24.426: INFO: prometheus-adapter-55674d9685-vvdz2 from openshift-monitoring started at 2020-02-17 14:41:40 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.426: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 17 18:16:24.426: INFO: 
Logging pods the kubelet thinks is on node 10.45.66.189 before test
Feb 17 18:16:24.547: INFO: telemeter-client-56f4c98664-jhjns from openshift-monitoring started at 2020-02-17 14:41:37 +0000 UTC (3 container statuses recorded)
Feb 17 18:16:24.547: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:16:24.547: INFO: 	Container reload ready: true, restart count 0
Feb 17 18:16:24.547: INFO: 	Container telemeter-client ready: true, restart count 0
Feb 17 18:16:24.547: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-02-17 14:42:08 +0000 UTC (3 container statuses recorded)
Feb 17 18:16:24.547: INFO: 	Container alertmanager ready: true, restart count 0
Feb 17 18:16:24.547: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 17 18:16:24.547: INFO: 	Container config-reloader ready: true, restart count 0
Feb 17 18:16:24.547: INFO: ibm-keepalived-watcher-5hn6l from kube-system started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.547: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 18:16:24.547: INFO: calico-node-cctn2 from kube-system started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.547: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:16:24.548: INFO: ibmcloud-block-storage-driver-jnxhv from kube-system started at 2020-02-17 14:36:32 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.548: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 17 18:16:24.548: INFO: tuned-kg4dp from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:28 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.548: INFO: 	Container tuned ready: true, restart count 0
Feb 17 18:16:24.548: INFO: thanos-querier-b949c5db-5nhn5 from openshift-monitoring started at 2020-02-17 14:42:35 +0000 UTC (4 container statuses recorded)
Feb 17 18:16:24.548: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:16:24.548: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 17 18:16:24.548: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 18:16:24.548: INFO: 	Container thanos-querier ready: true, restart count 0
Feb 17 18:16:24.548: INFO: redhat-operators-7cc9d7897-nsjpx from openshift-marketplace started at 2020-02-17 14:38:05 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.548: INFO: 	Container redhat-operators ready: true, restart count 0
Feb 17 18:16:24.548: INFO: console-7547c7bc6c-rjbss from openshift-console started at 2020-02-17 14:39:12 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.548: INFO: 	Container console ready: true, restart count 0
Feb 17 18:16:24.548: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-02-17 14:39:35 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.548: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Feb 17 18:16:24.548: INFO: calico-typha-76c597bc7d-h6b67 from kube-system started at 2020-02-17 14:37:29 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.548: INFO: 	Container calico-typha ready: true, restart count 0
Feb 17 18:16:24.548: INFO: dns-default-wkxng from openshift-dns started at 2020-02-17 14:37:34 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.548: INFO: 	Container dns ready: true, restart count 0
Feb 17 18:16:24.548: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 17 18:16:24.548: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-02-17 14:43:08 +0000 UTC (7 container statuses recorded)
Feb 17 18:16:24.548: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:16:24.548: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 18:16:24.549: INFO: 	Container prometheus ready: true, restart count 1
Feb 17 18:16:24.549: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Feb 17 18:16:24.549: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 17 18:16:24.549: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Feb 17 18:16:24.549: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 17 18:16:24.549: INFO: multus-admission-controller-lpf76 from openshift-multus started at 2020-02-17 14:37:28 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.549: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 17 18:16:24.549: INFO: sonobuoy-systemd-logs-daemon-set-67976b35ad084332-4jdsp from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.549: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 17 18:16:24.549: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 18:16:24.549: INFO: ibm-master-proxy-static-10.45.66.189 from kube-system started at 2020-02-17 14:37:14 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.549: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 18:16:24.549: INFO: 	Container pause ready: true, restart count 0
Feb 17 18:16:24.549: INFO: multus-df87p from openshift-multus started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.549: INFO: 	Container kube-multus ready: true, restart count 0
Feb 17 18:16:24.549: INFO: ibm-cloud-provider-ip-158-175-93-66-7b5b7d9cb-mhbr7 from ibm-system started at 2020-02-17 14:41:22 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.549: INFO: 	Container ibm-cloud-provider-ip-158-175-93-66 ready: true, restart count 0
Feb 17 18:16:24.549: INFO: openshift-kube-proxy-gwxkw from openshift-kube-proxy started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.549: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:16:24.549: INFO: node-ca-56bzx from openshift-image-registry started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.549: INFO: 	Container node-ca ready: true, restart count 0
Feb 17 18:16:24.549: INFO: prometheus-adapter-55674d9685-g5b96 from openshift-monitoring started at 2020-02-17 14:41:40 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.549: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 17 18:16:24.550: INFO: node-exporter-4fhhh from openshift-monitoring started at 2020-02-17 14:36:28 +0000 UTC (2 container statuses recorded)
Feb 17 18:16:24.550: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:16:24.550: INFO: 	Container node-exporter ready: true, restart count 0
Feb 17 18:16:24.550: INFO: community-operators-6c7f5d4977-fxhls from openshift-marketplace started at 2020-02-17 14:38:06 +0000 UTC (1 container statuses recorded)
Feb 17 18:16:24.550: INFO: 	Container community-operators ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8615649c-81a1-4fa7-8b66-597f1a4d272e 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-8615649c-81a1-4fa7-8b66-597f1a4d272e off the node 10.45.66.178
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8615649c-81a1-4fa7-8b66-597f1a4d272e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:17:06.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-326" for this suite.
Feb 17 18:17:21.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:17:23.084: INFO: namespace sched-pred-326 deletion completed in 16.118077158s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:59.178 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:17:23.084: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0217 18:17:33.465343      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 17 18:17:33.465: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:17:33.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1637" for this suite.
Feb 17 18:17:41.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:17:43.612: INFO: namespace gc-1637 deletion completed in 10.128333755s

• [SLOW TEST:20.527 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:17:43.612: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-f2f0af2d-4f92-4da7-b035-6b490b063c1d
STEP: Creating a pod to test consume configMaps
Feb 17 18:17:43.816: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a421ed07-757f-4f85-9141-41baebd428c6" in namespace "projected-1494" to be "success or failure"
Feb 17 18:17:43.827: INFO: Pod "pod-projected-configmaps-a421ed07-757f-4f85-9141-41baebd428c6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.614885ms
Feb 17 18:17:45.840: INFO: Pod "pod-projected-configmaps-a421ed07-757f-4f85-9141-41baebd428c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024298595s
Feb 17 18:17:47.854: INFO: Pod "pod-projected-configmaps-a421ed07-757f-4f85-9141-41baebd428c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037987409s
Feb 17 18:17:49.867: INFO: Pod "pod-projected-configmaps-a421ed07-757f-4f85-9141-41baebd428c6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051170476s
Feb 17 18:17:51.881: INFO: Pod "pod-projected-configmaps-a421ed07-757f-4f85-9141-41baebd428c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.06508738s
STEP: Saw pod success
Feb 17 18:17:51.881: INFO: Pod "pod-projected-configmaps-a421ed07-757f-4f85-9141-41baebd428c6" satisfied condition "success or failure"
Feb 17 18:17:51.895: INFO: Trying to get logs from node 10.45.66.177 pod pod-projected-configmaps-a421ed07-757f-4f85-9141-41baebd428c6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:17:51.966: INFO: Waiting for pod pod-projected-configmaps-a421ed07-757f-4f85-9141-41baebd428c6 to disappear
Feb 17 18:17:51.978: INFO: Pod pod-projected-configmaps-a421ed07-757f-4f85-9141-41baebd428c6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:17:51.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1494" for this suite.
Feb 17 18:18:00.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:18:02.136: INFO: namespace projected-1494 deletion completed in 10.140889813s

• [SLOW TEST:18.524 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:18:02.136: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-ac01f82d-1eb8-4909-996d-1469aab71b04
STEP: Creating a pod to test consume secrets
Feb 17 18:18:02.431: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a4a1a2de-344d-42b9-84f2-4abdf6681e00" in namespace "projected-1197" to be "success or failure"
Feb 17 18:18:02.445: INFO: Pod "pod-projected-secrets-a4a1a2de-344d-42b9-84f2-4abdf6681e00": Phase="Pending", Reason="", readiness=false. Elapsed: 13.693792ms
Feb 17 18:18:04.462: INFO: Pod "pod-projected-secrets-a4a1a2de-344d-42b9-84f2-4abdf6681e00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030497497s
Feb 17 18:18:06.474: INFO: Pod "pod-projected-secrets-a4a1a2de-344d-42b9-84f2-4abdf6681e00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04291192s
Feb 17 18:18:08.488: INFO: Pod "pod-projected-secrets-a4a1a2de-344d-42b9-84f2-4abdf6681e00": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056980601s
Feb 17 18:18:10.501: INFO: Pod "pod-projected-secrets-a4a1a2de-344d-42b9-84f2-4abdf6681e00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.069941996s
STEP: Saw pod success
Feb 17 18:18:10.501: INFO: Pod "pod-projected-secrets-a4a1a2de-344d-42b9-84f2-4abdf6681e00" satisfied condition "success or failure"
Feb 17 18:18:10.513: INFO: Trying to get logs from node 10.45.66.189 pod pod-projected-secrets-a4a1a2de-344d-42b9-84f2-4abdf6681e00 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 17 18:18:10.615: INFO: Waiting for pod pod-projected-secrets-a4a1a2de-344d-42b9-84f2-4abdf6681e00 to disappear
Feb 17 18:18:10.627: INFO: Pod pod-projected-secrets-a4a1a2de-344d-42b9-84f2-4abdf6681e00 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:18:10.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1197" for this suite.
Feb 17 18:18:18.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:18:20.827: INFO: namespace projected-1197 deletion completed in 10.141458363s

• [SLOW TEST:18.691 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:18:20.827: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Feb 17 18:18:29.622: INFO: Successfully updated pod "annotationupdateb8545b78-6c1e-4979-be2b-76b3544527cb"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:18:31.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9286" for this suite.
Feb 17 18:19:03.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:19:05.825: INFO: namespace downward-api-9286 deletion completed in 34.121291671s

• [SLOW TEST:44.998 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:19:05.825: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5793.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5793.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5793.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5793.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5793.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5793.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 18:19:16.265: INFO: DNS probes using dns-5793/dns-test-8121da17-ddeb-4baf-bb2d-d685bafa19fb succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:19:16.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5793" for this suite.
Feb 17 18:19:24.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:19:26.527: INFO: namespace dns-5793 deletion completed in 10.144715237s

• [SLOW TEST:20.702 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:19:26.528: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Feb 17 18:19:26.655: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:20:05.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7630" for this suite.
Feb 17 18:20:13.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:20:15.661: INFO: namespace crd-publish-openapi-7630 deletion completed in 10.184311059s

• [SLOW TEST:49.133 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:20:15.661: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:20:16.633: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:20:18.670: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:20:20.688: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:20:22.683: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:20:24.684: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560416, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:20:27.710: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:20:27.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1722" for this suite.
Feb 17 18:20:36.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:20:38.104: INFO: namespace webhook-1722 deletion completed in 10.104711148s
STEP: Destroying namespace "webhook-1722-markers" for this suite.
Feb 17 18:20:46.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:20:48.228: INFO: namespace webhook-1722-markers deletion completed in 10.124720938s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:32.640 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:20:48.302: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Feb 17 18:20:57.571: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:20:58.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-399" for this suite.
Feb 17 18:21:12.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:21:14.803: INFO: namespace replicaset-399 deletion completed in 16.116110159s

• [SLOW TEST:26.501 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:21:14.804: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:21:26.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1198" for this suite.
Feb 17 18:21:34.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:21:36.347: INFO: namespace resourcequota-1198 deletion completed in 10.168155991s

• [SLOW TEST:21.543 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:21:36.348: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:21:37.740: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:21:39.779: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560497, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560497, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560497, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560497, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:21:41.792: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560497, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560497, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560497, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560497, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:21:43.792: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560497, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560497, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560497, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560497, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:21:46.823: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 18:21:46.837: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1320-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:21:48.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8661" for this suite.
Feb 17 18:21:56.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:21:58.582: INFO: namespace webhook-8661 deletion completed in 10.227737147s
STEP: Destroying namespace "webhook-8661-markers" for this suite.
Feb 17 18:22:06.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:22:08.742: INFO: namespace webhook-8661-markers deletion completed in 10.15952554s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:32.463 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:22:08.811: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 17 18:22:08.963: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 17 18:22:09.081: INFO: Waiting for terminating namespaces to be deleted...
Feb 17 18:22:09.105: INFO: 
Logging pods the kubelet thinks is on node 10.45.66.177 before test
Feb 17 18:22:09.261: INFO: marketplace-operator-664f66c947-jckz5 from openshift-marketplace started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container marketplace-operator ready: true, restart count 0
Feb 17 18:22:09.261: INFO: configmap-cabundle-injector-c749c6984-hlkf5 from openshift-service-ca started at 2020-02-17 14:36:50 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Feb 17 18:22:09.261: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-02-17 14:41:40 +0000 UTC (3 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container alertmanager ready: true, restart count 0
Feb 17 18:22:09.261: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 17 18:22:09.261: INFO: 	Container config-reloader ready: true, restart count 0
Feb 17 18:22:09.261: INFO: network-operator-86974b9cf-spclm from openshift-network-operator started at 2020-02-17 14:34:58 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container network-operator ready: true, restart count 0
Feb 17 18:22:09.261: INFO: calico-typha-76c597bc7d-4tfhl from kube-system started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container calico-typha ready: true, restart count 0
Feb 17 18:22:09.261: INFO: ibm-storage-watcher-5cf8f8b4cb-pnqvx from kube-system started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Feb 17 18:22:09.261: INFO: openshift-state-metrics-86bbf546f-gglb8 from openshift-monitoring started at 2020-02-17 14:36:26 +0000 UTC (3 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 17 18:22:09.261: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 17 18:22:09.261: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Feb 17 18:22:09.261: INFO: node-exporter-rfpkb from openshift-monitoring started at 2020-02-17 14:36:28 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:22:09.261: INFO: 	Container node-exporter ready: true, restart count 0
Feb 17 18:22:09.261: INFO: multus-z6qv7 from openshift-multus started at 2020-02-17 14:35:36 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container kube-multus ready: true, restart count 0
Feb 17 18:22:09.261: INFO: openshift-service-catalog-controller-manager-operator-6857997dg from openshift-service-catalog-controller-manager-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container operator ready: true, restart count 1
Feb 17 18:22:09.261: INFO: dns-operator-586c9c56ff-nxk8c from openshift-dns-operator started at 2020-02-17 14:36:04 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container dns-operator ready: true, restart count 0
Feb 17 18:22:09.261: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:22:09.261: INFO: ibm-file-plugin-788cbbd987-7hzlx from kube-system started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Feb 17 18:22:09.261: INFO: cluster-monitoring-operator-55d99d5ffc-qlxs4 from openshift-monitoring started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Feb 17 18:22:09.261: INFO: ibm-master-proxy-static-10.45.66.177 from kube-system started at 2020-02-17 14:34:51 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 18:22:09.261: INFO: 	Container pause ready: true, restart count 0
Feb 17 18:22:09.261: INFO: openshift-kube-proxy-sz6st from openshift-kube-proxy started at 2020-02-17 14:35:42 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:22:09.261: INFO: ingress-operator-855959d8d9-xhrmn from openshift-ingress-operator started at 2020-02-17 14:36:04 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container ingress-operator ready: true, restart count 0
Feb 17 18:22:09.261: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:22:09.261: INFO: cluster-storage-operator-646c48994f-4nvbp from openshift-cluster-storage-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Feb 17 18:22:09.261: INFO: calico-kube-controllers-5df9fd5998-tgjvk from kube-system started at 2020-02-17 14:36:06 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 17 18:22:09.261: INFO: tuned-f4zpl from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:28 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container tuned ready: true, restart count 0
Feb 17 18:22:09.261: INFO: openshift-service-catalog-apiserver-operator-577c6ffb4f-r7vw8 from openshift-service-catalog-apiserver-operator started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container operator ready: true, restart count 1
Feb 17 18:22:09.261: INFO: cluster-image-registry-operator-79bfcb686-62774 from openshift-image-registry started at 2020-02-17 14:36:04 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Feb 17 18:22:09.261: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Feb 17 18:22:09.261: INFO: router-default-6c67c67864-hlm85 from openshift-ingress started at 2020-02-17 14:42:12 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container router ready: true, restart count 0
Feb 17 18:22:09.261: INFO: packageserver-749668c8dd-fpjbg from openshift-operator-lifecycle-manager started at 2020-02-17 15:40:00 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container packageserver ready: true, restart count 0
Feb 17 18:22:09.261: INFO: service-ca-operator-5f9bd445b8-k65l4 from openshift-service-ca-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container operator ready: true, restart count 0
Feb 17 18:22:09.261: INFO: dns-default-xw2km from openshift-dns started at 2020-02-17 14:37:34 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container dns ready: true, restart count 0
Feb 17 18:22:09.261: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 17 18:22:09.261: INFO: vpn-5d9c8dbf8-fwt5m from kube-system started at 2020-02-17 14:45:06 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container vpn ready: true, restart count 0
Feb 17 18:22:09.261: INFO: sonobuoy-e2e-job-a662486eb1ae4ce5 from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container e2e ready: true, restart count 0
Feb 17 18:22:09.261: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 18:22:09.261: INFO: sonobuoy-systemd-logs-daemon-set-67976b35ad084332-bvf8d from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 17 18:22:09.261: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 18:22:09.261: INFO: olm-operator-5f5f7bbd7d-8k49r from openshift-operator-lifecycle-manager started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container olm-operator ready: true, restart count 0
Feb 17 18:22:09.261: INFO: ibmcloud-block-storage-plugin-75cf87c786-cc9kd from kube-system started at 2020-02-17 14:34:58 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Feb 17 18:22:09.261: INFO: cluster-node-tuning-operator-5cbf9c8db5-mr2tk from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Feb 17 18:22:09.261: INFO: cloud-credential-operator-6fc5bb58fc-npgb8 from openshift-cloud-credential-operator started at 2020-02-17 14:36:06 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container manager ready: true, restart count 0
Feb 17 18:22:09.261: INFO: kube-state-metrics-5745cc99f5-vd9j6 from openshift-monitoring started at 2020-02-17 14:36:27 +0000 UTC (3 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 17 18:22:09.261: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 17 18:22:09.261: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 17 18:22:09.261: INFO: multus-admission-controller-x8vxh from openshift-multus started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 17 18:22:09.261: INFO: downloads-65dd498cf8-vvlwb from openshift-console started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container download-server ready: true, restart count 0
Feb 17 18:22:09.261: INFO: service-serving-cert-signer-6669ffbb98-s7bf6 from openshift-service-ca started at 2020-02-17 14:36:49 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Feb 17 18:22:09.261: INFO: node-ca-49d68 from openshift-image-registry started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container node-ca ready: true, restart count 0
Feb 17 18:22:09.261: INFO: ibmcloud-block-storage-driver-vnql8 from kube-system started at 2020-02-17 14:34:55 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 17 18:22:09.261: INFO: apiservice-cabundle-injector-745c6fc9d8-h2znf from openshift-service-ca started at 2020-02-17 14:36:50 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Feb 17 18:22:09.261: INFO: ibm-keepalived-watcher-684qj from kube-system started at 2020-02-17 14:34:53 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 18:22:09.261: INFO: console-operator-7897bcfbd8-m6vt2 from openshift-console-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container console-operator ready: true, restart count 1
Feb 17 18:22:09.261: INFO: calico-node-mjlf4 from kube-system started at 2020-02-17 14:34:53 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:22:09.261: INFO: catalog-operator-7fccd6877f-xzz4r from openshift-operator-lifecycle-manager started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container catalog-operator ready: true, restart count 0
Feb 17 18:22:09.261: INFO: downloads-65dd498cf8-cqpss from openshift-console started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container download-server ready: true, restart count 0
Feb 17 18:22:09.261: INFO: sonobuoy from sonobuoy started at 2020-02-17 16:29:56 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.261: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 17 18:22:09.261: INFO: 
Logging pods the kubelet thinks is on node 10.45.66.178 before test
Feb 17 18:22:09.383: INFO: prometheus-operator-7d5f885558-bv55g from openshift-monitoring started at 2020-02-17 14:41:26 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.383: INFO: 	Container prometheus-operator ready: true, restart count 0
Feb 17 18:22:09.383: INFO: node-ca-n4v6k from openshift-image-registry started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.383: INFO: 	Container node-ca ready: true, restart count 0
Feb 17 18:22:09.383: INFO: registry-pvc-permissions-jlk4p from openshift-image-registry started at 2020-02-17 14:40:17 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.383: INFO: 	Container pvc-permissions ready: false, restart count 0
Feb 17 18:22:09.383: INFO: thanos-querier-b949c5db-ns4qs from openshift-monitoring started at 2020-02-17 14:42:53 +0000 UTC (4 container statuses recorded)
Feb 17 18:22:09.383: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:22:09.384: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 17 18:22:09.384: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 18:22:09.384: INFO: 	Container thanos-querier ready: true, restart count 0
Feb 17 18:22:09.384: INFO: router-default-6c67c67864-f8hvb from openshift-ingress started at 2020-02-17 14:42:29 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.384: INFO: 	Container router ready: true, restart count 0
Feb 17 18:22:09.384: INFO: ibm-master-proxy-static-10.45.66.178 from kube-system started at 2020-02-17 14:37:31 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.384: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 18:22:09.384: INFO: 	Container pause ready: true, restart count 0
Feb 17 18:22:09.384: INFO: packageserver-749668c8dd-slgc8 from openshift-operator-lifecycle-manager started at 2020-02-17 15:40:09 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.384: INFO: 	Container packageserver ready: true, restart count 0
Feb 17 18:22:09.385: INFO: sonobuoy-systemd-logs-daemon-set-67976b35ad084332-thldq from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.385: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 17 18:22:09.385: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 18:22:09.385: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-02-17 14:41:51 +0000 UTC (3 container statuses recorded)
Feb 17 18:22:09.385: INFO: 	Container alertmanager ready: true, restart count 0
Feb 17 18:22:09.385: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 17 18:22:09.385: INFO: 	Container config-reloader ready: true, restart count 0
Feb 17 18:22:09.385: INFO: certified-operators-8577555dd-n9z77 from openshift-marketplace started at 2020-02-17 17:37:51 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.385: INFO: 	Container certified-operators ready: true, restart count 0
Feb 17 18:22:09.386: INFO: node-exporter-wmxf5 from openshift-monitoring started at 2020-02-17 14:36:43 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.386: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:22:09.386: INFO: 	Container node-exporter ready: true, restart count 0
Feb 17 18:22:09.386: INFO: dns-default-9fgff from openshift-dns started at 2020-02-17 14:38:00 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.386: INFO: 	Container dns ready: true, restart count 0
Feb 17 18:22:09.386: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 17 18:22:09.386: INFO: cluster-samples-operator-946f4d4c5-66g9k from openshift-cluster-samples-operator started at 2020-02-17 14:38:52 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.386: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Feb 17 18:22:09.387: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Feb 17 18:22:09.387: INFO: image-registry-6d4857c99d-q2dn4 from openshift-image-registry started at 2020-02-17 14:40:17 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.387: INFO: 	Container registry ready: true, restart count 0
Feb 17 18:22:09.387: INFO: console-7547c7bc6c-wschb from openshift-console started at 2020-02-17 14:39:26 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.387: INFO: 	Container console ready: true, restart count 0
Feb 17 18:22:09.387: INFO: calico-node-zkvdn from kube-system started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.387: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:22:09.387: INFO: ibm-keepalived-watcher-4nt2b from kube-system started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.387: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 18:22:09.388: INFO: multus-wbdmp from openshift-multus started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.388: INFO: 	Container kube-multus ready: true, restart count 0
Feb 17 18:22:09.388: INFO: ibm-cloud-provider-ip-158-175-93-66-7b5b7d9cb-jbd6r from ibm-system started at 2020-02-17 14:41:22 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.388: INFO: 	Container ibm-cloud-provider-ip-158-175-93-66 ready: true, restart count 0
Feb 17 18:22:09.388: INFO: grafana-66dff46d7c-l5jsk from openshift-monitoring started at 2020-02-17 14:41:46 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.388: INFO: 	Container grafana ready: true, restart count 0
Feb 17 18:22:09.388: INFO: 	Container grafana-proxy ready: true, restart count 0
Feb 17 18:22:09.388: INFO: tuned-2n22g from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.389: INFO: 	Container tuned ready: true, restart count 0
Feb 17 18:22:09.389: INFO: openshift-kube-proxy-kchsl from openshift-kube-proxy started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.389: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:22:09.389: INFO: calico-typha-76c597bc7d-72pj2 from kube-system started at 2020-02-17 14:38:05 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.389: INFO: 	Container calico-typha ready: true, restart count 0
Feb 17 18:22:09.389: INFO: prometheus-adapter-55674d9685-vvdz2 from openshift-monitoring started at 2020-02-17 14:41:40 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.389: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 17 18:22:09.389: INFO: ibmcloud-block-storage-driver-d7lnw from kube-system started at 2020-02-17 14:36:45 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.389: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 17 18:22:09.389: INFO: multus-admission-controller-s6fg5 from openshift-multus started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.390: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 17 18:22:09.390: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-02-17 14:42:48 +0000 UTC (7 container statuses recorded)
Feb 17 18:22:09.390: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:22:09.390: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 18:22:09.390: INFO: 	Container prometheus ready: true, restart count 1
Feb 17 18:22:09.390: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Feb 17 18:22:09.390: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 17 18:22:09.390: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Feb 17 18:22:09.390: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 17 18:22:09.390: INFO: 
Logging pods the kubelet thinks is on node 10.45.66.189 before test
Feb 17 18:22:09.500: INFO: ibm-master-proxy-static-10.45.66.189 from kube-system started at 2020-02-17 14:37:14 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.500: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 18:22:09.500: INFO: 	Container pause ready: true, restart count 0
Feb 17 18:22:09.500: INFO: multus-df87p from openshift-multus started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.500: INFO: 	Container kube-multus ready: true, restart count 0
Feb 17 18:22:09.500: INFO: ibm-cloud-provider-ip-158-175-93-66-7b5b7d9cb-mhbr7 from ibm-system started at 2020-02-17 14:41:22 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.501: INFO: 	Container ibm-cloud-provider-ip-158-175-93-66 ready: true, restart count 0
Feb 17 18:22:09.501: INFO: openshift-kube-proxy-gwxkw from openshift-kube-proxy started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.501: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 18:22:09.501: INFO: node-ca-56bzx from openshift-image-registry started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.501: INFO: 	Container node-ca ready: true, restart count 0
Feb 17 18:22:09.501: INFO: prometheus-adapter-55674d9685-g5b96 from openshift-monitoring started at 2020-02-17 14:41:40 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.501: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 17 18:22:09.501: INFO: node-exporter-4fhhh from openshift-monitoring started at 2020-02-17 14:36:28 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:22:09.502: INFO: 	Container node-exporter ready: true, restart count 0
Feb 17 18:22:09.502: INFO: community-operators-6c7f5d4977-fxhls from openshift-marketplace started at 2020-02-17 14:38:06 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.502: INFO: 	Container community-operators ready: true, restart count 0
Feb 17 18:22:09.502: INFO: telemeter-client-56f4c98664-jhjns from openshift-monitoring started at 2020-02-17 14:41:37 +0000 UTC (3 container statuses recorded)
Feb 17 18:22:09.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:22:09.502: INFO: 	Container reload ready: true, restart count 0
Feb 17 18:22:09.502: INFO: 	Container telemeter-client ready: true, restart count 0
Feb 17 18:22:09.503: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-02-17 14:42:08 +0000 UTC (3 container statuses recorded)
Feb 17 18:22:09.503: INFO: 	Container alertmanager ready: true, restart count 0
Feb 17 18:22:09.503: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 17 18:22:09.503: INFO: 	Container config-reloader ready: true, restart count 0
Feb 17 18:22:09.503: INFO: ibm-keepalived-watcher-5hn6l from kube-system started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.503: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 18:22:09.503: INFO: calico-node-cctn2 from kube-system started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.503: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 18:22:09.503: INFO: ibmcloud-block-storage-driver-jnxhv from kube-system started at 2020-02-17 14:36:32 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.503: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 17 18:22:09.504: INFO: tuned-kg4dp from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:28 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.504: INFO: 	Container tuned ready: true, restart count 0
Feb 17 18:22:09.504: INFO: thanos-querier-b949c5db-5nhn5 from openshift-monitoring started at 2020-02-17 14:42:35 +0000 UTC (4 container statuses recorded)
Feb 17 18:22:09.504: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:22:09.504: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 17 18:22:09.504: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 18:22:09.504: INFO: 	Container thanos-querier ready: true, restart count 0
Feb 17 18:22:09.504: INFO: redhat-operators-7cc9d7897-nsjpx from openshift-marketplace started at 2020-02-17 14:38:05 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.504: INFO: 	Container redhat-operators ready: true, restart count 0
Feb 17 18:22:09.504: INFO: console-7547c7bc6c-rjbss from openshift-console started at 2020-02-17 14:39:12 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.505: INFO: 	Container console ready: true, restart count 0
Feb 17 18:22:09.505: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-02-17 14:39:35 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.505: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Feb 17 18:22:09.505: INFO: calico-typha-76c597bc7d-h6b67 from kube-system started at 2020-02-17 14:37:29 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.505: INFO: 	Container calico-typha ready: true, restart count 0
Feb 17 18:22:09.505: INFO: dns-default-wkxng from openshift-dns started at 2020-02-17 14:37:34 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.505: INFO: 	Container dns ready: true, restart count 0
Feb 17 18:22:09.505: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 17 18:22:09.506: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-02-17 14:43:08 +0000 UTC (7 container statuses recorded)
Feb 17 18:22:09.506: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 18:22:09.506: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 18:22:09.506: INFO: 	Container prometheus ready: true, restart count 1
Feb 17 18:22:09.506: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Feb 17 18:22:09.506: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 17 18:22:09.506: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Feb 17 18:22:09.506: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 17 18:22:09.506: INFO: multus-admission-controller-lpf76 from openshift-multus started at 2020-02-17 14:37:28 +0000 UTC (1 container statuses recorded)
Feb 17 18:22:09.506: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 17 18:22:09.507: INFO: sonobuoy-systemd-logs-daemon-set-67976b35ad084332-4jdsp from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 18:22:09.507: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Feb 17 18:22:09.507: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15f443a1664b1225], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:22:10.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6242" for this suite.
Feb 17 18:22:18.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:22:20.777: INFO: namespace sched-pred-6242 deletion completed in 10.141891584s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:11.965 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:22:20.777: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 17 18:22:20.959: INFO: Waiting up to 5m0s for pod "pod-638f31c8-cb54-4b2e-b7a6-e5644254323d" in namespace "emptydir-5605" to be "success or failure"
Feb 17 18:22:20.971: INFO: Pod "pod-638f31c8-cb54-4b2e-b7a6-e5644254323d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.343017ms
Feb 17 18:22:22.984: INFO: Pod "pod-638f31c8-cb54-4b2e-b7a6-e5644254323d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025063743s
Feb 17 18:22:24.997: INFO: Pod "pod-638f31c8-cb54-4b2e-b7a6-e5644254323d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037924917s
Feb 17 18:22:27.010: INFO: Pod "pod-638f31c8-cb54-4b2e-b7a6-e5644254323d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050807809s
Feb 17 18:22:29.023: INFO: Pod "pod-638f31c8-cb54-4b2e-b7a6-e5644254323d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.064216165s
STEP: Saw pod success
Feb 17 18:22:29.023: INFO: Pod "pod-638f31c8-cb54-4b2e-b7a6-e5644254323d" satisfied condition "success or failure"
Feb 17 18:22:29.035: INFO: Trying to get logs from node 10.45.66.177 pod pod-638f31c8-cb54-4b2e-b7a6-e5644254323d container test-container: <nil>
STEP: delete the pod
Feb 17 18:22:29.109: INFO: Waiting for pod pod-638f31c8-cb54-4b2e-b7a6-e5644254323d to disappear
Feb 17 18:22:29.129: INFO: Pod pod-638f31c8-cb54-4b2e-b7a6-e5644254323d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:22:29.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5605" for this suite.
Feb 17 18:22:37.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:22:39.294: INFO: namespace emptydir-5605 deletion completed in 10.148122189s

• [SLOW TEST:18.517 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:22:39.294: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:22:39.473: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d5edd6e4-9f01-407a-9868-6ba171a75fe5" in namespace "projected-6784" to be "success or failure"
Feb 17 18:22:39.490: INFO: Pod "downwardapi-volume-d5edd6e4-9f01-407a-9868-6ba171a75fe5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.962963ms
Feb 17 18:22:41.505: INFO: Pod "downwardapi-volume-d5edd6e4-9f01-407a-9868-6ba171a75fe5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031426611s
Feb 17 18:22:43.518: INFO: Pod "downwardapi-volume-d5edd6e4-9f01-407a-9868-6ba171a75fe5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044143832s
Feb 17 18:22:45.531: INFO: Pod "downwardapi-volume-d5edd6e4-9f01-407a-9868-6ba171a75fe5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.057992594s
Feb 17 18:22:47.545: INFO: Pod "downwardapi-volume-d5edd6e4-9f01-407a-9868-6ba171a75fe5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.07112869s
Feb 17 18:22:49.567: INFO: Pod "downwardapi-volume-d5edd6e4-9f01-407a-9868-6ba171a75fe5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.093323766s
STEP: Saw pod success
Feb 17 18:22:49.574: INFO: Pod "downwardapi-volume-d5edd6e4-9f01-407a-9868-6ba171a75fe5" satisfied condition "success or failure"
Feb 17 18:22:49.596: INFO: Trying to get logs from node 10.45.66.177 pod downwardapi-volume-d5edd6e4-9f01-407a-9868-6ba171a75fe5 container client-container: <nil>
STEP: delete the pod
Feb 17 18:22:49.687: INFO: Waiting for pod downwardapi-volume-d5edd6e4-9f01-407a-9868-6ba171a75fe5 to disappear
Feb 17 18:22:49.708: INFO: Pod downwardapi-volume-d5edd6e4-9f01-407a-9868-6ba171a75fe5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:22:49.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6784" for this suite.
Feb 17 18:22:57.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:22:59.914: INFO: namespace projected-6784 deletion completed in 10.156805s

• [SLOW TEST:20.620 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:22:59.914: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 18:23:00.042: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:23:08.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-572" for this suite.
Feb 17 18:23:50.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:23:52.495: INFO: namespace pods-572 deletion completed in 44.135517883s

• [SLOW TEST:52.581 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:23:52.496: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Feb 17 18:23:52.714: INFO: Waiting up to 5m0s for pod "pod-2574b1da-cc6a-4e16-94ab-8e6efd9507c3" in namespace "emptydir-5705" to be "success or failure"
Feb 17 18:23:52.726: INFO: Pod "pod-2574b1da-cc6a-4e16-94ab-8e6efd9507c3": Phase="Pending", Reason="", readiness=false. Elapsed: 11.978861ms
Feb 17 18:23:54.739: INFO: Pod "pod-2574b1da-cc6a-4e16-94ab-8e6efd9507c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024745847s
Feb 17 18:23:56.752: INFO: Pod "pod-2574b1da-cc6a-4e16-94ab-8e6efd9507c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038017571s
Feb 17 18:23:58.765: INFO: Pod "pod-2574b1da-cc6a-4e16-94ab-8e6efd9507c3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051184985s
Feb 17 18:24:00.778: INFO: Pod "pod-2574b1da-cc6a-4e16-94ab-8e6efd9507c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.06417726s
STEP: Saw pod success
Feb 17 18:24:00.778: INFO: Pod "pod-2574b1da-cc6a-4e16-94ab-8e6efd9507c3" satisfied condition "success or failure"
Feb 17 18:24:00.791: INFO: Trying to get logs from node 10.45.66.178 pod pod-2574b1da-cc6a-4e16-94ab-8e6efd9507c3 container test-container: <nil>
STEP: delete the pod
Feb 17 18:24:00.886: INFO: Waiting for pod pod-2574b1da-cc6a-4e16-94ab-8e6efd9507c3 to disappear
Feb 17 18:24:00.899: INFO: Pod pod-2574b1da-cc6a-4e16-94ab-8e6efd9507c3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:24:00.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5705" for this suite.
Feb 17 18:24:08.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:24:11.079: INFO: namespace emptydir-5705 deletion completed in 10.152578874s

• [SLOW TEST:18.583 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:24:11.079: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 18:24:11.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-1423'
Feb 17 18:24:11.586: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 17 18:24:11.586: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Feb 17 18:24:15.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete deployment e2e-test-httpd-deployment --namespace=kubectl-1423'
Feb 17 18:24:15.821: INFO: stderr: ""
Feb 17 18:24:15.821: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:24:15.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1423" for this suite.
Feb 17 18:24:23.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:24:26.009: INFO: namespace kubectl-1423 deletion completed in 10.164023474s

• [SLOW TEST:14.929 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:24:26.009: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Feb 17 18:24:26.199: INFO: Waiting up to 5m0s for pod "downward-api-aef68a4a-7f8a-496b-bd57-ad91fcdd2e06" in namespace "downward-api-7190" to be "success or failure"
Feb 17 18:24:26.212: INFO: Pod "downward-api-aef68a4a-7f8a-496b-bd57-ad91fcdd2e06": Phase="Pending", Reason="", readiness=false. Elapsed: 13.156687ms
Feb 17 18:24:28.227: INFO: Pod "downward-api-aef68a4a-7f8a-496b-bd57-ad91fcdd2e06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028282834s
Feb 17 18:24:30.240: INFO: Pod "downward-api-aef68a4a-7f8a-496b-bd57-ad91fcdd2e06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041562852s
Feb 17 18:24:32.256: INFO: Pod "downward-api-aef68a4a-7f8a-496b-bd57-ad91fcdd2e06": Phase="Pending", Reason="", readiness=false. Elapsed: 6.057491749s
Feb 17 18:24:34.269: INFO: Pod "downward-api-aef68a4a-7f8a-496b-bd57-ad91fcdd2e06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.070473983s
STEP: Saw pod success
Feb 17 18:24:34.269: INFO: Pod "downward-api-aef68a4a-7f8a-496b-bd57-ad91fcdd2e06" satisfied condition "success or failure"
Feb 17 18:24:34.285: INFO: Trying to get logs from node 10.45.66.189 pod downward-api-aef68a4a-7f8a-496b-bd57-ad91fcdd2e06 container dapi-container: <nil>
STEP: delete the pod
Feb 17 18:24:34.414: INFO: Waiting for pod downward-api-aef68a4a-7f8a-496b-bd57-ad91fcdd2e06 to disappear
Feb 17 18:24:34.425: INFO: Pod downward-api-aef68a4a-7f8a-496b-bd57-ad91fcdd2e06 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:24:34.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7190" for this suite.
Feb 17 18:24:42.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:24:44.586: INFO: namespace downward-api-7190 deletion completed in 10.137521441s

• [SLOW TEST:18.576 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:24:44.586: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 18:24:44.720: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:24:50.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3799" for this suite.
Feb 17 18:24:58.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:25:00.959: INFO: namespace custom-resource-definition-3799 deletion completed in 10.162052815s

• [SLOW TEST:16.373 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:25:00.959: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Feb 17 18:25:01.188: INFO: Waiting up to 5m0s for pod "client-containers-590932fc-6a09-4259-a373-5d44ea29fc56" in namespace "containers-7528" to be "success or failure"
Feb 17 18:25:01.201: INFO: Pod "client-containers-590932fc-6a09-4259-a373-5d44ea29fc56": Phase="Pending", Reason="", readiness=false. Elapsed: 13.435799ms
Feb 17 18:25:03.214: INFO: Pod "client-containers-590932fc-6a09-4259-a373-5d44ea29fc56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025897124s
Feb 17 18:25:05.226: INFO: Pod "client-containers-590932fc-6a09-4259-a373-5d44ea29fc56": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038362326s
Feb 17 18:25:07.241: INFO: Pod "client-containers-590932fc-6a09-4259-a373-5d44ea29fc56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053233186s
Feb 17 18:25:09.254: INFO: Pod "client-containers-590932fc-6a09-4259-a373-5d44ea29fc56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.06654049s
STEP: Saw pod success
Feb 17 18:25:09.254: INFO: Pod "client-containers-590932fc-6a09-4259-a373-5d44ea29fc56" satisfied condition "success or failure"
Feb 17 18:25:09.267: INFO: Trying to get logs from node 10.45.66.189 pod client-containers-590932fc-6a09-4259-a373-5d44ea29fc56 container test-container: <nil>
STEP: delete the pod
Feb 17 18:25:09.329: INFO: Waiting for pod client-containers-590932fc-6a09-4259-a373-5d44ea29fc56 to disappear
Feb 17 18:25:09.342: INFO: Pod client-containers-590932fc-6a09-4259-a373-5d44ea29fc56 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:25:09.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7528" for this suite.
Feb 17 18:25:17.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:25:19.495: INFO: namespace containers-7528 deletion completed in 10.120964085s

• [SLOW TEST:18.536 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:25:19.495: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Feb 17 18:25:19.661: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:26:02.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5640" for this suite.
Feb 17 18:26:10.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:26:12.398: INFO: namespace crd-publish-openapi-5640 deletion completed in 10.13187648s

• [SLOW TEST:52.903 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:26:12.398: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-22c48008-8b69-4ece-9807-503a3af60794
STEP: Creating a pod to test consume configMaps
Feb 17 18:26:12.590: INFO: Waiting up to 5m0s for pod "pod-configmaps-33ea65d8-55fd-4210-89ac-41d50763f424" in namespace "configmap-8209" to be "success or failure"
Feb 17 18:26:12.602: INFO: Pod "pod-configmaps-33ea65d8-55fd-4210-89ac-41d50763f424": Phase="Pending", Reason="", readiness=false. Elapsed: 12.572484ms
Feb 17 18:26:14.615: INFO: Pod "pod-configmaps-33ea65d8-55fd-4210-89ac-41d50763f424": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025772531s
Feb 17 18:26:16.629: INFO: Pod "pod-configmaps-33ea65d8-55fd-4210-89ac-41d50763f424": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039560332s
Feb 17 18:26:18.642: INFO: Pod "pod-configmaps-33ea65d8-55fd-4210-89ac-41d50763f424": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052433994s
Feb 17 18:26:20.657: INFO: Pod "pod-configmaps-33ea65d8-55fd-4210-89ac-41d50763f424": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.067227047s
STEP: Saw pod success
Feb 17 18:26:20.661: INFO: Pod "pod-configmaps-33ea65d8-55fd-4210-89ac-41d50763f424" satisfied condition "success or failure"
Feb 17 18:26:20.672: INFO: Trying to get logs from node 10.45.66.177 pod pod-configmaps-33ea65d8-55fd-4210-89ac-41d50763f424 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:26:20.783: INFO: Waiting for pod pod-configmaps-33ea65d8-55fd-4210-89ac-41d50763f424 to disappear
Feb 17 18:26:20.795: INFO: Pod pod-configmaps-33ea65d8-55fd-4210-89ac-41d50763f424 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:26:20.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8209" for this suite.
Feb 17 18:26:28.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:26:30.989: INFO: namespace configmap-8209 deletion completed in 10.175743046s

• [SLOW TEST:18.592 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:26:30.990: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-49821c24-d55c-47cd-8e0c-ab1dfece5fd1 in namespace container-probe-7076
Feb 17 18:26:40.228: INFO: Started pod busybox-49821c24-d55c-47cd-8e0c-ab1dfece5fd1 in namespace container-probe-7076
STEP: checking the pod's current state and verifying that restartCount is present
Feb 17 18:26:40.241: INFO: Initial restart count of pod busybox-49821c24-d55c-47cd-8e0c-ab1dfece5fd1 is 0
Feb 17 18:27:36.640: INFO: Restart count of pod container-probe-7076/busybox-49821c24-d55c-47cd-8e0c-ab1dfece5fd1 is now 1 (56.399037944s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:27:36.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7076" for this suite.
Feb 17 18:27:44.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:27:46.840: INFO: namespace container-probe-7076 deletion completed in 10.114494499s

• [SLOW TEST:75.851 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:27:46.849: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-539
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-539
I0217 18:27:47.102680      26 runners.go:184] Created replication controller with name: externalname-service, namespace: services-539, replica count: 2
I0217 18:27:50.154936      26 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 18:27:53.155295      26 runners.go:184] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 18:27:56.155: INFO: Creating new exec pod
I0217 18:27:56.155579      26 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Feb 17 18:28:05.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=services-539 execpod5hxhk -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Feb 17 18:28:05.606: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Feb 17 18:28:05.606: INFO: stdout: ""
Feb 17 18:28:05.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=services-539 execpod5hxhk -- /bin/sh -x -c nc -zv -t -w 2 172.21.29.119 80'
Feb 17 18:28:05.998: INFO: stderr: "+ nc -zv -t -w 2 172.21.29.119 80\nConnection to 172.21.29.119 80 port [tcp/http] succeeded!\n"
Feb 17 18:28:05.998: INFO: stdout: ""
Feb 17 18:28:05.998: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:28:06.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-539" for this suite.
Feb 17 18:28:14.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:28:16.288: INFO: namespace services-539 deletion completed in 10.182634383s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:29.438 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:28:16.288: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:28:17.239: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:28:19.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560897, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560897, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560897, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560897, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:28:21.291: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560897, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560897, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560897, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560897, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:28:23.291: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560897, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560897, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560897, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560897, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:28:25.292: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560897, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560897, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560897, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717560897, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:28:28.326: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:28:28.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-176" for this suite.
Feb 17 18:28:36.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:28:38.760: INFO: namespace webhook-176 deletion completed in 10.121421649s
STEP: Destroying namespace "webhook-176-markers" for this suite.
Feb 17 18:28:46.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:28:48.900: INFO: namespace webhook-176-markers deletion completed in 10.139809203s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:32.682 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:28:48.970: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Feb 17 18:28:49.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 api-versions'
Feb 17 18:28:49.220: INFO: stderr: ""
Feb 17 18:28:49.220: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.openshift.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:28:49.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7199" for this suite.
Feb 17 18:28:57.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:28:59.360: INFO: namespace kubectl-7199 deletion completed in 10.113799283s

• [SLOW TEST:10.390 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:28:59.367: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 18:28:59.544: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-9828a649-c090-40f7-b957-1b68ad5de1ff" in namespace "security-context-test-586" to be "success or failure"
Feb 17 18:28:59.558: INFO: Pod "busybox-privileged-false-9828a649-c090-40f7-b957-1b68ad5de1ff": Phase="Pending", Reason="", readiness=false. Elapsed: 13.640665ms
Feb 17 18:29:01.571: INFO: Pod "busybox-privileged-false-9828a649-c090-40f7-b957-1b68ad5de1ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026808369s
Feb 17 18:29:03.586: INFO: Pod "busybox-privileged-false-9828a649-c090-40f7-b957-1b68ad5de1ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041863499s
Feb 17 18:29:05.599: INFO: Pod "busybox-privileged-false-9828a649-c090-40f7-b957-1b68ad5de1ff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055170374s
Feb 17 18:29:07.617: INFO: Pod "busybox-privileged-false-9828a649-c090-40f7-b957-1b68ad5de1ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.072677909s
Feb 17 18:29:07.617: INFO: Pod "busybox-privileged-false-9828a649-c090-40f7-b957-1b68ad5de1ff" satisfied condition "success or failure"
Feb 17 18:29:07.682: INFO: Got logs for pod "busybox-privileged-false-9828a649-c090-40f7-b957-1b68ad5de1ff": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:29:07.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-586" for this suite.
Feb 17 18:29:15.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:29:18.055: INFO: namespace security-context-test-586 deletion completed in 10.350647947s

• [SLOW TEST:18.689 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:29:18.056: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 18:29:18.317: INFO: Create a RollingUpdate DaemonSet
Feb 17 18:29:18.338: INFO: Check that daemon pods launch on every node of the cluster
Feb 17 18:29:18.374: INFO: Number of nodes with available pods: 0
Feb 17 18:29:18.374: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:29:19.410: INFO: Number of nodes with available pods: 0
Feb 17 18:29:19.410: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:29:20.412: INFO: Number of nodes with available pods: 0
Feb 17 18:29:20.412: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:29:21.412: INFO: Number of nodes with available pods: 0
Feb 17 18:29:21.413: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:29:22.412: INFO: Number of nodes with available pods: 0
Feb 17 18:29:22.412: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:29:23.411: INFO: Number of nodes with available pods: 0
Feb 17 18:29:23.411: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:29:24.411: INFO: Number of nodes with available pods: 0
Feb 17 18:29:24.411: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:29:25.409: INFO: Number of nodes with available pods: 0
Feb 17 18:29:25.409: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:29:26.417: INFO: Number of nodes with available pods: 3
Feb 17 18:29:26.417: INFO: Number of running nodes: 3, number of available pods: 3
Feb 17 18:29:26.417: INFO: Update the DaemonSet to trigger a rollout
Feb 17 18:29:26.494: INFO: Updating DaemonSet daemon-set
Feb 17 18:29:30.560: INFO: Roll back the DaemonSet before rollout is complete
Feb 17 18:29:30.589: INFO: Updating DaemonSet daemon-set
Feb 17 18:29:30.589: INFO: Make sure DaemonSet rollback is complete
Feb 17 18:29:30.603: INFO: Wrong image for pod: daemon-set-tg7zd. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 17 18:29:30.603: INFO: Pod daemon-set-tg7zd is not available
Feb 17 18:29:31.639: INFO: Wrong image for pod: daemon-set-tg7zd. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 17 18:29:31.639: INFO: Pod daemon-set-tg7zd is not available
Feb 17 18:29:32.640: INFO: Wrong image for pod: daemon-set-tg7zd. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 17 18:29:32.640: INFO: Pod daemon-set-tg7zd is not available
Feb 17 18:29:33.639: INFO: Wrong image for pod: daemon-set-tg7zd. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Feb 17 18:29:33.639: INFO: Pod daemon-set-tg7zd is not available
Feb 17 18:29:34.641: INFO: Pod daemon-set-r9wmh is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9393, will wait for the garbage collector to delete the pods
Feb 17 18:29:34.781: INFO: Deleting DaemonSet.extensions daemon-set took: 27.643068ms
Feb 17 18:29:35.381: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.360331ms
Feb 17 18:29:47.895: INFO: Number of nodes with available pods: 0
Feb 17 18:29:47.895: INFO: Number of running nodes: 0, number of available pods: 0
Feb 17 18:29:47.907: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9393/daemonsets","resourceVersion":"97974"},"items":null}

Feb 17 18:29:47.921: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9393/pods","resourceVersion":"97974"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:29:47.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9393" for this suite.
Feb 17 18:29:56.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:29:58.111: INFO: namespace daemonsets-9393 deletion completed in 10.110600497s

• [SLOW TEST:40.055 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:29:58.111: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 17 18:29:58.305: INFO: Waiting up to 5m0s for pod "pod-5028f9ce-ac5e-429a-b6d6-c8a195949816" in namespace "emptydir-7583" to be "success or failure"
Feb 17 18:29:58.323: INFO: Pod "pod-5028f9ce-ac5e-429a-b6d6-c8a195949816": Phase="Pending", Reason="", readiness=false. Elapsed: 17.674037ms
Feb 17 18:30:00.337: INFO: Pod "pod-5028f9ce-ac5e-429a-b6d6-c8a195949816": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032072427s
Feb 17 18:30:02.351: INFO: Pod "pod-5028f9ce-ac5e-429a-b6d6-c8a195949816": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045746145s
Feb 17 18:30:04.364: INFO: Pod "pod-5028f9ce-ac5e-429a-b6d6-c8a195949816": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058776503s
Feb 17 18:30:06.380: INFO: Pod "pod-5028f9ce-ac5e-429a-b6d6-c8a195949816": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.074942264s
STEP: Saw pod success
Feb 17 18:30:06.380: INFO: Pod "pod-5028f9ce-ac5e-429a-b6d6-c8a195949816" satisfied condition "success or failure"
Feb 17 18:30:06.393: INFO: Trying to get logs from node 10.45.66.189 pod pod-5028f9ce-ac5e-429a-b6d6-c8a195949816 container test-container: <nil>
STEP: delete the pod
Feb 17 18:30:06.504: INFO: Waiting for pod pod-5028f9ce-ac5e-429a-b6d6-c8a195949816 to disappear
Feb 17 18:30:06.520: INFO: Pod pod-5028f9ce-ac5e-429a-b6d6-c8a195949816 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:30:06.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7583" for this suite.
Feb 17 18:30:14.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:30:16.696: INFO: namespace emptydir-7583 deletion completed in 10.146208389s

• [SLOW TEST:18.586 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:30:16.697: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-6241
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6241
STEP: Deleting pre-stop pod
Feb 17 18:30:40.036: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:30:40.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6241" for this suite.
Feb 17 18:31:20.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:31:22.244: INFO: namespace prestop-6241 deletion completed in 42.151839958s

• [SLOW TEST:65.548 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:31:22.244: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:31:22.830: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:31:24.874: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561082, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561082, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561082, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561082, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:31:26.886: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561082, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561082, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561082, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561082, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:31:28.888: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561082, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561082, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561082, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561082, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:31:31.914: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 18:31:31.929: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6681-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:31:33.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4334" for this suite.
Feb 17 18:31:41.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:31:43.394: INFO: namespace webhook-4334 deletion completed in 10.124364682s
STEP: Destroying namespace "webhook-4334-markers" for this suite.
Feb 17 18:31:51.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:31:53.529: INFO: namespace webhook-4334-markers deletion completed in 10.135152704s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:31.352 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:31:53.597: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Feb 17 18:31:53.723: INFO: PodSpec: initContainers in spec.initContainers
Feb 17 18:32:44.297: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7cc8d2c1-b643-4d66-9d55-d5912a3e64fc", GenerateName:"", Namespace:"init-container-4663", SelfLink:"/api/v1/namespaces/init-container-4663/pods/pod-init-7cc8d2c1-b643-4d66-9d55-d5912a3e64fc", UID:"ec63363e-26ec-44fb-be67-c4f49e5ebc86", ResourceVersion:"99211", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63717561113, loc:(*time.Location)(0x84c02a0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"723253120"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.30.50.161/32", "cni.projectcalico.org/podIPs":"172.30.50.161/32", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.50.161\"\n    ],\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-rsf6j", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0026f2f80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rsf6j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0033f5450), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rsf6j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0033f54f0), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-rsf6j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0033f53b0), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc009c7e148), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.45.66.189", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002de0e40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc009c7e200)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc009c7e220)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc009c7e23c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc009c7e240), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561113, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561113, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561113, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561113, loc:(*time.Location)(0x84c02a0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.45.66.189", PodIP:"172.30.50.161", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.50.161"}}, StartTime:(*v1.Time)(0xc0081adee0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003272ee0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003272f50)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://bb97aec792efd013ca8207f7dd39ee94e40586d2f638a652e7fbd37ff19e0158", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0081adf20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0081adf00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc009c7e2bf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:32:44.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4663" for this suite.
Feb 17 18:32:58.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:33:00.439: INFO: namespace init-container-4663 deletion completed in 16.11806241s

• [SLOW TEST:66.842 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:33:00.439: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-d6abdb26-8b1a-4b4f-a4cc-3ede13ce67da in namespace container-probe-3884
Feb 17 18:33:08.695: INFO: Started pod liveness-d6abdb26-8b1a-4b4f-a4cc-3ede13ce67da in namespace container-probe-3884
STEP: checking the pod's current state and verifying that restartCount is present
Feb 17 18:33:08.708: INFO: Initial restart count of pod liveness-d6abdb26-8b1a-4b4f-a4cc-3ede13ce67da is 0
Feb 17 18:33:24.830: INFO: Restart count of pod container-probe-3884/liveness-d6abdb26-8b1a-4b4f-a4cc-3ede13ce67da is now 1 (16.122056337s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:33:24.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3884" for this suite.
Feb 17 18:33:32.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:33:35.010: INFO: namespace container-probe-3884 deletion completed in 10.107821052s

• [SLOW TEST:34.571 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:33:35.011: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Feb 17 18:33:35.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 cluster-info'
Feb 17 18:33:35.271: INFO: stderr: ""
Feb 17 18:33:35.271: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:33:35.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2369" for this suite.
Feb 17 18:33:43.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:33:45.457: INFO: namespace kubectl-2369 deletion completed in 10.152903046s

• [SLOW TEST:10.446 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:33:45.457: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:33:45.649: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42f1e521-e894-43c1-ac84-d5cb2f10746e" in namespace "downward-api-1667" to be "success or failure"
Feb 17 18:33:45.663: INFO: Pod "downwardapi-volume-42f1e521-e894-43c1-ac84-d5cb2f10746e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.916783ms
Feb 17 18:33:47.675: INFO: Pod "downwardapi-volume-42f1e521-e894-43c1-ac84-d5cb2f10746e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025859315s
Feb 17 18:33:49.689: INFO: Pod "downwardapi-volume-42f1e521-e894-43c1-ac84-d5cb2f10746e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039503982s
Feb 17 18:33:51.704: INFO: Pod "downwardapi-volume-42f1e521-e894-43c1-ac84-d5cb2f10746e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054318452s
Feb 17 18:33:53.717: INFO: Pod "downwardapi-volume-42f1e521-e894-43c1-ac84-d5cb2f10746e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.067053289s
STEP: Saw pod success
Feb 17 18:33:53.717: INFO: Pod "downwardapi-volume-42f1e521-e894-43c1-ac84-d5cb2f10746e" satisfied condition "success or failure"
Feb 17 18:33:53.729: INFO: Trying to get logs from node 10.45.66.178 pod downwardapi-volume-42f1e521-e894-43c1-ac84-d5cb2f10746e container client-container: <nil>
STEP: delete the pod
Feb 17 18:33:53.830: INFO: Waiting for pod downwardapi-volume-42f1e521-e894-43c1-ac84-d5cb2f10746e to disappear
Feb 17 18:33:53.842: INFO: Pod downwardapi-volume-42f1e521-e894-43c1-ac84-d5cb2f10746e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:33:53.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1667" for this suite.
Feb 17 18:34:01.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:34:03.985: INFO: namespace downward-api-1667 deletion completed in 10.110112321s

• [SLOW TEST:18.528 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:34:03.985: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-9348, will wait for the garbage collector to delete the pods
Feb 17 18:34:12.242: INFO: Deleting Job.batch foo took: 26.192527ms
Feb 17 18:34:12.742: INFO: Terminating Job.batch foo pods took: 500.488489ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:34:47.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9348" for this suite.
Feb 17 18:34:56.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:34:58.142: INFO: namespace job-9348 deletion completed in 10.159184521s

• [SLOW TEST:54.157 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:34:58.142: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:34:58.328: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3b8a925-53ff-4637-8751-50599ac673ea" in namespace "downward-api-7565" to be "success or failure"
Feb 17 18:34:58.344: INFO: Pod "downwardapi-volume-d3b8a925-53ff-4637-8751-50599ac673ea": Phase="Pending", Reason="", readiness=false. Elapsed: 15.160947ms
Feb 17 18:35:00.357: INFO: Pod "downwardapi-volume-d3b8a925-53ff-4637-8751-50599ac673ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0284319s
Feb 17 18:35:02.371: INFO: Pod "downwardapi-volume-d3b8a925-53ff-4637-8751-50599ac673ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042275468s
Feb 17 18:35:04.384: INFO: Pod "downwardapi-volume-d3b8a925-53ff-4637-8751-50599ac673ea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055419863s
Feb 17 18:35:06.397: INFO: Pod "downwardapi-volume-d3b8a925-53ff-4637-8751-50599ac673ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068278081s
STEP: Saw pod success
Feb 17 18:35:06.397: INFO: Pod "downwardapi-volume-d3b8a925-53ff-4637-8751-50599ac673ea" satisfied condition "success or failure"
Feb 17 18:35:06.414: INFO: Trying to get logs from node 10.45.66.177 pod downwardapi-volume-d3b8a925-53ff-4637-8751-50599ac673ea container client-container: <nil>
STEP: delete the pod
Feb 17 18:35:06.517: INFO: Waiting for pod downwardapi-volume-d3b8a925-53ff-4637-8751-50599ac673ea to disappear
Feb 17 18:35:06.530: INFO: Pod downwardapi-volume-d3b8a925-53ff-4637-8751-50599ac673ea no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:35:06.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7565" for this suite.
Feb 17 18:35:14.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:35:16.682: INFO: namespace downward-api-7565 deletion completed in 10.128077965s

• [SLOW TEST:18.540 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:35:16.683: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Feb 17 18:35:16.899: INFO: Waiting up to 5m0s for pod "client-containers-7ad7624d-d797-4c61-9182-c4b5b49c746b" in namespace "containers-8966" to be "success or failure"
Feb 17 18:35:16.914: INFO: Pod "client-containers-7ad7624d-d797-4c61-9182-c4b5b49c746b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.880182ms
Feb 17 18:35:18.929: INFO: Pod "client-containers-7ad7624d-d797-4c61-9182-c4b5b49c746b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0297896s
Feb 17 18:35:20.942: INFO: Pod "client-containers-7ad7624d-d797-4c61-9182-c4b5b49c746b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04292038s
Feb 17 18:35:22.955: INFO: Pod "client-containers-7ad7624d-d797-4c61-9182-c4b5b49c746b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05613606s
Feb 17 18:35:24.968: INFO: Pod "client-containers-7ad7624d-d797-4c61-9182-c4b5b49c746b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.069712546s
STEP: Saw pod success
Feb 17 18:35:24.969: INFO: Pod "client-containers-7ad7624d-d797-4c61-9182-c4b5b49c746b" satisfied condition "success or failure"
Feb 17 18:35:24.981: INFO: Trying to get logs from node 10.45.66.177 pod client-containers-7ad7624d-d797-4c61-9182-c4b5b49c746b container test-container: <nil>
STEP: delete the pod
Feb 17 18:35:25.046: INFO: Waiting for pod client-containers-7ad7624d-d797-4c61-9182-c4b5b49c746b to disappear
Feb 17 18:35:25.058: INFO: Pod client-containers-7ad7624d-d797-4c61-9182-c4b5b49c746b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:35:25.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8966" for this suite.
Feb 17 18:35:33.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:35:35.223: INFO: namespace containers-8966 deletion completed in 10.131233406s

• [SLOW TEST:18.540 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:35:35.224: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0217 18:35:36.530288      26 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Feb 17 18:35:36.530: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:35:36.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4078" for this suite.
Feb 17 18:35:44.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:35:46.690: INFO: namespace gc-4078 deletion completed in 10.124870161s

• [SLOW TEST:11.466 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:35:46.690: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Feb 17 18:35:46.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 --namespace=kubectl-2097 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Feb 17 18:35:55.069: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Feb 17 18:35:55.069: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:35:57.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2097" for this suite.
Feb 17 18:36:09.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:36:11.258: INFO: namespace kubectl-2097 deletion completed in 14.136136187s

• [SLOW TEST:24.568 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:36:11.258: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 18:36:11.426: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-a0ad5e22-148f-41d5-92e8-0dad9ae4e930
STEP: Creating secret with name s-test-opt-upd-fcfef6bf-d15e-486a-a898-e3d70fd4298f
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a0ad5e22-148f-41d5-92e8-0dad9ae4e930
STEP: Updating secret s-test-opt-upd-fcfef6bf-d15e-486a-a898-e3d70fd4298f
STEP: Creating secret with name s-test-opt-create-ee048b11-15de-40da-8bea-3ea41c5aa586
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:37:24.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2507" for this suite.
Feb 17 18:37:56.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:37:58.974: INFO: namespace projected-2507 deletion completed in 34.17406443s

• [SLOW TEST:107.716 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:37:58.982: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:37:59.156: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d593d8ae-52da-4ad4-be92-6fbdce9a7495" in namespace "downward-api-4356" to be "success or failure"
Feb 17 18:37:59.168: INFO: Pod "downwardapi-volume-d593d8ae-52da-4ad4-be92-6fbdce9a7495": Phase="Pending", Reason="", readiness=false. Elapsed: 12.25784ms
Feb 17 18:38:01.181: INFO: Pod "downwardapi-volume-d593d8ae-52da-4ad4-be92-6fbdce9a7495": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025452091s
Feb 17 18:38:03.194: INFO: Pod "downwardapi-volume-d593d8ae-52da-4ad4-be92-6fbdce9a7495": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038076486s
Feb 17 18:38:05.206: INFO: Pod "downwardapi-volume-d593d8ae-52da-4ad4-be92-6fbdce9a7495": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050191519s
Feb 17 18:38:07.219: INFO: Pod "downwardapi-volume-d593d8ae-52da-4ad4-be92-6fbdce9a7495": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.063380364s
STEP: Saw pod success
Feb 17 18:38:07.219: INFO: Pod "downwardapi-volume-d593d8ae-52da-4ad4-be92-6fbdce9a7495" satisfied condition "success or failure"
Feb 17 18:38:07.232: INFO: Trying to get logs from node 10.45.66.178 pod downwardapi-volume-d593d8ae-52da-4ad4-be92-6fbdce9a7495 container client-container: <nil>
STEP: delete the pod
Feb 17 18:38:07.295: INFO: Waiting for pod downwardapi-volume-d593d8ae-52da-4ad4-be92-6fbdce9a7495 to disappear
Feb 17 18:38:07.307: INFO: Pod downwardapi-volume-d593d8ae-52da-4ad4-be92-6fbdce9a7495 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:38:07.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4356" for this suite.
Feb 17 18:38:15.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:38:17.489: INFO: namespace downward-api-4356 deletion completed in 10.144077127s

• [SLOW TEST:18.508 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:38:17.489: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Feb 17 18:38:17.665: INFO: Waiting up to 5m0s for pod "var-expansion-43728a66-1415-4121-8e6b-6e157338920f" in namespace "var-expansion-4249" to be "success or failure"
Feb 17 18:38:17.681: INFO: Pod "var-expansion-43728a66-1415-4121-8e6b-6e157338920f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.856126ms
Feb 17 18:38:19.694: INFO: Pod "var-expansion-43728a66-1415-4121-8e6b-6e157338920f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026001075s
Feb 17 18:38:21.708: INFO: Pod "var-expansion-43728a66-1415-4121-8e6b-6e157338920f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039577183s
Feb 17 18:38:23.722: INFO: Pod "var-expansion-43728a66-1415-4121-8e6b-6e157338920f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05373498s
Feb 17 18:38:25.736: INFO: Pod "var-expansion-43728a66-1415-4121-8e6b-6e157338920f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068223157s
STEP: Saw pod success
Feb 17 18:38:25.736: INFO: Pod "var-expansion-43728a66-1415-4121-8e6b-6e157338920f" satisfied condition "success or failure"
Feb 17 18:38:25.754: INFO: Trying to get logs from node 10.45.66.189 pod var-expansion-43728a66-1415-4121-8e6b-6e157338920f container dapi-container: <nil>
STEP: delete the pod
Feb 17 18:38:25.856: INFO: Waiting for pod var-expansion-43728a66-1415-4121-8e6b-6e157338920f to disappear
Feb 17 18:38:25.868: INFO: Pod var-expansion-43728a66-1415-4121-8e6b-6e157338920f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:38:25.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4249" for this suite.
Feb 17 18:38:33.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:38:36.068: INFO: namespace var-expansion-4249 deletion completed in 10.172070962s

• [SLOW TEST:18.579 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:38:36.069: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1718
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1718
STEP: creating replication controller externalsvc in namespace services-1718
I0217 18:38:36.310364      26 runners.go:184] Created replication controller with name: externalsvc, namespace: services-1718, replica count: 2
I0217 18:38:39.370246      26 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 18:38:42.370567      26 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 18:38:45.370948      26 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Feb 17 18:38:45.430: INFO: Creating new exec pod
Feb 17 18:38:53.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=services-1718 execpodw22pn -- /bin/sh -x -c nslookup clusterip-service'
Feb 17 18:38:53.888: INFO: stderr: "+ nslookup clusterip-service\n"
Feb 17 18:38:53.889: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-1718.svc.cluster.local\tcanonical name = externalsvc.services-1718.svc.cluster.local.\nName:\texternalsvc.services-1718.svc.cluster.local\nAddress: 172.21.183.115\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1718, will wait for the garbage collector to delete the pods
Feb 17 18:38:53.986: INFO: Deleting ReplicationController externalsvc took: 32.707206ms
Feb 17 18:38:54.486: INFO: Terminating ReplicationController externalsvc pods took: 500.25152ms
Feb 17 18:39:08.763: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:39:08.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1718" for this suite.
Feb 17 18:39:16.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:39:18.982: INFO: namespace services-1718 deletion completed in 10.125643062s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:42.914 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:39:18.982: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-rjmv
STEP: Creating a pod to test atomic-volume-subpath
Feb 17 18:39:19.179: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-rjmv" in namespace "subpath-6792" to be "success or failure"
Feb 17 18:39:19.192: INFO: Pod "pod-subpath-test-projected-rjmv": Phase="Pending", Reason="", readiness=false. Elapsed: 13.040284ms
Feb 17 18:39:21.205: INFO: Pod "pod-subpath-test-projected-rjmv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026357929s
Feb 17 18:39:23.220: INFO: Pod "pod-subpath-test-projected-rjmv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041355736s
Feb 17 18:39:25.234: INFO: Pod "pod-subpath-test-projected-rjmv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055021788s
Feb 17 18:39:27.248: INFO: Pod "pod-subpath-test-projected-rjmv": Phase="Running", Reason="", readiness=true. Elapsed: 8.068657095s
Feb 17 18:39:29.261: INFO: Pod "pod-subpath-test-projected-rjmv": Phase="Running", Reason="", readiness=true. Elapsed: 10.081774918s
Feb 17 18:39:31.273: INFO: Pod "pod-subpath-test-projected-rjmv": Phase="Running", Reason="", readiness=true. Elapsed: 12.094516497s
Feb 17 18:39:33.286: INFO: Pod "pod-subpath-test-projected-rjmv": Phase="Running", Reason="", readiness=true. Elapsed: 14.107375678s
Feb 17 18:39:35.300: INFO: Pod "pod-subpath-test-projected-rjmv": Phase="Running", Reason="", readiness=true. Elapsed: 16.120997642s
Feb 17 18:39:37.313: INFO: Pod "pod-subpath-test-projected-rjmv": Phase="Running", Reason="", readiness=true. Elapsed: 18.133686743s
Feb 17 18:39:39.326: INFO: Pod "pod-subpath-test-projected-rjmv": Phase="Running", Reason="", readiness=true. Elapsed: 20.147442274s
Feb 17 18:39:41.340: INFO: Pod "pod-subpath-test-projected-rjmv": Phase="Running", Reason="", readiness=true. Elapsed: 22.16076216s
Feb 17 18:39:43.352: INFO: Pod "pod-subpath-test-projected-rjmv": Phase="Running", Reason="", readiness=true. Elapsed: 24.173544711s
Feb 17 18:39:45.365: INFO: Pod "pod-subpath-test-projected-rjmv": Phase="Running", Reason="", readiness=true. Elapsed: 26.186506847s
Feb 17 18:39:47.378: INFO: Pod "pod-subpath-test-projected-rjmv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.19933716s
STEP: Saw pod success
Feb 17 18:39:47.378: INFO: Pod "pod-subpath-test-projected-rjmv" satisfied condition "success or failure"
Feb 17 18:39:47.390: INFO: Trying to get logs from node 10.45.66.189 pod pod-subpath-test-projected-rjmv container test-container-subpath-projected-rjmv: <nil>
STEP: delete the pod
Feb 17 18:39:47.455: INFO: Waiting for pod pod-subpath-test-projected-rjmv to disappear
Feb 17 18:39:47.467: INFO: Pod pod-subpath-test-projected-rjmv no longer exists
STEP: Deleting pod pod-subpath-test-projected-rjmv
Feb 17 18:39:47.467: INFO: Deleting pod "pod-subpath-test-projected-rjmv" in namespace "subpath-6792"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:39:47.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6792" for this suite.
Feb 17 18:39:55.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:39:57.667: INFO: namespace subpath-6792 deletion completed in 10.137142936s

• [SLOW TEST:38.685 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:39:57.669: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:39:58.336: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:40:00.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561598, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561598, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561598, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561598, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:40:02.404: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561598, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561598, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561598, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561598, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:40:04.404: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561598, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561598, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561598, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561598, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:40:07.433: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:40:07.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9576" for this suite.
Feb 17 18:40:15.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:40:17.840: INFO: namespace webhook-9576 deletion completed in 10.13837167s
STEP: Destroying namespace "webhook-9576-markers" for this suite.
Feb 17 18:40:25.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:40:27.997: INFO: namespace webhook-9576-markers deletion completed in 10.157239918s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:30.399 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:40:28.068: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 18:40:28.256: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-56dacf87-2860-4229-8da2-ec106b048c58
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:40:38.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3068" for this suite.
Feb 17 18:41:10.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:41:12.603: INFO: namespace configmap-3068 deletion completed in 34.1439744s

• [SLOW TEST:44.535 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:41:12.604: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-478
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-478 to expose endpoints map[]
Feb 17 18:41:12.786: INFO: successfully validated that service multi-endpoint-test in namespace services-478 exposes endpoints map[] (16.41956ms elapsed)
STEP: Creating pod pod1 in namespace services-478
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-478 to expose endpoints map[pod1:[100]]
Feb 17 18:41:16.969: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.147055853s elapsed, will retry)
Feb 17 18:41:21.082: INFO: successfully validated that service multi-endpoint-test in namespace services-478 exposes endpoints map[pod1:[100]] (8.259300373s elapsed)
STEP: Creating pod pod2 in namespace services-478
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-478 to expose endpoints map[pod1:[100] pod2:[101]]
Feb 17 18:41:25.311: INFO: Unexpected endpoints: found map[5561b87b-398c-4e25-8406-e40b763c38e7:[100]], expected map[pod1:[100] pod2:[101]] (4.200873329s elapsed, will retry)
Feb 17 18:41:29.501: INFO: successfully validated that service multi-endpoint-test in namespace services-478 exposes endpoints map[pod1:[100] pod2:[101]] (8.391117574s elapsed)
STEP: Deleting pod pod1 in namespace services-478
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-478 to expose endpoints map[pod2:[101]]
Feb 17 18:41:29.561: INFO: successfully validated that service multi-endpoint-test in namespace services-478 exposes endpoints map[pod2:[101]] (26.428714ms elapsed)
STEP: Deleting pod pod2 in namespace services-478
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-478 to expose endpoints map[]
Feb 17 18:41:29.598: INFO: successfully validated that service multi-endpoint-test in namespace services-478 exposes endpoints map[] (14.861429ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:41:29.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-478" for this suite.
Feb 17 18:41:43.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:41:45.838: INFO: namespace services-478 deletion completed in 16.124206979s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:33.234 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:41:45.838: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Feb 17 18:41:54.149: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:41:54.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8112" for this suite.
Feb 17 18:42:02.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:42:04.345: INFO: namespace container-runtime-8112 deletion completed in 10.113698714s

• [SLOW TEST:18.507 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:42:04.345: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:42:04.530: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fc33e60c-5435-47bc-bf79-67a407413149" in namespace "downward-api-3414" to be "success or failure"
Feb 17 18:42:04.547: INFO: Pod "downwardapi-volume-fc33e60c-5435-47bc-bf79-67a407413149": Phase="Pending", Reason="", readiness=false. Elapsed: 17.166308ms
Feb 17 18:42:06.560: INFO: Pod "downwardapi-volume-fc33e60c-5435-47bc-bf79-67a407413149": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029910478s
Feb 17 18:42:08.574: INFO: Pod "downwardapi-volume-fc33e60c-5435-47bc-bf79-67a407413149": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043444716s
Feb 17 18:42:10.586: INFO: Pod "downwardapi-volume-fc33e60c-5435-47bc-bf79-67a407413149": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05580469s
Feb 17 18:42:12.598: INFO: Pod "downwardapi-volume-fc33e60c-5435-47bc-bf79-67a407413149": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068220804s
STEP: Saw pod success
Feb 17 18:42:12.599: INFO: Pod "downwardapi-volume-fc33e60c-5435-47bc-bf79-67a407413149" satisfied condition "success or failure"
Feb 17 18:42:12.611: INFO: Trying to get logs from node 10.45.66.189 pod downwardapi-volume-fc33e60c-5435-47bc-bf79-67a407413149 container client-container: <nil>
STEP: delete the pod
Feb 17 18:42:12.706: INFO: Waiting for pod downwardapi-volume-fc33e60c-5435-47bc-bf79-67a407413149 to disappear
Feb 17 18:42:12.719: INFO: Pod downwardapi-volume-fc33e60c-5435-47bc-bf79-67a407413149 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:42:12.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3414" for this suite.
Feb 17 18:42:20.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:42:22.891: INFO: namespace downward-api-3414 deletion completed in 10.148857281s

• [SLOW TEST:18.546 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:42:22.894: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-075d6373-5532-45b2-a65e-ebea5e43e634
STEP: Creating a pod to test consume secrets
Feb 17 18:42:23.092: INFO: Waiting up to 5m0s for pod "pod-secrets-3783a97d-e62e-4124-9a2a-13fd3e7dcacc" in namespace "secrets-9553" to be "success or failure"
Feb 17 18:42:23.106: INFO: Pod "pod-secrets-3783a97d-e62e-4124-9a2a-13fd3e7dcacc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.158295ms
Feb 17 18:42:25.123: INFO: Pod "pod-secrets-3783a97d-e62e-4124-9a2a-13fd3e7dcacc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030749089s
Feb 17 18:42:27.137: INFO: Pod "pod-secrets-3783a97d-e62e-4124-9a2a-13fd3e7dcacc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044131149s
Feb 17 18:42:29.150: INFO: Pod "pod-secrets-3783a97d-e62e-4124-9a2a-13fd3e7dcacc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.057614963s
Feb 17 18:42:31.164: INFO: Pod "pod-secrets-3783a97d-e62e-4124-9a2a-13fd3e7dcacc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.071153859s
STEP: Saw pod success
Feb 17 18:42:31.164: INFO: Pod "pod-secrets-3783a97d-e62e-4124-9a2a-13fd3e7dcacc" satisfied condition "success or failure"
Feb 17 18:42:31.176: INFO: Trying to get logs from node 10.45.66.189 pod pod-secrets-3783a97d-e62e-4124-9a2a-13fd3e7dcacc container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 18:42:31.240: INFO: Waiting for pod pod-secrets-3783a97d-e62e-4124-9a2a-13fd3e7dcacc to disappear
Feb 17 18:42:31.253: INFO: Pod pod-secrets-3783a97d-e62e-4124-9a2a-13fd3e7dcacc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:42:31.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9553" for this suite.
Feb 17 18:42:39.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:42:41.430: INFO: namespace secrets-9553 deletion completed in 10.153235499s

• [SLOW TEST:18.536 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:42:41.430: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-267
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 17 18:42:41.570: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 17 18:43:13.940: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.50.168:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-267 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:43:13.940: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 18:43:14.161: INFO: Found all expected endpoints: [netserver-0]
Feb 17 18:43:14.173: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.178.255:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-267 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:43:14.173: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 18:43:14.405: INFO: Found all expected endpoints: [netserver-1]
Feb 17 18:43:14.419: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.80.45:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-267 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 18:43:14.419: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 18:43:14.654: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:43:14.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-267" for this suite.
Feb 17 18:43:22.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:43:24.822: INFO: namespace pod-network-test-267 deletion completed in 10.144467868s

• [SLOW TEST:43.392 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:43:24.823: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Feb 17 18:43:24.993: INFO: Waiting up to 5m0s for pod "client-containers-8244be4e-1d6d-414d-ad00-0754f0677d92" in namespace "containers-4642" to be "success or failure"
Feb 17 18:43:25.008: INFO: Pod "client-containers-8244be4e-1d6d-414d-ad00-0754f0677d92": Phase="Pending", Reason="", readiness=false. Elapsed: 14.93984ms
Feb 17 18:43:27.021: INFO: Pod "client-containers-8244be4e-1d6d-414d-ad00-0754f0677d92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028242973s
Feb 17 18:43:29.035: INFO: Pod "client-containers-8244be4e-1d6d-414d-ad00-0754f0677d92": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042351358s
Feb 17 18:43:31.049: INFO: Pod "client-containers-8244be4e-1d6d-414d-ad00-0754f0677d92": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056003086s
Feb 17 18:43:33.065: INFO: Pod "client-containers-8244be4e-1d6d-414d-ad00-0754f0677d92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.072489613s
STEP: Saw pod success
Feb 17 18:43:33.065: INFO: Pod "client-containers-8244be4e-1d6d-414d-ad00-0754f0677d92" satisfied condition "success or failure"
Feb 17 18:43:33.078: INFO: Trying to get logs from node 10.45.66.178 pod client-containers-8244be4e-1d6d-414d-ad00-0754f0677d92 container test-container: <nil>
STEP: delete the pod
Feb 17 18:43:33.177: INFO: Waiting for pod client-containers-8244be4e-1d6d-414d-ad00-0754f0677d92 to disappear
Feb 17 18:43:33.189: INFO: Pod client-containers-8244be4e-1d6d-414d-ad00-0754f0677d92 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:43:33.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4642" for this suite.
Feb 17 18:43:41.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:43:43.388: INFO: namespace containers-4642 deletion completed in 10.169553684s

• [SLOW TEST:18.565 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:43:43.389: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 18:43:43.592: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9b4a33b8-2406-41c8-ab81-04bc6eac66fe" in namespace "projected-7934" to be "success or failure"
Feb 17 18:43:43.605: INFO: Pod "downwardapi-volume-9b4a33b8-2406-41c8-ab81-04bc6eac66fe": Phase="Pending", Reason="", readiness=false. Elapsed: 12.512689ms
Feb 17 18:43:45.618: INFO: Pod "downwardapi-volume-9b4a33b8-2406-41c8-ab81-04bc6eac66fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02599624s
Feb 17 18:43:47.641: INFO: Pod "downwardapi-volume-9b4a33b8-2406-41c8-ab81-04bc6eac66fe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048456192s
Feb 17 18:43:49.653: INFO: Pod "downwardapi-volume-9b4a33b8-2406-41c8-ab81-04bc6eac66fe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061108901s
Feb 17 18:43:51.667: INFO: Pod "downwardapi-volume-9b4a33b8-2406-41c8-ab81-04bc6eac66fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.074623375s
STEP: Saw pod success
Feb 17 18:43:51.667: INFO: Pod "downwardapi-volume-9b4a33b8-2406-41c8-ab81-04bc6eac66fe" satisfied condition "success or failure"
Feb 17 18:43:51.680: INFO: Trying to get logs from node 10.45.66.189 pod downwardapi-volume-9b4a33b8-2406-41c8-ab81-04bc6eac66fe container client-container: <nil>
STEP: delete the pod
Feb 17 18:43:51.755: INFO: Waiting for pod downwardapi-volume-9b4a33b8-2406-41c8-ab81-04bc6eac66fe to disappear
Feb 17 18:43:51.767: INFO: Pod downwardapi-volume-9b4a33b8-2406-41c8-ab81-04bc6eac66fe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:43:51.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7934" for this suite.
Feb 17 18:43:59.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:44:01.935: INFO: namespace projected-7934 deletion completed in 10.139352678s

• [SLOW TEST:18.546 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:44:01.936: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Feb 17 18:44:02.116: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:44:11.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2511" for this suite.
Feb 17 18:44:19.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:44:21.449: INFO: namespace init-container-2511 deletion completed in 10.133175744s

• [SLOW TEST:19.514 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:44:21.450: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Feb 17 18:44:21.709: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5918 /api/v1/namespaces/watch-5918/configmaps/e2e-watch-test-label-changed 71e42ec6-e8db-49a1-8282-617062ecd580 104282 0 2020-02-17 18:44:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 17 18:44:21.709: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5918 /api/v1/namespaces/watch-5918/configmaps/e2e-watch-test-label-changed 71e42ec6-e8db-49a1-8282-617062ecd580 104287 0 2020-02-17 18:44:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 17 18:44:21.709: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5918 /api/v1/namespaces/watch-5918/configmaps/e2e-watch-test-label-changed 71e42ec6-e8db-49a1-8282-617062ecd580 104288 0 2020-02-17 18:44:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Feb 17 18:44:31.847: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5918 /api/v1/namespaces/watch-5918/configmaps/e2e-watch-test-label-changed 71e42ec6-e8db-49a1-8282-617062ecd580 104332 0 2020-02-17 18:44:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 17 18:44:31.847: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5918 /api/v1/namespaces/watch-5918/configmaps/e2e-watch-test-label-changed 71e42ec6-e8db-49a1-8282-617062ecd580 104334 0 2020-02-17 18:44:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Feb 17 18:44:31.847: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5918 /api/v1/namespaces/watch-5918/configmaps/e2e-watch-test-label-changed 71e42ec6-e8db-49a1-8282-617062ecd580 104335 0 2020-02-17 18:44:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:44:31.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5918" for this suite.
Feb 17 18:44:39.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:44:42.006: INFO: namespace watch-5918 deletion completed in 10.130416058s

• [SLOW TEST:20.556 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:44:42.008: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:44:42.585: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:44:44.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561882, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561882, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561882, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561882, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:44:46.636: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561882, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561882, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561882, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561882, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:44:48.636: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561882, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561882, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561882, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717561882, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:44:51.666: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:44:52.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7057" for this suite.
Feb 17 18:45:00.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:45:02.493: INFO: namespace webhook-7057 deletion completed in 10.131726216s
STEP: Destroying namespace "webhook-7057-markers" for this suite.
Feb 17 18:45:10.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:45:12.659: INFO: namespace webhook-7057-markers deletion completed in 10.166429063s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:30.725 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:45:12.740: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-8ca53052-2d4a-4a3f-ac52-20cca37f0f79
STEP: Creating a pod to test consume secrets
Feb 17 18:45:12.949: INFO: Waiting up to 5m0s for pod "pod-secrets-ee8c8638-8f92-4475-8b33-cebc3886d8d1" in namespace "secrets-5823" to be "success or failure"
Feb 17 18:45:12.962: INFO: Pod "pod-secrets-ee8c8638-8f92-4475-8b33-cebc3886d8d1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.150306ms
Feb 17 18:45:14.979: INFO: Pod "pod-secrets-ee8c8638-8f92-4475-8b33-cebc3886d8d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030053661s
Feb 17 18:45:16.993: INFO: Pod "pod-secrets-ee8c8638-8f92-4475-8b33-cebc3886d8d1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043782839s
Feb 17 18:45:19.006: INFO: Pod "pod-secrets-ee8c8638-8f92-4475-8b33-cebc3886d8d1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056988166s
Feb 17 18:45:21.019: INFO: Pod "pod-secrets-ee8c8638-8f92-4475-8b33-cebc3886d8d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.06987935s
STEP: Saw pod success
Feb 17 18:45:21.019: INFO: Pod "pod-secrets-ee8c8638-8f92-4475-8b33-cebc3886d8d1" satisfied condition "success or failure"
Feb 17 18:45:21.031: INFO: Trying to get logs from node 10.45.66.189 pod pod-secrets-ee8c8638-8f92-4475-8b33-cebc3886d8d1 container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 18:45:21.111: INFO: Waiting for pod pod-secrets-ee8c8638-8f92-4475-8b33-cebc3886d8d1 to disappear
Feb 17 18:45:21.124: INFO: Pod pod-secrets-ee8c8638-8f92-4475-8b33-cebc3886d8d1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:45:21.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5823" for this suite.
Feb 17 18:45:29.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:45:31.295: INFO: namespace secrets-5823 deletion completed in 10.144328487s

• [SLOW TEST:18.555 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:45:31.295: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Feb 17 18:45:31.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-6119'
Feb 17 18:45:31.585: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Feb 17 18:45:31.585: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Feb 17 18:45:33.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 delete deployment e2e-test-httpd-deployment --namespace=kubectl-6119'
Feb 17 18:45:33.796: INFO: stderr: ""
Feb 17 18:45:33.796: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:45:33.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6119" for this suite.
Feb 17 18:45:47.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:45:49.938: INFO: namespace kubectl-6119 deletion completed in 16.107184347s

• [SLOW TEST:18.642 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:45:49.938: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Feb 17 18:45:50.067: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 17 18:46:50.232: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 18:46:50.248: INFO: Starting informer...
STEP: Starting pod...
Feb 17 18:46:50.511: INFO: Pod is running on 10.45.66.189. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Feb 17 18:46:50.562: INFO: Pod wasn't evicted. Proceeding
Feb 17 18:46:50.562: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Feb 17 18:48:05.629: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:48:05.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9771" for this suite.
Feb 17 18:48:19.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:48:21.779: INFO: namespace taint-single-pod-9771 deletion completed in 16.130062203s

• [SLOW TEST:151.841 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:48:21.780: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-3ffb4ebb-cdfc-4e68-a399-6341073e0e5a in namespace container-probe-9775
Feb 17 18:48:33.964: INFO: Started pod test-webserver-3ffb4ebb-cdfc-4e68-a399-6341073e0e5a in namespace container-probe-9775
STEP: checking the pod's current state and verifying that restartCount is present
Feb 17 18:48:33.976: INFO: Initial restart count of pod test-webserver-3ffb4ebb-cdfc-4e68-a399-6341073e0e5a is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:52:35.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9775" for this suite.
Feb 17 18:52:43.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:52:46.087: INFO: namespace container-probe-9775 deletion completed in 10.156383953s

• [SLOW TEST:264.308 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:52:46.088: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Feb 17 18:52:46.269: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4263 /api/v1/namespaces/watch-4263/configmaps/e2e-watch-test-configmap-a ea681f44-fe78-45b3-b03f-c702857a6458 107549 0 2020-02-17 18:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 17 18:52:46.270: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4263 /api/v1/namespaces/watch-4263/configmaps/e2e-watch-test-configmap-a ea681f44-fe78-45b3-b03f-c702857a6458 107549 0 2020-02-17 18:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Feb 17 18:52:56.304: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4263 /api/v1/namespaces/watch-4263/configmaps/e2e-watch-test-configmap-a ea681f44-fe78-45b3-b03f-c702857a6458 107595 0 2020-02-17 18:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Feb 17 18:52:56.305: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4263 /api/v1/namespaces/watch-4263/configmaps/e2e-watch-test-configmap-a ea681f44-fe78-45b3-b03f-c702857a6458 107595 0 2020-02-17 18:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Feb 17 18:53:06.336: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4263 /api/v1/namespaces/watch-4263/configmaps/e2e-watch-test-configmap-a ea681f44-fe78-45b3-b03f-c702857a6458 107635 0 2020-02-17 18:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 17 18:53:06.336: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4263 /api/v1/namespaces/watch-4263/configmaps/e2e-watch-test-configmap-a ea681f44-fe78-45b3-b03f-c702857a6458 107635 0 2020-02-17 18:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Feb 17 18:53:16.370: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4263 /api/v1/namespaces/watch-4263/configmaps/e2e-watch-test-configmap-a ea681f44-fe78-45b3-b03f-c702857a6458 107674 0 2020-02-17 18:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 17 18:53:16.370: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4263 /api/v1/namespaces/watch-4263/configmaps/e2e-watch-test-configmap-a ea681f44-fe78-45b3-b03f-c702857a6458 107674 0 2020-02-17 18:52:46 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Feb 17 18:53:26.402: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4263 /api/v1/namespaces/watch-4263/configmaps/e2e-watch-test-configmap-b c4f375f2-15d3-4eba-a96c-b8ab5229b5bb 107714 0 2020-02-17 18:53:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 17 18:53:26.402: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4263 /api/v1/namespaces/watch-4263/configmaps/e2e-watch-test-configmap-b c4f375f2-15d3-4eba-a96c-b8ab5229b5bb 107714 0 2020-02-17 18:53:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Feb 17 18:53:36.437: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4263 /api/v1/namespaces/watch-4263/configmaps/e2e-watch-test-configmap-b c4f375f2-15d3-4eba-a96c-b8ab5229b5bb 107755 0 2020-02-17 18:53:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 17 18:53:36.437: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4263 /api/v1/namespaces/watch-4263/configmaps/e2e-watch-test-configmap-b c4f375f2-15d3-4eba-a96c-b8ab5229b5bb 107755 0 2020-02-17 18:53:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:53:46.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4263" for this suite.
Feb 17 18:53:54.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:53:56.622: INFO: namespace watch-4263 deletion completed in 10.147339622s

• [SLOW TEST:70.534 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:53:56.624: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 18:54:24.833: INFO: Container started at 2020-02-17 18:54:03 +0000 UTC, pod became ready at 2020-02-17 18:54:23 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:54:24.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2911" for this suite.
Feb 17 18:54:56.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:54:59.029: INFO: namespace container-probe-2911 deletion completed in 34.142978791s

• [SLOW TEST:62.405 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:54:59.031: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Feb 17 18:55:00.247: INFO: Waiting up to 5m0s for pod "pod-fd34b28c-2406-4b30-aee2-997dfcaaf258" in namespace "emptydir-9605" to be "success or failure"
Feb 17 18:55:00.262: INFO: Pod "pod-fd34b28c-2406-4b30-aee2-997dfcaaf258": Phase="Pending", Reason="", readiness=false. Elapsed: 14.231828ms
Feb 17 18:55:02.275: INFO: Pod "pod-fd34b28c-2406-4b30-aee2-997dfcaaf258": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027857733s
Feb 17 18:55:04.291: INFO: Pod "pod-fd34b28c-2406-4b30-aee2-997dfcaaf258": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043622152s
Feb 17 18:55:06.305: INFO: Pod "pod-fd34b28c-2406-4b30-aee2-997dfcaaf258": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058048956s
Feb 17 18:55:08.319: INFO: Pod "pod-fd34b28c-2406-4b30-aee2-997dfcaaf258": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.071770916s
STEP: Saw pod success
Feb 17 18:55:08.319: INFO: Pod "pod-fd34b28c-2406-4b30-aee2-997dfcaaf258" satisfied condition "success or failure"
Feb 17 18:55:08.332: INFO: Trying to get logs from node 10.45.66.189 pod pod-fd34b28c-2406-4b30-aee2-997dfcaaf258 container test-container: <nil>
STEP: delete the pod
Feb 17 18:55:08.429: INFO: Waiting for pod pod-fd34b28c-2406-4b30-aee2-997dfcaaf258 to disappear
Feb 17 18:55:08.442: INFO: Pod pod-fd34b28c-2406-4b30-aee2-997dfcaaf258 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:55:08.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9605" for this suite.
Feb 17 18:55:16.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:55:18.619: INFO: namespace emptydir-9605 deletion completed in 10.144240006s

• [SLOW TEST:19.589 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:55:18.620: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-858e9895-6c67-45dd-8c32-74ae4bed8d4f
STEP: Creating a pod to test consume configMaps
Feb 17 18:55:18.826: INFO: Waiting up to 5m0s for pod "pod-configmaps-c574567b-873b-4115-91d7-dfbdeac6c5f2" in namespace "configmap-958" to be "success or failure"
Feb 17 18:55:18.841: INFO: Pod "pod-configmaps-c574567b-873b-4115-91d7-dfbdeac6c5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.467522ms
Feb 17 18:55:20.854: INFO: Pod "pod-configmaps-c574567b-873b-4115-91d7-dfbdeac6c5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028058003s
Feb 17 18:55:22.867: INFO: Pod "pod-configmaps-c574567b-873b-4115-91d7-dfbdeac6c5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04124725s
Feb 17 18:55:24.882: INFO: Pod "pod-configmaps-c574567b-873b-4115-91d7-dfbdeac6c5f2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055671432s
Feb 17 18:55:26.895: INFO: Pod "pod-configmaps-c574567b-873b-4115-91d7-dfbdeac6c5f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.069136241s
STEP: Saw pod success
Feb 17 18:55:26.895: INFO: Pod "pod-configmaps-c574567b-873b-4115-91d7-dfbdeac6c5f2" satisfied condition "success or failure"
Feb 17 18:55:26.909: INFO: Trying to get logs from node 10.45.66.189 pod pod-configmaps-c574567b-873b-4115-91d7-dfbdeac6c5f2 container configmap-volume-test: <nil>
STEP: delete the pod
Feb 17 18:55:26.975: INFO: Waiting for pod pod-configmaps-c574567b-873b-4115-91d7-dfbdeac6c5f2 to disappear
Feb 17 18:55:26.988: INFO: Pod pod-configmaps-c574567b-873b-4115-91d7-dfbdeac6c5f2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:55:26.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-958" for this suite.
Feb 17 18:55:35.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:55:37.160: INFO: namespace configmap-958 deletion completed in 10.13712526s

• [SLOW TEST:18.540 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:55:37.162: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-9kmx
STEP: Creating a pod to test atomic-volume-subpath
Feb 17 18:55:37.368: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-9kmx" in namespace "subpath-219" to be "success or failure"
Feb 17 18:55:37.381: INFO: Pod "pod-subpath-test-secret-9kmx": Phase="Pending", Reason="", readiness=false. Elapsed: 12.553388ms
Feb 17 18:55:39.395: INFO: Pod "pod-subpath-test-secret-9kmx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026718794s
Feb 17 18:55:41.409: INFO: Pod "pod-subpath-test-secret-9kmx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040264258s
Feb 17 18:55:43.423: INFO: Pod "pod-subpath-test-secret-9kmx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054795777s
Feb 17 18:55:45.437: INFO: Pod "pod-subpath-test-secret-9kmx": Phase="Running", Reason="", readiness=true. Elapsed: 8.068354154s
Feb 17 18:55:47.451: INFO: Pod "pod-subpath-test-secret-9kmx": Phase="Running", Reason="", readiness=true. Elapsed: 10.082679693s
Feb 17 18:55:49.465: INFO: Pod "pod-subpath-test-secret-9kmx": Phase="Running", Reason="", readiness=true. Elapsed: 12.09646041s
Feb 17 18:55:51.479: INFO: Pod "pod-subpath-test-secret-9kmx": Phase="Running", Reason="", readiness=true. Elapsed: 14.110702815s
Feb 17 18:55:53.493: INFO: Pod "pod-subpath-test-secret-9kmx": Phase="Running", Reason="", readiness=true. Elapsed: 16.124179138s
Feb 17 18:55:55.506: INFO: Pod "pod-subpath-test-secret-9kmx": Phase="Running", Reason="", readiness=true. Elapsed: 18.137719359s
Feb 17 18:55:57.519: INFO: Pod "pod-subpath-test-secret-9kmx": Phase="Running", Reason="", readiness=true. Elapsed: 20.15062818s
Feb 17 18:55:59.533: INFO: Pod "pod-subpath-test-secret-9kmx": Phase="Running", Reason="", readiness=true. Elapsed: 22.164709121s
Feb 17 18:56:01.546: INFO: Pod "pod-subpath-test-secret-9kmx": Phase="Running", Reason="", readiness=true. Elapsed: 24.177719227s
Feb 17 18:56:03.561: INFO: Pod "pod-subpath-test-secret-9kmx": Phase="Running", Reason="", readiness=true. Elapsed: 26.192317598s
Feb 17 18:56:05.574: INFO: Pod "pod-subpath-test-secret-9kmx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.205109717s
STEP: Saw pod success
Feb 17 18:56:05.574: INFO: Pod "pod-subpath-test-secret-9kmx" satisfied condition "success or failure"
Feb 17 18:56:05.587: INFO: Trying to get logs from node 10.45.66.189 pod pod-subpath-test-secret-9kmx container test-container-subpath-secret-9kmx: <nil>
STEP: delete the pod
Feb 17 18:56:05.652: INFO: Waiting for pod pod-subpath-test-secret-9kmx to disappear
Feb 17 18:56:05.664: INFO: Pod pod-subpath-test-secret-9kmx no longer exists
STEP: Deleting pod pod-subpath-test-secret-9kmx
Feb 17 18:56:05.664: INFO: Deleting pod "pod-subpath-test-secret-9kmx" in namespace "subpath-219"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:56:05.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-219" for this suite.
Feb 17 18:56:13.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:56:15.896: INFO: namespace subpath-219 deletion completed in 10.185840385s

• [SLOW TEST:38.734 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:56:15.896: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Feb 17 18:56:16.259: INFO: Number of nodes with available pods: 0
Feb 17 18:56:16.259: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:56:17.298: INFO: Number of nodes with available pods: 0
Feb 17 18:56:17.298: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:56:18.296: INFO: Number of nodes with available pods: 0
Feb 17 18:56:18.296: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:56:19.297: INFO: Number of nodes with available pods: 0
Feb 17 18:56:19.297: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:56:20.297: INFO: Number of nodes with available pods: 0
Feb 17 18:56:20.297: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:56:21.296: INFO: Number of nodes with available pods: 0
Feb 17 18:56:21.296: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:56:22.297: INFO: Number of nodes with available pods: 0
Feb 17 18:56:22.297: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:56:23.296: INFO: Number of nodes with available pods: 0
Feb 17 18:56:23.296: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:56:24.297: INFO: Number of nodes with available pods: 2
Feb 17 18:56:24.297: INFO: Node 10.45.66.177 is running more than one daemon pod
Feb 17 18:56:25.292: INFO: Number of nodes with available pods: 3
Feb 17 18:56:25.292: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Feb 17 18:56:25.391: INFO: Number of nodes with available pods: 2
Feb 17 18:56:25.391: INFO: Node 10.45.66.189 is running more than one daemon pod
Feb 17 18:56:26.423: INFO: Number of nodes with available pods: 2
Feb 17 18:56:26.423: INFO: Node 10.45.66.189 is running more than one daemon pod
Feb 17 18:56:27.423: INFO: Number of nodes with available pods: 2
Feb 17 18:56:27.424: INFO: Node 10.45.66.189 is running more than one daemon pod
Feb 17 18:56:28.428: INFO: Number of nodes with available pods: 2
Feb 17 18:56:28.428: INFO: Node 10.45.66.189 is running more than one daemon pod
Feb 17 18:56:29.425: INFO: Number of nodes with available pods: 2
Feb 17 18:56:29.425: INFO: Node 10.45.66.189 is running more than one daemon pod
Feb 17 18:56:30.423: INFO: Number of nodes with available pods: 2
Feb 17 18:56:30.424: INFO: Node 10.45.66.189 is running more than one daemon pod
Feb 17 18:56:31.425: INFO: Number of nodes with available pods: 2
Feb 17 18:56:31.425: INFO: Node 10.45.66.189 is running more than one daemon pod
Feb 17 18:56:32.424: INFO: Number of nodes with available pods: 2
Feb 17 18:56:32.424: INFO: Node 10.45.66.189 is running more than one daemon pod
Feb 17 18:56:33.424: INFO: Number of nodes with available pods: 3
Feb 17 18:56:33.424: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8473, will wait for the garbage collector to delete the pods
Feb 17 18:56:33.538: INFO: Deleting DaemonSet.extensions daemon-set took: 27.442832ms
Feb 17 18:56:34.038: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.289759ms
Feb 17 18:56:47.951: INFO: Number of nodes with available pods: 0
Feb 17 18:56:47.951: INFO: Number of running nodes: 0, number of available pods: 0
Feb 17 18:56:47.963: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8473/daemonsets","resourceVersion":"109021"},"items":null}

Feb 17 18:56:47.974: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8473/pods","resourceVersion":"109021"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:56:48.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8473" for this suite.
Feb 17 18:56:56.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:56:58.222: INFO: namespace daemonsets-8473 deletion completed in 10.174009592s

• [SLOW TEST:42.326 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:56:58.224: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:56:59.341: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:57:01.379: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562619, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562619, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562619, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562619, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:57:03.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562619, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562619, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562619, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562619, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:57:05.394: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562619, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562619, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562619, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562619, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:57:07.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562619, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562619, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562619, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562619, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:57:10.421: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Feb 17 18:57:10.508: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:57:10.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-821" for this suite.
Feb 17 18:57:18.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:57:20.714: INFO: namespace webhook-821 deletion completed in 10.135423s
STEP: Destroying namespace "webhook-821-markers" for this suite.
Feb 17 18:57:28.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:57:30.858: INFO: namespace webhook-821-markers deletion completed in 10.14426901s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:32.703 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:57:30.926: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:57:31.647: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 17 18:57:33.686: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562651, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562651, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562651, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562651, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:57:35.699: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562651, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562651, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562651, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562651, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:57:37.700: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562651, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562651, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562651, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562651, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:57:39.700: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562651, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562651, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562651, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562651, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:57:42.728: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 18:57:42.744: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:57:44.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2307" for this suite.
Feb 17 18:57:52.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:57:54.442: INFO: namespace crd-webhook-2307 deletion completed in 10.141452079s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:23.584 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:57:54.510: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 18:57:54.666: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Creating first CR 
Feb 17 18:57:55.349: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T18:57:55Z generation:1 name:name1 resourceVersion:109645 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:67aa44e9-9ef7-401f-af4f-c3cd9f1dba7e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Feb 17 18:58:05.367: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T18:58:05Z generation:1 name:name2 resourceVersion:109684 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:db63c805-eb9f-49fd-92c4-a1a47db70da4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Feb 17 18:58:15.383: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T18:57:55Z generation:2 name:name1 resourceVersion:109726 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:67aa44e9-9ef7-401f-af4f-c3cd9f1dba7e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Feb 17 18:58:25.399: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T18:58:05Z generation:2 name:name2 resourceVersion:109767 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:db63c805-eb9f-49fd-92c4-a1a47db70da4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Feb 17 18:58:35.425: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T18:57:55Z generation:2 name:name1 resourceVersion:109807 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:67aa44e9-9ef7-401f-af4f-c3cd9f1dba7e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Feb 17 18:58:45.455: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-02-17T18:58:05Z generation:2 name:name2 resourceVersion:109851 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:db63c805-eb9f-49fd-92c4-a1a47db70da4] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:58:55.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5189" for this suite.
Feb 17 18:59:04.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:59:06.161: INFO: namespace crd-watch-5189 deletion completed in 10.14315866s

• [SLOW TEST:71.651 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:59:06.162: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:59:06.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3806" for this suite.
Feb 17 18:59:20.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:59:22.546: INFO: namespace pods-3806 deletion completed in 16.165907619s

• [SLOW TEST:16.384 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:59:22.548: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Feb 17 18:59:23.391: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Feb 17 18:59:25.429: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562763, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562763, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562763, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562763, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:59:27.443: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562763, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562763, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562763, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562763, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:59:29.444: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562763, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562763, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562763, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562763, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 18:59:31.443: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562763, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562763, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562763, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717562763, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 18:59:34.471: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
Feb 17 18:59:34.555: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 18:59:34.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9189" for this suite.
Feb 17 18:59:42.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:59:45.005: INFO: namespace webhook-9189 deletion completed in 10.142731251s
STEP: Destroying namespace "webhook-9189-markers" for this suite.
Feb 17 18:59:53.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 18:59:55.146: INFO: namespace webhook-9189-markers deletion completed in 10.140767596s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:32.667 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 18:59:55.215: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Feb 17 18:59:55.408: INFO: Waiting up to 5m0s for pod "downward-api-f5492896-a7fb-4fb9-ac10-d6658048e652" in namespace "downward-api-7506" to be "success or failure"
Feb 17 18:59:55.420: INFO: Pod "downward-api-f5492896-a7fb-4fb9-ac10-d6658048e652": Phase="Pending", Reason="", readiness=false. Elapsed: 12.381548ms
Feb 17 18:59:57.433: INFO: Pod "downward-api-f5492896-a7fb-4fb9-ac10-d6658048e652": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025377631s
Feb 17 18:59:59.449: INFO: Pod "downward-api-f5492896-a7fb-4fb9-ac10-d6658048e652": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041347525s
Feb 17 19:00:01.463: INFO: Pod "downward-api-f5492896-a7fb-4fb9-ac10-d6658048e652": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055065304s
Feb 17 19:00:03.476: INFO: Pod "downward-api-f5492896-a7fb-4fb9-ac10-d6658048e652": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068199002s
STEP: Saw pod success
Feb 17 19:00:03.476: INFO: Pod "downward-api-f5492896-a7fb-4fb9-ac10-d6658048e652" satisfied condition "success or failure"
Feb 17 19:00:03.489: INFO: Trying to get logs from node 10.45.66.189 pod downward-api-f5492896-a7fb-4fb9-ac10-d6658048e652 container dapi-container: <nil>
STEP: delete the pod
Feb 17 19:00:03.587: INFO: Waiting for pod downward-api-f5492896-a7fb-4fb9-ac10-d6658048e652 to disappear
Feb 17 19:00:03.601: INFO: Pod downward-api-f5492896-a7fb-4fb9-ac10-d6658048e652 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:00:03.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7506" for this suite.
Feb 17 19:00:11.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:00:13.788: INFO: namespace downward-api-7506 deletion completed in 10.151794733s

• [SLOW TEST:18.572 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:00:13.789: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 19:00:14.017: INFO: (0) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 54.226303ms)
Feb 17 19:00:14.043: INFO: (1) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 25.918611ms)
Feb 17 19:00:14.067: INFO: (2) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.757675ms)
Feb 17 19:00:14.087: INFO: (3) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.444867ms)
Feb 17 19:00:14.108: INFO: (4) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.310713ms)
Feb 17 19:00:14.135: INFO: (5) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 27.10177ms)
Feb 17 19:00:14.156: INFO: (6) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.711027ms)
Feb 17 19:00:14.179: INFO: (7) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.606199ms)
Feb 17 19:00:14.199: INFO: (8) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.530059ms)
Feb 17 19:00:14.220: INFO: (9) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.006374ms)
Feb 17 19:00:14.241: INFO: (10) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.070516ms)
Feb 17 19:00:14.261: INFO: (11) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.443553ms)
Feb 17 19:00:14.282: INFO: (12) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.314159ms)
Feb 17 19:00:14.303: INFO: (13) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.539384ms)
Feb 17 19:00:14.325: INFO: (14) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.538745ms)
Feb 17 19:00:14.346: INFO: (15) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.506438ms)
Feb 17 19:00:14.367: INFO: (16) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.358756ms)
Feb 17 19:00:14.388: INFO: (17) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.522813ms)
Feb 17 19:00:14.409: INFO: (18) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.668342ms)
Feb 17 19:00:14.429: INFO: (19) /api/v1/nodes/10.45.66.177/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.154296ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:00:14.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9303" for this suite.
Feb 17 19:00:22.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:00:23.527: INFO: namespace proxy-9303 deletion completed in 9.074648632s

• [SLOW TEST:9.738 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:00:23.527: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Feb 17 19:00:23.725: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5832 /api/v1/namespaces/watch-5832/configmaps/e2e-watch-test-watch-closed 76e90ad8-b2b6-4277-b1c3-ecca2507e177 110654 0 2020-02-17 19:00:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Feb 17 19:00:23.725: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5832 /api/v1/namespaces/watch-5832/configmaps/e2e-watch-test-watch-closed 76e90ad8-b2b6-4277-b1c3-ecca2507e177 110659 0 2020-02-17 19:00:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Feb 17 19:00:23.796: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5832 /api/v1/namespaces/watch-5832/configmaps/e2e-watch-test-watch-closed 76e90ad8-b2b6-4277-b1c3-ecca2507e177 110660 0 2020-02-17 19:00:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Feb 17 19:00:23.796: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5832 /api/v1/namespaces/watch-5832/configmaps/e2e-watch-test-watch-closed 76e90ad8-b2b6-4277-b1c3-ecca2507e177 110662 0 2020-02-17 19:00:23 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:00:23.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5832" for this suite.
Feb 17 19:00:31.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:00:33.953: INFO: namespace watch-5832 deletion completed in 10.128967822s

• [SLOW TEST:10.426 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:00:33.954: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Feb 17 19:00:34.140: INFO: Waiting up to 5m0s for pod "pod-b742cddd-ec16-449f-a469-f9d49fd30303" in namespace "emptydir-1848" to be "success or failure"
Feb 17 19:00:34.152: INFO: Pod "pod-b742cddd-ec16-449f-a469-f9d49fd30303": Phase="Pending", Reason="", readiness=false. Elapsed: 11.967109ms
Feb 17 19:00:36.166: INFO: Pod "pod-b742cddd-ec16-449f-a469-f9d49fd30303": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025441321s
Feb 17 19:00:38.179: INFO: Pod "pod-b742cddd-ec16-449f-a469-f9d49fd30303": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038559303s
Feb 17 19:00:40.193: INFO: Pod "pod-b742cddd-ec16-449f-a469-f9d49fd30303": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052270726s
Feb 17 19:00:42.207: INFO: Pod "pod-b742cddd-ec16-449f-a469-f9d49fd30303": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.066861496s
STEP: Saw pod success
Feb 17 19:00:42.207: INFO: Pod "pod-b742cddd-ec16-449f-a469-f9d49fd30303" satisfied condition "success or failure"
Feb 17 19:00:42.220: INFO: Trying to get logs from node 10.45.66.189 pod pod-b742cddd-ec16-449f-a469-f9d49fd30303 container test-container: <nil>
STEP: delete the pod
Feb 17 19:00:42.309: INFO: Waiting for pod pod-b742cddd-ec16-449f-a469-f9d49fd30303 to disappear
Feb 17 19:00:42.321: INFO: Pod pod-b742cddd-ec16-449f-a469-f9d49fd30303 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:00:42.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1848" for this suite.
Feb 17 19:00:50.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:00:52.480: INFO: namespace emptydir-1848 deletion completed in 10.135607296s

• [SLOW TEST:18.526 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:00:52.480: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 19:00:53.696: INFO: Waiting up to 5m0s for pod "downwardapi-volume-595ef9fb-1016-44f0-b803-1c40383a9b84" in namespace "downward-api-9705" to be "success or failure"
Feb 17 19:00:53.713: INFO: Pod "downwardapi-volume-595ef9fb-1016-44f0-b803-1c40383a9b84": Phase="Pending", Reason="", readiness=false. Elapsed: 16.836572ms
Feb 17 19:00:55.726: INFO: Pod "downwardapi-volume-595ef9fb-1016-44f0-b803-1c40383a9b84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029668661s
Feb 17 19:00:57.740: INFO: Pod "downwardapi-volume-595ef9fb-1016-44f0-b803-1c40383a9b84": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043510881s
Feb 17 19:00:59.754: INFO: Pod "downwardapi-volume-595ef9fb-1016-44f0-b803-1c40383a9b84": Phase="Pending", Reason="", readiness=false. Elapsed: 6.057147783s
Feb 17 19:01:01.767: INFO: Pod "downwardapi-volume-595ef9fb-1016-44f0-b803-1c40383a9b84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.070500355s
STEP: Saw pod success
Feb 17 19:01:01.767: INFO: Pod "downwardapi-volume-595ef9fb-1016-44f0-b803-1c40383a9b84" satisfied condition "success or failure"
Feb 17 19:01:01.781: INFO: Trying to get logs from node 10.45.66.189 pod downwardapi-volume-595ef9fb-1016-44f0-b803-1c40383a9b84 container client-container: <nil>
STEP: delete the pod
Feb 17 19:01:04.701: INFO: Waiting for pod downwardapi-volume-595ef9fb-1016-44f0-b803-1c40383a9b84 to disappear
Feb 17 19:01:04.720: INFO: Pod downwardapi-volume-595ef9fb-1016-44f0-b803-1c40383a9b84 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:01:04.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9705" for this suite.
Feb 17 19:01:12.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:01:14.913: INFO: namespace downward-api-9705 deletion completed in 10.163089322s

• [SLOW TEST:22.433 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:01:14.914: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 19:01:15.124: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-bcb0de51-ac6c-4d88-b71b-a9eeaf7d184a
STEP: Creating secret with name s-test-opt-upd-13fa8347-61bf-40ed-b0ac-4bed5e9fd08d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-bcb0de51-ac6c-4d88-b71b-a9eeaf7d184a
STEP: Updating secret s-test-opt-upd-13fa8347-61bf-40ed-b0ac-4bed5e9fd08d
STEP: Creating secret with name s-test-opt-create-395f9e9f-36ad-45c2-bc71-ad0e51ffd4b6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:02:50.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4421" for this suite.
Feb 17 19:03:04.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:03:07.043: INFO: namespace secrets-4421 deletion completed in 16.119760971s

• [SLOW TEST:112.129 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:03:07.043: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6173.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6173.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6173.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6173.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6173.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6173.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6173.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6173.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6173.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6173.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6173.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 19:03:29.320: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6173.svc.cluster.local from pod dns-6173/dns-test-0daaa26a-975c-420e-b58a-3d32e904f570: the server could not find the requested resource (get pods dns-test-0daaa26a-975c-420e-b58a-3d32e904f570)
Feb 17 19:03:29.341: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6173.svc.cluster.local from pod dns-6173/dns-test-0daaa26a-975c-420e-b58a-3d32e904f570: the server could not find the requested resource (get pods dns-test-0daaa26a-975c-420e-b58a-3d32e904f570)
Feb 17 19:03:29.386: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6173.svc.cluster.local from pod dns-6173/dns-test-0daaa26a-975c-420e-b58a-3d32e904f570: the server could not find the requested resource (get pods dns-test-0daaa26a-975c-420e-b58a-3d32e904f570)
Feb 17 19:03:29.455: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6173.svc.cluster.local from pod dns-6173/dns-test-0daaa26a-975c-420e-b58a-3d32e904f570: the server could not find the requested resource (get pods dns-test-0daaa26a-975c-420e-b58a-3d32e904f570)
Feb 17 19:03:29.476: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6173.svc.cluster.local from pod dns-6173/dns-test-0daaa26a-975c-420e-b58a-3d32e904f570: the server could not find the requested resource (get pods dns-test-0daaa26a-975c-420e-b58a-3d32e904f570)
Feb 17 19:03:29.570: INFO: Lookups using dns-6173/dns-test-0daaa26a-975c-420e-b58a-3d32e904f570 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6173.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6173.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6173.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6173.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6173.svc.cluster.local]

Feb 17 19:03:34.841: INFO: DNS probes using dns-6173/dns-test-0daaa26a-975c-420e-b58a-3d32e904f570 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:03:34.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6173" for this suite.
Feb 17 19:03:43.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:03:45.118: INFO: namespace dns-6173 deletion completed in 10.14280619s

• [SLOW TEST:38.076 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:03:45.120: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9547.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9547.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9547.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9547.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9547.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9547.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9547.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9547.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9547.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9547.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9547.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9547.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9547.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 178.67.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.67.178_udp@PTR;check="$$(dig +tcp +noall +answer +search 178.67.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.67.178_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9547.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9547.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9547.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9547.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9547.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9547.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9547.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9547.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9547.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9547.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9547.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9547.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9547.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 178.67.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.67.178_udp@PTR;check="$$(dig +tcp +noall +answer +search 178.67.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.67.178_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Feb 17 19:03:53.432: INFO: Unable to read wheezy_udp@dns-test-service.dns-9547.svc.cluster.local from pod dns-9547/dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af: the server could not find the requested resource (get pods dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af)
Feb 17 19:03:53.455: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9547.svc.cluster.local from pod dns-9547/dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af: the server could not find the requested resource (get pods dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af)
Feb 17 19:03:53.476: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9547.svc.cluster.local from pod dns-9547/dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af: the server could not find the requested resource (get pods dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af)
Feb 17 19:03:53.500: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9547.svc.cluster.local from pod dns-9547/dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af: the server could not find the requested resource (get pods dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af)
Feb 17 19:03:53.655: INFO: Unable to read jessie_udp@dns-test-service.dns-9547.svc.cluster.local from pod dns-9547/dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af: the server could not find the requested resource (get pods dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af)
Feb 17 19:03:53.676: INFO: Unable to read jessie_tcp@dns-test-service.dns-9547.svc.cluster.local from pod dns-9547/dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af: the server could not find the requested resource (get pods dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af)
Feb 17 19:03:53.697: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9547.svc.cluster.local from pod dns-9547/dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af: the server could not find the requested resource (get pods dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af)
Feb 17 19:03:53.732: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9547.svc.cluster.local from pod dns-9547/dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af: the server could not find the requested resource (get pods dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af)
Feb 17 19:03:53.865: INFO: Lookups using dns-9547/dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af failed for: [wheezy_udp@dns-test-service.dns-9547.svc.cluster.local wheezy_tcp@dns-test-service.dns-9547.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9547.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9547.svc.cluster.local jessie_udp@dns-test-service.dns-9547.svc.cluster.local jessie_tcp@dns-test-service.dns-9547.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9547.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9547.svc.cluster.local]

Feb 17 19:03:59.308: INFO: DNS probes using dns-9547/dns-test-2146f9c7-53f9-4641-b867-f71c2ed684af succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:03:59.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9547" for this suite.
Feb 17 19:04:07.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:04:09.666: INFO: namespace dns-9547 deletion completed in 10.139228814s

• [SLOW TEST:24.546 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:04:09.666: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Feb 17 19:04:26.072: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 19:04:26.085: INFO: Pod pod-with-poststart-http-hook still exists
Feb 17 19:04:28.085: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 19:04:28.100: INFO: Pod pod-with-poststart-http-hook still exists
Feb 17 19:04:30.085: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 19:04:30.099: INFO: Pod pod-with-poststart-http-hook still exists
Feb 17 19:04:32.085: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 19:04:32.098: INFO: Pod pod-with-poststart-http-hook still exists
Feb 17 19:04:34.085: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 19:04:34.098: INFO: Pod pod-with-poststart-http-hook still exists
Feb 17 19:04:36.085: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 19:04:36.098: INFO: Pod pod-with-poststart-http-hook still exists
Feb 17 19:04:38.085: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Feb 17 19:04:38.098: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:04:38.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6161" for this suite.
Feb 17 19:04:54.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:04:56.266: INFO: namespace container-lifecycle-hook-6161 deletion completed in 18.139669713s

• [SLOW TEST:46.600 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:04:56.267: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 19:04:56.449: INFO: Creating deployment "test-recreate-deployment"
Feb 17 19:04:56.464: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Feb 17 19:04:56.499: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Feb 17 19:04:58.546: INFO: Waiting deployment "test-recreate-deployment" to complete
Feb 17 19:04:58.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563096, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563096, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563096, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563096, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 19:05:00.571: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563096, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563096, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563096, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563096, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 19:05:02.572: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563096, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563096, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563096, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563096, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-68fc85c7bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 19:05:04.572: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Feb 17 19:05:04.598: INFO: Updating deployment test-recreate-deployment
Feb 17 19:05:04.598: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Feb 17 19:05:04.850: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1219 /apis/apps/v1/namespaces/deployment-1219/deployments/test-recreate-deployment c5db9192-3b63-4e79-92e1-2c1bd334d15b 112492 2 2020-02-17 19:04:56 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc007783c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-02-17 19:05:04 +0000 UTC,LastTransitionTime:2020-02-17 19:05:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-02-17 19:05:04 +0000 UTC,LastTransitionTime:2020-02-17 19:04:56 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Feb 17 19:05:04.863: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-1219 /apis/apps/v1/namespaces/deployment-1219/replicasets/test-recreate-deployment-5f94c574ff d65b66bf-7bf3-4257-863d-8cd6a380c0b7 112491 1 2020-02-17 19:05:04 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment c5db9192-3b63-4e79-92e1-2c1bd334d15b 0xc00aa50af7 0xc00aa50af8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00aa50b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 19:05:04.863: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Feb 17 19:05:04.863: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-1219 /apis/apps/v1/namespaces/deployment-1219/replicasets/test-recreate-deployment-68fc85c7bb 119b57f9-7724-40df-9225-f0e4fbff50bf 112480 2 2020-02-17 19:04:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment c5db9192-3b63-4e79-92e1-2c1bd334d15b 0xc00aa50bc7 0xc00aa50bc8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00aa50c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 19:05:04.876: INFO: Pod "test-recreate-deployment-5f94c574ff-bx7zh" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-bx7zh test-recreate-deployment-5f94c574ff- deployment-1219 /api/v1/namespaces/deployment-1219/pods/test-recreate-deployment-5f94c574ff-bx7zh 6909c139-6822-45f4-954c-d9850df41671 112490 0 2020-02-17 19:05:04 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff d65b66bf-7bf3-4257-863d-8cd6a380c0b7 0xc002c0d227 0xc002c0d228}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5jbsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5jbsq,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5jbsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-brl9m,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:05:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:05:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:05:04 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:05:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.189,PodIP:,StartTime:2020-02-17 19:05:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:05:04.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1219" for this suite.
Feb 17 19:05:12.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:05:15.031: INFO: namespace deployment-1219 deletion completed in 10.132201332s

• [SLOW TEST:18.765 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:05:15.033: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Feb 17 19:05:23.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec pod-sharedvolume-c7cd0135-8d25-4eb3-8593-71ceed47f90b -c busybox-main-container --namespace=emptydir-6730 -- cat /usr/share/volumeshare/shareddata.txt'
Feb 17 19:05:23.879: INFO: stderr: ""
Feb 17 19:05:23.879: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:05:23.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6730" for this suite.
Feb 17 19:05:31.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:05:34.031: INFO: namespace emptydir-6730 deletion completed in 10.109393967s

• [SLOW TEST:18.998 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:05:34.032: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-bl8c
STEP: Creating a pod to test atomic-volume-subpath
Feb 17 19:05:34.253: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-bl8c" in namespace "subpath-6925" to be "success or failure"
Feb 17 19:05:34.266: INFO: Pod "pod-subpath-test-configmap-bl8c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.270028ms
Feb 17 19:05:36.283: INFO: Pod "pod-subpath-test-configmap-bl8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03025581s
Feb 17 19:05:38.297: INFO: Pod "pod-subpath-test-configmap-bl8c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043832197s
Feb 17 19:05:40.311: INFO: Pod "pod-subpath-test-configmap-bl8c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.057594721s
Feb 17 19:05:42.324: INFO: Pod "pod-subpath-test-configmap-bl8c": Phase="Running", Reason="", readiness=true. Elapsed: 8.070708785s
Feb 17 19:05:44.337: INFO: Pod "pod-subpath-test-configmap-bl8c": Phase="Running", Reason="", readiness=true. Elapsed: 10.084451288s
Feb 17 19:05:46.352: INFO: Pod "pod-subpath-test-configmap-bl8c": Phase="Running", Reason="", readiness=true. Elapsed: 12.098616224s
Feb 17 19:05:48.365: INFO: Pod "pod-subpath-test-configmap-bl8c": Phase="Running", Reason="", readiness=true. Elapsed: 14.112314031s
Feb 17 19:05:50.379: INFO: Pod "pod-subpath-test-configmap-bl8c": Phase="Running", Reason="", readiness=true. Elapsed: 16.125662193s
Feb 17 19:05:52.392: INFO: Pod "pod-subpath-test-configmap-bl8c": Phase="Running", Reason="", readiness=true. Elapsed: 18.139509167s
Feb 17 19:05:54.406: INFO: Pod "pod-subpath-test-configmap-bl8c": Phase="Running", Reason="", readiness=true. Elapsed: 20.153061022s
Feb 17 19:05:56.422: INFO: Pod "pod-subpath-test-configmap-bl8c": Phase="Running", Reason="", readiness=true. Elapsed: 22.169499331s
Feb 17 19:05:58.436: INFO: Pod "pod-subpath-test-configmap-bl8c": Phase="Running", Reason="", readiness=true. Elapsed: 24.182722254s
Feb 17 19:06:00.450: INFO: Pod "pod-subpath-test-configmap-bl8c": Phase="Running", Reason="", readiness=true. Elapsed: 26.197313059s
Feb 17 19:06:02.464: INFO: Pod "pod-subpath-test-configmap-bl8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.210934415s
STEP: Saw pod success
Feb 17 19:06:02.464: INFO: Pod "pod-subpath-test-configmap-bl8c" satisfied condition "success or failure"
Feb 17 19:06:02.476: INFO: Trying to get logs from node 10.45.66.189 pod pod-subpath-test-configmap-bl8c container test-container-subpath-configmap-bl8c: <nil>
STEP: delete the pod
Feb 17 19:06:02.577: INFO: Waiting for pod pod-subpath-test-configmap-bl8c to disappear
Feb 17 19:06:02.590: INFO: Pod pod-subpath-test-configmap-bl8c no longer exists
STEP: Deleting pod pod-subpath-test-configmap-bl8c
Feb 17 19:06:02.590: INFO: Deleting pod "pod-subpath-test-configmap-bl8c" in namespace "subpath-6925"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:06:02.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6925" for this suite.
Feb 17 19:06:10.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:06:12.787: INFO: namespace subpath-6925 deletion completed in 10.149979337s

• [SLOW TEST:38.755 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:06:12.790: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Feb 17 19:06:12.953: INFO: Waiting up to 5m0s for pod "pod-aba85fdf-16d8-45b3-84f4-da6a5ad89b1c" in namespace "emptydir-4944" to be "success or failure"
Feb 17 19:06:12.965: INFO: Pod "pod-aba85fdf-16d8-45b3-84f4-da6a5ad89b1c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.294565ms
Feb 17 19:06:14.979: INFO: Pod "pod-aba85fdf-16d8-45b3-84f4-da6a5ad89b1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025379164s
Feb 17 19:06:16.995: INFO: Pod "pod-aba85fdf-16d8-45b3-84f4-da6a5ad89b1c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041678398s
Feb 17 19:06:19.010: INFO: Pod "pod-aba85fdf-16d8-45b3-84f4-da6a5ad89b1c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056374284s
Feb 17 19:06:21.046: INFO: Pod "pod-aba85fdf-16d8-45b3-84f4-da6a5ad89b1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.092617745s
STEP: Saw pod success
Feb 17 19:06:21.046: INFO: Pod "pod-aba85fdf-16d8-45b3-84f4-da6a5ad89b1c" satisfied condition "success or failure"
Feb 17 19:06:21.060: INFO: Trying to get logs from node 10.45.66.189 pod pod-aba85fdf-16d8-45b3-84f4-da6a5ad89b1c container test-container: <nil>
STEP: delete the pod
Feb 17 19:06:21.125: INFO: Waiting for pod pod-aba85fdf-16d8-45b3-84f4-da6a5ad89b1c to disappear
Feb 17 19:06:21.137: INFO: Pod pod-aba85fdf-16d8-45b3-84f4-da6a5ad89b1c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:06:21.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4944" for this suite.
Feb 17 19:06:29.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:06:31.338: INFO: namespace emptydir-4944 deletion completed in 10.16793397s

• [SLOW TEST:18.548 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:06:31.343: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:06:31.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1523" for this suite.
Feb 17 19:06:39.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:06:41.744: INFO: namespace custom-resource-definition-1523 deletion completed in 10.161998023s

• [SLOW TEST:10.402 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:06:41.745: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Feb 17 19:06:42.485: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Feb 17 19:06:44.526: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563202, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563202, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563202, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563202, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 19:06:46.539: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563202, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563202, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563202, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563202, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 19:06:48.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563202, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563202, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563202, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563202, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Feb 17 19:06:51.569: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 19:06:51.588: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:06:52.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5963" for this suite.
Feb 17 19:07:00.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:07:02.625: INFO: namespace crd-webhook-5963 deletion completed in 10.158063152s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:20.957 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:07:02.703: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Feb 17 19:07:02.845: INFO: Waiting up to 1m0s for all nodes to be ready
Feb 17 19:08:03.020: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 19:08:03.037: INFO: Starting informer...
STEP: Starting pods...
Feb 17 19:08:03.313: INFO: Pod1 is running on 10.45.66.189. Tainting Node
Feb 17 19:08:11.619: INFO: Pod2 is running on 10.45.66.189. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Feb 17 19:08:27.866: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Feb 17 19:08:47.868: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:08:47.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-6143" for this suite.
Feb 17 19:08:56.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:08:58.129: INFO: namespace taint-multiple-pods-6143 deletion completed in 10.185487275s

• [SLOW TEST:115.427 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:08:58.130: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 17 19:08:58.337: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 17 19:08:58.394: INFO: Waiting for terminating namespaces to be deleted...
Feb 17 19:08:58.410: INFO: 
Logging pods the kubelet thinks is on node 10.45.66.177 before test
Feb 17 19:08:58.539: INFO: calico-node-mjlf4 from kube-system started at 2020-02-17 14:34:53 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.540: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 19:08:58.540: INFO: catalog-operator-7fccd6877f-xzz4r from openshift-operator-lifecycle-manager started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.540: INFO: 	Container catalog-operator ready: true, restart count 0
Feb 17 19:08:58.540: INFO: downloads-65dd498cf8-cqpss from openshift-console started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.540: INFO: 	Container download-server ready: true, restart count 0
Feb 17 19:08:58.540: INFO: sonobuoy from sonobuoy started at 2020-02-17 16:29:56 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.540: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 17 19:08:58.540: INFO: marketplace-operator-664f66c947-jckz5 from openshift-marketplace started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.540: INFO: 	Container marketplace-operator ready: true, restart count 0
Feb 17 19:08:58.540: INFO: configmap-cabundle-injector-c749c6984-hlkf5 from openshift-service-ca started at 2020-02-17 14:36:50 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.540: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Feb 17 19:08:58.540: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-02-17 14:41:40 +0000 UTC (3 container statuses recorded)
Feb 17 19:08:58.540: INFO: 	Container alertmanager ready: true, restart count 0
Feb 17 19:08:58.540: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 17 19:08:58.540: INFO: 	Container config-reloader ready: true, restart count 0
Feb 17 19:08:58.540: INFO: network-operator-86974b9cf-spclm from openshift-network-operator started at 2020-02-17 14:34:58 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.540: INFO: 	Container network-operator ready: true, restart count 0
Feb 17 19:08:58.540: INFO: calico-typha-76c597bc7d-4tfhl from kube-system started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.540: INFO: 	Container calico-typha ready: true, restart count 0
Feb 17 19:08:58.540: INFO: ibm-storage-watcher-5cf8f8b4cb-pnqvx from kube-system started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.540: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Feb 17 19:08:58.540: INFO: openshift-state-metrics-86bbf546f-gglb8 from openshift-monitoring started at 2020-02-17 14:36:26 +0000 UTC (3 container statuses recorded)
Feb 17 19:08:58.540: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 17 19:08:58.540: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 17 19:08:58.540: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Feb 17 19:08:58.540: INFO: node-exporter-rfpkb from openshift-monitoring started at 2020-02-17 14:36:28 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.540: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:08:58.540: INFO: 	Container node-exporter ready: true, restart count 0
Feb 17 19:08:58.540: INFO: multus-z6qv7 from openshift-multus started at 2020-02-17 14:35:36 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.540: INFO: 	Container kube-multus ready: true, restart count 0
Feb 17 19:08:58.541: INFO: openshift-service-catalog-controller-manager-operator-6857997dg from openshift-service-catalog-controller-manager-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container operator ready: true, restart count 1
Feb 17 19:08:58.541: INFO: dns-operator-586c9c56ff-nxk8c from openshift-dns-operator started at 2020-02-17 14:36:04 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container dns-operator ready: true, restart count 0
Feb 17 19:08:58.541: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:08:58.541: INFO: ibm-file-plugin-788cbbd987-7hzlx from kube-system started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Feb 17 19:08:58.541: INFO: cluster-monitoring-operator-55d99d5ffc-qlxs4 from openshift-monitoring started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Feb 17 19:08:58.541: INFO: console-7547c7bc6c-n79r2 from openshift-console started at 2020-02-17 19:08:11 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container console ready: true, restart count 0
Feb 17 19:08:58.541: INFO: tuned-f4zpl from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:28 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container tuned ready: true, restart count 0
Feb 17 19:08:58.541: INFO: ibm-master-proxy-static-10.45.66.177 from kube-system started at 2020-02-17 14:34:51 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 19:08:58.541: INFO: 	Container pause ready: true, restart count 0
Feb 17 19:08:58.541: INFO: openshift-kube-proxy-sz6st from openshift-kube-proxy started at 2020-02-17 14:35:42 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 19:08:58.541: INFO: ingress-operator-855959d8d9-xhrmn from openshift-ingress-operator started at 2020-02-17 14:36:04 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container ingress-operator ready: true, restart count 0
Feb 17 19:08:58.541: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:08:58.541: INFO: cluster-storage-operator-646c48994f-4nvbp from openshift-cluster-storage-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Feb 17 19:08:58.541: INFO: calico-kube-controllers-5df9fd5998-tgjvk from kube-system started at 2020-02-17 14:36:06 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 17 19:08:58.541: INFO: openshift-service-catalog-apiserver-operator-577c6ffb4f-r7vw8 from openshift-service-catalog-apiserver-operator started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container operator ready: true, restart count 1
Feb 17 19:08:58.541: INFO: cluster-image-registry-operator-79bfcb686-62774 from openshift-image-registry started at 2020-02-17 14:36:04 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Feb 17 19:08:58.541: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Feb 17 19:08:58.541: INFO: router-default-6c67c67864-hlm85 from openshift-ingress started at 2020-02-17 14:42:12 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container router ready: true, restart count 0
Feb 17 19:08:58.541: INFO: packageserver-749668c8dd-fpjbg from openshift-operator-lifecycle-manager started at 2020-02-17 15:40:00 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container packageserver ready: true, restart count 0
Feb 17 19:08:58.541: INFO: service-ca-operator-5f9bd445b8-k65l4 from openshift-service-ca-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container operator ready: true, restart count 0
Feb 17 19:08:58.541: INFO: dns-default-xw2km from openshift-dns started at 2020-02-17 14:37:34 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container dns ready: true, restart count 0
Feb 17 19:08:58.541: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 17 19:08:58.541: INFO: vpn-5d9c8dbf8-fwt5m from kube-system started at 2020-02-17 14:45:06 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container vpn ready: true, restart count 0
Feb 17 19:08:58.541: INFO: sonobuoy-e2e-job-a662486eb1ae4ce5 from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container e2e ready: true, restart count 0
Feb 17 19:08:58.541: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 19:08:58.541: INFO: sonobuoy-systemd-logs-daemon-set-67976b35ad084332-bvf8d from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.541: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Feb 17 19:08:58.541: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 19:08:58.542: INFO: olm-operator-5f5f7bbd7d-8k49r from openshift-operator-lifecycle-manager started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.542: INFO: 	Container olm-operator ready: true, restart count 0
Feb 17 19:08:58.542: INFO: ibmcloud-block-storage-plugin-75cf87c786-cc9kd from kube-system started at 2020-02-17 14:34:58 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.542: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Feb 17 19:08:58.542: INFO: prometheus-adapter-55674d9685-pfwsr from openshift-monitoring started at 2020-02-17 19:08:11 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.542: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 17 19:08:58.542: INFO: cluster-node-tuning-operator-5cbf9c8db5-mr2tk from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.542: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Feb 17 19:08:58.542: INFO: cloud-credential-operator-6fc5bb58fc-npgb8 from openshift-cloud-credential-operator started at 2020-02-17 14:36:06 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.542: INFO: 	Container manager ready: true, restart count 0
Feb 17 19:08:58.542: INFO: kube-state-metrics-5745cc99f5-vd9j6 from openshift-monitoring started at 2020-02-17 14:36:27 +0000 UTC (3 container statuses recorded)
Feb 17 19:08:58.542: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 17 19:08:58.542: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 17 19:08:58.542: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 17 19:08:58.542: INFO: multus-admission-controller-x8vxh from openshift-multus started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.542: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 17 19:08:58.542: INFO: downloads-65dd498cf8-vvlwb from openshift-console started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.542: INFO: 	Container download-server ready: true, restart count 0
Feb 17 19:08:58.542: INFO: service-serving-cert-signer-6669ffbb98-s7bf6 from openshift-service-ca started at 2020-02-17 14:36:49 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.542: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Feb 17 19:08:58.542: INFO: node-ca-49d68 from openshift-image-registry started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.542: INFO: 	Container node-ca ready: true, restart count 0
Feb 17 19:08:58.542: INFO: ibm-cloud-provider-ip-158-175-93-66-7b5b7d9cb-w5zjw from ibm-system started at 2020-02-17 18:46:50 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.542: INFO: 	Container ibm-cloud-provider-ip-158-175-93-66 ready: true, restart count 0
Feb 17 19:08:58.542: INFO: ibmcloud-block-storage-driver-vnql8 from kube-system started at 2020-02-17 14:34:55 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.542: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 17 19:08:58.542: INFO: apiservice-cabundle-injector-745c6fc9d8-h2znf from openshift-service-ca started at 2020-02-17 14:36:50 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.542: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Feb 17 19:08:58.542: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-02-17 19:08:18 +0000 UTC (7 container statuses recorded)
Feb 17 19:08:58.542: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:08:58.542: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 19:08:58.542: INFO: 	Container prometheus ready: true, restart count 1
Feb 17 19:08:58.542: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Feb 17 19:08:58.542: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 17 19:08:58.542: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Feb 17 19:08:58.542: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 17 19:08:58.542: INFO: ibm-keepalived-watcher-684qj from kube-system started at 2020-02-17 14:34:53 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.543: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 19:08:58.543: INFO: console-operator-7897bcfbd8-m6vt2 from openshift-console-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.543: INFO: 	Container console-operator ready: true, restart count 1
Feb 17 19:08:58.543: INFO: telemeter-client-56f4c98664-kc4sq from openshift-monitoring started at 2020-02-17 18:46:50 +0000 UTC (3 container statuses recorded)
Feb 17 19:08:58.543: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:08:58.543: INFO: 	Container reload ready: true, restart count 0
Feb 17 19:08:58.543: INFO: 	Container telemeter-client ready: true, restart count 0
Feb 17 19:08:58.543: INFO: 
Logging pods the kubelet thinks is on node 10.45.66.178 before test
Feb 17 19:08:58.662: INFO: prometheus-operator-7d5f885558-bv55g from openshift-monitoring started at 2020-02-17 14:41:26 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.662: INFO: 	Container prometheus-operator ready: true, restart count 0
Feb 17 19:08:58.662: INFO: community-operators-6c7f5d4977-z2vjq from openshift-marketplace started at 2020-02-17 18:46:50 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.662: INFO: 	Container community-operators ready: true, restart count 0
Feb 17 19:08:58.662: INFO: ibm-master-proxy-static-10.45.66.178 from kube-system started at 2020-02-17 14:37:31 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.662: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 19:08:58.662: INFO: 	Container pause ready: true, restart count 0
Feb 17 19:08:58.662: INFO: node-ca-n4v6k from openshift-image-registry started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.662: INFO: 	Container node-ca ready: true, restart count 0
Feb 17 19:08:58.662: INFO: registry-pvc-permissions-jlk4p from openshift-image-registry started at 2020-02-17 14:40:17 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.662: INFO: 	Container pvc-permissions ready: false, restart count 0
Feb 17 19:08:58.662: INFO: thanos-querier-b949c5db-ns4qs from openshift-monitoring started at 2020-02-17 14:42:53 +0000 UTC (4 container statuses recorded)
Feb 17 19:08:58.662: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:08:58.662: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 17 19:08:58.662: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 19:08:58.662: INFO: 	Container thanos-querier ready: true, restart count 0
Feb 17 19:08:58.662: INFO: router-default-6c67c67864-f8hvb from openshift-ingress started at 2020-02-17 14:42:29 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.662: INFO: 	Container router ready: true, restart count 0
Feb 17 19:08:58.662: INFO: node-exporter-wmxf5 from openshift-monitoring started at 2020-02-17 14:36:43 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.662: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:08:58.662: INFO: 	Container node-exporter ready: true, restart count 0
Feb 17 19:08:58.662: INFO: dns-default-9fgff from openshift-dns started at 2020-02-17 14:38:00 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.662: INFO: 	Container dns ready: true, restart count 0
Feb 17 19:08:58.662: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 17 19:08:58.662: INFO: packageserver-749668c8dd-slgc8 from openshift-operator-lifecycle-manager started at 2020-02-17 15:40:09 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.662: INFO: 	Container packageserver ready: true, restart count 0
Feb 17 19:08:58.662: INFO: sonobuoy-systemd-logs-daemon-set-67976b35ad084332-thldq from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.662: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Feb 17 19:08:58.662: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 19:08:58.662: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-02-17 14:41:51 +0000 UTC (3 container statuses recorded)
Feb 17 19:08:58.662: INFO: 	Container alertmanager ready: true, restart count 0
Feb 17 19:08:58.662: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 17 19:08:58.662: INFO: 	Container config-reloader ready: true, restart count 0
Feb 17 19:08:58.662: INFO: calico-node-zkvdn from kube-system started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.662: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 19:08:58.662: INFO: ibm-keepalived-watcher-4nt2b from kube-system started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 19:08:58.663: INFO: cluster-samples-operator-946f4d4c5-66g9k from openshift-cluster-samples-operator started at 2020-02-17 14:38:52 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Feb 17 19:08:58.663: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Feb 17 19:08:58.663: INFO: image-registry-6d4857c99d-q2dn4 from openshift-image-registry started at 2020-02-17 14:40:17 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container registry ready: true, restart count 0
Feb 17 19:08:58.663: INFO: console-7547c7bc6c-wschb from openshift-console started at 2020-02-17 14:39:26 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container console ready: true, restart count 0
Feb 17 19:08:58.663: INFO: multus-wbdmp from openshift-multus started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container kube-multus ready: true, restart count 0
Feb 17 19:08:58.663: INFO: ibm-cloud-provider-ip-158-175-93-66-7b5b7d9cb-jbd6r from ibm-system started at 2020-02-17 14:41:22 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container ibm-cloud-provider-ip-158-175-93-66 ready: true, restart count 0
Feb 17 19:08:58.663: INFO: redhat-operators-7cc9d7897-rw5tc from openshift-marketplace started at 2020-02-17 19:08:11 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container redhat-operators ready: true, restart count 0
Feb 17 19:08:58.663: INFO: thanos-querier-b949c5db-f9jx6 from openshift-monitoring started at 2020-02-17 19:08:11 +0000 UTC (4 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:08:58.663: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 17 19:08:58.663: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 19:08:58.663: INFO: 	Container thanos-querier ready: true, restart count 0
Feb 17 19:08:58.663: INFO: tuned-2n22g from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container tuned ready: true, restart count 0
Feb 17 19:08:58.663: INFO: openshift-kube-proxy-kchsl from openshift-kube-proxy started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 19:08:58.663: INFO: grafana-66dff46d7c-l5jsk from openshift-monitoring started at 2020-02-17 14:41:46 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container grafana ready: true, restart count 0
Feb 17 19:08:58.663: INFO: 	Container grafana-proxy ready: true, restart count 0
Feb 17 19:08:58.663: INFO: certified-operators-549b955645-dv7mk from openshift-marketplace started at 2020-02-17 19:08:11 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container certified-operators ready: true, restart count 0
Feb 17 19:08:58.663: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-02-17 19:08:18 +0000 UTC (3 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container alertmanager ready: true, restart count 0
Feb 17 19:08:58.663: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 17 19:08:58.663: INFO: 	Container config-reloader ready: true, restart count 0
Feb 17 19:08:58.663: INFO: ibmcloud-block-storage-driver-d7lnw from kube-system started at 2020-02-17 14:36:45 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 17 19:08:58.663: INFO: multus-admission-controller-s6fg5 from openshift-multus started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 17 19:08:58.663: INFO: calico-typha-76c597bc7d-72pj2 from kube-system started at 2020-02-17 14:38:05 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container calico-typha ready: true, restart count 0
Feb 17 19:08:58.663: INFO: prometheus-adapter-55674d9685-vvdz2 from openshift-monitoring started at 2020-02-17 14:41:40 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 17 19:08:58.663: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-02-17 14:42:48 +0000 UTC (7 container statuses recorded)
Feb 17 19:08:58.663: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:08:58.663: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 19:08:58.664: INFO: 	Container prometheus ready: true, restart count 1
Feb 17 19:08:58.664: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Feb 17 19:08:58.664: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 17 19:08:58.664: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Feb 17 19:08:58.664: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 17 19:08:58.664: INFO: 
Logging pods the kubelet thinks is on node 10.45.66.189 before test
Feb 17 19:08:58.746: INFO: ibm-master-proxy-static-10.45.66.189 from kube-system started at 2020-02-17 14:37:14 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.746: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 19:08:58.746: INFO: 	Container pause ready: true, restart count 0
Feb 17 19:08:58.746: INFO: multus-df87p from openshift-multus started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.746: INFO: 	Container kube-multus ready: true, restart count 0
Feb 17 19:08:58.746: INFO: openshift-kube-proxy-gwxkw from openshift-kube-proxy started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.746: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 19:08:58.746: INFO: multus-admission-controller-dp92c from openshift-multus started at 2020-02-17 19:08:48 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.746: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 17 19:08:58.746: INFO: calico-typha-76c597bc7d-6zmf9 from kube-system started at 2020-02-17 19:08:52 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.746: INFO: 	Container calico-typha ready: false, restart count 0
Feb 17 19:08:58.746: INFO: node-exporter-4fhhh from openshift-monitoring started at 2020-02-17 14:36:28 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.746: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:08:58.746: INFO: 	Container node-exporter ready: true, restart count 0
Feb 17 19:08:58.746: INFO: dns-default-d8gsb from openshift-dns started at 2020-02-17 19:08:47 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.746: INFO: 	Container dns ready: false, restart count 0
Feb 17 19:08:58.746: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 17 19:08:58.746: INFO: ibm-keepalived-watcher-5hn6l from kube-system started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.746: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 19:08:58.746: INFO: calico-node-cctn2 from kube-system started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.746: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 19:08:58.746: INFO: ibmcloud-block-storage-driver-jnxhv from kube-system started at 2020-02-17 14:36:32 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.746: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 17 19:08:58.746: INFO: tuned-kg4dp from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:28 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.746: INFO: 	Container tuned ready: true, restart count 0
Feb 17 19:08:58.746: INFO: node-ca-vjvhw from openshift-image-registry started at 2020-02-17 19:08:48 +0000 UTC (1 container statuses recorded)
Feb 17 19:08:58.746: INFO: 	Container node-ca ready: true, restart count 0
Feb 17 19:08:58.746: INFO: sonobuoy-systemd-logs-daemon-set-67976b35ad084332-4jdsp from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 19:08:58.746: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Feb 17 19:08:58.746: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-1aea27e2-74ee-4728-bee1-6a03e74a7c26 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-1aea27e2-74ee-4728-bee1-6a03e74a7c26 off the node 10.45.66.189
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1aea27e2-74ee-4728-bee1-6a03e74a7c26
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:09:15.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9364" for this suite.
Feb 17 19:09:27.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:09:29.224: INFO: namespace sched-pred-9364 deletion completed in 14.166922342s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:31.094 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:09:29.224: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 19:09:29.356: INFO: Creating ReplicaSet my-hostname-basic-f6dbc238-41b9-4f0d-84fd-0e588e9ad63a
Feb 17 19:09:29.392: INFO: Pod name my-hostname-basic-f6dbc238-41b9-4f0d-84fd-0e588e9ad63a: Found 0 pods out of 1
Feb 17 19:09:34.405: INFO: Pod name my-hostname-basic-f6dbc238-41b9-4f0d-84fd-0e588e9ad63a: Found 1 pods out of 1
Feb 17 19:09:34.405: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f6dbc238-41b9-4f0d-84fd-0e588e9ad63a" is running
Feb 17 19:09:38.430: INFO: Pod "my-hostname-basic-f6dbc238-41b9-4f0d-84fd-0e588e9ad63a-wnhmg" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 19:09:29 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 19:09:29 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-f6dbc238-41b9-4f0d-84fd-0e588e9ad63a]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 19:09:29 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-f6dbc238-41b9-4f0d-84fd-0e588e9ad63a]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-02-17 19:09:29 +0000 UTC Reason: Message:}])
Feb 17 19:09:38.430: INFO: Trying to dial the pod
Feb 17 19:09:43.488: INFO: Controller my-hostname-basic-f6dbc238-41b9-4f0d-84fd-0e588e9ad63a: Got expected result from replica 1 [my-hostname-basic-f6dbc238-41b9-4f0d-84fd-0e588e9ad63a-wnhmg]: "my-hostname-basic-f6dbc238-41b9-4f0d-84fd-0e588e9ad63a-wnhmg", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:09:43.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2911" for this suite.
Feb 17 19:09:51.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:09:53.636: INFO: namespace replicaset-2911 deletion completed in 10.129170568s

• [SLOW TEST:24.412 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:09:53.636: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2876
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2876
STEP: creating replication controller externalsvc in namespace services-2876
I0217 19:09:53.849723      26 runners.go:184] Created replication controller with name: externalsvc, namespace: services-2876, replica count: 2
I0217 19:09:56.900801      26 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 19:09:59.901143      26 runners.go:184] externalsvc Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0217 19:10:02.901363      26 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Feb 17 19:10:02.975: INFO: Creating new exec pod
Feb 17 19:10:11.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559420064 exec --namespace=services-2876 execpodgzlsb -- /bin/sh -x -c nslookup nodeport-service'
Feb 17 19:10:11.432: INFO: stderr: "+ nslookup nodeport-service\n"
Feb 17 19:10:11.432: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-2876.svc.cluster.local\tcanonical name = externalsvc.services-2876.svc.cluster.local.\nName:\texternalsvc.services-2876.svc.cluster.local\nAddress: 172.21.107.215\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2876, will wait for the garbage collector to delete the pods
Feb 17 19:10:11.536: INFO: Deleting ReplicationController externalsvc took: 38.872711ms
Feb 17 19:10:12.037: INFO: Terminating ReplicationController externalsvc pods took: 500.312077ms
Feb 17 19:10:18.011: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:10:18.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2876" for this suite.
Feb 17 19:10:26.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:10:28.312: INFO: namespace services-2876 deletion completed in 10.224183906s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:34.676 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:10:28.313: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:10:41.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3401" for this suite.
Feb 17 19:10:49.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:10:51.929: INFO: namespace resourcequota-3401 deletion completed in 10.195644611s

• [SLOW TEST:23.616 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:10:51.929: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Feb 17 19:10:52.666: INFO: created pod pod-service-account-defaultsa
Feb 17 19:10:52.666: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Feb 17 19:10:52.704: INFO: created pod pod-service-account-mountsa
Feb 17 19:10:52.704: INFO: pod pod-service-account-mountsa service account token volume mount: true
Feb 17 19:10:52.732: INFO: created pod pod-service-account-nomountsa
Feb 17 19:10:52.732: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Feb 17 19:10:52.765: INFO: created pod pod-service-account-defaultsa-mountspec
Feb 17 19:10:52.765: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Feb 17 19:10:52.798: INFO: created pod pod-service-account-mountsa-mountspec
Feb 17 19:10:52.798: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Feb 17 19:10:52.854: INFO: created pod pod-service-account-nomountsa-mountspec
Feb 17 19:10:52.854: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Feb 17 19:10:52.888: INFO: created pod pod-service-account-defaultsa-nomountspec
Feb 17 19:10:52.888: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Feb 17 19:10:52.923: INFO: created pod pod-service-account-mountsa-nomountspec
Feb 17 19:10:52.923: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Feb 17 19:10:52.955: INFO: created pod pod-service-account-nomountsa-nomountspec
Feb 17 19:10:52.955: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:10:52.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3152" for this suite.
Feb 17 19:11:15.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:11:17.151: INFO: namespace svcaccounts-3152 deletion completed in 24.156359463s

• [SLOW TEST:25.221 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:11:17.151: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Feb 17 19:11:18.343: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6631" to be "success or failure"
Feb 17 19:11:18.356: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 12.903454ms
Feb 17 19:11:20.369: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025822477s
Feb 17 19:11:22.382: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038972204s
Feb 17 19:11:24.397: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054039781s
Feb 17 19:11:26.409: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.066203591s
STEP: Saw pod success
Feb 17 19:11:26.409: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Feb 17 19:11:26.421: INFO: Trying to get logs from node 10.45.66.189 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Feb 17 19:11:26.522: INFO: Waiting for pod pod-host-path-test to disappear
Feb 17 19:11:26.535: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:11:26.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6631" for this suite.
Feb 17 19:11:34.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:11:36.729: INFO: namespace hostpath-6631 deletion completed in 10.165140066s

• [SLOW TEST:19.578 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:11:36.729: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Feb 17 19:11:36.871: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Feb 17 19:11:36.910: INFO: Pod name sample-pod: Found 0 pods out of 1
Feb 17 19:11:41.924: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Feb 17 19:11:45.951: INFO: Creating deployment "test-rolling-update-deployment"
Feb 17 19:11:45.966: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Feb 17 19:11:45.990: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Feb 17 19:11:48.019: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Feb 17 19:11:48.033: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563506, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563506, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563506, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563506, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 19:11:50.046: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563506, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563506, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563506, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563506, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 19:11:52.051: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563506, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563506, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563506, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63717563506, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Feb 17 19:11:54.045: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Feb 17 19:11:54.082: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8168 /apis/apps/v1/namespaces/deployment-8168/deployments/test-rolling-update-deployment 15432aea-15ca-486e-9e89-7bb776066d31 116030 1 2020-02-17 19:11:45 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0029358d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-02-17 19:11:46 +0000 UTC,LastTransitionTime:2020-02-17 19:11:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-02-17 19:11:53 +0000 UTC,LastTransitionTime:2020-02-17 19:11:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Feb 17 19:11:54.104: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-8168 /apis/apps/v1/namespaces/deployment-8168/replicasets/test-rolling-update-deployment-55d946486 416e8cf3-27d1-4c7a-b326-36e97303352b 116019 1 2020-02-17 19:11:45 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 15432aea-15ca-486e-9e89-7bb776066d31 0xc002935df0 0xc002935df1}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc002935e58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Feb 17 19:11:54.105: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Feb 17 19:11:54.105: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8168 /apis/apps/v1/namespaces/deployment-8168/replicasets/test-rolling-update-controller 9baebdf2-c4e3-4063-a7b7-2688d5cd3be7 116028 2 2020-02-17 19:11:36 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 15432aea-15ca-486e-9e89-7bb776066d31 0xc002935d27 0xc002935d28}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002935d88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Feb 17 19:11:54.118: INFO: Pod "test-rolling-update-deployment-55d946486-jv5fm" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-jv5fm test-rolling-update-deployment-55d946486- deployment-8168 /api/v1/namespaces/deployment-8168/pods/test-rolling-update-deployment-55d946486-jv5fm 926cc05e-bcb6-4b78-8dd1-2f064e5a5310 116018 0 2020-02-17 19:11:46 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:172.30.50.137/32 cni.projectcalico.org/podIPs:172.30.50.137/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.50.137"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 416e8cf3-27d1-4c7a-b326-36e97303352b 0xc0077820c0 0xc0077820c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-7dbb2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-7dbb2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-7dbb2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.45.66.189,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-zjvpj,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:11:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:11:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:11:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-02-17 19:11:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.66.189,PodIP:172.30.50.137,StartTime:2020-02-17 19:11:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-02-17 19:11:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://997edcd10c401897f556c2af89eef9227668e8d67b65ad1edf76b2e19568510e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.50.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:11:54.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8168" for this suite.
Feb 17 19:12:02.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:12:04.324: INFO: namespace deployment-8168 deletion completed in 10.172977597s

• [SLOW TEST:27.594 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:12:04.326: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 19:12:04.512: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c44c3251-6bc8-4eef-afe4-6422a9be9d38" in namespace "projected-6931" to be "success or failure"
Feb 17 19:12:04.526: INFO: Pod "downwardapi-volume-c44c3251-6bc8-4eef-afe4-6422a9be9d38": Phase="Pending", Reason="", readiness=false. Elapsed: 13.083886ms
Feb 17 19:12:06.541: INFO: Pod "downwardapi-volume-c44c3251-6bc8-4eef-afe4-6422a9be9d38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028149307s
Feb 17 19:12:08.555: INFO: Pod "downwardapi-volume-c44c3251-6bc8-4eef-afe4-6422a9be9d38": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042683311s
Feb 17 19:12:10.568: INFO: Pod "downwardapi-volume-c44c3251-6bc8-4eef-afe4-6422a9be9d38": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055699733s
Feb 17 19:12:12.581: INFO: Pod "downwardapi-volume-c44c3251-6bc8-4eef-afe4-6422a9be9d38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.068857714s
STEP: Saw pod success
Feb 17 19:12:12.581: INFO: Pod "downwardapi-volume-c44c3251-6bc8-4eef-afe4-6422a9be9d38" satisfied condition "success or failure"
Feb 17 19:12:12.594: INFO: Trying to get logs from node 10.45.66.189 pod downwardapi-volume-c44c3251-6bc8-4eef-afe4-6422a9be9d38 container client-container: <nil>
STEP: delete the pod
Feb 17 19:12:12.659: INFO: Waiting for pod downwardapi-volume-c44c3251-6bc8-4eef-afe4-6422a9be9d38 to disappear
Feb 17 19:12:12.673: INFO: Pod downwardapi-volume-c44c3251-6bc8-4eef-afe4-6422a9be9d38 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:12:12.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6931" for this suite.
Feb 17 19:12:20.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:12:22.862: INFO: namespace projected-6931 deletion completed in 10.155168094s

• [SLOW TEST:18.536 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:12:22.867: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 19:12:23.059: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f802bc9a-954a-49f1-8d2b-93046058a100" in namespace "projected-3434" to be "success or failure"
Feb 17 19:12:23.072: INFO: Pod "downwardapi-volume-f802bc9a-954a-49f1-8d2b-93046058a100": Phase="Pending", Reason="", readiness=false. Elapsed: 13.287039ms
Feb 17 19:12:25.087: INFO: Pod "downwardapi-volume-f802bc9a-954a-49f1-8d2b-93046058a100": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028257445s
Feb 17 19:12:27.101: INFO: Pod "downwardapi-volume-f802bc9a-954a-49f1-8d2b-93046058a100": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042317411s
Feb 17 19:12:29.115: INFO: Pod "downwardapi-volume-f802bc9a-954a-49f1-8d2b-93046058a100": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055742811s
Feb 17 19:12:31.130: INFO: Pod "downwardapi-volume-f802bc9a-954a-49f1-8d2b-93046058a100": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.071113476s
STEP: Saw pod success
Feb 17 19:12:31.130: INFO: Pod "downwardapi-volume-f802bc9a-954a-49f1-8d2b-93046058a100" satisfied condition "success or failure"
Feb 17 19:12:31.144: INFO: Trying to get logs from node 10.45.66.189 pod downwardapi-volume-f802bc9a-954a-49f1-8d2b-93046058a100 container client-container: <nil>
STEP: delete the pod
Feb 17 19:12:31.209: INFO: Waiting for pod downwardapi-volume-f802bc9a-954a-49f1-8d2b-93046058a100 to disappear
Feb 17 19:12:31.221: INFO: Pod downwardapi-volume-f802bc9a-954a-49f1-8d2b-93046058a100 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:12:31.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3434" for this suite.
Feb 17 19:12:39.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:12:41.441: INFO: namespace projected-3434 deletion completed in 10.174553263s

• [SLOW TEST:18.574 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:12:41.443: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Feb 17 19:12:41.631: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Feb 17 19:12:41.708: INFO: Waiting for terminating namespaces to be deleted...
Feb 17 19:12:41.730: INFO: 
Logging pods the kubelet thinks is on node 10.45.66.177 before test
Feb 17 19:12:41.888: INFO: multus-z6qv7 from openshift-multus started at 2020-02-17 14:35:36 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.888: INFO: 	Container kube-multus ready: true, restart count 0
Feb 17 19:12:41.888: INFO: openshift-service-catalog-controller-manager-operator-6857997dg from openshift-service-catalog-controller-manager-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.888: INFO: 	Container operator ready: true, restart count 1
Feb 17 19:12:41.888: INFO: dns-operator-586c9c56ff-nxk8c from openshift-dns-operator started at 2020-02-17 14:36:04 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:41.888: INFO: 	Container dns-operator ready: true, restart count 0
Feb 17 19:12:41.888: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:12:41.888: INFO: ibm-file-plugin-788cbbd987-7hzlx from kube-system started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.888: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Feb 17 19:12:41.888: INFO: cluster-monitoring-operator-55d99d5ffc-qlxs4 from openshift-monitoring started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.888: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Feb 17 19:12:41.888: INFO: console-7547c7bc6c-n79r2 from openshift-console started at 2020-02-17 19:08:11 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container console ready: true, restart count 0
Feb 17 19:12:41.889: INFO: ibm-master-proxy-static-10.45.66.177 from kube-system started at 2020-02-17 14:34:51 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container pause ready: true, restart count 0
Feb 17 19:12:41.889: INFO: openshift-kube-proxy-sz6st from openshift-kube-proxy started at 2020-02-17 14:35:42 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 19:12:41.889: INFO: ingress-operator-855959d8d9-xhrmn from openshift-ingress-operator started at 2020-02-17 14:36:04 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container ingress-operator ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:12:41.889: INFO: cluster-storage-operator-646c48994f-4nvbp from openshift-cluster-storage-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Feb 17 19:12:41.889: INFO: calico-kube-controllers-5df9fd5998-tgjvk from kube-system started at 2020-02-17 14:36:06 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Feb 17 19:12:41.889: INFO: tuned-f4zpl from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:28 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container tuned ready: true, restart count 0
Feb 17 19:12:41.889: INFO: openshift-service-catalog-apiserver-operator-577c6ffb4f-r7vw8 from openshift-service-catalog-apiserver-operator started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container operator ready: true, restart count 1
Feb 17 19:12:41.889: INFO: cluster-image-registry-operator-79bfcb686-62774 from openshift-image-registry started at 2020-02-17 14:36:04 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Feb 17 19:12:41.889: INFO: router-default-6c67c67864-hlm85 from openshift-ingress started at 2020-02-17 14:42:12 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container router ready: true, restart count 0
Feb 17 19:12:41.889: INFO: packageserver-749668c8dd-fpjbg from openshift-operator-lifecycle-manager started at 2020-02-17 15:40:00 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container packageserver ready: true, restart count 0
Feb 17 19:12:41.889: INFO: service-ca-operator-5f9bd445b8-k65l4 from openshift-service-ca-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container operator ready: true, restart count 0
Feb 17 19:12:41.889: INFO: dns-default-xw2km from openshift-dns started at 2020-02-17 14:37:34 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container dns ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 17 19:12:41.889: INFO: vpn-5d9c8dbf8-fwt5m from kube-system started at 2020-02-17 14:45:06 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container vpn ready: true, restart count 0
Feb 17 19:12:41.889: INFO: sonobuoy-e2e-job-a662486eb1ae4ce5 from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container e2e ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Feb 17 19:12:41.889: INFO: sonobuoy-systemd-logs-daemon-set-67976b35ad084332-bvf8d from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Feb 17 19:12:41.889: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 19:12:41.889: INFO: olm-operator-5f5f7bbd7d-8k49r from openshift-operator-lifecycle-manager started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container olm-operator ready: true, restart count 0
Feb 17 19:12:41.889: INFO: ibmcloud-block-storage-plugin-75cf87c786-cc9kd from kube-system started at 2020-02-17 14:34:58 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Feb 17 19:12:41.889: INFO: prometheus-adapter-55674d9685-pfwsr from openshift-monitoring started at 2020-02-17 19:08:11 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 17 19:12:41.889: INFO: cluster-node-tuning-operator-5cbf9c8db5-mr2tk from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Feb 17 19:12:41.889: INFO: cloud-credential-operator-6fc5bb58fc-npgb8 from openshift-cloud-credential-operator started at 2020-02-17 14:36:06 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container manager ready: true, restart count 0
Feb 17 19:12:41.889: INFO: kube-state-metrics-5745cc99f5-vd9j6 from openshift-monitoring started at 2020-02-17 14:36:27 +0000 UTC (3 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container kube-state-metrics ready: true, restart count 0
Feb 17 19:12:41.889: INFO: multus-admission-controller-x8vxh from openshift-multus started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 17 19:12:41.889: INFO: downloads-65dd498cf8-vvlwb from openshift-console started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container download-server ready: true, restart count 0
Feb 17 19:12:41.889: INFO: service-serving-cert-signer-6669ffbb98-s7bf6 from openshift-service-ca started at 2020-02-17 14:36:49 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Feb 17 19:12:41.889: INFO: node-ca-49d68 from openshift-image-registry started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container node-ca ready: true, restart count 0
Feb 17 19:12:41.889: INFO: ibm-cloud-provider-ip-158-175-93-66-7b5b7d9cb-w5zjw from ibm-system started at 2020-02-17 18:46:50 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container ibm-cloud-provider-ip-158-175-93-66 ready: true, restart count 0
Feb 17 19:12:41.889: INFO: ibmcloud-block-storage-driver-vnql8 from kube-system started at 2020-02-17 14:34:55 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 17 19:12:41.889: INFO: apiservice-cabundle-injector-745c6fc9d8-h2znf from openshift-service-ca started at 2020-02-17 14:36:50 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Feb 17 19:12:41.889: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-02-17 19:08:18 +0000 UTC (7 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container prometheus ready: true, restart count 1
Feb 17 19:12:41.889: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 17 19:12:41.889: INFO: ibm-keepalived-watcher-684qj from kube-system started at 2020-02-17 14:34:53 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 19:12:41.889: INFO: console-operator-7897bcfbd8-m6vt2 from openshift-console-operator started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container console-operator ready: true, restart count 1
Feb 17 19:12:41.889: INFO: telemeter-client-56f4c98664-kc4sq from openshift-monitoring started at 2020-02-17 18:46:50 +0000 UTC (3 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container reload ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container telemeter-client ready: true, restart count 0
Feb 17 19:12:41.889: INFO: calico-node-mjlf4 from kube-system started at 2020-02-17 14:34:53 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 19:12:41.889: INFO: catalog-operator-7fccd6877f-xzz4r from openshift-operator-lifecycle-manager started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container catalog-operator ready: true, restart count 0
Feb 17 19:12:41.889: INFO: downloads-65dd498cf8-cqpss from openshift-console started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container download-server ready: true, restart count 0
Feb 17 19:12:41.889: INFO: sonobuoy from sonobuoy started at 2020-02-17 16:29:56 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Feb 17 19:12:41.889: INFO: marketplace-operator-664f66c947-jckz5 from openshift-marketplace started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container marketplace-operator ready: true, restart count 0
Feb 17 19:12:41.889: INFO: configmap-cabundle-injector-c749c6984-hlkf5 from openshift-service-ca started at 2020-02-17 14:36:50 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Feb 17 19:12:41.889: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-02-17 14:41:40 +0000 UTC (3 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container alertmanager ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container config-reloader ready: true, restart count 0
Feb 17 19:12:41.889: INFO: network-operator-86974b9cf-spclm from openshift-network-operator started at 2020-02-17 14:34:58 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container network-operator ready: true, restart count 0
Feb 17 19:12:41.889: INFO: calico-typha-76c597bc7d-4tfhl from kube-system started at 2020-02-17 14:36:03 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container calico-typha ready: true, restart count 0
Feb 17 19:12:41.889: INFO: ibm-storage-watcher-5cf8f8b4cb-pnqvx from kube-system started at 2020-02-17 14:36:04 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Feb 17 19:12:41.889: INFO: openshift-state-metrics-86bbf546f-gglb8 from openshift-monitoring started at 2020-02-17 14:36:26 +0000 UTC (3 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Feb 17 19:12:41.889: INFO: node-exporter-rfpkb from openshift-monitoring started at 2020-02-17 14:36:28 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:41.889: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 	Container node-exporter ready: true, restart count 0
Feb 17 19:12:41.889: INFO: 
Logging pods the kubelet thinks is on node 10.45.66.178 before test
Feb 17 19:12:42.017: INFO: redhat-operators-7cc9d7897-rw5tc from openshift-marketplace started at 2020-02-17 19:08:11 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container redhat-operators ready: true, restart count 0
Feb 17 19:12:42.017: INFO: thanos-querier-b949c5db-f9jx6 from openshift-monitoring started at 2020-02-17 19:08:11 +0000 UTC (4 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:12:42.017: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 17 19:12:42.017: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 19:12:42.017: INFO: 	Container thanos-querier ready: true, restart count 0
Feb 17 19:12:42.017: INFO: multus-wbdmp from openshift-multus started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container kube-multus ready: true, restart count 0
Feb 17 19:12:42.017: INFO: ibm-cloud-provider-ip-158-175-93-66-7b5b7d9cb-jbd6r from ibm-system started at 2020-02-17 14:41:22 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container ibm-cloud-provider-ip-158-175-93-66 ready: true, restart count 0
Feb 17 19:12:42.017: INFO: grafana-66dff46d7c-l5jsk from openshift-monitoring started at 2020-02-17 14:41:46 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container grafana ready: true, restart count 0
Feb 17 19:12:42.017: INFO: 	Container grafana-proxy ready: true, restart count 0
Feb 17 19:12:42.017: INFO: certified-operators-549b955645-dv7mk from openshift-marketplace started at 2020-02-17 19:08:11 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container certified-operators ready: true, restart count 0
Feb 17 19:12:42.017: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-02-17 19:08:18 +0000 UTC (3 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container alertmanager ready: true, restart count 0
Feb 17 19:12:42.017: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 17 19:12:42.017: INFO: 	Container config-reloader ready: true, restart count 0
Feb 17 19:12:42.017: INFO: tuned-2n22g from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container tuned ready: true, restart count 0
Feb 17 19:12:42.017: INFO: openshift-kube-proxy-kchsl from openshift-kube-proxy started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 19:12:42.017: INFO: calico-typha-76c597bc7d-72pj2 from kube-system started at 2020-02-17 14:38:05 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container calico-typha ready: true, restart count 0
Feb 17 19:12:42.017: INFO: prometheus-adapter-55674d9685-vvdz2 from openshift-monitoring started at 2020-02-17 14:41:40 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container prometheus-adapter ready: true, restart count 0
Feb 17 19:12:42.017: INFO: ibmcloud-block-storage-driver-d7lnw from kube-system started at 2020-02-17 14:36:45 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 17 19:12:42.017: INFO: multus-admission-controller-s6fg5 from openshift-multus started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 17 19:12:42.017: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-02-17 14:42:48 +0000 UTC (7 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:12:42.017: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 19:12:42.017: INFO: 	Container prometheus ready: true, restart count 1
Feb 17 19:12:42.017: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Feb 17 19:12:42.017: INFO: 	Container prometheus-proxy ready: true, restart count 0
Feb 17 19:12:42.017: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Feb 17 19:12:42.017: INFO: 	Container thanos-sidecar ready: true, restart count 0
Feb 17 19:12:42.017: INFO: prometheus-operator-7d5f885558-bv55g from openshift-monitoring started at 2020-02-17 14:41:26 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container prometheus-operator ready: true, restart count 0
Feb 17 19:12:42.017: INFO: community-operators-6c7f5d4977-z2vjq from openshift-marketplace started at 2020-02-17 18:46:50 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container community-operators ready: true, restart count 0
Feb 17 19:12:42.017: INFO: node-ca-n4v6k from openshift-image-registry started at 2020-02-17 14:38:00 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container node-ca ready: true, restart count 0
Feb 17 19:12:42.017: INFO: registry-pvc-permissions-jlk4p from openshift-image-registry started at 2020-02-17 14:40:17 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container pvc-permissions ready: false, restart count 0
Feb 17 19:12:42.017: INFO: thanos-querier-b949c5db-ns4qs from openshift-monitoring started at 2020-02-17 14:42:53 +0000 UTC (4 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:12:42.017: INFO: 	Container oauth-proxy ready: true, restart count 0
Feb 17 19:12:42.017: INFO: 	Container prom-label-proxy ready: true, restart count 0
Feb 17 19:12:42.017: INFO: 	Container thanos-querier ready: true, restart count 0
Feb 17 19:12:42.017: INFO: router-default-6c67c67864-f8hvb from openshift-ingress started at 2020-02-17 14:42:29 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container router ready: true, restart count 0
Feb 17 19:12:42.017: INFO: ibm-master-proxy-static-10.45.66.178 from kube-system started at 2020-02-17 14:37:31 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 19:12:42.017: INFO: 	Container pause ready: true, restart count 0
Feb 17 19:12:42.017: INFO: packageserver-749668c8dd-slgc8 from openshift-operator-lifecycle-manager started at 2020-02-17 15:40:09 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.017: INFO: 	Container packageserver ready: true, restart count 0
Feb 17 19:12:42.017: INFO: sonobuoy-systemd-logs-daemon-set-67976b35ad084332-thldq from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:42.018: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Feb 17 19:12:42.018: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 19:12:42.018: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-02-17 14:41:51 +0000 UTC (3 container statuses recorded)
Feb 17 19:12:42.018: INFO: 	Container alertmanager ready: true, restart count 0
Feb 17 19:12:42.018: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Feb 17 19:12:42.018: INFO: 	Container config-reloader ready: true, restart count 0
Feb 17 19:12:42.018: INFO: node-exporter-wmxf5 from openshift-monitoring started at 2020-02-17 14:36:43 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:42.018: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:12:42.018: INFO: 	Container node-exporter ready: true, restart count 0
Feb 17 19:12:42.018: INFO: dns-default-9fgff from openshift-dns started at 2020-02-17 14:38:00 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:42.018: INFO: 	Container dns ready: true, restart count 0
Feb 17 19:12:42.018: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 17 19:12:42.018: INFO: cluster-samples-operator-946f4d4c5-66g9k from openshift-cluster-samples-operator started at 2020-02-17 14:38:52 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:42.018: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Feb 17 19:12:42.018: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Feb 17 19:12:42.018: INFO: image-registry-6d4857c99d-q2dn4 from openshift-image-registry started at 2020-02-17 14:40:17 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.018: INFO: 	Container registry ready: true, restart count 0
Feb 17 19:12:42.018: INFO: console-7547c7bc6c-wschb from openshift-console started at 2020-02-17 14:39:26 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.018: INFO: 	Container console ready: true, restart count 0
Feb 17 19:12:42.018: INFO: calico-node-zkvdn from kube-system started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.018: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 19:12:42.018: INFO: ibm-keepalived-watcher-4nt2b from kube-system started at 2020-02-17 14:36:43 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.018: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 19:12:42.018: INFO: 
Logging pods the kubelet thinks is on node 10.45.66.189 before test
Feb 17 19:12:42.090: INFO: sonobuoy-systemd-logs-daemon-set-67976b35ad084332-4jdsp from sonobuoy started at 2020-02-17 16:30:13 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:42.090: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Feb 17 19:12:42.090: INFO: 	Container systemd-logs ready: true, restart count 0
Feb 17 19:12:42.090: INFO: multus-df87p from openshift-multus started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.090: INFO: 	Container kube-multus ready: true, restart count 0
Feb 17 19:12:42.090: INFO: ibm-master-proxy-static-10.45.66.189 from kube-system started at 2020-02-17 14:37:14 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:42.090: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Feb 17 19:12:42.090: INFO: 	Container pause ready: true, restart count 0
Feb 17 19:12:42.090: INFO: multus-admission-controller-dp92c from openshift-multus started at 2020-02-17 19:08:48 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.090: INFO: 	Container multus-admission-controller ready: true, restart count 0
Feb 17 19:12:42.090: INFO: calico-typha-76c597bc7d-6zmf9 from kube-system started at 2020-02-17 19:08:52 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.090: INFO: 	Container calico-typha ready: true, restart count 0
Feb 17 19:12:42.090: INFO: openshift-kube-proxy-gwxkw from openshift-kube-proxy started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.090: INFO: 	Container kube-proxy ready: true, restart count 0
Feb 17 19:12:42.090: INFO: dns-default-d8gsb from openshift-dns started at 2020-02-17 19:08:47 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:42.090: INFO: 	Container dns ready: true, restart count 0
Feb 17 19:12:42.090: INFO: 	Container dns-node-resolver ready: true, restart count 0
Feb 17 19:12:42.090: INFO: node-exporter-4fhhh from openshift-monitoring started at 2020-02-17 14:36:28 +0000 UTC (2 container statuses recorded)
Feb 17 19:12:42.090: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Feb 17 19:12:42.090: INFO: 	Container node-exporter ready: true, restart count 0
Feb 17 19:12:42.090: INFO: calico-node-cctn2 from kube-system started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.090: INFO: 	Container calico-node ready: true, restart count 0
Feb 17 19:12:42.090: INFO: ibmcloud-block-storage-driver-jnxhv from kube-system started at 2020-02-17 14:36:32 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.090: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Feb 17 19:12:42.090: INFO: ibm-keepalived-watcher-5hn6l from kube-system started at 2020-02-17 14:36:25 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.091: INFO: 	Container keepalived-watcher ready: true, restart count 0
Feb 17 19:12:42.091: INFO: node-ca-vjvhw from openshift-image-registry started at 2020-02-17 19:08:48 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.091: INFO: 	Container node-ca ready: true, restart count 0
Feb 17 19:12:42.091: INFO: tuned-kg4dp from openshift-cluster-node-tuning-operator started at 2020-02-17 14:36:28 +0000 UTC (1 container statuses recorded)
Feb 17 19:12:42.091: INFO: 	Container tuned ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node 10.45.66.177
STEP: verifying the node has the label node 10.45.66.178
STEP: verifying the node has the label node 10.45.66.189
Feb 17 19:12:42.273: INFO: Pod ibm-cloud-provider-ip-158-175-93-66-7b5b7d9cb-jbd6r requesting resource cpu=5m on Node 10.45.66.178
Feb 17 19:12:42.273: INFO: Pod ibm-cloud-provider-ip-158-175-93-66-7b5b7d9cb-w5zjw requesting resource cpu=5m on Node 10.45.66.177
Feb 17 19:12:42.273: INFO: Pod calico-kube-controllers-5df9fd5998-tgjvk requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.273: INFO: Pod calico-node-cctn2 requesting resource cpu=250m on Node 10.45.66.189
Feb 17 19:12:42.273: INFO: Pod calico-node-mjlf4 requesting resource cpu=250m on Node 10.45.66.177
Feb 17 19:12:42.273: INFO: Pod calico-node-zkvdn requesting resource cpu=250m on Node 10.45.66.178
Feb 17 19:12:42.273: INFO: Pod calico-typha-76c597bc7d-4tfhl requesting resource cpu=250m on Node 10.45.66.177
Feb 17 19:12:42.273: INFO: Pod calico-typha-76c597bc7d-6zmf9 requesting resource cpu=250m on Node 10.45.66.189
Feb 17 19:12:42.273: INFO: Pod calico-typha-76c597bc7d-72pj2 requesting resource cpu=250m on Node 10.45.66.178
Feb 17 19:12:42.273: INFO: Pod ibm-file-plugin-788cbbd987-7hzlx requesting resource cpu=50m on Node 10.45.66.177
Feb 17 19:12:42.273: INFO: Pod ibm-keepalived-watcher-4nt2b requesting resource cpu=5m on Node 10.45.66.178
Feb 17 19:12:42.274: INFO: Pod ibm-keepalived-watcher-5hn6l requesting resource cpu=5m on Node 10.45.66.189
Feb 17 19:12:42.274: INFO: Pod ibm-keepalived-watcher-684qj requesting resource cpu=5m on Node 10.45.66.177
Feb 17 19:12:42.274: INFO: Pod ibm-master-proxy-static-10.45.66.177 requesting resource cpu=25m on Node 10.45.66.177
Feb 17 19:12:42.274: INFO: Pod ibm-master-proxy-static-10.45.66.178 requesting resource cpu=25m on Node 10.45.66.178
Feb 17 19:12:42.274: INFO: Pod ibm-master-proxy-static-10.45.66.189 requesting resource cpu=25m on Node 10.45.66.189
Feb 17 19:12:42.274: INFO: Pod ibm-storage-watcher-5cf8f8b4cb-pnqvx requesting resource cpu=50m on Node 10.45.66.177
Feb 17 19:12:42.274: INFO: Pod ibmcloud-block-storage-driver-d7lnw requesting resource cpu=0m on Node 10.45.66.178
Feb 17 19:12:42.274: INFO: Pod ibmcloud-block-storage-driver-jnxhv requesting resource cpu=0m on Node 10.45.66.189
Feb 17 19:12:42.274: INFO: Pod ibmcloud-block-storage-driver-vnql8 requesting resource cpu=0m on Node 10.45.66.177
Feb 17 19:12:42.274: INFO: Pod ibmcloud-block-storage-plugin-75cf87c786-cc9kd requesting resource cpu=50m on Node 10.45.66.177
Feb 17 19:12:42.274: INFO: Pod vpn-5d9c8dbf8-fwt5m requesting resource cpu=5m on Node 10.45.66.177
Feb 17 19:12:42.274: INFO: Pod cloud-credential-operator-6fc5bb58fc-npgb8 requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.274: INFO: Pod cluster-node-tuning-operator-5cbf9c8db5-mr2tk requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.275: INFO: Pod tuned-2n22g requesting resource cpu=10m on Node 10.45.66.178
Feb 17 19:12:42.275: INFO: Pod tuned-f4zpl requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.275: INFO: Pod tuned-kg4dp requesting resource cpu=10m on Node 10.45.66.189
Feb 17 19:12:42.275: INFO: Pod cluster-samples-operator-946f4d4c5-66g9k requesting resource cpu=20m on Node 10.45.66.178
Feb 17 19:12:42.275: INFO: Pod cluster-storage-operator-646c48994f-4nvbp requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.275: INFO: Pod console-operator-7897bcfbd8-m6vt2 requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.275: INFO: Pod console-7547c7bc6c-n79r2 requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.275: INFO: Pod console-7547c7bc6c-wschb requesting resource cpu=10m on Node 10.45.66.178
Feb 17 19:12:42.275: INFO: Pod downloads-65dd498cf8-cqpss requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.276: INFO: Pod downloads-65dd498cf8-vvlwb requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.276: INFO: Pod dns-operator-586c9c56ff-nxk8c requesting resource cpu=20m on Node 10.45.66.177
Feb 17 19:12:42.276: INFO: Pod dns-default-9fgff requesting resource cpu=110m on Node 10.45.66.178
Feb 17 19:12:42.276: INFO: Pod dns-default-d8gsb requesting resource cpu=110m on Node 10.45.66.189
Feb 17 19:12:42.276: INFO: Pod dns-default-xw2km requesting resource cpu=110m on Node 10.45.66.177
Feb 17 19:12:42.276: INFO: Pod cluster-image-registry-operator-79bfcb686-62774 requesting resource cpu=20m on Node 10.45.66.177
Feb 17 19:12:42.276: INFO: Pod image-registry-6d4857c99d-q2dn4 requesting resource cpu=100m on Node 10.45.66.178
Feb 17 19:12:42.276: INFO: Pod node-ca-49d68 requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.276: INFO: Pod node-ca-n4v6k requesting resource cpu=10m on Node 10.45.66.178
Feb 17 19:12:42.276: INFO: Pod node-ca-vjvhw requesting resource cpu=10m on Node 10.45.66.189
Feb 17 19:12:42.276: INFO: Pod ingress-operator-855959d8d9-xhrmn requesting resource cpu=20m on Node 10.45.66.177
Feb 17 19:12:42.276: INFO: Pod router-default-6c67c67864-f8hvb requesting resource cpu=100m on Node 10.45.66.178
Feb 17 19:12:42.277: INFO: Pod router-default-6c67c67864-hlm85 requesting resource cpu=100m on Node 10.45.66.177
Feb 17 19:12:42.277: INFO: Pod openshift-kube-proxy-gwxkw requesting resource cpu=0m on Node 10.45.66.189
Feb 17 19:12:42.277: INFO: Pod openshift-kube-proxy-kchsl requesting resource cpu=0m on Node 10.45.66.178
Feb 17 19:12:42.277: INFO: Pod openshift-kube-proxy-sz6st requesting resource cpu=0m on Node 10.45.66.177
Feb 17 19:12:42.277: INFO: Pod certified-operators-549b955645-dv7mk requesting resource cpu=10m on Node 10.45.66.178
Feb 17 19:12:42.277: INFO: Pod community-operators-6c7f5d4977-z2vjq requesting resource cpu=10m on Node 10.45.66.178
Feb 17 19:12:42.277: INFO: Pod marketplace-operator-664f66c947-jckz5 requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.277: INFO: Pod redhat-operators-7cc9d7897-rw5tc requesting resource cpu=10m on Node 10.45.66.178
Feb 17 19:12:42.277: INFO: Pod alertmanager-main-0 requesting resource cpu=100m on Node 10.45.66.178
Feb 17 19:12:42.277: INFO: Pod alertmanager-main-1 requesting resource cpu=100m on Node 10.45.66.178
Feb 17 19:12:42.277: INFO: Pod alertmanager-main-2 requesting resource cpu=100m on Node 10.45.66.177
Feb 17 19:12:42.277: INFO: Pod cluster-monitoring-operator-55d99d5ffc-qlxs4 requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.277: INFO: Pod grafana-66dff46d7c-l5jsk requesting resource cpu=100m on Node 10.45.66.178
Feb 17 19:12:42.277: INFO: Pod kube-state-metrics-5745cc99f5-vd9j6 requesting resource cpu=30m on Node 10.45.66.177
Feb 17 19:12:42.277: INFO: Pod node-exporter-4fhhh requesting resource cpu=10m on Node 10.45.66.189
Feb 17 19:12:42.278: INFO: Pod node-exporter-rfpkb requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.278: INFO: Pod node-exporter-wmxf5 requesting resource cpu=10m on Node 10.45.66.178
Feb 17 19:12:42.278: INFO: Pod openshift-state-metrics-86bbf546f-gglb8 requesting resource cpu=120m on Node 10.45.66.177
Feb 17 19:12:42.278: INFO: Pod prometheus-adapter-55674d9685-pfwsr requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.278: INFO: Pod prometheus-adapter-55674d9685-vvdz2 requesting resource cpu=10m on Node 10.45.66.178
Feb 17 19:12:42.278: INFO: Pod prometheus-k8s-0 requesting resource cpu=430m on Node 10.45.66.177
Feb 17 19:12:42.278: INFO: Pod prometheus-k8s-1 requesting resource cpu=430m on Node 10.45.66.178
Feb 17 19:12:42.278: INFO: Pod prometheus-operator-7d5f885558-bv55g requesting resource cpu=10m on Node 10.45.66.178
Feb 17 19:12:42.278: INFO: Pod telemeter-client-56f4c98664-kc4sq requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.278: INFO: Pod thanos-querier-b949c5db-f9jx6 requesting resource cpu=40m on Node 10.45.66.178
Feb 17 19:12:42.278: INFO: Pod thanos-querier-b949c5db-ns4qs requesting resource cpu=40m on Node 10.45.66.178
Feb 17 19:12:42.278: INFO: Pod multus-admission-controller-dp92c requesting resource cpu=10m on Node 10.45.66.189
Feb 17 19:12:42.278: INFO: Pod multus-admission-controller-s6fg5 requesting resource cpu=10m on Node 10.45.66.178
Feb 17 19:12:42.278: INFO: Pod multus-admission-controller-x8vxh requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.278: INFO: Pod multus-df87p requesting resource cpu=10m on Node 10.45.66.189
Feb 17 19:12:42.278: INFO: Pod multus-wbdmp requesting resource cpu=10m on Node 10.45.66.178
Feb 17 19:12:42.278: INFO: Pod multus-z6qv7 requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.278: INFO: Pod network-operator-86974b9cf-spclm requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.279: INFO: Pod catalog-operator-7fccd6877f-xzz4r requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.279: INFO: Pod olm-operator-5f5f7bbd7d-8k49r requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.279: INFO: Pod packageserver-749668c8dd-fpjbg requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.279: INFO: Pod packageserver-749668c8dd-slgc8 requesting resource cpu=10m on Node 10.45.66.178
Feb 17 19:12:42.279: INFO: Pod service-ca-operator-5f9bd445b8-k65l4 requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.279: INFO: Pod apiservice-cabundle-injector-745c6fc9d8-h2znf requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.279: INFO: Pod configmap-cabundle-injector-c749c6984-hlkf5 requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.279: INFO: Pod service-serving-cert-signer-6669ffbb98-s7bf6 requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.279: INFO: Pod openshift-service-catalog-apiserver-operator-577c6ffb4f-r7vw8 requesting resource cpu=0m on Node 10.45.66.177
Feb 17 19:12:42.279: INFO: Pod openshift-service-catalog-controller-manager-operator-6857997dg requesting resource cpu=10m on Node 10.45.66.177
Feb 17 19:12:42.279: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.45.66.177
Feb 17 19:12:42.279: INFO: Pod sonobuoy-e2e-job-a662486eb1ae4ce5 requesting resource cpu=0m on Node 10.45.66.177
Feb 17 19:12:42.279: INFO: Pod sonobuoy-systemd-logs-daemon-set-67976b35ad084332-4jdsp requesting resource cpu=0m on Node 10.45.66.189
Feb 17 19:12:42.279: INFO: Pod sonobuoy-systemd-logs-daemon-set-67976b35ad084332-bvf8d requesting resource cpu=0m on Node 10.45.66.177
Feb 17 19:12:42.279: INFO: Pod sonobuoy-systemd-logs-daemon-set-67976b35ad084332-thldq requesting resource cpu=0m on Node 10.45.66.178
STEP: Starting Pods to consume most of the cluster CPU.
Feb 17 19:12:42.279: INFO: Creating a pod which consumes cpu=1407m on Node 10.45.66.177
Feb 17 19:12:42.318: INFO: Creating a pod which consumes cpu=1480m on Node 10.45.66.178
Feb 17 19:12:42.356: INFO: Creating a pod which consumes cpu=2254m on Node 10.45.66.189
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-76b563ba-a88f-4e21-bd86-22843f844f74.15f446638765d267], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2964/filler-pod-76b563ba-a88f-4e21-bd86-22843f844f74 to 10.45.66.189]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-76b563ba-a88f-4e21-bd86-22843f844f74.15f446651719dfd7], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-76b563ba-a88f-4e21-bd86-22843f844f74.15f4466525dca4c3], Reason = [Created], Message = [Created container filler-pod-76b563ba-a88f-4e21-bd86-22843f844f74]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-76b563ba-a88f-4e21-bd86-22843f844f74.15f4466527ec48b3], Reason = [Started], Message = [Started container filler-pod-76b563ba-a88f-4e21-bd86-22843f844f74]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-80e91747-36f7-4042-829e-8a0abaf095e0.15f4466385832b97], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2964/filler-pod-80e91747-36f7-4042-829e-8a0abaf095e0 to 10.45.66.178]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-80e91747-36f7-4042-829e-8a0abaf095e0.15f44665166d50c8], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-80e91747-36f7-4042-829e-8a0abaf095e0.15f4466527413e6f], Reason = [Created], Message = [Created container filler-pod-80e91747-36f7-4042-829e-8a0abaf095e0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-80e91747-36f7-4042-829e-8a0abaf095e0.15f4466529b86e94], Reason = [Started], Message = [Started container filler-pod-80e91747-36f7-4042-829e-8a0abaf095e0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e23f67de-b6cd-413e-94d7-4cbad596c35f.15f44663839c5434], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2964/filler-pod-e23f67de-b6cd-413e-94d7-4cbad596c35f to 10.45.66.177]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e23f67de-b6cd-413e-94d7-4cbad596c35f.15f446651e9096f4], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e23f67de-b6cd-413e-94d7-4cbad596c35f.15f4466530f0dffa], Reason = [Created], Message = [Created container filler-pod-e23f67de-b6cd-413e-94d7-4cbad596c35f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e23f67de-b6cd-413e-94d7-4cbad596c35f.15f44665336a5101], Reason = [Started], Message = [Started container filler-pod-e23f67de-b6cd-413e-94d7-4cbad596c35f]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15f4466569e6b68b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.45.66.177
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.45.66.178
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.45.66.189
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:12:51.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2964" for this suite.
Feb 17 19:12:59.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:13:01.888: INFO: namespace sched-pred-2964 deletion completed in 10.190378292s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:20.446 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:13:01.889: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-b5354e8e-7dba-418e-96a5-5e9476c85eca in namespace container-probe-7069
Feb 17 19:13:10.113: INFO: Started pod liveness-b5354e8e-7dba-418e-96a5-5e9476c85eca in namespace container-probe-7069
STEP: checking the pod's current state and verifying that restartCount is present
Feb 17 19:13:10.126: INFO: Initial restart count of pod liveness-b5354e8e-7dba-418e-96a5-5e9476c85eca is 0
Feb 17 19:13:22.219: INFO: Restart count of pod container-probe-7069/liveness-b5354e8e-7dba-418e-96a5-5e9476c85eca is now 1 (12.092122471s elapsed)
Feb 17 19:13:42.354: INFO: Restart count of pod container-probe-7069/liveness-b5354e8e-7dba-418e-96a5-5e9476c85eca is now 2 (32.227860906s elapsed)
Feb 17 19:14:02.496: INFO: Restart count of pod container-probe-7069/liveness-b5354e8e-7dba-418e-96a5-5e9476c85eca is now 3 (52.369203018s elapsed)
Feb 17 19:14:22.646: INFO: Restart count of pod container-probe-7069/liveness-b5354e8e-7dba-418e-96a5-5e9476c85eca is now 4 (1m12.519274596s elapsed)
Feb 17 19:15:35.160: INFO: Restart count of pod container-probe-7069/liveness-b5354e8e-7dba-418e-96a5-5e9476c85eca is now 5 (2m25.033610975s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:15:35.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7069" for this suite.
Feb 17 19:15:43.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:15:45.369: INFO: namespace container-probe-7069 deletion completed in 10.156160836s

• [SLOW TEST:163.480 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:15:45.369: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:15:56.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9992" for this suite.
Feb 17 19:16:04.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:16:06.894: INFO: namespace resourcequota-9992 deletion completed in 10.141280705s

• [SLOW TEST:21.525 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:16:06.896: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-abcd3df6-ea2c-4540-a649-c5700092a7c4
STEP: Creating a pod to test consume secrets
Feb 17 19:16:07.105: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c85f2ead-5e00-4ca6-b8e8-1f25c9d82e27" in namespace "projected-8165" to be "success or failure"
Feb 17 19:16:07.118: INFO: Pod "pod-projected-secrets-c85f2ead-5e00-4ca6-b8e8-1f25c9d82e27": Phase="Pending", Reason="", readiness=false. Elapsed: 13.718599ms
Feb 17 19:16:09.135: INFO: Pod "pod-projected-secrets-c85f2ead-5e00-4ca6-b8e8-1f25c9d82e27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03012329s
Feb 17 19:16:11.148: INFO: Pod "pod-projected-secrets-c85f2ead-5e00-4ca6-b8e8-1f25c9d82e27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042958689s
Feb 17 19:16:13.161: INFO: Pod "pod-projected-secrets-c85f2ead-5e00-4ca6-b8e8-1f25c9d82e27": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05626112s
Feb 17 19:16:15.175: INFO: Pod "pod-projected-secrets-c85f2ead-5e00-4ca6-b8e8-1f25c9d82e27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.070002747s
STEP: Saw pod success
Feb 17 19:16:15.175: INFO: Pod "pod-projected-secrets-c85f2ead-5e00-4ca6-b8e8-1f25c9d82e27" satisfied condition "success or failure"
Feb 17 19:16:15.188: INFO: Trying to get logs from node 10.45.66.189 pod pod-projected-secrets-c85f2ead-5e00-4ca6-b8e8-1f25c9d82e27 container projected-secret-volume-test: <nil>
STEP: delete the pod
Feb 17 19:16:15.279: INFO: Waiting for pod pod-projected-secrets-c85f2ead-5e00-4ca6-b8e8-1f25c9d82e27 to disappear
Feb 17 19:16:15.291: INFO: Pod pod-projected-secrets-c85f2ead-5e00-4ca6-b8e8-1f25c9d82e27 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:16:15.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8165" for this suite.
Feb 17 19:16:23.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:16:25.458: INFO: namespace projected-8165 deletion completed in 10.139007876s

• [SLOW TEST:18.562 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:16:25.458: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:16:34.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7320" for this suite.
Feb 17 19:16:56.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:16:59.006: INFO: namespace replication-controller-7320 deletion completed in 24.243140469s

• [SLOW TEST:33.548 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:16:59.007: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Feb 17 19:16:59.182: INFO: Waiting up to 5m0s for pod "downwardapi-volume-01e8a438-e720-4376-abb4-db2473f813fb" in namespace "projected-7679" to be "success or failure"
Feb 17 19:16:59.195: INFO: Pod "downwardapi-volume-01e8a438-e720-4376-abb4-db2473f813fb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.98833ms
Feb 17 19:17:01.210: INFO: Pod "downwardapi-volume-01e8a438-e720-4376-abb4-db2473f813fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027380305s
Feb 17 19:17:03.223: INFO: Pod "downwardapi-volume-01e8a438-e720-4376-abb4-db2473f813fb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040544114s
Feb 17 19:17:05.237: INFO: Pod "downwardapi-volume-01e8a438-e720-4376-abb4-db2473f813fb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054593492s
Feb 17 19:17:07.257: INFO: Pod "downwardapi-volume-01e8a438-e720-4376-abb4-db2473f813fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.074839294s
STEP: Saw pod success
Feb 17 19:17:07.257: INFO: Pod "downwardapi-volume-01e8a438-e720-4376-abb4-db2473f813fb" satisfied condition "success or failure"
Feb 17 19:17:07.270: INFO: Trying to get logs from node 10.45.66.189 pod downwardapi-volume-01e8a438-e720-4376-abb4-db2473f813fb container client-container: <nil>
STEP: delete the pod
Feb 17 19:17:07.352: INFO: Waiting for pod downwardapi-volume-01e8a438-e720-4376-abb4-db2473f813fb to disappear
Feb 17 19:17:07.378: INFO: Pod downwardapi-volume-01e8a438-e720-4376-abb4-db2473f813fb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:17:07.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7679" for this suite.
Feb 17 19:17:15.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:17:17.569: INFO: namespace projected-7679 deletion completed in 10.158864561s

• [SLOW TEST:18.565 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:17:17.571: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:17:28.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9796" for this suite.
Feb 17 19:17:37.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:17:39.097: INFO: namespace resourcequota-9796 deletion completed in 10.136405697s

• [SLOW TEST:21.526 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:17:39.098: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-25d0bdc9-e368-4a4f-a8c8-46b36e3cf442
STEP: Creating a pod to test consume secrets
Feb 17 19:17:39.281: INFO: Waiting up to 5m0s for pod "pod-secrets-3ce82d68-b79d-4261-92a6-847e248c12cb" in namespace "secrets-6505" to be "success or failure"
Feb 17 19:17:39.295: INFO: Pod "pod-secrets-3ce82d68-b79d-4261-92a6-847e248c12cb": Phase="Pending", Reason="", readiness=false. Elapsed: 13.223427ms
Feb 17 19:17:41.308: INFO: Pod "pod-secrets-3ce82d68-b79d-4261-92a6-847e248c12cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026231895s
Feb 17 19:17:43.321: INFO: Pod "pod-secrets-3ce82d68-b79d-4261-92a6-847e248c12cb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0399014s
Feb 17 19:17:45.334: INFO: Pod "pod-secrets-3ce82d68-b79d-4261-92a6-847e248c12cb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052814688s
Feb 17 19:17:47.347: INFO: Pod "pod-secrets-3ce82d68-b79d-4261-92a6-847e248c12cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.065724144s
STEP: Saw pod success
Feb 17 19:17:47.347: INFO: Pod "pod-secrets-3ce82d68-b79d-4261-92a6-847e248c12cb" satisfied condition "success or failure"
Feb 17 19:17:47.360: INFO: Trying to get logs from node 10.45.66.189 pod pod-secrets-3ce82d68-b79d-4261-92a6-847e248c12cb container secret-volume-test: <nil>
STEP: delete the pod
Feb 17 19:17:47.424: INFO: Waiting for pod pod-secrets-3ce82d68-b79d-4261-92a6-847e248c12cb to disappear
Feb 17 19:17:47.436: INFO: Pod pod-secrets-3ce82d68-b79d-4261-92a6-847e248c12cb no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:17:47.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6505" for this suite.
Feb 17 19:17:55.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:17:57.590: INFO: namespace secrets-6505 deletion completed in 10.129586778s

• [SLOW TEST:18.492 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Feb 17 19:17:57.590: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-9566
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Feb 17 19:17:57.710: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Feb 17 19:18:30.111: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.50.135:8080/dial?request=hostName&protocol=http&host=172.30.80.52&port=8080&tries=1'] Namespace:pod-network-test-9566 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 19:18:30.111: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 19:18:30.315: INFO: Waiting for endpoints: map[]
Feb 17 19:18:30.328: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.50.135:8080/dial?request=hostName&protocol=http&host=172.30.178.228&port=8080&tries=1'] Namespace:pod-network-test-9566 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 19:18:30.328: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 19:18:30.587: INFO: Waiting for endpoints: map[]
Feb 17 19:18:30.600: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.50.135:8080/dial?request=hostName&protocol=http&host=172.30.50.136&port=8080&tries=1'] Namespace:pod-network-test-9566 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Feb 17 19:18:30.600: INFO: >>> kubeConfig: /tmp/kubeconfig-559420064
Feb 17 19:18:30.801: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Feb 17 19:18:30.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9566" for this suite.
Feb 17 19:18:38.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Feb 17 19:18:41.001: INFO: namespace pod-network-test-9566 deletion completed in 10.17604895s

• [SLOW TEST:43.411 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSFeb 17 19:18:41.001: INFO: Running AfterSuite actions on all nodes
Feb 17 19:18:41.001: INFO: Running AfterSuite actions on node 1
Feb 17 19:18:41.001: INFO: Skipping dumping logs from cluster

Ran 276 of 4897 Specs in 10071.229 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4621 Skipped
PASS

Ginkgo ran 1 suite in 2h47m53.232955184s
Test Suite Passed
