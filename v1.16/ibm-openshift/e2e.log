I0417 20:47:51.533182      22 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-531062789
I0417 20:47:51.533718      22 e2e.go:92] Starting e2e run "5766aa68-21ab-4498-aad4-2935d50e9967" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1587156469 - Will randomize all specs
Will run 276 of 4897 specs

Apr 17 20:47:51.568: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 20:47:51.572: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr 17 20:47:51.625: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr 17 20:47:51.685: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr 17 20:47:51.685: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Apr 17 20:47:51.685: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr 17 20:47:51.699: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Apr 17 20:47:51.700: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Apr 17 20:47:51.700: INFO: e2e test version: v1.16.2
Apr 17 20:47:51.704: INFO: kube-apiserver version: v1.16.2
Apr 17 20:47:51.704: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 20:47:51.717: INFO: Cluster IP family: ipv4
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:47:51.718: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename container-lifecycle-hook
Apr 17 20:47:51.858: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 17 20:48:07.989: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 17 20:48:07.997: INFO: Pod pod-with-prestop-http-hook still exists
Apr 17 20:48:09.997: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 17 20:48:10.006: INFO: Pod pod-with-prestop-http-hook still exists
Apr 17 20:48:11.997: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 17 20:48:12.006: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:48:12.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9875" for this suite.
Apr 17 20:48:44.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:48:46.008: INFO: namespace container-lifecycle-hook-9875 deletion completed in 33.935455892s

â€¢ [SLOW TEST:54.290 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:48:46.009: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Apr 17 20:48:46.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 create -f - --namespace=kubectl-4174'
Apr 17 20:48:46.786: INFO: stderr: ""
Apr 17 20:48:46.786: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 17 20:48:46.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4174'
Apr 17 20:48:46.932: INFO: stderr: ""
Apr 17 20:48:46.932: INFO: stdout: "update-demo-nautilus-2hg5b update-demo-nautilus-8pt4x "
Apr 17 20:48:46.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-2hg5b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4174'
Apr 17 20:48:47.077: INFO: stderr: ""
Apr 17 20:48:47.077: INFO: stdout: ""
Apr 17 20:48:47.077: INFO: update-demo-nautilus-2hg5b is created but not running
Apr 17 20:48:52.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4174'
Apr 17 20:48:52.218: INFO: stderr: ""
Apr 17 20:48:52.218: INFO: stdout: "update-demo-nautilus-2hg5b update-demo-nautilus-8pt4x "
Apr 17 20:48:52.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-2hg5b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4174'
Apr 17 20:48:52.366: INFO: stderr: ""
Apr 17 20:48:52.366: INFO: stdout: "true"
Apr 17 20:48:52.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-2hg5b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4174'
Apr 17 20:48:52.534: INFO: stderr: ""
Apr 17 20:48:52.534: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 17 20:48:52.534: INFO: validating pod update-demo-nautilus-2hg5b
Apr 17 20:48:52.556: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 20:48:52.556: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 20:48:52.556: INFO: update-demo-nautilus-2hg5b is verified up and running
Apr 17 20:48:52.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-8pt4x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4174'
Apr 17 20:48:52.700: INFO: stderr: ""
Apr 17 20:48:52.700: INFO: stdout: "true"
Apr 17 20:48:52.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-8pt4x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4174'
Apr 17 20:48:52.834: INFO: stderr: ""
Apr 17 20:48:52.834: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 17 20:48:52.834: INFO: validating pod update-demo-nautilus-8pt4x
Apr 17 20:48:52.854: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 20:48:52.854: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 20:48:52.855: INFO: update-demo-nautilus-8pt4x is verified up and running
STEP: scaling down the replication controller
Apr 17 20:48:52.858: INFO: scanned /root for discovery docs: <nil>
Apr 17 20:48:52.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-4174'
Apr 17 20:48:54.171: INFO: stderr: ""
Apr 17 20:48:54.171: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 17 20:48:54.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4174'
Apr 17 20:48:54.346: INFO: stderr: ""
Apr 17 20:48:54.346: INFO: stdout: "update-demo-nautilus-2hg5b update-demo-nautilus-8pt4x "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 17 20:48:59.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4174'
Apr 17 20:48:59.517: INFO: stderr: ""
Apr 17 20:48:59.517: INFO: stdout: "update-demo-nautilus-2hg5b update-demo-nautilus-8pt4x "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr 17 20:49:04.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4174'
Apr 17 20:49:04.672: INFO: stderr: ""
Apr 17 20:49:04.672: INFO: stdout: "update-demo-nautilus-8pt4x "
Apr 17 20:49:04.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-8pt4x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4174'
Apr 17 20:49:04.798: INFO: stderr: ""
Apr 17 20:49:04.798: INFO: stdout: "true"
Apr 17 20:49:04.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-8pt4x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4174'
Apr 17 20:49:04.957: INFO: stderr: ""
Apr 17 20:49:04.957: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 17 20:49:04.957: INFO: validating pod update-demo-nautilus-8pt4x
Apr 17 20:49:04.983: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 20:49:04.983: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 20:49:04.983: INFO: update-demo-nautilus-8pt4x is verified up and running
STEP: scaling up the replication controller
Apr 17 20:49:04.987: INFO: scanned /root for discovery docs: <nil>
Apr 17 20:49:04.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-4174'
Apr 17 20:49:06.228: INFO: stderr: ""
Apr 17 20:49:06.228: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 17 20:49:06.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4174'
Apr 17 20:49:06.414: INFO: stderr: ""
Apr 17 20:49:06.414: INFO: stdout: "update-demo-nautilus-8pt4x update-demo-nautilus-drg2g "
Apr 17 20:49:06.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-8pt4x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4174'
Apr 17 20:49:06.566: INFO: stderr: ""
Apr 17 20:49:06.566: INFO: stdout: "true"
Apr 17 20:49:06.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-8pt4x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4174'
Apr 17 20:49:06.703: INFO: stderr: ""
Apr 17 20:49:06.703: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 17 20:49:06.703: INFO: validating pod update-demo-nautilus-8pt4x
Apr 17 20:49:06.718: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 20:49:06.718: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 20:49:06.718: INFO: update-demo-nautilus-8pt4x is verified up and running
Apr 17 20:49:06.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-drg2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4174'
Apr 17 20:49:06.844: INFO: stderr: ""
Apr 17 20:49:06.844: INFO: stdout: ""
Apr 17 20:49:06.844: INFO: update-demo-nautilus-drg2g is created but not running
Apr 17 20:49:11.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4174'
Apr 17 20:49:11.991: INFO: stderr: ""
Apr 17 20:49:11.991: INFO: stdout: "update-demo-nautilus-8pt4x update-demo-nautilus-drg2g "
Apr 17 20:49:11.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-8pt4x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4174'
Apr 17 20:49:12.145: INFO: stderr: ""
Apr 17 20:49:12.145: INFO: stdout: "true"
Apr 17 20:49:12.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-8pt4x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4174'
Apr 17 20:49:12.277: INFO: stderr: ""
Apr 17 20:49:12.277: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 17 20:49:12.277: INFO: validating pod update-demo-nautilus-8pt4x
Apr 17 20:49:12.291: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 20:49:12.291: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 20:49:12.291: INFO: update-demo-nautilus-8pt4x is verified up and running
Apr 17 20:49:12.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-drg2g -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4174'
Apr 17 20:49:12.439: INFO: stderr: ""
Apr 17 20:49:12.439: INFO: stdout: "true"
Apr 17 20:49:12.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-drg2g -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4174'
Apr 17 20:49:12.570: INFO: stderr: ""
Apr 17 20:49:12.570: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 17 20:49:12.570: INFO: validating pod update-demo-nautilus-drg2g
Apr 17 20:49:12.595: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 20:49:12.595: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 20:49:12.595: INFO: update-demo-nautilus-drg2g is verified up and running
STEP: using delete to clean up resources
Apr 17 20:49:12.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete --grace-period=0 --force -f - --namespace=kubectl-4174'
Apr 17 20:49:12.745: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 20:49:12.745: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 17 20:49:12.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4174'
Apr 17 20:49:12.913: INFO: stderr: "No resources found in kubectl-4174 namespace.\n"
Apr 17 20:49:12.913: INFO: stdout: ""
Apr 17 20:49:12.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -l name=update-demo --namespace=kubectl-4174 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 17 20:49:13.064: INFO: stderr: ""
Apr 17 20:49:13.064: INFO: stdout: "update-demo-nautilus-8pt4x\nupdate-demo-nautilus-drg2g\n"
Apr 17 20:49:13.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4174'
Apr 17 20:49:13.737: INFO: stderr: "No resources found in kubectl-4174 namespace.\n"
Apr 17 20:49:13.737: INFO: stdout: ""
Apr 17 20:49:13.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -l name=update-demo --namespace=kubectl-4174 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 17 20:49:13.897: INFO: stderr: ""
Apr 17 20:49:13.897: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:49:13.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4174" for this suite.
Apr 17 20:49:29.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:49:31.803: INFO: namespace kubectl-4174 deletion completed in 17.886547678s

â€¢ [SLOW TEST:45.795 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:49:31.804: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-f3ba188c-d804-43b4-8fc3-0c8a806defcf
STEP: Creating a pod to test consume secrets
Apr 17 20:49:31.990: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-35479def-f386-4f64-91e3-d28ca9c378dd" in namespace "projected-5256" to be "success or failure"
Apr 17 20:49:31.998: INFO: Pod "pod-projected-secrets-35479def-f386-4f64-91e3-d28ca9c378dd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.604782ms
Apr 17 20:49:34.008: INFO: Pod "pod-projected-secrets-35479def-f386-4f64-91e3-d28ca9c378dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018492745s
Apr 17 20:49:36.017: INFO: Pod "pod-projected-secrets-35479def-f386-4f64-91e3-d28ca9c378dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02733139s
Apr 17 20:49:38.027: INFO: Pod "pod-projected-secrets-35479def-f386-4f64-91e3-d28ca9c378dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037024188s
STEP: Saw pod success
Apr 17 20:49:38.027: INFO: Pod "pod-projected-secrets-35479def-f386-4f64-91e3-d28ca9c378dd" satisfied condition "success or failure"
Apr 17 20:49:38.035: INFO: Trying to get logs from node 10.72.119.74 pod pod-projected-secrets-35479def-f386-4f64-91e3-d28ca9c378dd container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 17 20:49:38.079: INFO: Waiting for pod pod-projected-secrets-35479def-f386-4f64-91e3-d28ca9c378dd to disappear
Apr 17 20:49:38.087: INFO: Pod pod-projected-secrets-35479def-f386-4f64-91e3-d28ca9c378dd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:49:38.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5256" for this suite.
Apr 17 20:49:46.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:49:47.998: INFO: namespace projected-5256 deletion completed in 9.895655971s

â€¢ [SLOW TEST:16.195 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:49:47.998: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 20:49:48.118: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:49:56.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7790" for this suite.
Apr 17 20:50:46.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:50:48.276: INFO: namespace pods-7790 deletion completed in 51.897213248s

â€¢ [SLOW TEST:60.278 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:50:48.277: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 17 20:50:48.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1527'
Apr 17 20:50:48.609: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 17 20:50:48.609: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Apr 17 20:50:48.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete jobs e2e-test-httpd-job --namespace=kubectl-1527'
Apr 17 20:50:48.780: INFO: stderr: ""
Apr 17 20:50:48.780: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:50:48.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1527" for this suite.
Apr 17 20:50:56.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:50:58.693: INFO: namespace kubectl-1527 deletion completed in 9.895307175s

â€¢ [SLOW TEST:10.417 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:50:58.695: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-7598
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-7598
STEP: creating replication controller externalsvc in namespace services-7598
I0417 20:50:58.951014      22 runners.go:184] Created replication controller with name: externalsvc, namespace: services-7598, replica count: 2
I0417 20:51:02.001856      22 runners.go:184] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0417 20:51:05.002093      22 runners.go:184] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0417 20:51:08.002641      22 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Apr 17 20:51:08.073: INFO: Creating new exec pod
Apr 17 20:51:16.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=services-7598 execpodw5z6s -- /bin/sh -x -c nslookup nodeport-service'
Apr 17 20:51:16.497: INFO: stderr: "+ nslookup nodeport-service\n"
Apr 17 20:51:16.497: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-7598.svc.cluster.local\tcanonical name = externalsvc.services-7598.svc.cluster.local.\nName:\texternalsvc.services-7598.svc.cluster.local\nAddress: 172.21.68.61\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7598, will wait for the garbage collector to delete the pods
Apr 17 20:51:16.611: INFO: Deleting ReplicationController externalsvc took: 46.500744ms
Apr 17 20:51:17.211: INFO: Terminating ReplicationController externalsvc pods took: 600.2752ms
Apr 17 20:51:32.485: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:51:32.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7598" for this suite.
Apr 17 20:51:40.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:51:42.471: INFO: namespace services-7598 deletion completed in 9.919909222s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:43.776 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:51:42.472: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Apr 17 20:51:50.679: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-531062789 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Apr 17 20:52:05.872: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:52:05.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5256" for this suite.
Apr 17 20:52:13.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:52:15.815: INFO: namespace pods-5256 deletion completed in 9.918384768s

â€¢ [SLOW TEST:33.343 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:52:15.816: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-t7j6
STEP: Creating a pod to test atomic-volume-subpath
Apr 17 20:52:16.038: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-t7j6" in namespace "subpath-9896" to be "success or failure"
Apr 17 20:52:16.047: INFO: Pod "pod-subpath-test-downwardapi-t7j6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.914311ms
Apr 17 20:52:18.056: INFO: Pod "pod-subpath-test-downwardapi-t7j6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017628634s
Apr 17 20:52:20.065: INFO: Pod "pod-subpath-test-downwardapi-t7j6": Phase="Running", Reason="", readiness=true. Elapsed: 4.026669081s
Apr 17 20:52:22.086: INFO: Pod "pod-subpath-test-downwardapi-t7j6": Phase="Running", Reason="", readiness=true. Elapsed: 6.048153795s
Apr 17 20:52:24.102: INFO: Pod "pod-subpath-test-downwardapi-t7j6": Phase="Running", Reason="", readiness=true. Elapsed: 8.064506911s
Apr 17 20:52:26.112: INFO: Pod "pod-subpath-test-downwardapi-t7j6": Phase="Running", Reason="", readiness=true. Elapsed: 10.074097272s
Apr 17 20:52:28.121: INFO: Pod "pod-subpath-test-downwardapi-t7j6": Phase="Running", Reason="", readiness=true. Elapsed: 12.083322681s
Apr 17 20:52:30.131: INFO: Pod "pod-subpath-test-downwardapi-t7j6": Phase="Running", Reason="", readiness=true. Elapsed: 14.092647811s
Apr 17 20:52:32.141: INFO: Pod "pod-subpath-test-downwardapi-t7j6": Phase="Running", Reason="", readiness=true. Elapsed: 16.103128118s
Apr 17 20:52:34.150: INFO: Pod "pod-subpath-test-downwardapi-t7j6": Phase="Running", Reason="", readiness=true. Elapsed: 18.111715078s
Apr 17 20:52:36.158: INFO: Pod "pod-subpath-test-downwardapi-t7j6": Phase="Running", Reason="", readiness=true. Elapsed: 20.120081662s
Apr 17 20:52:38.166: INFO: Pod "pod-subpath-test-downwardapi-t7j6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.12793541s
STEP: Saw pod success
Apr 17 20:52:38.166: INFO: Pod "pod-subpath-test-downwardapi-t7j6" satisfied condition "success or failure"
Apr 17 20:52:38.174: INFO: Trying to get logs from node 10.72.119.74 pod pod-subpath-test-downwardapi-t7j6 container test-container-subpath-downwardapi-t7j6: <nil>
STEP: delete the pod
Apr 17 20:52:38.228: INFO: Waiting for pod pod-subpath-test-downwardapi-t7j6 to disappear
Apr 17 20:52:38.235: INFO: Pod pod-subpath-test-downwardapi-t7j6 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-t7j6
Apr 17 20:52:38.235: INFO: Deleting pod "pod-subpath-test-downwardapi-t7j6" in namespace "subpath-9896"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:52:38.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9896" for this suite.
Apr 17 20:52:46.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:52:48.141: INFO: namespace subpath-9896 deletion completed in 9.883013304s

â€¢ [SLOW TEST:32.325 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:52:48.141: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-d2059fa0-d35e-44a8-a2db-5d0bfca1d479
STEP: Creating a pod to test consume secrets
Apr 17 20:52:48.342: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-600b025c-ce99-4809-ab3d-36926e7afb73" in namespace "projected-6180" to be "success or failure"
Apr 17 20:52:48.350: INFO: Pod "pod-projected-secrets-600b025c-ce99-4809-ab3d-36926e7afb73": Phase="Pending", Reason="", readiness=false. Elapsed: 8.312689ms
Apr 17 20:52:50.360: INFO: Pod "pod-projected-secrets-600b025c-ce99-4809-ab3d-36926e7afb73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018062271s
STEP: Saw pod success
Apr 17 20:52:50.360: INFO: Pod "pod-projected-secrets-600b025c-ce99-4809-ab3d-36926e7afb73" satisfied condition "success or failure"
Apr 17 20:52:50.369: INFO: Trying to get logs from node 10.72.119.74 pod pod-projected-secrets-600b025c-ce99-4809-ab3d-36926e7afb73 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 17 20:52:50.424: INFO: Waiting for pod pod-projected-secrets-600b025c-ce99-4809-ab3d-36926e7afb73 to disappear
Apr 17 20:52:50.435: INFO: Pod pod-projected-secrets-600b025c-ce99-4809-ab3d-36926e7afb73 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:52:50.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6180" for this suite.
Apr 17 20:52:58.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:53:00.388: INFO: namespace projected-6180 deletion completed in 9.934810486s

â€¢ [SLOW TEST:12.247 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:53:00.389: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:53:11.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4190" for this suite.
Apr 17 20:53:19.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:53:21.575: INFO: namespace resourcequota-4190 deletion completed in 9.912165884s

â€¢ [SLOW TEST:21.187 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:53:21.575: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-1446/secret-test-9cfd2622-a5f4-4acc-8475-74a27567872d
STEP: Creating a pod to test consume secrets
Apr 17 20:53:21.785: INFO: Waiting up to 5m0s for pod "pod-configmaps-db807c2a-3198-4ae3-b001-a5401bc0c4eb" in namespace "secrets-1446" to be "success or failure"
Apr 17 20:53:21.804: INFO: Pod "pod-configmaps-db807c2a-3198-4ae3-b001-a5401bc0c4eb": Phase="Pending", Reason="", readiness=false. Elapsed: 19.120732ms
Apr 17 20:53:23.823: INFO: Pod "pod-configmaps-db807c2a-3198-4ae3-b001-a5401bc0c4eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037445087s
Apr 17 20:53:25.832: INFO: Pod "pod-configmaps-db807c2a-3198-4ae3-b001-a5401bc0c4eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04732287s
STEP: Saw pod success
Apr 17 20:53:25.833: INFO: Pod "pod-configmaps-db807c2a-3198-4ae3-b001-a5401bc0c4eb" satisfied condition "success or failure"
Apr 17 20:53:25.841: INFO: Trying to get logs from node 10.72.119.74 pod pod-configmaps-db807c2a-3198-4ae3-b001-a5401bc0c4eb container env-test: <nil>
STEP: delete the pod
Apr 17 20:53:25.908: INFO: Waiting for pod pod-configmaps-db807c2a-3198-4ae3-b001-a5401bc0c4eb to disappear
Apr 17 20:53:25.915: INFO: Pod pod-configmaps-db807c2a-3198-4ae3-b001-a5401bc0c4eb no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:53:25.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1446" for this suite.
Apr 17 20:53:33.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:53:35.840: INFO: namespace secrets-1446 deletion completed in 9.891349217s

â€¢ [SLOW TEST:14.265 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:53:35.843: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr 17 20:53:35.987: INFO: Pod name pod-release: Found 0 pods out of 1
Apr 17 20:53:40.996: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:53:42.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7512" for this suite.
Apr 17 20:53:50.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:53:51.958: INFO: namespace replication-controller-7512 deletion completed in 9.886042706s

â€¢ [SLOW TEST:16.115 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:53:51.958: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 20:53:52.105: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-fc76ddc6-51ed-4c4c-a3aa-91227107422f
STEP: Creating configMap with name cm-test-opt-upd-1c677e1a-1674-469b-8bbd-6250c8de31e8
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-fc76ddc6-51ed-4c4c-a3aa-91227107422f
STEP: Updating configmap cm-test-opt-upd-1c677e1a-1674-469b-8bbd-6250c8de31e8
STEP: Creating configMap with name cm-test-opt-create-b5783863-7286-446c-9dfc-e51be21f26e2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:55:23.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-80" for this suite.
Apr 17 20:55:55.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:55:57.517: INFO: namespace configmap-80 deletion completed in 33.951355544s

â€¢ [SLOW TEST:125.559 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:55:57.519: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 17 20:55:57.725: INFO: Waiting up to 5m0s for pod "pod-c49550da-bde5-47a7-b1a6-49b2ef4239ee" in namespace "emptydir-6545" to be "success or failure"
Apr 17 20:55:57.733: INFO: Pod "pod-c49550da-bde5-47a7-b1a6-49b2ef4239ee": Phase="Pending", Reason="", readiness=false. Elapsed: 8.093566ms
Apr 17 20:55:59.742: INFO: Pod "pod-c49550da-bde5-47a7-b1a6-49b2ef4239ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017369218s
Apr 17 20:56:01.751: INFO: Pod "pod-c49550da-bde5-47a7-b1a6-49b2ef4239ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026263575s
Apr 17 20:56:03.761: INFO: Pod "pod-c49550da-bde5-47a7-b1a6-49b2ef4239ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036217615s
STEP: Saw pod success
Apr 17 20:56:03.761: INFO: Pod "pod-c49550da-bde5-47a7-b1a6-49b2ef4239ee" satisfied condition "success or failure"
Apr 17 20:56:03.769: INFO: Trying to get logs from node 10.72.119.74 pod pod-c49550da-bde5-47a7-b1a6-49b2ef4239ee container test-container: <nil>
STEP: delete the pod
Apr 17 20:56:03.840: INFO: Waiting for pod pod-c49550da-bde5-47a7-b1a6-49b2ef4239ee to disappear
Apr 17 20:56:03.847: INFO: Pod pod-c49550da-bde5-47a7-b1a6-49b2ef4239ee no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:56:03.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6545" for this suite.
Apr 17 20:56:11.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:56:13.766: INFO: namespace emptydir-6545 deletion completed in 9.897613994s

â€¢ [SLOW TEST:16.248 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:56:13.769: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Apr 17 20:56:13.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 cluster-info'
Apr 17 20:56:14.050: INFO: stderr: ""
Apr 17 20:56:14.050: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:56:14.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7739" for this suite.
Apr 17 20:56:22.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:56:23.963: INFO: namespace kubectl-7739 deletion completed in 9.89077475s

â€¢ [SLOW TEST:10.194 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:56:23.963: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:56:28.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4904" for this suite.
Apr 17 20:56:36.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:56:38.056: INFO: namespace kubelet-test-4904 deletion completed in 9.887434554s

â€¢ [SLOW TEST:14.093 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:56:38.056: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 20:56:38.619: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 17 20:56:40.667: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722753798, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722753798, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722753798, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722753798, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 20:56:43.708: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:56:43.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3144" for this suite.
Apr 17 20:56:51.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:56:53.853: INFO: namespace webhook-3144 deletion completed in 9.928936991s
STEP: Destroying namespace "webhook-3144-markers" for this suite.
Apr 17 20:57:01.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:57:03.878: INFO: namespace webhook-3144-markers deletion completed in 10.025099223s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:25.894 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:57:03.951: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-165b12c0-d75e-4bde-8c2d-5b7b17a320f2
STEP: Creating a pod to test consume secrets
Apr 17 20:57:04.154: INFO: Waiting up to 5m0s for pod "pod-secrets-98f5f4f5-400a-48da-89a2-a6a97caed4c9" in namespace "secrets-5687" to be "success or failure"
Apr 17 20:57:04.163: INFO: Pod "pod-secrets-98f5f4f5-400a-48da-89a2-a6a97caed4c9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.212587ms
Apr 17 20:57:06.171: INFO: Pod "pod-secrets-98f5f4f5-400a-48da-89a2-a6a97caed4c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017138647s
STEP: Saw pod success
Apr 17 20:57:06.171: INFO: Pod "pod-secrets-98f5f4f5-400a-48da-89a2-a6a97caed4c9" satisfied condition "success or failure"
Apr 17 20:57:06.178: INFO: Trying to get logs from node 10.72.119.74 pod pod-secrets-98f5f4f5-400a-48da-89a2-a6a97caed4c9 container secret-volume-test: <nil>
STEP: delete the pod
Apr 17 20:57:06.230: INFO: Waiting for pod pod-secrets-98f5f4f5-400a-48da-89a2-a6a97caed4c9 to disappear
Apr 17 20:57:06.238: INFO: Pod pod-secrets-98f5f4f5-400a-48da-89a2-a6a97caed4c9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:57:06.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5687" for this suite.
Apr 17 20:57:14.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:57:16.207: INFO: namespace secrets-5687 deletion completed in 9.933034154s

â€¢ [SLOW TEST:12.256 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:57:16.207: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-469df9d4-8062-4adf-9f84-5770f664b9d1
STEP: Creating secret with name secret-projected-all-test-volume-1354a71d-617d-455f-9c0b-9dd5f4e642ff
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr 17 20:57:16.414: INFO: Waiting up to 5m0s for pod "projected-volume-89bd5432-3aea-4d28-a653-e82802a57a65" in namespace "projected-2352" to be "success or failure"
Apr 17 20:57:16.422: INFO: Pod "projected-volume-89bd5432-3aea-4d28-a653-e82802a57a65": Phase="Pending", Reason="", readiness=false. Elapsed: 8.877812ms
Apr 17 20:57:18.431: INFO: Pod "projected-volume-89bd5432-3aea-4d28-a653-e82802a57a65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017809193s
Apr 17 20:57:20.440: INFO: Pod "projected-volume-89bd5432-3aea-4d28-a653-e82802a57a65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026724368s
STEP: Saw pod success
Apr 17 20:57:20.440: INFO: Pod "projected-volume-89bd5432-3aea-4d28-a653-e82802a57a65" satisfied condition "success or failure"
Apr 17 20:57:20.450: INFO: Trying to get logs from node 10.72.119.74 pod projected-volume-89bd5432-3aea-4d28-a653-e82802a57a65 container projected-all-volume-test: <nil>
STEP: delete the pod
Apr 17 20:57:20.498: INFO: Waiting for pod projected-volume-89bd5432-3aea-4d28-a653-e82802a57a65 to disappear
Apr 17 20:57:20.506: INFO: Pod projected-volume-89bd5432-3aea-4d28-a653-e82802a57a65 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:57:20.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2352" for this suite.
Apr 17 20:57:28.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:57:30.427: INFO: namespace projected-2352 deletion completed in 9.895966315s

â€¢ [SLOW TEST:14.220 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:57:30.429: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Apr 17 20:57:30.551: INFO: PodSpec: initContainers in spec.initContainers
Apr 17 20:58:16.955: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-431e893d-d10c-46b3-97f0-178ee46b463b", GenerateName:"", Namespace:"init-container-7424", SelfLink:"/api/v1/namespaces/init-container-7424/pods/pod-init-431e893d-d10c-46b3-97f0-178ee46b463b", UID:"dc0449d5-fb46-498c-921d-cd765a489bc4", ResourceVersion:"46320", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63722753850, loc:(*time.Location)(0x84c02a0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"551375004"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.30.194.98/32", "cni.projectcalico.org/podIPs":"172.30.194.98/32", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.194.98\"\n    ],\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-r8n8f", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00168e2c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-r8n8f", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00155fef0), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-r8n8f", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00155ff90), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-r8n8f", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc00155fe50), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001a35ef0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.72.119.74", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002955c80), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001a35fa0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001a35fc0)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001a35fdc), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001a35fe0), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722753850, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722753850, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722753850, loc:(*time.Location)(0x84c02a0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722753850, loc:(*time.Location)(0x84c02a0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.72.119.74", PodIP:"172.30.194.98", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.194.98"}}, StartTime:(*v1.Time)(0xc0017c00c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003060e00)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc003060e70)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"cri-o://29fe0512d0fa5e6db68cc393997ed712b558c702daf7f6441afbc530329979d1", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0017c0140), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0017c0100), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc001ee005f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:58:16.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7424" for this suite.
Apr 17 20:58:31.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:58:32.906: INFO: namespace init-container-7424 deletion completed in 15.917295722s

â€¢ [SLOW TEST:62.477 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:58:32.906: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Apr 17 20:58:37.130: INFO: Pod pod-hostip-e6e106f1-d9ff-4361-8363-c3a9531d50bd has hostIP: 10.72.119.74
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:58:37.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8220" for this suite.
Apr 17 20:59:09.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:59:11.044: INFO: namespace pods-8220 deletion completed in 33.887505196s

â€¢ [SLOW TEST:38.138 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 20:59:11.044: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 20:59:12.226: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 17 20:59:14.267: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722753952, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722753952, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722753952, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722753952, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 20:59:17.313: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Apr 17 20:59:21.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 attach --namespace=webhook-2864 to-be-attached-pod -i -c=container1'
Apr 17 20:59:21.862: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 20:59:21.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2864" for this suite.
Apr 17 20:59:53.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 20:59:55.866: INFO: namespace webhook-2864 deletion completed in 33.95058018s
STEP: Destroying namespace "webhook-2864-markers" for this suite.
Apr 17 21:00:03.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:00:05.766: INFO: namespace webhook-2864-markers deletion completed in 9.899060849s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:54.793 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:00:05.838: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-b560ba34-5f9b-444d-89cd-c416d152861d
Apr 17 21:00:05.997: INFO: Pod name my-hostname-basic-b560ba34-5f9b-444d-89cd-c416d152861d: Found 0 pods out of 1
Apr 17 21:00:11.008: INFO: Pod name my-hostname-basic-b560ba34-5f9b-444d-89cd-c416d152861d: Found 1 pods out of 1
Apr 17 21:00:11.008: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-b560ba34-5f9b-444d-89cd-c416d152861d" are running
Apr 17 21:00:11.016: INFO: Pod "my-hostname-basic-b560ba34-5f9b-444d-89cd-c416d152861d-xnshx" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-17 21:00:06 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-17 21:00:08 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-17 21:00:08 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-17 21:00:06 +0000 UTC Reason: Message:}])
Apr 17 21:00:11.016: INFO: Trying to dial the pod
Apr 17 21:00:16.060: INFO: Controller my-hostname-basic-b560ba34-5f9b-444d-89cd-c416d152861d: Got expected result from replica 1 [my-hostname-basic-b560ba34-5f9b-444d-89cd-c416d152861d-xnshx]: "my-hostname-basic-b560ba34-5f9b-444d-89cd-c416d152861d-xnshx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:00:16.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2937" for this suite.
Apr 17 21:00:24.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:00:26.146: INFO: namespace replication-controller-2937 deletion completed in 10.058099018s

â€¢ [SLOW TEST:20.308 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:00:26.146: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Apr 17 21:00:26.311: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:00:34.039: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:01:09.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9886" for this suite.
Apr 17 21:01:17.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:01:19.736: INFO: namespace crd-publish-openapi-9886 deletion completed in 10.178495717s

â€¢ [SLOW TEST:53.590 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:01:19.738: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6708, will wait for the garbage collector to delete the pods
Apr 17 21:01:26.060: INFO: Deleting Job.batch foo took: 37.866481ms
Apr 17 21:01:26.661: INFO: Terminating Job.batch foo pods took: 600.34808ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:02:07.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6708" for this suite.
Apr 17 21:02:16.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:02:18.313: INFO: namespace job-6708 deletion completed in 10.297667174s

â€¢ [SLOW TEST:58.576 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:02:18.314: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Apr 17 21:02:58.643: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0417 21:02:58.643922      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:02:58.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8415" for this suite.
Apr 17 21:03:08.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:03:10.893: INFO: namespace gc-8415 deletion completed in 12.224863365s

â€¢ [SLOW TEST:52.580 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:03:10.894: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 21:03:11.123: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a44dd6fc-4310-4bde-8d29-ef5e65417ea1" in namespace "projected-7746" to be "success or failure"
Apr 17 21:03:11.139: INFO: Pod "downwardapi-volume-a44dd6fc-4310-4bde-8d29-ef5e65417ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.795243ms
Apr 17 21:03:13.155: INFO: Pod "downwardapi-volume-a44dd6fc-4310-4bde-8d29-ef5e65417ea1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032014815s
Apr 17 21:03:15.166: INFO: Pod "downwardapi-volume-a44dd6fc-4310-4bde-8d29-ef5e65417ea1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043617838s
STEP: Saw pod success
Apr 17 21:03:15.166: INFO: Pod "downwardapi-volume-a44dd6fc-4310-4bde-8d29-ef5e65417ea1" satisfied condition "success or failure"
Apr 17 21:03:15.178: INFO: Trying to get logs from node 10.72.119.72 pod downwardapi-volume-a44dd6fc-4310-4bde-8d29-ef5e65417ea1 container client-container: <nil>
STEP: delete the pod
Apr 17 21:03:15.273: INFO: Waiting for pod downwardapi-volume-a44dd6fc-4310-4bde-8d29-ef5e65417ea1 to disappear
Apr 17 21:03:15.283: INFO: Pod downwardapi-volume-a44dd6fc-4310-4bde-8d29-ef5e65417ea1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:03:15.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7746" for this suite.
Apr 17 21:03:23.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:03:25.690: INFO: namespace projected-7746 deletion completed in 10.380232124s

â€¢ [SLOW TEST:14.797 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:03:25.693: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Apr 17 21:03:25.898: INFO: Waiting up to 5m0s for pod "downward-api-82d80e81-ca49-4abe-9b27-a9b4d5abe476" in namespace "downward-api-4760" to be "success or failure"
Apr 17 21:03:25.912: INFO: Pod "downward-api-82d80e81-ca49-4abe-9b27-a9b4d5abe476": Phase="Pending", Reason="", readiness=false. Elapsed: 13.575992ms
Apr 17 21:03:27.925: INFO: Pod "downward-api-82d80e81-ca49-4abe-9b27-a9b4d5abe476": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027302841s
Apr 17 21:03:29.940: INFO: Pod "downward-api-82d80e81-ca49-4abe-9b27-a9b4d5abe476": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041641347s
STEP: Saw pod success
Apr 17 21:03:29.940: INFO: Pod "downward-api-82d80e81-ca49-4abe-9b27-a9b4d5abe476" satisfied condition "success or failure"
Apr 17 21:03:29.956: INFO: Trying to get logs from node 10.72.119.72 pod downward-api-82d80e81-ca49-4abe-9b27-a9b4d5abe476 container dapi-container: <nil>
STEP: delete the pod
Apr 17 21:03:30.017: INFO: Waiting for pod downward-api-82d80e81-ca49-4abe-9b27-a9b4d5abe476 to disappear
Apr 17 21:03:30.043: INFO: Pod downward-api-82d80e81-ca49-4abe-9b27-a9b4d5abe476 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:03:30.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4760" for this suite.
Apr 17 21:03:38.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:03:40.405: INFO: namespace downward-api-4760 deletion completed in 10.33769942s

â€¢ [SLOW TEST:14.713 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:03:40.406: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:03:40.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 version'
Apr 17 21:03:40.670: INFO: stderr: ""
Apr 17 21:03:40.670: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.2\", GitCommit:\"c97fe5036ef3df2967d086711e6c0c405941e14b\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T19:18:23Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16+\", GitVersion:\"v1.16.2\", GitCommit:\"65ad866\", GitTreeState:\"clean\", BuildDate:\"2020-04-09T20:15:09Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:03:40.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2632" for this suite.
Apr 17 21:03:48.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:03:50.897: INFO: namespace kubectl-2632 deletion completed in 10.191609733s

â€¢ [SLOW TEST:10.491 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:03:50.897: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-9280/configmap-test-07de533c-c28c-488e-b727-1c7c6d46826e
STEP: Creating a pod to test consume configMaps
Apr 17 21:03:51.158: INFO: Waiting up to 5m0s for pod "pod-configmaps-827e0046-b986-4e7b-afb3-82514aa8b998" in namespace "configmap-9280" to be "success or failure"
Apr 17 21:03:51.176: INFO: Pod "pod-configmaps-827e0046-b986-4e7b-afb3-82514aa8b998": Phase="Pending", Reason="", readiness=false. Elapsed: 18.807698ms
Apr 17 21:03:53.191: INFO: Pod "pod-configmaps-827e0046-b986-4e7b-afb3-82514aa8b998": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033252908s
STEP: Saw pod success
Apr 17 21:03:53.191: INFO: Pod "pod-configmaps-827e0046-b986-4e7b-afb3-82514aa8b998" satisfied condition "success or failure"
Apr 17 21:03:53.205: INFO: Trying to get logs from node 10.72.119.74 pod pod-configmaps-827e0046-b986-4e7b-afb3-82514aa8b998 container env-test: <nil>
STEP: delete the pod
Apr 17 21:03:53.315: INFO: Waiting for pod pod-configmaps-827e0046-b986-4e7b-afb3-82514aa8b998 to disappear
Apr 17 21:03:53.341: INFO: Pod pod-configmaps-827e0046-b986-4e7b-afb3-82514aa8b998 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:03:53.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9280" for this suite.
Apr 17 21:04:01.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:04:03.516: INFO: namespace configmap-9280 deletion completed in 10.154039733s

â€¢ [SLOW TEST:12.620 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:04:03.521: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Apr 17 21:04:03.646: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 17 21:04:03.725: INFO: Waiting for terminating namespaces to be deleted...
Apr 17 21:04:03.749: INFO: 
Logging pods the kubelet thinks is on node 10.72.119.72 before test
Apr 17 21:04:03.867: INFO: ibm-keepalived-watcher-58jb7 from kube-system started at 2020-04-17 19:09:24 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr 17 21:04:03.867: INFO: service-ca-operator-5d59f48888-hk9zs from openshift-service-ca-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container operator ready: true, restart count 0
Apr 17 21:04:03.867: INFO: olm-operator-7bd9dc9457-2wn6n from openshift-operator-lifecycle-manager started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container olm-operator ready: true, restart count 0
Apr 17 21:04:03.867: INFO: calico-kube-controllers-84d976f9ff-g9864 from calico-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr 17 21:04:03.867: INFO: downloads-64bb8b89c9-sccw2 from openshift-console started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container download-server ready: true, restart count 0
Apr 17 21:04:03.867: INFO: tuned-xrlm4 from openshift-cluster-node-tuning-operator started at 2020-04-17 19:11:12 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container tuned ready: true, restart count 0
Apr 17 21:04:03.867: INFO: packageserver-6bfd584bf6-xvcxz from openshift-operator-lifecycle-manager started at 2020-04-17 19:53:44 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container packageserver ready: true, restart count 0
Apr 17 21:04:03.867: INFO: ibm-master-proxy-static-10.72.119.72 from kube-system started at 2020-04-17 19:09:18 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr 17 21:04:03.867: INFO: 	Container pause ready: true, restart count 0
Apr 17 21:04:03.867: INFO: cluster-node-tuning-operator-77bdbd4f-79qvb from openshift-cluster-node-tuning-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Apr 17 21:04:03.867: INFO: openshift-service-catalog-apiserver-operator-76969db7f5-wpk2m from openshift-service-catalog-apiserver-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container operator ready: true, restart count 1
Apr 17 21:04:03.867: INFO: node-exporter-5qztm from openshift-monitoring started at 2020-04-17 19:11:29 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:04:03.867: INFO: 	Container node-exporter ready: true, restart count 0
Apr 17 21:04:03.867: INFO: dns-default-ffsfn from openshift-dns started at 2020-04-17 19:12:00 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container dns ready: true, restart count 0
Apr 17 21:04:03.867: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr 17 21:04:03.867: INFO: openshift-state-metrics-6c465bc47f-p9qr6 from openshift-monitoring started at 2020-04-17 19:11:29 +0000 UTC (3 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr 17 21:04:03.867: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr 17 21:04:03.867: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Apr 17 21:04:03.867: INFO: node-ca-x5gt7 from openshift-image-registry started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container node-ca ready: true, restart count 0
Apr 17 21:04:03.867: INFO: tigera-operator-df8f4c87c-c7sjz from tigera-operator started at 2020-04-17 19:09:24 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container tigera-operator ready: true, restart count 0
Apr 17 21:04:03.867: INFO: calico-node-tcc87 from calico-system started at 2020-04-17 19:09:40 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:04:03.867: INFO: cluster-storage-operator-56475d49d7-fsvdb from openshift-cluster-storage-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Apr 17 21:04:03.867: INFO: marketplace-operator-699fb8f5d-5nrcq from openshift-marketplace started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container marketplace-operator ready: true, restart count 0
Apr 17 21:04:03.867: INFO: ibm-file-plugin-7cbd86d68f-9tm42 from kube-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Apr 17 21:04:03.867: INFO: ibm-storage-watcher-d9c7cf586-f72vk from kube-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Apr 17 21:04:03.867: INFO: sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-l8m9b from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:04:03.867: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:04:03.867: INFO: ingress-operator-66cf4674d8-467pw from openshift-ingress-operator started at 2020-04-17 19:10:34 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container ingress-operator ready: true, restart count 0
Apr 17 21:04:03.867: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:04:03.867: INFO: multus-admission-controller-2jxfm from openshift-multus started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr 17 21:04:03.867: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-04-17 19:16:48 +0000 UTC (3 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container alertmanager ready: true, restart count 0
Apr 17 21:04:03.867: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr 17 21:04:03.867: INFO: 	Container config-reloader ready: true, restart count 0
Apr 17 21:04:03.867: INFO: console-operator-db5d785db-r4qh6 from openshift-console-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container console-operator ready: true, restart count 1
Apr 17 21:04:03.867: INFO: sonobuoy from sonobuoy started at 2020-04-17 20:47:07 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 17 21:04:03.867: INFO: configmap-cabundle-injector-5cf6d9695-kfct9 from openshift-service-ca started at 2020-04-17 19:11:11 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Apr 17 21:04:03.867: INFO: ibm-cloud-provider-ip-158-176-122-172-6f57dbd74b-cdqd6 from ibm-system started at 2020-04-17 19:13:49 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container ibm-cloud-provider-ip-158-176-122-172 ready: true, restart count 0
Apr 17 21:04:03.867: INFO: network-operator-64f597f5d-8tg4w from openshift-network-operator started at 2020-04-17 19:09:28 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container network-operator ready: true, restart count 0
Apr 17 21:04:03.867: INFO: multus-wlttp from openshift-multus started at 2020-04-17 19:09:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container kube-multus ready: true, restart count 0
Apr 17 21:04:03.867: INFO: openshift-kube-proxy-bkf2z from openshift-kube-proxy started at 2020-04-17 19:10:02 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:04:03.867: INFO: cluster-image-registry-operator-7857d56744-c2pch from openshift-image-registry started at 2020-04-17 19:10:34 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Apr 17 21:04:03.867: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Apr 17 21:04:03.867: INFO: calico-typha-76b588567c-d6d9k from calico-system started at 2020-04-17 19:09:39 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container calico-typha ready: true, restart count 1
Apr 17 21:04:03.867: INFO: catalog-operator-5665d988d5-kvkfk from openshift-operator-lifecycle-manager started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container catalog-operator ready: true, restart count 0
Apr 17 21:04:03.867: INFO: dns-operator-74d97d58d-56295 from openshift-dns-operator started at 2020-04-17 19:10:34 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container dns-operator ready: true, restart count 0
Apr 17 21:04:03.867: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:04:03.867: INFO: openshift-service-catalog-controller-manager-operator-f954mtxlp from openshift-service-catalog-controller-manager-operator started at 2020-04-17 19:10:35 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container operator ready: true, restart count 1
Apr 17 21:04:03.867: INFO: kube-state-metrics-5bc9b987bc-hvx9k from openshift-monitoring started at 2020-04-17 19:11:29 +0000 UTC (3 container statuses recorded)
Apr 17 21:04:03.867: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr 17 21:04:03.867: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr 17 21:04:03.868: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 17 21:04:03.868: INFO: ibmcloud-block-storage-driver-g7lth from kube-system started at 2020-04-17 19:09:28 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.868: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Apr 17 21:04:03.868: INFO: cluster-monitoring-operator-668c7b8d6d-khmrg from openshift-monitoring started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.868: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Apr 17 21:04:03.868: INFO: ibmcloud-block-storage-plugin-75f7cd767-lcw2w from kube-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.868: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Apr 17 21:04:03.868: INFO: downloads-64bb8b89c9-jxkw4 from openshift-console started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.868: INFO: 	Container download-server ready: true, restart count 0
Apr 17 21:04:03.868: INFO: service-serving-cert-signer-5968cc5d5c-m6hb2 from openshift-service-ca started at 2020-04-17 19:11:10 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.868: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Apr 17 21:04:03.868: INFO: apiservice-cabundle-injector-7bf8cddb9-kxhvx from openshift-service-ca started at 2020-04-17 19:11:11 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.868: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Apr 17 21:04:03.868: INFO: 
Logging pods the kubelet thinks is on node 10.72.119.74 before test
Apr 17 21:04:03.930: INFO: ibm-keepalived-watcher-hm7n9 from kube-system started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr 17 21:04:03.930: INFO: ibmcloud-block-storage-driver-6j6qk from kube-system started at 2020-04-17 19:10:53 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Apr 17 21:04:03.930: INFO: calico-typha-76b588567c-khgqc from calico-system started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 21:04:03.930: INFO: sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-tg59r from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:04:03.930: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:04:03.930: INFO: packageserver-6bfd584bf6-8l96s from openshift-operator-lifecycle-manager started at 2020-04-17 19:53:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container packageserver ready: true, restart count 0
Apr 17 21:04:03.930: INFO: openshift-kube-proxy-gsgx2 from openshift-kube-proxy started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:04:03.930: INFO: multus-admission-controller-px85k from openshift-multus started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr 17 21:04:03.930: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-04-17 19:17:02 +0000 UTC (3 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container alertmanager ready: true, restart count 0
Apr 17 21:04:03.930: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr 17 21:04:03.930: INFO: 	Container config-reloader ready: true, restart count 0
Apr 17 21:04:03.930: INFO: thanos-querier-5c84fd9f68-rxgq7 from openshift-monitoring started at 2020-04-17 19:17:38 +0000 UTC (4 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:04:03.930: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr 17 21:04:03.930: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 21:04:03.930: INFO: 	Container thanos-querier ready: true, restart count 0
Apr 17 21:04:03.930: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-04-17 19:17:57 +0000 UTC (7 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:04:03.930: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 21:04:03.930: INFO: 	Container prometheus ready: true, restart count 1
Apr 17 21:04:03.930: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Apr 17 21:04:03.930: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr 17 21:04:03.930: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Apr 17 21:04:03.930: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr 17 21:04:03.930: INFO: calico-node-xm4xf from calico-system started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:04:03.930: INFO: router-default-8779c94d4-drwq9 from openshift-ingress started at 2020-04-17 19:12:06 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container router ready: true, restart count 0
Apr 17 21:04:03.930: INFO: tuned-tsvx9 from openshift-cluster-node-tuning-operator started at 2020-04-17 19:11:12 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container tuned ready: true, restart count 0
Apr 17 21:04:03.930: INFO: multus-s62pc from openshift-multus started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container kube-multus ready: true, restart count 0
Apr 17 21:04:03.930: INFO: node-ca-5dp5t from openshift-image-registry started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container node-ca ready: true, restart count 0
Apr 17 21:04:03.930: INFO: grafana-6b4f8c85c5-ggf9s from openshift-monitoring started at 2020-04-17 19:16:49 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container grafana ready: true, restart count 0
Apr 17 21:04:03.930: INFO: 	Container grafana-proxy ready: true, restart count 0
Apr 17 21:04:03.930: INFO: ibm-master-proxy-static-10.72.119.74 from kube-system started at 2020-04-17 19:10:42 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr 17 21:04:03.930: INFO: 	Container pause ready: true, restart count 0
Apr 17 21:04:03.930: INFO: node-exporter-949xg from openshift-monitoring started at 2020-04-17 19:11:30 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:04:03.930: INFO: 	Container node-exporter ready: true, restart count 0
Apr 17 21:04:03.930: INFO: dns-default-kvkts from openshift-dns started at 2020-04-17 19:12:00 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container dns ready: true, restart count 0
Apr 17 21:04:03.930: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr 17 21:04:03.930: INFO: vpn-5d7df69b48-txcqf from kube-system started at 2020-04-17 19:28:14 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.930: INFO: 	Container vpn ready: true, restart count 0
Apr 17 21:04:03.930: INFO: prometheus-adapter-5cd8cd848d-vtjmp from openshift-monitoring started at 2020-04-17 19:16:43 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.931: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr 17 21:04:03.931: INFO: sonobuoy-e2e-job-793c74f2197d4b50 from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:03.931: INFO: 	Container e2e ready: true, restart count 0
Apr 17 21:04:03.931: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:04:03.931: INFO: console-5b98d99db9-cmb7l from openshift-console started at 2020-04-17 19:12:41 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:03.931: INFO: 	Container console ready: true, restart count 0
Apr 17 21:04:03.931: INFO: 
Logging pods the kubelet thinks is on node 10.72.119.98 before test
Apr 17 21:04:04.145: INFO: certified-operators-586b58fc67-nk9d4 from openshift-marketplace started at 2020-04-17 19:12:23 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.145: INFO: 	Container certified-operators ready: true, restart count 0
Apr 17 21:04:04.145: INFO: image-registry-5655cc46bf-gzrxj from openshift-image-registry started at 2020-04-17 19:14:35 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.145: INFO: 	Container registry ready: true, restart count 0
Apr 17 21:04:04.145: INFO: router-default-8779c94d4-mdtbq from openshift-ingress started at 2020-04-17 19:12:06 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.145: INFO: 	Container router ready: true, restart count 0
Apr 17 21:04:04.145: INFO: openshift-kube-proxy-7dshq from openshift-kube-proxy started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.145: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:04:04.145: INFO: prometheus-adapter-5cd8cd848d-xspwf from openshift-monitoring started at 2020-04-17 19:16:43 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.145: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr 17 21:04:04.145: INFO: calico-node-6sr5x from calico-system started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.145: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:04:04.145: INFO: dns-default-z6b7d from openshift-dns started at 2020-04-17 19:12:00 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:04.145: INFO: 	Container dns ready: true, restart count 0
Apr 17 21:04:04.145: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr 17 21:04:04.145: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-04-17 19:14:09 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.145: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Apr 17 21:04:04.145: INFO: registry-pvc-permissions-d9dpb from openshift-image-registry started at 2020-04-17 19:14:35 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.145: INFO: 	Container pvc-permissions ready: false, restart count 0
Apr 17 21:04:04.145: INFO: tuned-sz2br from openshift-cluster-node-tuning-operator started at 2020-04-17 19:11:12 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.145: INFO: 	Container tuned ready: true, restart count 0
Apr 17 21:04:04.145: INFO: node-ca-wr66h from openshift-image-registry started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.145: INFO: 	Container node-ca ready: true, restart count 0
Apr 17 21:04:04.145: INFO: ibm-cloud-provider-ip-158-176-122-172-6f57dbd74b-bpkpw from ibm-system started at 2020-04-17 19:13:51 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.145: INFO: 	Container ibm-cloud-provider-ip-158-176-122-172 ready: true, restart count 0
Apr 17 21:04:04.145: INFO: console-5b98d99db9-2z7vw from openshift-console started at 2020-04-17 19:12:51 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.145: INFO: 	Container console ready: true, restart count 0
Apr 17 21:04:04.145: INFO: ibmcloud-block-storage-driver-6jgj5 from kube-system started at 2020-04-17 19:10:47 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.146: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Apr 17 21:04:04.146: INFO: multus-admission-controller-72gwb from openshift-multus started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.146: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr 17 21:04:04.146: INFO: community-operators-5574589d55-swkx4 from openshift-marketplace started at 2020-04-17 19:12:21 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.146: INFO: 	Container community-operators ready: true, restart count 0
Apr 17 21:04:04.146: INFO: redhat-operators-7b557fbbc4-pq5mj from openshift-marketplace started at 2020-04-17 19:12:24 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.146: INFO: 	Container redhat-operators ready: true, restart count 0
Apr 17 21:04:04.146: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-04-17 19:17:47 +0000 UTC (7 container statuses recorded)
Apr 17 21:04:04.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:04:04.146: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 21:04:04.146: INFO: 	Container prometheus ready: true, restart count 1
Apr 17 21:04:04.146: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Apr 17 21:04:04.146: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr 17 21:04:04.146: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Apr 17 21:04:04.146: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr 17 21:04:04.146: INFO: cluster-samples-operator-6d4b68977f-5q62h from openshift-cluster-samples-operator started at 2020-04-17 19:11:57 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:04.146: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Apr 17 21:04:04.146: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Apr 17 21:04:04.146: INFO: node-exporter-lh6vl from openshift-monitoring started at 2020-04-17 19:11:30 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:04.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:04:04.146: INFO: 	Container node-exporter ready: true, restart count 0
Apr 17 21:04:04.146: INFO: prometheus-operator-75c7889c9b-47f2p from openshift-monitoring started at 2020-04-17 19:16:29 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.146: INFO: 	Container prometheus-operator ready: true, restart count 0
Apr 17 21:04:04.146: INFO: ibm-master-proxy-static-10.72.119.98 from kube-system started at 2020-04-17 19:10:37 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:04.146: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr 17 21:04:04.146: INFO: 	Container pause ready: true, restart count 0
Apr 17 21:04:04.146: INFO: sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-kbntn from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 21:04:04.146: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:04:04.146: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:04:04.146: INFO: telemeter-client-5f4f6fb5fc-rwg74 from openshift-monitoring started at 2020-04-17 19:16:40 +0000 UTC (3 container statuses recorded)
Apr 17 21:04:04.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:04:04.146: INFO: 	Container reload ready: true, restart count 0
Apr 17 21:04:04.146: INFO: 	Container telemeter-client ready: true, restart count 0
Apr 17 21:04:04.146: INFO: multus-lddbc from openshift-multus started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.146: INFO: 	Container kube-multus ready: true, restart count 0
Apr 17 21:04:04.146: INFO: calico-typha-76b588567c-kfx7z from calico-system started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.146: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 21:04:04.146: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-04-17 19:17:09 +0000 UTC (3 container statuses recorded)
Apr 17 21:04:04.146: INFO: 	Container alertmanager ready: true, restart count 0
Apr 17 21:04:04.146: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr 17 21:04:04.146: INFO: 	Container config-reloader ready: true, restart count 0
Apr 17 21:04:04.146: INFO: thanos-querier-5c84fd9f68-vbdc4 from openshift-monitoring started at 2020-04-17 19:17:49 +0000 UTC (4 container statuses recorded)
Apr 17 21:04:04.146: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:04:04.146: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr 17 21:04:04.146: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 21:04:04.146: INFO: 	Container thanos-querier ready: true, restart count 0
Apr 17 21:04:04.146: INFO: ibm-keepalived-watcher-h2bpf from kube-system started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 21:04:04.146: INFO: 	Container keepalived-watcher ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1606b749552323a3], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:04:05.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7536" for this suite.
Apr 17 21:04:13.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:04:15.485: INFO: namespace sched-pred-7536 deletion completed in 10.170765104s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:11.964 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:04:15.485: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 17 21:04:15.761: INFO: Waiting up to 5m0s for pod "pod-98f8c1f0-00e0-406f-bb7b-a8e1d6dd0e39" in namespace "emptydir-6555" to be "success or failure"
Apr 17 21:04:15.774: INFO: Pod "pod-98f8c1f0-00e0-406f-bb7b-a8e1d6dd0e39": Phase="Pending", Reason="", readiness=false. Elapsed: 13.227475ms
Apr 17 21:04:17.787: INFO: Pod "pod-98f8c1f0-00e0-406f-bb7b-a8e1d6dd0e39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026070514s
STEP: Saw pod success
Apr 17 21:04:17.787: INFO: Pod "pod-98f8c1f0-00e0-406f-bb7b-a8e1d6dd0e39" satisfied condition "success or failure"
Apr 17 21:04:17.798: INFO: Trying to get logs from node 10.72.119.74 pod pod-98f8c1f0-00e0-406f-bb7b-a8e1d6dd0e39 container test-container: <nil>
STEP: delete the pod
Apr 17 21:04:17.867: INFO: Waiting for pod pod-98f8c1f0-00e0-406f-bb7b-a8e1d6dd0e39 to disappear
Apr 17 21:04:17.878: INFO: Pod pod-98f8c1f0-00e0-406f-bb7b-a8e1d6dd0e39 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:04:17.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6555" for this suite.
Apr 17 21:04:25.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:04:28.114: INFO: namespace emptydir-6555 deletion completed in 10.203908907s

â€¢ [SLOW TEST:12.630 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:04:28.115: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 21:04:28.306: INFO: Waiting up to 5m0s for pod "downwardapi-volume-314f49cd-0fc6-444c-a72b-9a759f03f833" in namespace "projected-2610" to be "success or failure"
Apr 17 21:04:28.317: INFO: Pod "downwardapi-volume-314f49cd-0fc6-444c-a72b-9a759f03f833": Phase="Pending", Reason="", readiness=false. Elapsed: 10.558126ms
Apr 17 21:04:30.332: INFO: Pod "downwardapi-volume-314f49cd-0fc6-444c-a72b-9a759f03f833": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025773081s
STEP: Saw pod success
Apr 17 21:04:30.332: INFO: Pod "downwardapi-volume-314f49cd-0fc6-444c-a72b-9a759f03f833" satisfied condition "success or failure"
Apr 17 21:04:30.344: INFO: Trying to get logs from node 10.72.119.74 pod downwardapi-volume-314f49cd-0fc6-444c-a72b-9a759f03f833 container client-container: <nil>
STEP: delete the pod
Apr 17 21:04:30.428: INFO: Waiting for pod downwardapi-volume-314f49cd-0fc6-444c-a72b-9a759f03f833 to disappear
Apr 17 21:04:30.440: INFO: Pod downwardapi-volume-314f49cd-0fc6-444c-a72b-9a759f03f833 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:04:30.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2610" for this suite.
Apr 17 21:04:38.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:04:40.627: INFO: namespace projected-2610 deletion completed in 10.149659061s

â€¢ [SLOW TEST:12.512 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:04:40.627: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:04:40.942: INFO: Create a RollingUpdate DaemonSet
Apr 17 21:04:40.967: INFO: Check that daemon pods launch on every node of the cluster
Apr 17 21:04:41.003: INFO: Number of nodes with available pods: 0
Apr 17 21:04:41.003: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:04:42.036: INFO: Number of nodes with available pods: 0
Apr 17 21:04:42.036: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:04:43.035: INFO: Number of nodes with available pods: 0
Apr 17 21:04:43.035: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:04:44.037: INFO: Number of nodes with available pods: 2
Apr 17 21:04:44.037: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:04:45.040: INFO: Number of nodes with available pods: 2
Apr 17 21:04:45.040: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:04:46.046: INFO: Number of nodes with available pods: 2
Apr 17 21:04:46.046: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:04:47.034: INFO: Number of nodes with available pods: 2
Apr 17 21:04:47.034: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:04:48.037: INFO: Number of nodes with available pods: 2
Apr 17 21:04:48.037: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:04:49.051: INFO: Number of nodes with available pods: 2
Apr 17 21:04:49.052: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:04:50.034: INFO: Number of nodes with available pods: 2
Apr 17 21:04:50.034: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:04:51.039: INFO: Number of nodes with available pods: 2
Apr 17 21:04:51.039: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:04:52.046: INFO: Number of nodes with available pods: 3
Apr 17 21:04:52.046: INFO: Number of running nodes: 3, number of available pods: 3
Apr 17 21:04:52.046: INFO: Update the DaemonSet to trigger a rollout
Apr 17 21:04:52.142: INFO: Updating DaemonSet daemon-set
Apr 17 21:05:03.205: INFO: Roll back the DaemonSet before rollout is complete
Apr 17 21:05:03.230: INFO: Updating DaemonSet daemon-set
Apr 17 21:05:03.230: INFO: Make sure DaemonSet rollback is complete
Apr 17 21:05:03.244: INFO: Wrong image for pod: daemon-set-r9xrj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 17 21:05:03.244: INFO: Pod daemon-set-r9xrj is not available
Apr 17 21:05:04.274: INFO: Wrong image for pod: daemon-set-r9xrj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 17 21:05:04.275: INFO: Pod daemon-set-r9xrj is not available
Apr 17 21:05:05.273: INFO: Wrong image for pod: daemon-set-r9xrj. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Apr 17 21:05:05.273: INFO: Pod daemon-set-r9xrj is not available
Apr 17 21:05:06.273: INFO: Pod daemon-set-9wpj5 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7772, will wait for the garbage collector to delete the pods
Apr 17 21:05:06.412: INFO: Deleting DaemonSet.extensions daemon-set took: 21.83387ms
Apr 17 21:05:07.012: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.251699ms
Apr 17 21:06:22.426: INFO: Number of nodes with available pods: 0
Apr 17 21:06:22.426: INFO: Number of running nodes: 0, number of available pods: 0
Apr 17 21:06:22.439: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7772/daemonsets","resourceVersion":"49673"},"items":null}

Apr 17 21:06:22.453: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7772/pods","resourceVersion":"49673"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:06:22.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7772" for this suite.
Apr 17 21:06:30.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:06:32.850: INFO: namespace daemonsets-7772 deletion completed in 10.29832156s

â€¢ [SLOW TEST:112.224 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:06:32.852: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-34a681bf-c9fb-4f2d-a64e-0b443037a737
STEP: Creating a pod to test consume configMaps
Apr 17 21:06:33.085: INFO: Waiting up to 5m0s for pod "pod-configmaps-6810a062-bdf9-48b7-b165-0e70f6feb77d" in namespace "configmap-4644" to be "success or failure"
Apr 17 21:06:33.096: INFO: Pod "pod-configmaps-6810a062-bdf9-48b7-b165-0e70f6feb77d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.677483ms
Apr 17 21:06:35.109: INFO: Pod "pod-configmaps-6810a062-bdf9-48b7-b165-0e70f6feb77d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024110783s
STEP: Saw pod success
Apr 17 21:06:35.110: INFO: Pod "pod-configmaps-6810a062-bdf9-48b7-b165-0e70f6feb77d" satisfied condition "success or failure"
Apr 17 21:06:35.121: INFO: Trying to get logs from node 10.72.119.74 pod pod-configmaps-6810a062-bdf9-48b7-b165-0e70f6feb77d container configmap-volume-test: <nil>
STEP: delete the pod
Apr 17 21:06:35.234: INFO: Waiting for pod pod-configmaps-6810a062-bdf9-48b7-b165-0e70f6feb77d to disappear
Apr 17 21:06:35.248: INFO: Pod pod-configmaps-6810a062-bdf9-48b7-b165-0e70f6feb77d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:06:35.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4644" for this suite.
Apr 17 21:06:43.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:06:45.604: INFO: namespace configmap-4644 deletion completed in 10.332075269s

â€¢ [SLOW TEST:12.752 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:06:45.604: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Apr 17 21:06:45.906: INFO: Waiting up to 5m0s for pod "client-containers-0ac0868c-34b8-466e-b553-0740e2d99db2" in namespace "containers-3219" to be "success or failure"
Apr 17 21:06:45.953: INFO: Pod "client-containers-0ac0868c-34b8-466e-b553-0740e2d99db2": Phase="Pending", Reason="", readiness=false. Elapsed: 47.076415ms
Apr 17 21:06:47.965: INFO: Pod "client-containers-0ac0868c-34b8-466e-b553-0740e2d99db2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.059005766s
STEP: Saw pod success
Apr 17 21:06:47.965: INFO: Pod "client-containers-0ac0868c-34b8-466e-b553-0740e2d99db2" satisfied condition "success or failure"
Apr 17 21:06:47.976: INFO: Trying to get logs from node 10.72.119.74 pod client-containers-0ac0868c-34b8-466e-b553-0740e2d99db2 container test-container: <nil>
STEP: delete the pod
Apr 17 21:06:48.041: INFO: Waiting for pod client-containers-0ac0868c-34b8-466e-b553-0740e2d99db2 to disappear
Apr 17 21:06:48.056: INFO: Pod client-containers-0ac0868c-34b8-466e-b553-0740e2d99db2 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:06:48.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3219" for this suite.
Apr 17 21:06:56.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:06:58.306: INFO: namespace containers-3219 deletion completed in 10.227145562s

â€¢ [SLOW TEST:12.702 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:06:58.306: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 21:06:59.412: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 17 21:07:01.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722754419, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722754419, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722754419, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722754419, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 21:07:04.532: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:07:04.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3081" for this suite.
Apr 17 21:07:12.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:07:14.775: INFO: namespace webhook-3081 deletion completed in 10.178318378s
STEP: Destroying namespace "webhook-3081-markers" for this suite.
Apr 17 21:07:22.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:07:24.990: INFO: namespace webhook-3081-markers deletion completed in 10.214206457s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:26.762 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:07:25.072: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:07:25.401: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"694c6057-1a37-4fed-b732-cdaa81c67898", Controller:(*bool)(0xc00b8867a2), BlockOwnerDeletion:(*bool)(0xc00b8867a3)}}
Apr 17 21:07:25.424: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"0b4c55ef-50fa-4f6e-8f5d-3f313853988f", Controller:(*bool)(0xc0064c98c2), BlockOwnerDeletion:(*bool)(0xc0064c98c3)}}
Apr 17 21:07:25.454: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"bd0bf5d4-6a74-418b-b04e-ec82f72c8d5d", Controller:(*bool)(0xc00b9722e6), BlockOwnerDeletion:(*bool)(0xc00b9722e7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:07:30.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1543" for this suite.
Apr 17 21:07:38.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:07:40.736: INFO: namespace gc-1543 deletion completed in 10.220803432s

â€¢ [SLOW TEST:15.664 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:07:40.737: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 21:07:40.930: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04941149-6115-4b6b-8ec7-d253700dce10" in namespace "projected-7415" to be "success or failure"
Apr 17 21:07:40.943: INFO: Pod "downwardapi-volume-04941149-6115-4b6b-8ec7-d253700dce10": Phase="Pending", Reason="", readiness=false. Elapsed: 13.537135ms
Apr 17 21:07:42.985: INFO: Pod "downwardapi-volume-04941149-6115-4b6b-8ec7-d253700dce10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.055204021s
STEP: Saw pod success
Apr 17 21:07:42.985: INFO: Pod "downwardapi-volume-04941149-6115-4b6b-8ec7-d253700dce10" satisfied condition "success or failure"
Apr 17 21:07:43.001: INFO: Trying to get logs from node 10.72.119.74 pod downwardapi-volume-04941149-6115-4b6b-8ec7-d253700dce10 container client-container: <nil>
STEP: delete the pod
Apr 17 21:07:43.067: INFO: Waiting for pod downwardapi-volume-04941149-6115-4b6b-8ec7-d253700dce10 to disappear
Apr 17 21:07:43.078: INFO: Pod downwardapi-volume-04941149-6115-4b6b-8ec7-d253700dce10 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:07:43.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7415" for this suite.
Apr 17 21:07:51.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:07:53.364: INFO: namespace projected-7415 deletion completed in 10.264767922s

â€¢ [SLOW TEST:12.627 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:07:53.365: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-ba8fe767-eb54-47c2-9b54-13266a2d7821 in namespace container-probe-4249
Apr 17 21:07:57.624: INFO: Started pod liveness-ba8fe767-eb54-47c2-9b54-13266a2d7821 in namespace container-probe-4249
STEP: checking the pod's current state and verifying that restartCount is present
Apr 17 21:07:57.638: INFO: Initial restart count of pod liveness-ba8fe767-eb54-47c2-9b54-13266a2d7821 is 0
Apr 17 21:08:11.747: INFO: Restart count of pod container-probe-4249/liveness-ba8fe767-eb54-47c2-9b54-13266a2d7821 is now 1 (14.108075237s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:08:11.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4249" for this suite.
Apr 17 21:08:19.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:08:22.162: INFO: namespace container-probe-4249 deletion completed in 10.342021622s

â€¢ [SLOW TEST:28.797 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:08:22.163: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 21:08:22.386: INFO: Waiting up to 5m0s for pod "downwardapi-volume-feb17779-6061-43ef-8baa-ccb454f32460" in namespace "downward-api-1369" to be "success or failure"
Apr 17 21:08:22.399: INFO: Pod "downwardapi-volume-feb17779-6061-43ef-8baa-ccb454f32460": Phase="Pending", Reason="", readiness=false. Elapsed: 13.729481ms
Apr 17 21:08:24.414: INFO: Pod "downwardapi-volume-feb17779-6061-43ef-8baa-ccb454f32460": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028018762s
STEP: Saw pod success
Apr 17 21:08:24.414: INFO: Pod "downwardapi-volume-feb17779-6061-43ef-8baa-ccb454f32460" satisfied condition "success or failure"
Apr 17 21:08:24.425: INFO: Trying to get logs from node 10.72.119.74 pod downwardapi-volume-feb17779-6061-43ef-8baa-ccb454f32460 container client-container: <nil>
STEP: delete the pod
Apr 17 21:08:24.490: INFO: Waiting for pod downwardapi-volume-feb17779-6061-43ef-8baa-ccb454f32460 to disappear
Apr 17 21:08:24.508: INFO: Pod downwardapi-volume-feb17779-6061-43ef-8baa-ccb454f32460 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:08:24.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1369" for this suite.
Apr 17 21:08:32.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:08:34.724: INFO: namespace downward-api-1369 deletion completed in 10.184241763s

â€¢ [SLOW TEST:12.561 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:08:34.724: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-05479858-40b5-42a2-a7bd-e4e92d0163ba
STEP: Creating a pod to test consume configMaps
Apr 17 21:08:34.946: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-062025ca-20c3-40b3-827e-b84ee81792ef" in namespace "projected-6417" to be "success or failure"
Apr 17 21:08:34.959: INFO: Pod "pod-projected-configmaps-062025ca-20c3-40b3-827e-b84ee81792ef": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011564ms
Apr 17 21:08:36.972: INFO: Pod "pod-projected-configmaps-062025ca-20c3-40b3-827e-b84ee81792ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025186603s
Apr 17 21:08:38.988: INFO: Pod "pod-projected-configmaps-062025ca-20c3-40b3-827e-b84ee81792ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041286352s
STEP: Saw pod success
Apr 17 21:08:38.988: INFO: Pod "pod-projected-configmaps-062025ca-20c3-40b3-827e-b84ee81792ef" satisfied condition "success or failure"
Apr 17 21:08:39.000: INFO: Trying to get logs from node 10.72.119.74 pod pod-projected-configmaps-062025ca-20c3-40b3-827e-b84ee81792ef container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 17 21:08:39.074: INFO: Waiting for pod pod-projected-configmaps-062025ca-20c3-40b3-827e-b84ee81792ef to disappear
Apr 17 21:08:39.092: INFO: Pod pod-projected-configmaps-062025ca-20c3-40b3-827e-b84ee81792ef no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:08:39.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6417" for this suite.
Apr 17 21:08:47.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:08:49.375: INFO: namespace projected-6417 deletion completed in 10.239481069s

â€¢ [SLOW TEST:14.652 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:08:49.376: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Apr 17 21:08:51.256: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:08:51.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0417 21:08:51.256251      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-8499" for this suite.
Apr 17 21:08:59.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:09:01.535: INFO: namespace gc-8499 deletion completed in 10.249411404s

â€¢ [SLOW TEST:12.159 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:09:01.538: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-20eb2539-6f6d-426c-9070-3d9e8efdbf39
STEP: Creating a pod to test consume secrets
Apr 17 21:09:01.742: INFO: Waiting up to 5m0s for pod "pod-secrets-22bbe9fd-b6ac-425d-8c50-61b6c25a0bb4" in namespace "secrets-4253" to be "success or failure"
Apr 17 21:09:01.754: INFO: Pod "pod-secrets-22bbe9fd-b6ac-425d-8c50-61b6c25a0bb4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.037704ms
Apr 17 21:09:03.767: INFO: Pod "pod-secrets-22bbe9fd-b6ac-425d-8c50-61b6c25a0bb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024422824s
Apr 17 21:09:05.784: INFO: Pod "pod-secrets-22bbe9fd-b6ac-425d-8c50-61b6c25a0bb4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041307105s
STEP: Saw pod success
Apr 17 21:09:05.784: INFO: Pod "pod-secrets-22bbe9fd-b6ac-425d-8c50-61b6c25a0bb4" satisfied condition "success or failure"
Apr 17 21:09:05.802: INFO: Trying to get logs from node 10.72.119.74 pod pod-secrets-22bbe9fd-b6ac-425d-8c50-61b6c25a0bb4 container secret-volume-test: <nil>
STEP: delete the pod
Apr 17 21:09:05.871: INFO: Waiting for pod pod-secrets-22bbe9fd-b6ac-425d-8c50-61b6c25a0bb4 to disappear
Apr 17 21:09:05.881: INFO: Pod pod-secrets-22bbe9fd-b6ac-425d-8c50-61b6c25a0bb4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:09:05.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4253" for this suite.
Apr 17 21:09:13.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:09:16.103: INFO: namespace secrets-4253 deletion completed in 10.193211531s

â€¢ [SLOW TEST:14.566 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:09:16.104: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-223ac14f-c855-46b4-b3a1-3469813b707a
STEP: Creating a pod to test consume configMaps
Apr 17 21:09:16.330: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4d557428-d2ee-4afb-b279-e05b1871c426" in namespace "projected-3366" to be "success or failure"
Apr 17 21:09:16.341: INFO: Pod "pod-projected-configmaps-4d557428-d2ee-4afb-b279-e05b1871c426": Phase="Pending", Reason="", readiness=false. Elapsed: 10.431673ms
Apr 17 21:09:18.354: INFO: Pod "pod-projected-configmaps-4d557428-d2ee-4afb-b279-e05b1871c426": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023659158s
STEP: Saw pod success
Apr 17 21:09:18.354: INFO: Pod "pod-projected-configmaps-4d557428-d2ee-4afb-b279-e05b1871c426" satisfied condition "success or failure"
Apr 17 21:09:18.367: INFO: Trying to get logs from node 10.72.119.74 pod pod-projected-configmaps-4d557428-d2ee-4afb-b279-e05b1871c426 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 17 21:09:18.429: INFO: Waiting for pod pod-projected-configmaps-4d557428-d2ee-4afb-b279-e05b1871c426 to disappear
Apr 17 21:09:18.439: INFO: Pod pod-projected-configmaps-4d557428-d2ee-4afb-b279-e05b1871c426 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:09:18.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3366" for this suite.
Apr 17 21:09:26.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:09:28.645: INFO: namespace projected-3366 deletion completed in 10.169884462s

â€¢ [SLOW TEST:12.542 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:09:28.646: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:09:28.837: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-f6fb521a-c271-492a-8f5a-7e5abc95cec9" in namespace "security-context-test-4840" to be "success or failure"
Apr 17 21:09:28.854: INFO: Pod "busybox-privileged-false-f6fb521a-c271-492a-8f5a-7e5abc95cec9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.822622ms
Apr 17 21:09:30.866: INFO: Pod "busybox-privileged-false-f6fb521a-c271-492a-8f5a-7e5abc95cec9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028735118s
Apr 17 21:09:30.866: INFO: Pod "busybox-privileged-false-f6fb521a-c271-492a-8f5a-7e5abc95cec9" satisfied condition "success or failure"
Apr 17 21:09:30.891: INFO: Got logs for pod "busybox-privileged-false-f6fb521a-c271-492a-8f5a-7e5abc95cec9": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:09:30.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4840" for this suite.
Apr 17 21:09:38.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:09:41.145: INFO: namespace security-context-test-4840 deletion completed in 10.202999491s

â€¢ [SLOW TEST:12.499 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:09:41.146: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr 17 21:09:41.837: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 21:09:44.929: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:09:44.952: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:09:46.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-6829" for this suite.
Apr 17 21:09:54.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:09:56.595: INFO: namespace crd-webhook-6829 deletion completed in 10.161016282s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:15.520 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:09:56.668: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 21:09:56.881: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b58eb529-8c8f-44be-8a62-f382222ed3b7" in namespace "projected-5231" to be "success or failure"
Apr 17 21:09:56.899: INFO: Pod "downwardapi-volume-b58eb529-8c8f-44be-8a62-f382222ed3b7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.679054ms
Apr 17 21:09:58.911: INFO: Pod "downwardapi-volume-b58eb529-8c8f-44be-8a62-f382222ed3b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029059363s
STEP: Saw pod success
Apr 17 21:09:58.911: INFO: Pod "downwardapi-volume-b58eb529-8c8f-44be-8a62-f382222ed3b7" satisfied condition "success or failure"
Apr 17 21:09:58.922: INFO: Trying to get logs from node 10.72.119.72 pod downwardapi-volume-b58eb529-8c8f-44be-8a62-f382222ed3b7 container client-container: <nil>
STEP: delete the pod
Apr 17 21:09:59.020: INFO: Waiting for pod downwardapi-volume-b58eb529-8c8f-44be-8a62-f382222ed3b7 to disappear
Apr 17 21:09:59.033: INFO: Pod downwardapi-volume-b58eb529-8c8f-44be-8a62-f382222ed3b7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:09:59.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5231" for this suite.
Apr 17 21:10:07.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:10:09.382: INFO: namespace projected-5231 deletion completed in 10.321188712s

â€¢ [SLOW TEST:12.714 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:10:09.382: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Apr 17 21:10:09.595: INFO: Waiting up to 5m0s for pod "pod-a6fddf32-dc00-4d2d-ac4f-b3434194a6bd" in namespace "emptydir-8931" to be "success or failure"
Apr 17 21:10:09.607: INFO: Pod "pod-a6fddf32-dc00-4d2d-ac4f-b3434194a6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.035191ms
Apr 17 21:10:11.647: INFO: Pod "pod-a6fddf32-dc00-4d2d-ac4f-b3434194a6bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.05155814s
STEP: Saw pod success
Apr 17 21:10:11.647: INFO: Pod "pod-a6fddf32-dc00-4d2d-ac4f-b3434194a6bd" satisfied condition "success or failure"
Apr 17 21:10:11.691: INFO: Trying to get logs from node 10.72.119.74 pod pod-a6fddf32-dc00-4d2d-ac4f-b3434194a6bd container test-container: <nil>
STEP: delete the pod
Apr 17 21:10:11.754: INFO: Waiting for pod pod-a6fddf32-dc00-4d2d-ac4f-b3434194a6bd to disappear
Apr 17 21:10:11.770: INFO: Pod pod-a6fddf32-dc00-4d2d-ac4f-b3434194a6bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:10:11.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8931" for this suite.
Apr 17 21:10:19.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:10:22.111: INFO: namespace emptydir-8931 deletion completed in 10.282996048s

â€¢ [SLOW TEST:12.729 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:10:22.112: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-e09796ea-f0aa-44ef-9608-31b72dcf5fb7
STEP: Creating a pod to test consume configMaps
Apr 17 21:10:22.360: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9bef270-e73d-4d02-be9f-39f703c87550" in namespace "configmap-6850" to be "success or failure"
Apr 17 21:10:22.374: INFO: Pod "pod-configmaps-e9bef270-e73d-4d02-be9f-39f703c87550": Phase="Pending", Reason="", readiness=false. Elapsed: 13.347255ms
Apr 17 21:10:24.387: INFO: Pod "pod-configmaps-e9bef270-e73d-4d02-be9f-39f703c87550": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026781267s
Apr 17 21:10:26.399: INFO: Pod "pod-configmaps-e9bef270-e73d-4d02-be9f-39f703c87550": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038236859s
STEP: Saw pod success
Apr 17 21:10:26.399: INFO: Pod "pod-configmaps-e9bef270-e73d-4d02-be9f-39f703c87550" satisfied condition "success or failure"
Apr 17 21:10:26.411: INFO: Trying to get logs from node 10.72.119.74 pod pod-configmaps-e9bef270-e73d-4d02-be9f-39f703c87550 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 17 21:10:26.484: INFO: Waiting for pod pod-configmaps-e9bef270-e73d-4d02-be9f-39f703c87550 to disappear
Apr 17 21:10:26.495: INFO: Pod pod-configmaps-e9bef270-e73d-4d02-be9f-39f703c87550 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:10:26.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6850" for this suite.
Apr 17 21:10:34.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:10:36.758: INFO: namespace configmap-6850 deletion completed in 10.236571475s

â€¢ [SLOW TEST:14.646 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:10:36.758: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-c673c853-4858-4348-8e29-0e41a7c48759 in namespace container-probe-74
Apr 17 21:10:41.066: INFO: Started pod test-webserver-c673c853-4858-4348-8e29-0e41a7c48759 in namespace container-probe-74
STEP: checking the pod's current state and verifying that restartCount is present
Apr 17 21:10:41.078: INFO: Initial restart count of pod test-webserver-c673c853-4858-4348-8e29-0e41a7c48759 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:14:42.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-74" for this suite.
Apr 17 21:14:50.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:14:52.719: INFO: namespace container-probe-74 deletion completed in 10.271161587s

â€¢ [SLOW TEST:255.961 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:14:52.720: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 17 21:14:54.958: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:14:55.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7147" for this suite.
Apr 17 21:15:03.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:15:05.171: INFO: namespace container-runtime-7147 deletion completed in 10.137326316s

â€¢ [SLOW TEST:12.451 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:15:05.171: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 17 21:15:09.571: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 17 21:15:09.583: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 17 21:15:11.583: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 17 21:15:11.595: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 17 21:15:13.583: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 17 21:15:13.596: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 17 21:15:15.583: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 17 21:15:15.595: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 17 21:15:17.583: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 17 21:15:17.654: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:15:17.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2411" for this suite.
Apr 17 21:15:49.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:15:51.870: INFO: namespace container-lifecycle-hook-2411 deletion completed in 34.190726829s

â€¢ [SLOW TEST:46.699 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:15:51.871: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:16:00.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5702" for this suite.
Apr 17 21:16:08.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:16:10.675: INFO: namespace namespaces-5702 deletion completed in 10.162513424s
STEP: Destroying namespace "nsdeletetest-6833" for this suite.
Apr 17 21:16:10.686: INFO: Namespace nsdeletetest-6833 was already deleted
STEP: Destroying namespace "nsdeletetest-3540" for this suite.
Apr 17 21:16:18.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:16:20.908: INFO: namespace nsdeletetest-3540 deletion completed in 10.221645065s

â€¢ [SLOW TEST:29.037 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:16:20.911: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:16:21.103: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-c9fdf071-09f2-42b8-88f4-0e51247a1dac
STEP: Creating secret with name s-test-opt-upd-8fb27619-913e-42aa-b9a8-e961a0d886a3
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c9fdf071-09f2-42b8-88f4-0e51247a1dac
STEP: Updating secret s-test-opt-upd-8fb27619-913e-42aa-b9a8-e961a0d886a3
STEP: Creating secret with name s-test-opt-create-3a71020b-0f75-4201-8e65-8061b8625933
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:17:36.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4610" for this suite.
Apr 17 21:17:52.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:17:54.853: INFO: namespace secrets-4610 deletion completed in 18.194380679s

â€¢ [SLOW TEST:93.943 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:17:54.860: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Apr 17 21:17:57.779: INFO: Successfully updated pod "labelsupdateb1f229e0-a126-433c-b86d-ea65c2a0247e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:17:59.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1662" for this suite.
Apr 17 21:18:13.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:18:16.228: INFO: namespace downward-api-1662 deletion completed in 16.365932751s

â€¢ [SLOW TEST:21.369 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:18:16.232: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:18:16.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3797" for this suite.
Apr 17 21:18:24.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:18:26.596: INFO: namespace tables-3797 deletion completed in 10.165956451s

â€¢ [SLOW TEST:10.364 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:18:26.596: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 21:18:26.813: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bb70446e-d05a-4583-8fed-9e1f9f4da6ee" in namespace "downward-api-6317" to be "success or failure"
Apr 17 21:18:26.824: INFO: Pod "downwardapi-volume-bb70446e-d05a-4583-8fed-9e1f9f4da6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 11.511515ms
Apr 17 21:18:28.836: INFO: Pod "downwardapi-volume-bb70446e-d05a-4583-8fed-9e1f9f4da6ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022948471s
Apr 17 21:18:30.847: INFO: Pod "downwardapi-volume-bb70446e-d05a-4583-8fed-9e1f9f4da6ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034550227s
STEP: Saw pod success
Apr 17 21:18:30.847: INFO: Pod "downwardapi-volume-bb70446e-d05a-4583-8fed-9e1f9f4da6ee" satisfied condition "success or failure"
Apr 17 21:18:30.857: INFO: Trying to get logs from node 10.72.119.74 pod downwardapi-volume-bb70446e-d05a-4583-8fed-9e1f9f4da6ee container client-container: <nil>
STEP: delete the pod
Apr 17 21:18:30.925: INFO: Waiting for pod downwardapi-volume-bb70446e-d05a-4583-8fed-9e1f9f4da6ee to disappear
Apr 17 21:18:30.936: INFO: Pod downwardapi-volume-bb70446e-d05a-4583-8fed-9e1f9f4da6ee no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:18:30.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6317" for this suite.
Apr 17 21:18:39.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:18:41.145: INFO: namespace downward-api-6317 deletion completed in 10.179130828s

â€¢ [SLOW TEST:14.549 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:18:41.145: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:18:43.471: INFO: Waiting up to 5m0s for pod "client-envvars-fa085857-699c-436c-be7b-8af6730278f8" in namespace "pods-1319" to be "success or failure"
Apr 17 21:18:43.490: INFO: Pod "client-envvars-fa085857-699c-436c-be7b-8af6730278f8": Phase="Pending", Reason="", readiness=false. Elapsed: 19.668513ms
Apr 17 21:18:45.503: INFO: Pod "client-envvars-fa085857-699c-436c-be7b-8af6730278f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032572061s
Apr 17 21:18:47.515: INFO: Pod "client-envvars-fa085857-699c-436c-be7b-8af6730278f8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044683387s
Apr 17 21:18:49.529: INFO: Pod "client-envvars-fa085857-699c-436c-be7b-8af6730278f8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058030944s
Apr 17 21:18:51.540: INFO: Pod "client-envvars-fa085857-699c-436c-be7b-8af6730278f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.069153535s
STEP: Saw pod success
Apr 17 21:18:51.540: INFO: Pod "client-envvars-fa085857-699c-436c-be7b-8af6730278f8" satisfied condition "success or failure"
Apr 17 21:18:51.551: INFO: Trying to get logs from node 10.72.119.98 pod client-envvars-fa085857-699c-436c-be7b-8af6730278f8 container env3cont: <nil>
STEP: delete the pod
Apr 17 21:18:51.667: INFO: Waiting for pod client-envvars-fa085857-699c-436c-be7b-8af6730278f8 to disappear
Apr 17 21:18:51.685: INFO: Pod client-envvars-fa085857-699c-436c-be7b-8af6730278f8 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:18:51.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1319" for this suite.
Apr 17 21:19:23.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:19:25.966: INFO: namespace pods-1319 deletion completed in 34.255149828s

â€¢ [SLOW TEST:44.821 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:19:25.966: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:19:26.141: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:19:27.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2042" for this suite.
Apr 17 21:19:35.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:19:37.539: INFO: namespace custom-resource-definition-2042 deletion completed in 10.211430888s

â€¢ [SLOW TEST:11.573 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:19:37.539: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-30f2a7c7-ebb0-4555-8d27-0aa032306c83
STEP: Creating a pod to test consume secrets
Apr 17 21:19:37.736: INFO: Waiting up to 5m0s for pod "pod-secrets-b644c5a8-31ac-48ac-8527-1ad5f2b828f9" in namespace "secrets-6490" to be "success or failure"
Apr 17 21:19:37.757: INFO: Pod "pod-secrets-b644c5a8-31ac-48ac-8527-1ad5f2b828f9": Phase="Pending", Reason="", readiness=false. Elapsed: 21.076388ms
Apr 17 21:19:39.769: INFO: Pod "pod-secrets-b644c5a8-31ac-48ac-8527-1ad5f2b828f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.033593443s
STEP: Saw pod success
Apr 17 21:19:39.770: INFO: Pod "pod-secrets-b644c5a8-31ac-48ac-8527-1ad5f2b828f9" satisfied condition "success or failure"
Apr 17 21:19:39.781: INFO: Trying to get logs from node 10.72.119.74 pod pod-secrets-b644c5a8-31ac-48ac-8527-1ad5f2b828f9 container secret-volume-test: <nil>
STEP: delete the pod
Apr 17 21:19:39.846: INFO: Waiting for pod pod-secrets-b644c5a8-31ac-48ac-8527-1ad5f2b828f9 to disappear
Apr 17 21:19:39.857: INFO: Pod pod-secrets-b644c5a8-31ac-48ac-8527-1ad5f2b828f9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:19:39.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6490" for this suite.
Apr 17 21:19:47.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:19:50.044: INFO: namespace secrets-6490 deletion completed in 10.159268415s

â€¢ [SLOW TEST:12.505 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:19:50.045: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Apr 17 21:19:50.187: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:19:54.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6727" for this suite.
Apr 17 21:20:26.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:20:28.486: INFO: namespace init-container-6727 deletion completed in 34.235744438s

â€¢ [SLOW TEST:38.441 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:20:28.486: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 21:20:28.717: INFO: Waiting up to 5m0s for pod "downwardapi-volume-86bccc27-0b52-42c8-a8d0-6085c65a3748" in namespace "projected-2904" to be "success or failure"
Apr 17 21:20:28.728: INFO: Pod "downwardapi-volume-86bccc27-0b52-42c8-a8d0-6085c65a3748": Phase="Pending", Reason="", readiness=false. Elapsed: 11.650305ms
Apr 17 21:20:30.740: INFO: Pod "downwardapi-volume-86bccc27-0b52-42c8-a8d0-6085c65a3748": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02306624s
STEP: Saw pod success
Apr 17 21:20:30.740: INFO: Pod "downwardapi-volume-86bccc27-0b52-42c8-a8d0-6085c65a3748" satisfied condition "success or failure"
Apr 17 21:20:30.751: INFO: Trying to get logs from node 10.72.119.74 pod downwardapi-volume-86bccc27-0b52-42c8-a8d0-6085c65a3748 container client-container: <nil>
STEP: delete the pod
Apr 17 21:20:30.826: INFO: Waiting for pod downwardapi-volume-86bccc27-0b52-42c8-a8d0-6085c65a3748 to disappear
Apr 17 21:20:30.836: INFO: Pod downwardapi-volume-86bccc27-0b52-42c8-a8d0-6085c65a3748 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:20:30.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2904" for this suite.
Apr 17 21:20:38.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:20:41.332: INFO: namespace projected-2904 deletion completed in 10.452054087s

â€¢ [SLOW TEST:12.846 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:20:41.332: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:20:41.619: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr 17 21:20:41.701: INFO: Number of nodes with available pods: 0
Apr 17 21:20:41.701: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:20:42.751: INFO: Number of nodes with available pods: 0
Apr 17 21:20:42.751: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:20:43.740: INFO: Number of nodes with available pods: 3
Apr 17 21:20:43.740: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr 17 21:20:43.865: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:43.865: INFO: Wrong image for pod: daemon-set-ztkp6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:43.865: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:44.907: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:44.908: INFO: Wrong image for pod: daemon-set-ztkp6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:44.908: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:45.902: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:45.903: INFO: Wrong image for pod: daemon-set-ztkp6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:45.903: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:46.901: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:46.901: INFO: Wrong image for pod: daemon-set-ztkp6. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:46.901: INFO: Pod daemon-set-ztkp6 is not available
Apr 17 21:20:46.901: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:47.900: INFO: Pod daemon-set-9hwv5 is not available
Apr 17 21:20:47.900: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:47.900: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:48.901: INFO: Pod daemon-set-9hwv5 is not available
Apr 17 21:20:48.901: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:48.901: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:49.930: INFO: Pod daemon-set-9hwv5 is not available
Apr 17 21:20:49.930: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:49.930: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:50.902: INFO: Pod daemon-set-9hwv5 is not available
Apr 17 21:20:50.902: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:50.902: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:51.900: INFO: Pod daemon-set-9hwv5 is not available
Apr 17 21:20:51.900: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:51.900: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:52.906: INFO: Pod daemon-set-9hwv5 is not available
Apr 17 21:20:52.906: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:52.906: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:53.901: INFO: Pod daemon-set-9hwv5 is not available
Apr 17 21:20:53.901: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:53.901: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:54.902: INFO: Pod daemon-set-9hwv5 is not available
Apr 17 21:20:54.902: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:54.902: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:55.901: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:55.901: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:56.903: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:56.904: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:56.904: INFO: Pod daemon-set-zw2gc is not available
Apr 17 21:20:57.901: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:57.901: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:57.901: INFO: Pod daemon-set-zw2gc is not available
Apr 17 21:20:58.901: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:58.901: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:58.901: INFO: Pod daemon-set-zw2gc is not available
Apr 17 21:20:59.901: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:59.901: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:20:59.901: INFO: Pod daemon-set-zw2gc is not available
Apr 17 21:21:00.900: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:00.900: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:00.900: INFO: Pod daemon-set-zw2gc is not available
Apr 17 21:21:01.903: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:01.903: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:01.903: INFO: Pod daemon-set-zw2gc is not available
Apr 17 21:21:02.903: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:02.903: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:02.903: INFO: Pod daemon-set-zw2gc is not available
Apr 17 21:21:03.900: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:03.900: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:03.900: INFO: Pod daemon-set-zw2gc is not available
Apr 17 21:21:04.900: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:04.900: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:04.900: INFO: Pod daemon-set-zw2gc is not available
Apr 17 21:21:05.900: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:05.900: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:05.900: INFO: Pod daemon-set-zw2gc is not available
Apr 17 21:21:06.900: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:06.900: INFO: Wrong image for pod: daemon-set-zw2gc. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:06.900: INFO: Pod daemon-set-zw2gc is not available
Apr 17 21:21:07.900: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:08.901: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:08.901: INFO: Pod daemon-set-frvbb is not available
Apr 17 21:21:09.901: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:09.901: INFO: Pod daemon-set-frvbb is not available
Apr 17 21:21:10.900: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:10.900: INFO: Pod daemon-set-frvbb is not available
Apr 17 21:21:11.899: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:11.899: INFO: Pod daemon-set-frvbb is not available
Apr 17 21:21:12.900: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:12.900: INFO: Pod daemon-set-frvbb is not available
Apr 17 21:21:13.902: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:13.902: INFO: Pod daemon-set-frvbb is not available
Apr 17 21:21:14.903: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:14.903: INFO: Pod daemon-set-frvbb is not available
Apr 17 21:21:15.901: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:16.905: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:17.900: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:17.900: INFO: Pod daemon-set-dvqk5 is not available
Apr 17 21:21:18.901: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:18.901: INFO: Pod daemon-set-dvqk5 is not available
Apr 17 21:21:19.902: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:19.902: INFO: Pod daemon-set-dvqk5 is not available
Apr 17 21:21:20.900: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:20.901: INFO: Pod daemon-set-dvqk5 is not available
Apr 17 21:21:21.900: INFO: Wrong image for pod: daemon-set-dvqk5. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Apr 17 21:21:21.900: INFO: Pod daemon-set-dvqk5 is not available
Apr 17 21:21:22.900: INFO: Pod daemon-set-ntnsd is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Apr 17 21:21:22.950: INFO: Number of nodes with available pods: 2
Apr 17 21:21:22.950: INFO: Node 10.72.119.74 is running more than one daemon pod
Apr 17 21:21:24.003: INFO: Number of nodes with available pods: 2
Apr 17 21:21:24.003: INFO: Node 10.72.119.74 is running more than one daemon pod
Apr 17 21:21:24.982: INFO: Number of nodes with available pods: 2
Apr 17 21:21:24.982: INFO: Node 10.72.119.74 is running more than one daemon pod
Apr 17 21:21:25.982: INFO: Number of nodes with available pods: 2
Apr 17 21:21:25.982: INFO: Node 10.72.119.74 is running more than one daemon pod
Apr 17 21:21:26.983: INFO: Number of nodes with available pods: 2
Apr 17 21:21:26.983: INFO: Node 10.72.119.74 is running more than one daemon pod
Apr 17 21:21:27.979: INFO: Number of nodes with available pods: 2
Apr 17 21:21:27.979: INFO: Node 10.72.119.74 is running more than one daemon pod
Apr 17 21:21:28.980: INFO: Number of nodes with available pods: 2
Apr 17 21:21:28.980: INFO: Node 10.72.119.74 is running more than one daemon pod
Apr 17 21:21:29.981: INFO: Number of nodes with available pods: 3
Apr 17 21:21:29.981: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4541, will wait for the garbage collector to delete the pods
Apr 17 21:21:30.135: INFO: Deleting DaemonSet.extensions daemon-set took: 30.881736ms
Apr 17 21:21:30.735: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.322922ms
Apr 17 21:21:42.449: INFO: Number of nodes with available pods: 0
Apr 17 21:21:42.449: INFO: Number of running nodes: 0, number of available pods: 0
Apr 17 21:21:42.462: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4541/daemonsets","resourceVersion":"55937"},"items":null}

Apr 17 21:21:42.473: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4541/pods","resourceVersion":"55937"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:21:42.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4541" for this suite.
Apr 17 21:21:52.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:21:54.761: INFO: namespace daemonsets-4541 deletion completed in 12.206715616s

â€¢ [SLOW TEST:73.428 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:21:54.761: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-71622d98-a72c-4720-80ef-f53eb007a5c2
STEP: Creating a pod to test consume secrets
Apr 17 21:21:55.022: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e8b9e3d6-383a-4c55-80bf-d7f8ab9aa09a" in namespace "projected-3376" to be "success or failure"
Apr 17 21:21:55.034: INFO: Pod "pod-projected-secrets-e8b9e3d6-383a-4c55-80bf-d7f8ab9aa09a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.228709ms
Apr 17 21:21:57.046: INFO: Pod "pod-projected-secrets-e8b9e3d6-383a-4c55-80bf-d7f8ab9aa09a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024417515s
Apr 17 21:21:59.059: INFO: Pod "pod-projected-secrets-e8b9e3d6-383a-4c55-80bf-d7f8ab9aa09a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03677823s
STEP: Saw pod success
Apr 17 21:21:59.059: INFO: Pod "pod-projected-secrets-e8b9e3d6-383a-4c55-80bf-d7f8ab9aa09a" satisfied condition "success or failure"
Apr 17 21:21:59.070: INFO: Trying to get logs from node 10.72.119.74 pod pod-projected-secrets-e8b9e3d6-383a-4c55-80bf-d7f8ab9aa09a container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 17 21:21:59.145: INFO: Waiting for pod pod-projected-secrets-e8b9e3d6-383a-4c55-80bf-d7f8ab9aa09a to disappear
Apr 17 21:21:59.158: INFO: Pod pod-projected-secrets-e8b9e3d6-383a-4c55-80bf-d7f8ab9aa09a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:21:59.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3376" for this suite.
Apr 17 21:22:07.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:22:09.427: INFO: namespace projected-3376 deletion completed in 10.251896009s

â€¢ [SLOW TEST:14.666 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:22:09.429: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-15d6cf4b-2f49-4c22-8d6f-dea3b9d53b60
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:22:09.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-43" for this suite.
Apr 17 21:22:17.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:22:19.786: INFO: namespace configmap-43 deletion completed in 10.192119056s

â€¢ [SLOW TEST:10.357 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:22:19.786: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Apr 17 21:22:19.977: INFO: Waiting up to 5m0s for pod "downward-api-ce6da128-b343-4054-99e6-19b9bd0c8b64" in namespace "downward-api-138" to be "success or failure"
Apr 17 21:22:19.992: INFO: Pod "downward-api-ce6da128-b343-4054-99e6-19b9bd0c8b64": Phase="Pending", Reason="", readiness=false. Elapsed: 14.699678ms
Apr 17 21:22:22.004: INFO: Pod "downward-api-ce6da128-b343-4054-99e6-19b9bd0c8b64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026559783s
STEP: Saw pod success
Apr 17 21:22:22.004: INFO: Pod "downward-api-ce6da128-b343-4054-99e6-19b9bd0c8b64" satisfied condition "success or failure"
Apr 17 21:22:22.015: INFO: Trying to get logs from node 10.72.119.74 pod downward-api-ce6da128-b343-4054-99e6-19b9bd0c8b64 container dapi-container: <nil>
STEP: delete the pod
Apr 17 21:22:22.091: INFO: Waiting for pod downward-api-ce6da128-b343-4054-99e6-19b9bd0c8b64 to disappear
Apr 17 21:22:22.103: INFO: Pod downward-api-ce6da128-b343-4054-99e6-19b9bd0c8b64 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:22:22.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-138" for this suite.
Apr 17 21:22:30.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:22:32.445: INFO: namespace downward-api-138 deletion completed in 10.323633297s

â€¢ [SLOW TEST:12.659 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:22:32.446: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-pvm9
STEP: Creating a pod to test atomic-volume-subpath
Apr 17 21:22:32.743: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pvm9" in namespace "subpath-2515" to be "success or failure"
Apr 17 21:22:32.757: INFO: Pod "pod-subpath-test-projected-pvm9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.255678ms
Apr 17 21:22:34.784: INFO: Pod "pod-subpath-test-projected-pvm9": Phase="Running", Reason="", readiness=true. Elapsed: 2.041556295s
Apr 17 21:22:36.799: INFO: Pod "pod-subpath-test-projected-pvm9": Phase="Running", Reason="", readiness=true. Elapsed: 4.056157814s
Apr 17 21:22:38.811: INFO: Pod "pod-subpath-test-projected-pvm9": Phase="Running", Reason="", readiness=true. Elapsed: 6.068300644s
Apr 17 21:22:40.823: INFO: Pod "pod-subpath-test-projected-pvm9": Phase="Running", Reason="", readiness=true. Elapsed: 8.080400493s
Apr 17 21:22:42.835: INFO: Pod "pod-subpath-test-projected-pvm9": Phase="Running", Reason="", readiness=true. Elapsed: 10.092615852s
Apr 17 21:22:44.858: INFO: Pod "pod-subpath-test-projected-pvm9": Phase="Running", Reason="", readiness=true. Elapsed: 12.114912173s
Apr 17 21:22:46.869: INFO: Pod "pod-subpath-test-projected-pvm9": Phase="Running", Reason="", readiness=true. Elapsed: 14.126248817s
Apr 17 21:22:48.883: INFO: Pod "pod-subpath-test-projected-pvm9": Phase="Running", Reason="", readiness=true. Elapsed: 16.139725484s
Apr 17 21:22:50.894: INFO: Pod "pod-subpath-test-projected-pvm9": Phase="Running", Reason="", readiness=true. Elapsed: 18.15148254s
Apr 17 21:22:52.906: INFO: Pod "pod-subpath-test-projected-pvm9": Phase="Running", Reason="", readiness=true. Elapsed: 20.163545168s
Apr 17 21:22:54.921: INFO: Pod "pod-subpath-test-projected-pvm9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.177813404s
STEP: Saw pod success
Apr 17 21:22:54.921: INFO: Pod "pod-subpath-test-projected-pvm9" satisfied condition "success or failure"
Apr 17 21:22:54.933: INFO: Trying to get logs from node 10.72.119.74 pod pod-subpath-test-projected-pvm9 container test-container-subpath-projected-pvm9: <nil>
STEP: delete the pod
Apr 17 21:22:54.993: INFO: Waiting for pod pod-subpath-test-projected-pvm9 to disappear
Apr 17 21:22:55.013: INFO: Pod pod-subpath-test-projected-pvm9 no longer exists
STEP: Deleting pod pod-subpath-test-projected-pvm9
Apr 17 21:22:55.013: INFO: Deleting pod "pod-subpath-test-projected-pvm9" in namespace "subpath-2515"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:22:55.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2515" for this suite.
Apr 17 21:23:03.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:23:05.289: INFO: namespace subpath-2515 deletion completed in 10.246531167s

â€¢ [SLOW TEST:32.843 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:23:05.289: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:23:05.445: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ea1400c3-110a-45a6-aa07-2e14618625d6
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-ea1400c3-110a-45a6-aa07-2e14618625d6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:24:14.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-946" for this suite.
Apr 17 21:24:34.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:24:36.889: INFO: namespace projected-946 deletion completed in 22.195430025s

â€¢ [SLOW TEST:91.600 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:24:36.889: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 17 21:24:37.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-6027'
Apr 17 21:24:37.508: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 17 21:24:37.508: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
Apr 17 21:24:37.534: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Apr 17 21:24:37.549: INFO: Waiting for rc e2e-test-httpd-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Apr 17 21:24:37.571: INFO: scanned /root for discovery docs: <nil>
Apr 17 21:24:37.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-6027'
Apr 17 21:24:53.774: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr 17 21:24:53.774: INFO: stdout: "Created e2e-test-httpd-rc-175d873ec2cae0f218eb1f7042e4d4cd\nScaling up e2e-test-httpd-rc-175d873ec2cae0f218eb1f7042e4d4cd from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-175d873ec2cae0f218eb1f7042e4d4cd up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-175d873ec2cae0f218eb1f7042e4d4cd to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Apr 17 21:24:53.774: INFO: stdout: "Created e2e-test-httpd-rc-175d873ec2cae0f218eb1f7042e4d4cd\nScaling up e2e-test-httpd-rc-175d873ec2cae0f218eb1f7042e4d4cd from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-175d873ec2cae0f218eb1f7042e4d4cd up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-175d873ec2cae0f218eb1f7042e4d4cd to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Apr 17 21:24:53.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-6027'
Apr 17 21:24:53.924: INFO: stderr: ""
Apr 17 21:24:53.924: INFO: stdout: "e2e-test-httpd-rc-175d873ec2cae0f218eb1f7042e4d4cd-92lj8 "
Apr 17 21:24:53.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods e2e-test-httpd-rc-175d873ec2cae0f218eb1f7042e4d4cd-92lj8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6027'
Apr 17 21:24:54.055: INFO: stderr: ""
Apr 17 21:24:54.055: INFO: stdout: "true"
Apr 17 21:24:54.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods e2e-test-httpd-rc-175d873ec2cae0f218eb1f7042e4d4cd-92lj8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6027'
Apr 17 21:24:54.189: INFO: stderr: ""
Apr 17 21:24:54.189: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Apr 17 21:24:54.189: INFO: e2e-test-httpd-rc-175d873ec2cae0f218eb1f7042e4d4cd-92lj8 is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Apr 17 21:24:54.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete rc e2e-test-httpd-rc --namespace=kubectl-6027'
Apr 17 21:24:54.413: INFO: stderr: ""
Apr 17 21:24:54.413: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:24:54.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6027" for this suite.
Apr 17 21:25:02.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:25:04.608: INFO: namespace kubectl-6027 deletion completed in 10.163702803s

â€¢ [SLOW TEST:27.719 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:25:04.608: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr 17 21:25:10.925: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-805 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 21:25:10.925: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:25:11.164: INFO: Exec stderr: ""
Apr 17 21:25:11.164: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-805 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 21:25:11.164: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:25:11.377: INFO: Exec stderr: ""
Apr 17 21:25:11.377: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-805 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 21:25:11.377: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:25:11.597: INFO: Exec stderr: ""
Apr 17 21:25:11.597: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-805 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 21:25:11.597: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:25:11.789: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr 17 21:25:11.789: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-805 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 21:25:11.789: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:25:12.030: INFO: Exec stderr: ""
Apr 17 21:25:12.031: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-805 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 21:25:12.031: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:25:12.242: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr 17 21:25:12.243: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-805 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 21:25:12.243: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:25:12.453: INFO: Exec stderr: ""
Apr 17 21:25:12.453: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-805 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 21:25:12.453: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:25:12.642: INFO: Exec stderr: ""
Apr 17 21:25:12.642: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-805 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 21:25:12.642: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:25:12.870: INFO: Exec stderr: ""
Apr 17 21:25:12.870: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-805 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 21:25:12.870: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:25:13.090: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:25:13.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-805" for this suite.
Apr 17 21:26:07.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:26:09.434: INFO: namespace e2e-kubelet-etc-hosts-805 deletion completed in 56.31173545s

â€¢ [SLOW TEST:64.826 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:26:09.434: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Apr 17 21:26:11.683: INFO: &Pod{ObjectMeta:{send-events-5a8475af-737a-4854-948a-a4d9a0de1b13  events-9724 /api/v1/namespaces/events-9724/pods/send-events-5a8475af-737a-4854-948a-a4d9a0de1b13 746c8ae4-da86-42b3-a82b-3ce4da1d4dd1 57649 0 2020-04-17 21:26:09 +0000 UTC <nil> <nil> map[name:foo time:579300022] map[cni.projectcalico.org/podIP:172.30.194.93/32 cni.projectcalico.org/podIPs:172.30.194.93/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.194.93"
    ],
    "dns": {}
}] openshift.io/scc:anyuid] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bvtk2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bvtk2,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bvtk2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c51,c45,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 21:26:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 21:26:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 21:26:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 21:26:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.74,PodIP:172.30.194.93,StartTime:2020-04-17 21:26:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-17 21:26:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:cri-o://229bcce7413be383b30aaf820b22296097347255c908f3859e1ddc1f7c113ba3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.194.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Apr 17 21:26:13.701: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Apr 17 21:26:15.742: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:26:15.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9724" for this suite.
Apr 17 21:26:51.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:26:54.283: INFO: namespace events-9724 deletion completed in 38.474896821s

â€¢ [SLOW TEST:44.848 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:26:54.283: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 21:26:55.330: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 17 21:26:57.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722755615, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722755615, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722755615, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722755615, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 21:27:00.434: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Apr 17 21:27:00.536: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:27:00.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4184" for this suite.
Apr 17 21:27:08.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:27:10.963: INFO: namespace webhook-4184 deletion completed in 10.342179138s
STEP: Destroying namespace "webhook-4184-markers" for this suite.
Apr 17 21:27:19.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:27:21.159: INFO: namespace webhook-4184-markers deletion completed in 10.195661355s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:26.965 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:27:21.248: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr 17 21:27:22.615: INFO: Pod name wrapped-volume-race-31bd7a2d-1f4c-4fb5-9713-bb847fd8723e: Found 0 pods out of 5
Apr 17 21:27:27.649: INFO: Pod name wrapped-volume-race-31bd7a2d-1f4c-4fb5-9713-bb847fd8723e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-31bd7a2d-1f4c-4fb5-9713-bb847fd8723e in namespace emptydir-wrapper-3776, will wait for the garbage collector to delete the pods
Apr 17 21:27:27.865: INFO: Deleting ReplicationController wrapped-volume-race-31bd7a2d-1f4c-4fb5-9713-bb847fd8723e took: 60.816969ms
Apr 17 21:27:28.465: INFO: Terminating ReplicationController wrapped-volume-race-31bd7a2d-1f4c-4fb5-9713-bb847fd8723e pods took: 600.374039ms
STEP: Creating RC which spawns configmap-volume pods
Apr 17 21:28:08.250: INFO: Pod name wrapped-volume-race-866c49eb-dca0-4c66-87d3-25bbbeddb4fc: Found 0 pods out of 5
Apr 17 21:28:13.277: INFO: Pod name wrapped-volume-race-866c49eb-dca0-4c66-87d3-25bbbeddb4fc: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-866c49eb-dca0-4c66-87d3-25bbbeddb4fc in namespace emptydir-wrapper-3776, will wait for the garbage collector to delete the pods
Apr 17 21:28:13.469: INFO: Deleting ReplicationController wrapped-volume-race-866c49eb-dca0-4c66-87d3-25bbbeddb4fc took: 44.531775ms
Apr 17 21:28:14.070: INFO: Terminating ReplicationController wrapped-volume-race-866c49eb-dca0-4c66-87d3-25bbbeddb4fc pods took: 600.321553ms
STEP: Creating RC which spawns configmap-volume pods
Apr 17 21:28:58.058: INFO: Pod name wrapped-volume-race-3fa055a4-be3f-4903-abff-1bd2c93de0f4: Found 0 pods out of 5
Apr 17 21:29:03.102: INFO: Pod name wrapped-volume-race-3fa055a4-be3f-4903-abff-1bd2c93de0f4: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3fa055a4-be3f-4903-abff-1bd2c93de0f4 in namespace emptydir-wrapper-3776, will wait for the garbage collector to delete the pods
Apr 17 21:29:03.281: INFO: Deleting ReplicationController wrapped-volume-race-3fa055a4-be3f-4903-abff-1bd2c93de0f4 took: 47.427378ms
Apr 17 21:29:03.882: INFO: Terminating ReplicationController wrapped-volume-race-3fa055a4-be3f-4903-abff-1bd2c93de0f4 pods took: 600.225274ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:29:38.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3776" for this suite.
Apr 17 21:29:48.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:29:50.366: INFO: namespace emptydir-wrapper-3776 deletion completed in 12.216487991s

â€¢ [SLOW TEST:149.119 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:29:50.367: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-3195d16f-752c-48c5-b410-480603f3e8f0
STEP: Creating a pod to test consume secrets
Apr 17 21:29:50.606: INFO: Waiting up to 5m0s for pod "pod-secrets-8b685efb-bb36-429d-905a-1ef9d9b2588b" in namespace "secrets-8962" to be "success or failure"
Apr 17 21:29:50.621: INFO: Pod "pod-secrets-8b685efb-bb36-429d-905a-1ef9d9b2588b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.565314ms
Apr 17 21:29:52.633: INFO: Pod "pod-secrets-8b685efb-bb36-429d-905a-1ef9d9b2588b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026884937s
STEP: Saw pod success
Apr 17 21:29:52.633: INFO: Pod "pod-secrets-8b685efb-bb36-429d-905a-1ef9d9b2588b" satisfied condition "success or failure"
Apr 17 21:29:52.645: INFO: Trying to get logs from node 10.72.119.74 pod pod-secrets-8b685efb-bb36-429d-905a-1ef9d9b2588b container secret-env-test: <nil>
STEP: delete the pod
Apr 17 21:29:52.748: INFO: Waiting for pod pod-secrets-8b685efb-bb36-429d-905a-1ef9d9b2588b to disappear
Apr 17 21:29:52.768: INFO: Pod pod-secrets-8b685efb-bb36-429d-905a-1ef9d9b2588b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:29:52.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8962" for this suite.
Apr 17 21:30:00.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:30:03.019: INFO: namespace secrets-8962 deletion completed in 10.214265091s

â€¢ [SLOW TEST:12.652 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:30:03.020: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4354
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4354
I0417 21:30:03.305000      22 runners.go:184] Created replication controller with name: externalname-service, namespace: services-4354, replica count: 2
Apr 17 21:30:06.355: INFO: Creating new exec pod
I0417 21:30:06.355453      22 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 17 21:30:09.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=services-4354 execpodb2p88 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Apr 17 21:30:09.855: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 17 21:30:09.855: INFO: stdout: ""
Apr 17 21:30:09.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=services-4354 execpodb2p88 -- /bin/sh -x -c nc -zv -t -w 2 172.21.172.161 80'
Apr 17 21:30:10.236: INFO: stderr: "+ nc -zv -t -w 2 172.21.172.161 80\nConnection to 172.21.172.161 80 port [tcp/http] succeeded!\n"
Apr 17 21:30:10.236: INFO: stdout: ""
Apr 17 21:30:10.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=services-4354 execpodb2p88 -- /bin/sh -x -c nc -zv -t -w 2 10.72.119.72 31064'
Apr 17 21:30:10.674: INFO: stderr: "+ nc -zv -t -w 2 10.72.119.72 31064\nConnection to 10.72.119.72 31064 port [tcp/31064] succeeded!\n"
Apr 17 21:30:10.674: INFO: stdout: ""
Apr 17 21:30:10.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=services-4354 execpodb2p88 -- /bin/sh -x -c nc -zv -t -w 2 10.72.119.74 31064'
Apr 17 21:30:11.016: INFO: stderr: "+ nc -zv -t -w 2 10.72.119.74 31064\nConnection to 10.72.119.74 31064 port [tcp/31064] succeeded!\n"
Apr 17 21:30:11.016: INFO: stdout: ""
Apr 17 21:30:11.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=services-4354 execpodb2p88 -- /bin/sh -x -c nc -zv -t -w 2 158.176.124.206 31064'
Apr 17 21:30:11.376: INFO: stderr: "+ nc -zv -t -w 2 158.176.124.206 31064\nConnection to 158.176.124.206 31064 port [tcp/31064] succeeded!\n"
Apr 17 21:30:11.376: INFO: stdout: ""
Apr 17 21:30:11.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=services-4354 execpodb2p88 -- /bin/sh -x -c nc -zv -t -w 2 158.176.124.194 31064'
Apr 17 21:30:11.769: INFO: stderr: "+ nc -zv -t -w 2 158.176.124.194 31064\nConnection to 158.176.124.194 31064 port [tcp/31064] succeeded!\n"
Apr 17 21:30:11.769: INFO: stdout: ""
Apr 17 21:30:11.769: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:30:11.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4354" for this suite.
Apr 17 21:30:19.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:30:22.134: INFO: namespace services-4354 deletion completed in 10.24390387s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:19.114 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:30:22.134: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:30:22.511: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr 17 21:30:22.559: INFO: Number of nodes with available pods: 0
Apr 17 21:30:22.559: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Apr 17 21:30:22.638: INFO: Number of nodes with available pods: 0
Apr 17 21:30:22.638: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:30:23.651: INFO: Number of nodes with available pods: 0
Apr 17 21:30:23.651: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:30:24.658: INFO: Number of nodes with available pods: 0
Apr 17 21:30:24.658: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:30:25.651: INFO: Number of nodes with available pods: 1
Apr 17 21:30:25.651: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr 17 21:30:25.724: INFO: Number of nodes with available pods: 1
Apr 17 21:30:25.724: INFO: Number of running nodes: 0, number of available pods: 1
Apr 17 21:30:26.736: INFO: Number of nodes with available pods: 0
Apr 17 21:30:26.736: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr 17 21:30:26.764: INFO: Number of nodes with available pods: 0
Apr 17 21:30:26.764: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:30:27.785: INFO: Number of nodes with available pods: 0
Apr 17 21:30:27.785: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:30:28.777: INFO: Number of nodes with available pods: 0
Apr 17 21:30:28.777: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:30:29.778: INFO: Number of nodes with available pods: 0
Apr 17 21:30:29.779: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:30:30.777: INFO: Number of nodes with available pods: 0
Apr 17 21:30:30.777: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:30:31.783: INFO: Number of nodes with available pods: 0
Apr 17 21:30:31.783: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:30:32.777: INFO: Number of nodes with available pods: 0
Apr 17 21:30:32.778: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:30:33.776: INFO: Number of nodes with available pods: 0
Apr 17 21:30:33.776: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:30:34.777: INFO: Number of nodes with available pods: 0
Apr 17 21:30:34.777: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:30:35.776: INFO: Number of nodes with available pods: 0
Apr 17 21:30:35.776: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:30:36.776: INFO: Number of nodes with available pods: 0
Apr 17 21:30:36.777: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:30:37.793: INFO: Number of nodes with available pods: 0
Apr 17 21:30:37.793: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:30:38.779: INFO: Number of nodes with available pods: 0
Apr 17 21:30:38.779: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 21:30:39.785: INFO: Number of nodes with available pods: 1
Apr 17 21:30:39.785: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-766, will wait for the garbage collector to delete the pods
Apr 17 21:30:39.925: INFO: Deleting DaemonSet.extensions daemon-set took: 48.141174ms
Apr 17 21:30:40.526: INFO: Terminating DaemonSet.extensions daemon-set pods took: 600.39895ms
Apr 17 21:30:47.941: INFO: Number of nodes with available pods: 0
Apr 17 21:30:47.941: INFO: Number of running nodes: 0, number of available pods: 0
Apr 17 21:30:47.955: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-766/daemonsets","resourceVersion":"59816"},"items":null}

Apr 17 21:30:47.967: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-766/pods","resourceVersion":"59817"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:30:48.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-766" for this suite.
Apr 17 21:30:56.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:30:58.311: INFO: namespace daemonsets-766 deletion completed in 10.218939124s

â€¢ [SLOW TEST:36.177 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:30:58.311: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-609d0947-78d5-45ab-bba4-640de60a2fa3
STEP: Creating a pod to test consume secrets
Apr 17 21:30:58.761: INFO: Waiting up to 5m0s for pod "pod-secrets-e476b454-7db5-4336-98b4-6ba16df4604b" in namespace "secrets-8359" to be "success or failure"
Apr 17 21:30:58.782: INFO: Pod "pod-secrets-e476b454-7db5-4336-98b4-6ba16df4604b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.988589ms
Apr 17 21:31:00.795: INFO: Pod "pod-secrets-e476b454-7db5-4336-98b4-6ba16df4604b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.034452743s
STEP: Saw pod success
Apr 17 21:31:00.796: INFO: Pod "pod-secrets-e476b454-7db5-4336-98b4-6ba16df4604b" satisfied condition "success or failure"
Apr 17 21:31:00.810: INFO: Trying to get logs from node 10.72.119.74 pod pod-secrets-e476b454-7db5-4336-98b4-6ba16df4604b container secret-volume-test: <nil>
STEP: delete the pod
Apr 17 21:31:00.884: INFO: Waiting for pod pod-secrets-e476b454-7db5-4336-98b4-6ba16df4604b to disappear
Apr 17 21:31:00.896: INFO: Pod pod-secrets-e476b454-7db5-4336-98b4-6ba16df4604b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:31:00.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8359" for this suite.
Apr 17 21:31:08.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:31:11.139: INFO: namespace secrets-8359 deletion completed in 10.224977424s
STEP: Destroying namespace "secret-namespace-3908" for this suite.
Apr 17 21:31:19.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:31:21.302: INFO: namespace secret-namespace-3908 deletion completed in 10.162779893s

â€¢ [SLOW TEST:22.991 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:31:21.310: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr 17 21:31:21.530: INFO: Waiting up to 5m0s for pod "pod-afe96cc0-c4f3-4ef9-8480-6fff253cc0a7" in namespace "emptydir-8098" to be "success or failure"
Apr 17 21:31:21.544: INFO: Pod "pod-afe96cc0-c4f3-4ef9-8480-6fff253cc0a7": Phase="Pending", Reason="", readiness=false. Elapsed: 13.937234ms
Apr 17 21:31:23.556: INFO: Pod "pod-afe96cc0-c4f3-4ef9-8480-6fff253cc0a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02673441s
STEP: Saw pod success
Apr 17 21:31:23.557: INFO: Pod "pod-afe96cc0-c4f3-4ef9-8480-6fff253cc0a7" satisfied condition "success or failure"
Apr 17 21:31:23.568: INFO: Trying to get logs from node 10.72.119.74 pod pod-afe96cc0-c4f3-4ef9-8480-6fff253cc0a7 container test-container: <nil>
STEP: delete the pod
Apr 17 21:31:23.625: INFO: Waiting for pod pod-afe96cc0-c4f3-4ef9-8480-6fff253cc0a7 to disappear
Apr 17 21:31:23.635: INFO: Pod pod-afe96cc0-c4f3-4ef9-8480-6fff253cc0a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:31:23.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8098" for this suite.
Apr 17 21:31:31.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:31:33.842: INFO: namespace emptydir-8098 deletion completed in 10.189317535s

â€¢ [SLOW TEST:12.531 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:31:33.842: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-4124
STEP: creating replication controller nodeport-test in namespace services-4124
I0417 21:31:34.095536      22 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-4124, replica count: 2
I0417 21:31:37.150751      22 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 17 21:31:37.151: INFO: Creating new exec pod
Apr 17 21:31:40.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=services-4124 execpodx8fnz -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Apr 17 21:31:40.615: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Apr 17 21:31:40.615: INFO: stdout: ""
Apr 17 21:31:40.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=services-4124 execpodx8fnz -- /bin/sh -x -c nc -zv -t -w 2 172.21.135.58 80'
Apr 17 21:31:41.013: INFO: stderr: "+ nc -zv -t -w 2 172.21.135.58 80\nConnection to 172.21.135.58 80 port [tcp/http] succeeded!\n"
Apr 17 21:31:41.013: INFO: stdout: ""
Apr 17 21:31:41.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=services-4124 execpodx8fnz -- /bin/sh -x -c nc -zv -t -w 2 10.72.119.72 30792'
Apr 17 21:31:41.358: INFO: stderr: "+ nc -zv -t -w 2 10.72.119.72 30792\nConnection to 10.72.119.72 30792 port [tcp/30792] succeeded!\n"
Apr 17 21:31:41.358: INFO: stdout: ""
Apr 17 21:31:41.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=services-4124 execpodx8fnz -- /bin/sh -x -c nc -zv -t -w 2 10.72.119.74 30792'
Apr 17 21:31:41.726: INFO: stderr: "+ nc -zv -t -w 2 10.72.119.74 30792\nConnection to 10.72.119.74 30792 port [tcp/30792] succeeded!\n"
Apr 17 21:31:41.726: INFO: stdout: ""
Apr 17 21:31:41.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=services-4124 execpodx8fnz -- /bin/sh -x -c nc -zv -t -w 2 158.176.124.206 30792'
Apr 17 21:31:42.089: INFO: stderr: "+ nc -zv -t -w 2 158.176.124.206 30792\nConnection to 158.176.124.206 30792 port [tcp/30792] succeeded!\n"
Apr 17 21:31:42.089: INFO: stdout: ""
Apr 17 21:31:42.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=services-4124 execpodx8fnz -- /bin/sh -x -c nc -zv -t -w 2 158.176.124.194 30792'
Apr 17 21:31:42.445: INFO: stderr: "+ nc -zv -t -w 2 158.176.124.194 30792\nConnection to 158.176.124.194 30792 port [tcp/30792] succeeded!\n"
Apr 17 21:31:42.445: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:31:42.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4124" for this suite.
Apr 17 21:31:50.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:31:52.703: INFO: namespace services-4124 deletion completed in 10.231620845s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:18.861 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:31:52.704: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:31:52.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 create -f - --namespace=kubectl-6235'
Apr 17 21:31:53.376: INFO: stderr: ""
Apr 17 21:31:53.377: INFO: stdout: "replicationcontroller/redis-master created\n"
Apr 17 21:31:53.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 create -f - --namespace=kubectl-6235'
Apr 17 21:31:53.749: INFO: stderr: ""
Apr 17 21:31:53.749: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 17 21:31:54.765: INFO: Selector matched 1 pods for map[app:redis]
Apr 17 21:31:54.765: INFO: Found 0 / 1
Apr 17 21:31:55.762: INFO: Selector matched 1 pods for map[app:redis]
Apr 17 21:31:55.762: INFO: Found 1 / 1
Apr 17 21:31:55.762: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 17 21:31:55.773: INFO: Selector matched 1 pods for map[app:redis]
Apr 17 21:31:55.773: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 17 21:31:55.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 describe pod redis-master-c56lj --namespace=kubectl-6235'
Apr 17 21:31:55.944: INFO: stderr: ""
Apr 17 21:31:55.944: INFO: stdout: "Name:         redis-master-c56lj\nNamespace:    kubectl-6235\nPriority:     0\nNode:         10.72.119.74/10.72.119.74\nStart Time:   Fri, 17 Apr 2020 21:31:53 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  cni.projectcalico.org/podIP: 172.30.194.102/32\n              cni.projectcalico.org/podIPs: 172.30.194.102/32\n              k8s.v1.cni.cncf.io/networks-status:\n                [{\n                    \"name\": \"k8s-pod-network\",\n                    \"ips\": [\n                        \"172.30.194.102\"\n                    ],\n                    \"dns\": {}\n                }]\n              openshift.io/scc: privileged\nStatus:       Running\nIP:           172.30.194.102\nIPs:\n  IP:           172.30.194.102\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   cri-o://6b914659320c91ae20e15827b0ab984f061886dbafcc24c5fc25be0c69348df5\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 17 Apr 2020 21:31:54 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-6r964 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-6r964:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-6r964\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                   Message\n  ----    ------     ----       ----                   -------\n  Normal  Scheduled  <unknown>  default-scheduler      Successfully assigned kubectl-6235/redis-master-c56lj to 10.72.119.74\n  Normal  Pulled     1s         kubelet, 10.72.119.74  Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    1s         kubelet, 10.72.119.74  Created container redis-master\n  Normal  Started    1s         kubelet, 10.72.119.74  Started container redis-master\n"
Apr 17 21:31:55.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 describe rc redis-master --namespace=kubectl-6235'
Apr 17 21:31:56.157: INFO: stderr: ""
Apr 17 21:31:56.157: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-6235\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-c56lj\n"
Apr 17 21:31:56.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 describe service redis-master --namespace=kubectl-6235'
Apr 17 21:31:56.354: INFO: stderr: ""
Apr 17 21:31:56.354: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-6235\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.21.3.120\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.30.194.102:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr 17 21:31:56.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 describe node 10.72.119.72'
Apr 17 21:31:56.697: INFO: stderr: ""
Apr 17 21:31:56.698: INFO: stdout: "Name:               10.72.119.72\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-gb\n                    failure-domain.beta.kubernetes.io/zone=lon06\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=158.176.124.206\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.72.119.72\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_7_64\n                    ibm-cloud.kubernetes.io/region=eu-gb\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-bqcujj6l08no7db3cah0-kubee2epvge-default-00000103\n                    ibm-cloud.kubernetes.io/worker-pool-id=bqcujj6l08no7db3cah0-3423bac\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.3.10_1518_openshift\n                    ibm-cloud.kubernetes.io/zone=lon06\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.72.119.72\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    privateVLAN=2722974\n                    publicVLAN=2722972\n                    topology.kubernetes.io/region=eu-gb\n                    topology.kubernetes.io/zone=lon06\nAnnotations:        projectcalico.org/IPv4Address: 10.72.119.72/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.53.0\nCreationTimestamp:  Fri, 17 Apr 2020 19:09:24 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 17 Apr 2020 19:10:24 +0000   Fri, 17 Apr 2020 19:10:24 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Fri, 17 Apr 2020 21:31:24 +0000   Fri, 17 Apr 2020 19:09:24 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 17 Apr 2020 21:31:24 +0000   Fri, 17 Apr 2020 19:09:24 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 17 Apr 2020 21:31:24 +0000   Fri, 17 Apr 2020 19:09:24 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 17 Apr 2020 21:31:24 +0000   Fri, 17 Apr 2020 19:10:34 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.72.119.72\n  ExternalIP:  158.176.124.206\n  Hostname:    10.72.119.72\nCapacity:\n cpu:                4\n ephemeral-storage:  103078840Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16260880Ki\n pods:               110\nAllocatable:\n cpu:                3910m\n ephemeral-storage:  100275095474\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             13484816Ki\n pods:               110\nSystem Info:\n Machine ID:                                             e123a3f77e1d4affb6ccd13c728c8bc6\n System UUID:                                            CF07AEDE-9A31-3DDA-A18C-320B2B9328B2\n Boot ID:                                                8955f607-233e-4e98-aee0-eee6f0492462\n Kernel Version:                                         3.10.0-1127.el7.x86_64\n OS Image:                                               Red Hat\n Operating System:                                       linux\n Architecture:                                           amd64\n Container Runtime Version:                              cri-o://1.16.4-1.dev.rhaos4.3.git9238eee.el7\n Kubelet Version:                                        v1.16.2\n Kube-Proxy Version:                                     v1.16.2\nProviderID:                                              ibm://fee034388aa6435883a1f720010ab3a2///bqcujj6l08no7db3cah0/kube-bqcujj6l08no7db3cah0-kubee2epvge-default-00000103\nNon-terminated Pods:                                     (43 in total)\n  Namespace                                              Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                                              ----                                                               ------------  ----------  ---------------  -------------  ---\n  calico-system                                          calico-kube-controllers-84d976f9ff-g9864                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         142m\n  calico-system                                          calico-node-tcc87                                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         142m\n  calico-system                                          calico-typha-76b588567c-d6d9k                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         142m\n  ibm-system                                             ibm-cloud-provider-ip-158-176-122-172-6f57dbd74b-cdqd6             5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         138m\n  kube-system                                            ibm-file-plugin-7cbd86d68f-9tm42                                   50m (1%)      200m (5%)   100Mi (0%)       0 (0%)         149m\n  kube-system                                            ibm-keepalived-watcher-58jb7                                       5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         142m\n  kube-system                                            ibm-master-proxy-static-10.72.119.72                               25m (0%)      300m (7%)   32M (0%)         512M (3%)      141m\n  kube-system                                            ibm-storage-watcher-d9c7cf586-f72vk                                50m (1%)      200m (5%)   100Mi (0%)       0 (0%)         149m\n  kube-system                                            ibmcloud-block-storage-driver-g7lth                                25m (0%)      100m (2%)   50Mi (0%)        200Mi (1%)     142m\n  kube-system                                            ibmcloud-block-storage-plugin-75f7cd767-lcw2w                      50m (1%)      200m (5%)   100Mi (0%)       0 (0%)         149m\n  openshift-cluster-node-tuning-operator                 cluster-node-tuning-operator-77bdbd4f-79qvb                        10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         151m\n  openshift-cluster-node-tuning-operator                 tuned-xrlm4                                                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         140m\n  openshift-cluster-storage-operator                     cluster-storage-operator-56475d49d7-fsvdb                          10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         151m\n  openshift-console-operator                             console-operator-db5d785db-r4qh6                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         151m\n  openshift-console                                      downloads-64bb8b89c9-jxkw4                                         10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         151m\n  openshift-console                                      downloads-64bb8b89c9-sccw2                                         10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         151m\n  openshift-dns-operator                                 dns-operator-74d97d58d-56295                                       20m (0%)      0 (0%)      40Mi (0%)        0 (0%)         152m\n  openshift-dns                                          dns-default-ffsfn                                                  110m (2%)     0 (0%)      70Mi (0%)        512Mi (3%)     139m\n  openshift-image-registry                               cluster-image-registry-operator-7857d56744-c2pch                   20m (0%)      0 (0%)      0 (0%)           0 (0%)         151m\n  openshift-image-registry                               node-ca-x5gt7                                                      10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         140m\n  openshift-ingress-operator                             ingress-operator-66cf4674d8-467pw                                  20m (0%)      0 (0%)      40Mi (0%)        0 (0%)         151m\n  openshift-kube-proxy                                   openshift-kube-proxy-bkf2z                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         141m\n  openshift-marketplace                                  marketplace-operator-699fb8f5d-5nrcq                               10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         151m\n  openshift-monitoring                                   alertmanager-main-2                                                110m (2%)     100m (2%)   245Mi (1%)       25Mi (0%)      135m\n  openshift-monitoring                                   cluster-monitoring-operator-668c7b8d6d-khmrg                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         151m\n  openshift-monitoring                                   kube-state-metrics-5bc9b987bc-hvx9k                                30m (0%)      0 (0%)      120Mi (0%)       0 (0%)         140m\n  openshift-monitoring                                   node-exporter-5qztm                                                112m (2%)     0 (0%)      200Mi (1%)       0 (0%)         140m\n  openshift-monitoring                                   openshift-state-metrics-6c465bc47f-p9qr6                           120m (3%)     0 (0%)      190Mi (1%)       0 (0%)         140m\n  openshift-multus                                       multus-admission-controller-2jxfm                                  10m (0%)      0 (0%)      0 (0%)           0 (0%)         141m\n  openshift-multus                                       multus-wlttp                                                       10m (0%)      0 (0%)      150Mi (1%)       0 (0%)         142m\n  openshift-network-operator                             network-operator-64f597f5d-8tg4w                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         152m\n  openshift-operator-lifecycle-manager                   catalog-operator-5665d988d5-kvkfk                                  10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         152m\n  openshift-operator-lifecycle-manager                   olm-operator-7bd9dc9457-2wn6n                                      10m (0%)      0 (0%)      160Mi (1%)       0 (0%)         152m\n  openshift-operator-lifecycle-manager                   packageserver-6bfd584bf6-xvcxz                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         98m\n  openshift-service-ca-operator                          service-ca-operator-5d59f48888-hk9zs                               10m (0%)      0 (0%)      80Mi (0%)        0 (0%)         152m\n  openshift-service-ca                                   apiservice-cabundle-injector-7bf8cddb9-kxhvx                       10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         140m\n  openshift-service-ca                                   configmap-cabundle-injector-5cf6d9695-kfct9                        10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         140m\n  openshift-service-ca                                   service-serving-cert-signer-5968cc5d5c-m6hb2                       10m (0%)      0 (0%)      120Mi (0%)       0 (0%)         140m\n  openshift-service-catalog-apiserver-operator           openshift-service-catalog-apiserver-operator-76969db7f5-wpk2m      0 (0%)        0 (0%)      50Mi (0%)        0 (0%)         152m\n  openshift-service-catalog-controller-manager-operator  openshift-service-catalog-controller-manager-operator-f954mtxlp    10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         152m\n  sonobuoy                                               sonobuoy                                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         44m\n  sonobuoy                                               sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-l8m9b            0 (0%)        0 (0%)      0 (0%)           0 (0%)         44m\n  tigera-operator                                        tigera-operator-df8f4c87c-c7sjz                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         150m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                952m (24%)       1100m (28%)\n  memory             2657810Ki (19%)  1284800512 (9%)\n  ephemeral-storage  0 (0%)           0 (0%)\nEvents:\n  Type    Reason                   Age                  From                      Message\n  ----    ------                   ----                 ----                      -------\n  Normal  Starting                 142m                 kubelet, 10.72.119.72     Starting kubelet.\n  Normal  NodeAllocatableEnforced  142m                 kubelet, 10.72.119.72     Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  142m (x8 over 142m)  kubelet, 10.72.119.72     Node 10.72.119.72 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    142m (x8 over 142m)  kubelet, 10.72.119.72     Node 10.72.119.72 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     142m (x7 over 142m)  kubelet, 10.72.119.72     Node 10.72.119.72 status is now: NodeHasSufficientPID\n  Normal  Starting                 141m                 kube-proxy, 10.72.119.72  Starting kube-proxy.\n"
Apr 17 21:31:56.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 describe namespace kubectl-6235'
Apr 17 21:31:56.867: INFO: stderr: ""
Apr 17 21:31:56.867: INFO: stdout: "Name:         kubectl-6235\nLabels:       e2e-framework=kubectl\n              e2e-run=5766aa68-21ab-4498-aad4-2935d50e9967\nAnnotations:  openshift.io/sa.scc.mcs: s0:c52,c49\n              openshift.io/sa.scc.supplemental-groups: 1002750000/10000\n              openshift.io/sa.scc.uid-range: 1002750000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:31:56.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6235" for this suite.
Apr 17 21:32:12.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:32:15.127: INFO: namespace kubectl-6235 deletion completed in 18.238402432s

â€¢ [SLOW TEST:22.423 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:32:15.127: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 17 21:32:18.394: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:32:18.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9435" for this suite.
Apr 17 21:32:26.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:32:28.768: INFO: namespace container-runtime-9435 deletion completed in 10.291792417s

â€¢ [SLOW TEST:13.641 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:32:28.768: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr 17 21:32:28.982: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5669 /api/v1/namespaces/watch-5669/configmaps/e2e-watch-test-configmap-a 1776c6df-2377-4837-b723-dec162637bc2 60796 0 2020-04-17 21:32:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 17 21:32:28.982: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5669 /api/v1/namespaces/watch-5669/configmaps/e2e-watch-test-configmap-a 1776c6df-2377-4837-b723-dec162637bc2 60796 0 2020-04-17 21:32:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr 17 21:32:39.013: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5669 /api/v1/namespaces/watch-5669/configmaps/e2e-watch-test-configmap-a 1776c6df-2377-4837-b723-dec162637bc2 60838 0 2020-04-17 21:32:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr 17 21:32:39.014: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5669 /api/v1/namespaces/watch-5669/configmaps/e2e-watch-test-configmap-a 1776c6df-2377-4837-b723-dec162637bc2 60838 0 2020-04-17 21:32:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr 17 21:32:49.047: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5669 /api/v1/namespaces/watch-5669/configmaps/e2e-watch-test-configmap-a 1776c6df-2377-4837-b723-dec162637bc2 60876 0 2020-04-17 21:32:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 17 21:32:49.047: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5669 /api/v1/namespaces/watch-5669/configmaps/e2e-watch-test-configmap-a 1776c6df-2377-4837-b723-dec162637bc2 60876 0 2020-04-17 21:32:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr 17 21:32:59.073: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5669 /api/v1/namespaces/watch-5669/configmaps/e2e-watch-test-configmap-a 1776c6df-2377-4837-b723-dec162637bc2 60912 0 2020-04-17 21:32:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 17 21:32:59.073: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5669 /api/v1/namespaces/watch-5669/configmaps/e2e-watch-test-configmap-a 1776c6df-2377-4837-b723-dec162637bc2 60912 0 2020-04-17 21:32:28 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr 17 21:33:09.100: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5669 /api/v1/namespaces/watch-5669/configmaps/e2e-watch-test-configmap-b 1946ef7d-5841-4c95-8762-16040b3902e0 60948 0 2020-04-17 21:33:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 17 21:33:09.100: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5669 /api/v1/namespaces/watch-5669/configmaps/e2e-watch-test-configmap-b 1946ef7d-5841-4c95-8762-16040b3902e0 60948 0 2020-04-17 21:33:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr 17 21:33:19.127: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5669 /api/v1/namespaces/watch-5669/configmaps/e2e-watch-test-configmap-b 1946ef7d-5841-4c95-8762-16040b3902e0 60990 0 2020-04-17 21:33:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 17 21:33:19.128: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5669 /api/v1/namespaces/watch-5669/configmaps/e2e-watch-test-configmap-b 1946ef7d-5841-4c95-8762-16040b3902e0 60990 0 2020-04-17 21:33:09 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:33:29.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5669" for this suite.
Apr 17 21:33:37.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:33:39.505: INFO: namespace watch-5669 deletion completed in 10.313719087s

â€¢ [SLOW TEST:70.736 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:33:39.505: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5377
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Apr 17 21:33:39.831: INFO: Found 0 stateful pods, waiting for 3
Apr 17 21:33:49.849: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 21:33:49.849: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 21:33:49.849: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 21:33:49.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-5377 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 21:33:50.239: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 21:33:50.239: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 21:33:50.239: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Apr 17 21:34:00.374: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr 17 21:34:10.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-5377 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 21:34:10.941: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 17 21:34:10.941: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 21:34:10.941: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 17 21:34:21.067: INFO: Waiting for StatefulSet statefulset-5377/ss2 to complete update
Apr 17 21:34:21.067: INFO: Waiting for Pod statefulset-5377/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 17 21:34:21.067: INFO: Waiting for Pod statefulset-5377/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 17 21:34:31.103: INFO: Waiting for StatefulSet statefulset-5377/ss2 to complete update
Apr 17 21:34:31.103: INFO: Waiting for Pod statefulset-5377/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 17 21:34:31.103: INFO: Waiting for Pod statefulset-5377/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 17 21:34:41.101: INFO: Waiting for StatefulSet statefulset-5377/ss2 to complete update
Apr 17 21:34:41.101: INFO: Waiting for Pod statefulset-5377/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 17 21:34:51.104: INFO: Waiting for StatefulSet statefulset-5377/ss2 to complete update
Apr 17 21:34:51.104: INFO: Waiting for Pod statefulset-5377/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 17 21:35:01.102: INFO: Waiting for StatefulSet statefulset-5377/ss2 to complete update
STEP: Rolling back to a previous revision
Apr 17 21:35:11.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-5377 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 21:35:11.695: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 21:35:11.695: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 21:35:11.695: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 21:35:21.819: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr 17 21:35:31.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-5377 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 21:35:32.309: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 17 21:35:32.309: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 21:35:32.309: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Apr 17 21:36:02.463: INFO: Deleting all statefulset in ns statefulset-5377
Apr 17 21:36:02.480: INFO: Scaling statefulset ss2 to 0
Apr 17 21:36:22.552: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 21:36:22.567: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:36:22.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5377" for this suite.
Apr 17 21:36:32.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:36:34.843: INFO: namespace statefulset-5377 deletion completed in 12.184180428s

â€¢ [SLOW TEST:175.339 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:36:34.847: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-327cf05d-b5a9-48f7-bb54-f443eeea2282
STEP: Creating a pod to test consume configMaps
Apr 17 21:36:35.122: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d18d3a5-6e1a-45c6-a51b-4242e8ea5d21" in namespace "configmap-2318" to be "success or failure"
Apr 17 21:36:35.135: INFO: Pod "pod-configmaps-9d18d3a5-6e1a-45c6-a51b-4242e8ea5d21": Phase="Pending", Reason="", readiness=false. Elapsed: 13.327313ms
Apr 17 21:36:37.149: INFO: Pod "pod-configmaps-9d18d3a5-6e1a-45c6-a51b-4242e8ea5d21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026843727s
Apr 17 21:36:39.161: INFO: Pod "pod-configmaps-9d18d3a5-6e1a-45c6-a51b-4242e8ea5d21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038896953s
STEP: Saw pod success
Apr 17 21:36:39.161: INFO: Pod "pod-configmaps-9d18d3a5-6e1a-45c6-a51b-4242e8ea5d21" satisfied condition "success or failure"
Apr 17 21:36:39.172: INFO: Trying to get logs from node 10.72.119.74 pod pod-configmaps-9d18d3a5-6e1a-45c6-a51b-4242e8ea5d21 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 17 21:36:39.285: INFO: Waiting for pod pod-configmaps-9d18d3a5-6e1a-45c6-a51b-4242e8ea5d21 to disappear
Apr 17 21:36:39.305: INFO: Pod pod-configmaps-9d18d3a5-6e1a-45c6-a51b-4242e8ea5d21 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:36:39.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2318" for this suite.
Apr 17 21:36:47.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:36:49.604: INFO: namespace configmap-2318 deletion completed in 10.268294006s

â€¢ [SLOW TEST:14.757 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:36:49.604: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8809
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 17 21:36:49.730: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 17 21:37:10.161: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.194.108:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8809 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 21:37:10.161: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:37:10.364: INFO: Found all expected endpoints: [netserver-0]
Apr 17 21:37:10.383: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.232.197:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8809 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 21:37:10.383: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:37:10.778: INFO: Found all expected endpoints: [netserver-1]
Apr 17 21:37:10.794: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.53.49:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8809 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 21:37:10.794: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:37:11.097: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:37:11.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8809" for this suite.
Apr 17 21:37:19.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:37:21.670: INFO: namespace pod-network-test-8809 deletion completed in 10.545729765s

â€¢ [SLOW TEST:32.066 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:37:21.670: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 17 21:37:24.033: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:37:24.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7100" for this suite.
Apr 17 21:37:32.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:37:34.345: INFO: namespace container-runtime-7100 deletion completed in 10.232459671s

â€¢ [SLOW TEST:12.675 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:37:34.345: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Apr 17 21:37:38.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec pod-sharedvolume-0eba43b4-07ff-4ecd-9681-09d8deb239a4 -c busybox-main-container --namespace=emptydir-7407 -- cat /usr/share/volumeshare/shareddata.txt'
Apr 17 21:37:38.986: INFO: stderr: ""
Apr 17 21:37:38.986: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:37:38.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7407" for this suite.
Apr 17 21:37:47.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:37:49.246: INFO: namespace emptydir-7407 deletion completed in 10.238346077s

â€¢ [SLOW TEST:14.901 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:37:49.247: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Apr 17 21:37:49.459: INFO: Waiting up to 5m0s for pod "var-expansion-b97222b1-4a04-4231-97d6-00313a2decff" in namespace "var-expansion-986" to be "success or failure"
Apr 17 21:37:49.472: INFO: Pod "var-expansion-b97222b1-4a04-4231-97d6-00313a2decff": Phase="Pending", Reason="", readiness=false. Elapsed: 12.264163ms
Apr 17 21:37:51.484: INFO: Pod "var-expansion-b97222b1-4a04-4231-97d6-00313a2decff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024424816s
Apr 17 21:37:53.497: INFO: Pod "var-expansion-b97222b1-4a04-4231-97d6-00313a2decff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037297036s
STEP: Saw pod success
Apr 17 21:37:53.497: INFO: Pod "var-expansion-b97222b1-4a04-4231-97d6-00313a2decff" satisfied condition "success or failure"
Apr 17 21:37:53.510: INFO: Trying to get logs from node 10.72.119.74 pod var-expansion-b97222b1-4a04-4231-97d6-00313a2decff container dapi-container: <nil>
STEP: delete the pod
Apr 17 21:37:53.575: INFO: Waiting for pod var-expansion-b97222b1-4a04-4231-97d6-00313a2decff to disappear
Apr 17 21:37:53.586: INFO: Pod var-expansion-b97222b1-4a04-4231-97d6-00313a2decff no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:37:53.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-986" for this suite.
Apr 17 21:38:01.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:38:03.950: INFO: namespace var-expansion-986 deletion completed in 10.337243726s

â€¢ [SLOW TEST:14.704 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:38:03.950: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Apr 17 21:38:04.159: INFO: Waiting up to 5m0s for pod "downward-api-002cdd1a-e376-4f17-92f0-d3eb51184762" in namespace "downward-api-9232" to be "success or failure"
Apr 17 21:38:04.172: INFO: Pod "downward-api-002cdd1a-e376-4f17-92f0-d3eb51184762": Phase="Pending", Reason="", readiness=false. Elapsed: 12.983865ms
Apr 17 21:38:06.186: INFO: Pod "downward-api-002cdd1a-e376-4f17-92f0-d3eb51184762": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026294168s
Apr 17 21:38:08.199: INFO: Pod "downward-api-002cdd1a-e376-4f17-92f0-d3eb51184762": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039907613s
STEP: Saw pod success
Apr 17 21:38:08.199: INFO: Pod "downward-api-002cdd1a-e376-4f17-92f0-d3eb51184762" satisfied condition "success or failure"
Apr 17 21:38:08.216: INFO: Trying to get logs from node 10.72.119.72 pod downward-api-002cdd1a-e376-4f17-92f0-d3eb51184762 container dapi-container: <nil>
STEP: delete the pod
Apr 17 21:38:08.343: INFO: Waiting for pod downward-api-002cdd1a-e376-4f17-92f0-d3eb51184762 to disappear
Apr 17 21:38:08.354: INFO: Pod downward-api-002cdd1a-e376-4f17-92f0-d3eb51184762 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:38:08.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9232" for this suite.
Apr 17 21:38:16.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:38:18.591: INFO: namespace downward-api-9232 deletion completed in 10.213107249s

â€¢ [SLOW TEST:14.641 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:38:18.592: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:38:18.779: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-9d21dd07-da0d-4c04-975b-5db978fc306f
STEP: Creating secret with name s-test-opt-upd-fde12546-1e60-486d-8191-bd6e038c92bc
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9d21dd07-da0d-4c04-975b-5db978fc306f
STEP: Updating secret s-test-opt-upd-fde12546-1e60-486d-8191-bd6e038c92bc
STEP: Creating secret with name s-test-opt-create-f5249683-b5c6-4ccd-8414-0061db1a2639
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:39:52.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5988" for this suite.
Apr 17 21:40:24.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:40:26.965: INFO: namespace projected-5988 deletion completed in 34.264296501s

â€¢ [SLOW TEST:128.374 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:40:26.966: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 17 21:40:27.201: INFO: Waiting up to 5m0s for pod "pod-007ed46a-555d-4cc3-ac2a-1f8b4a11a04b" in namespace "emptydir-5381" to be "success or failure"
Apr 17 21:40:27.219: INFO: Pod "pod-007ed46a-555d-4cc3-ac2a-1f8b4a11a04b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.874391ms
Apr 17 21:40:29.237: INFO: Pod "pod-007ed46a-555d-4cc3-ac2a-1f8b4a11a04b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035248717s
STEP: Saw pod success
Apr 17 21:40:29.237: INFO: Pod "pod-007ed46a-555d-4cc3-ac2a-1f8b4a11a04b" satisfied condition "success or failure"
Apr 17 21:40:29.248: INFO: Trying to get logs from node 10.72.119.74 pod pod-007ed46a-555d-4cc3-ac2a-1f8b4a11a04b container test-container: <nil>
STEP: delete the pod
Apr 17 21:40:29.344: INFO: Waiting for pod pod-007ed46a-555d-4cc3-ac2a-1f8b4a11a04b to disappear
Apr 17 21:40:29.356: INFO: Pod pod-007ed46a-555d-4cc3-ac2a-1f8b4a11a04b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:40:29.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5381" for this suite.
Apr 17 21:40:37.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:40:39.657: INFO: namespace emptydir-5381 deletion completed in 10.254004515s

â€¢ [SLOW TEST:12.691 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:40:39.657: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-m79c
STEP: Creating a pod to test atomic-volume-subpath
Apr 17 21:40:39.943: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-m79c" in namespace "subpath-4559" to be "success or failure"
Apr 17 21:40:39.956: INFO: Pod "pod-subpath-test-configmap-m79c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.226475ms
Apr 17 21:40:41.968: INFO: Pod "pod-subpath-test-configmap-m79c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025230999s
Apr 17 21:40:43.981: INFO: Pod "pod-subpath-test-configmap-m79c": Phase="Running", Reason="", readiness=true. Elapsed: 4.037757261s
Apr 17 21:40:45.993: INFO: Pod "pod-subpath-test-configmap-m79c": Phase="Running", Reason="", readiness=true. Elapsed: 6.050373624s
Apr 17 21:40:48.005: INFO: Pod "pod-subpath-test-configmap-m79c": Phase="Running", Reason="", readiness=true. Elapsed: 8.062247747s
Apr 17 21:40:50.018: INFO: Pod "pod-subpath-test-configmap-m79c": Phase="Running", Reason="", readiness=true. Elapsed: 10.075134071s
Apr 17 21:40:52.030: INFO: Pod "pod-subpath-test-configmap-m79c": Phase="Running", Reason="", readiness=true. Elapsed: 12.087461767s
Apr 17 21:40:54.044: INFO: Pod "pod-subpath-test-configmap-m79c": Phase="Running", Reason="", readiness=true. Elapsed: 14.100912873s
Apr 17 21:40:56.057: INFO: Pod "pod-subpath-test-configmap-m79c": Phase="Running", Reason="", readiness=true. Elapsed: 16.114290893s
Apr 17 21:40:58.071: INFO: Pod "pod-subpath-test-configmap-m79c": Phase="Running", Reason="", readiness=true. Elapsed: 18.127854006s
Apr 17 21:41:00.084: INFO: Pod "pod-subpath-test-configmap-m79c": Phase="Running", Reason="", readiness=true. Elapsed: 20.141373433s
Apr 17 21:41:02.096: INFO: Pod "pod-subpath-test-configmap-m79c": Phase="Running", Reason="", readiness=true. Elapsed: 22.153186334s
Apr 17 21:41:04.109: INFO: Pod "pod-subpath-test-configmap-m79c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.165989042s
STEP: Saw pod success
Apr 17 21:41:04.109: INFO: Pod "pod-subpath-test-configmap-m79c" satisfied condition "success or failure"
Apr 17 21:41:04.123: INFO: Trying to get logs from node 10.72.119.74 pod pod-subpath-test-configmap-m79c container test-container-subpath-configmap-m79c: <nil>
STEP: delete the pod
Apr 17 21:41:04.197: INFO: Waiting for pod pod-subpath-test-configmap-m79c to disappear
Apr 17 21:41:04.208: INFO: Pod pod-subpath-test-configmap-m79c no longer exists
STEP: Deleting pod pod-subpath-test-configmap-m79c
Apr 17 21:41:04.208: INFO: Deleting pod "pod-subpath-test-configmap-m79c" in namespace "subpath-4559"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:41:04.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4559" for this suite.
Apr 17 21:41:12.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:41:14.719: INFO: namespace subpath-4559 deletion completed in 10.466811083s

â€¢ [SLOW TEST:35.061 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:41:14.719: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 21:41:16.110: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 17 21:41:18.159: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722756476, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722756476, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722756476, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722756476, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 21:41:21.211: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:41:21.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9277" for this suite.
Apr 17 21:41:29.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:41:32.342: INFO: namespace webhook-9277 deletion completed in 10.442756355s
STEP: Destroying namespace "webhook-9277-markers" for this suite.
Apr 17 21:41:40.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:41:42.504: INFO: namespace webhook-9277-markers deletion completed in 10.162563576s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:27.862 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:41:42.582: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:41:53.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9021" for this suite.
Apr 17 21:42:02.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:42:04.239: INFO: namespace resourcequota-9021 deletion completed in 10.244687218s

â€¢ [SLOW TEST:21.657 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:42:04.240: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:42:04.437: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-b5654b10-ed52-435c-8ba7-aa7cb72045c5" in namespace "security-context-test-5594" to be "success or failure"
Apr 17 21:42:04.449: INFO: Pod "busybox-readonly-false-b5654b10-ed52-435c-8ba7-aa7cb72045c5": Phase="Pending", Reason="", readiness=false. Elapsed: 11.805161ms
Apr 17 21:42:06.460: INFO: Pod "busybox-readonly-false-b5654b10-ed52-435c-8ba7-aa7cb72045c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023137069s
Apr 17 21:42:08.473: INFO: Pod "busybox-readonly-false-b5654b10-ed52-435c-8ba7-aa7cb72045c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03572492s
Apr 17 21:42:08.473: INFO: Pod "busybox-readonly-false-b5654b10-ed52-435c-8ba7-aa7cb72045c5" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:42:08.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5594" for this suite.
Apr 17 21:42:16.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:42:18.758: INFO: namespace security-context-test-5594 deletion completed in 10.254626217s

â€¢ [SLOW TEST:14.518 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:42:18.759: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-kdlk6 in namespace proxy-9028
I0417 21:42:18.985156      22 runners.go:184] Created replication controller with name: proxy-service-kdlk6, namespace: proxy-9028, replica count: 1
I0417 21:42:20.036658      22 runners.go:184] proxy-service-kdlk6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0417 21:42:21.036931      22 runners.go:184] proxy-service-kdlk6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0417 21:42:22.037264      22 runners.go:184] proxy-service-kdlk6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0417 21:42:23.037586      22 runners.go:184] proxy-service-kdlk6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 17 21:42:23.053: INFO: setup took 4.137833289s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr 17 21:42:23.093: INFO: (0) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 38.529989ms)
Apr 17 21:42:23.093: INFO: (0) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 39.162001ms)
Apr 17 21:42:23.093: INFO: (0) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 39.230382ms)
Apr 17 21:42:23.103: INFO: (0) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 48.495009ms)
Apr 17 21:42:23.103: INFO: (0) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 48.837427ms)
Apr 17 21:42:23.103: INFO: (0) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 48.675624ms)
Apr 17 21:42:23.113: INFO: (0) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 58.79481ms)
Apr 17 21:42:23.113: INFO: (0) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 59.456762ms)
Apr 17 21:42:23.113: INFO: (0) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 58.975514ms)
Apr 17 21:42:23.113: INFO: (0) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 58.749809ms)
Apr 17 21:42:23.117: INFO: (0) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 62.779588ms)
Apr 17 21:42:23.117: INFO: (0) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 62.704075ms)
Apr 17 21:42:23.120: INFO: (0) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 65.774231ms)
Apr 17 21:42:23.121: INFO: (0) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 66.851316ms)
Apr 17 21:42:23.124: INFO: (0) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 69.848692ms)
Apr 17 21:42:23.126: INFO: (0) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 72.35601ms)
Apr 17 21:42:23.154: INFO: (1) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 27.11327ms)
Apr 17 21:42:23.156: INFO: (1) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 29.021237ms)
Apr 17 21:42:23.156: INFO: (1) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 29.623196ms)
Apr 17 21:42:23.160: INFO: (1) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 32.309011ms)
Apr 17 21:42:23.163: INFO: (1) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 35.480739ms)
Apr 17 21:42:23.163: INFO: (1) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 35.597431ms)
Apr 17 21:42:23.163: INFO: (1) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 36.2412ms)
Apr 17 21:42:23.164: INFO: (1) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 36.330943ms)
Apr 17 21:42:23.164: INFO: (1) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 36.600429ms)
Apr 17 21:42:23.164: INFO: (1) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 36.487724ms)
Apr 17 21:42:23.167: INFO: (1) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 40.95062ms)
Apr 17 21:42:23.168: INFO: (1) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 40.247991ms)
Apr 17 21:42:23.171: INFO: (1) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 43.511124ms)
Apr 17 21:42:23.171: INFO: (1) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 43.664039ms)
Apr 17 21:42:23.172: INFO: (1) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 44.078483ms)
Apr 17 21:42:23.175: INFO: (1) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 47.558286ms)
Apr 17 21:42:23.192: INFO: (2) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 16.675281ms)
Apr 17 21:42:23.194: INFO: (2) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 19.221087ms)
Apr 17 21:42:23.203: INFO: (2) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 27.249434ms)
Apr 17 21:42:23.203: INFO: (2) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 27.398966ms)
Apr 17 21:42:23.207: INFO: (2) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 31.915849ms)
Apr 17 21:42:23.207: INFO: (2) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 31.535301ms)
Apr 17 21:42:23.207: INFO: (2) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 31.565222ms)
Apr 17 21:42:23.207: INFO: (2) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 31.464924ms)
Apr 17 21:42:23.207: INFO: (2) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 31.955586ms)
Apr 17 21:42:23.207: INFO: (2) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 32.111724ms)
Apr 17 21:42:23.208: INFO: (2) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 31.840812ms)
Apr 17 21:42:23.217: INFO: (2) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 41.408741ms)
Apr 17 21:42:23.219: INFO: (2) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 42.702234ms)
Apr 17 21:42:23.219: INFO: (2) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 43.057645ms)
Apr 17 21:42:23.221: INFO: (2) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 45.500635ms)
Apr 17 21:42:23.221: INFO: (2) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 45.649056ms)
Apr 17 21:42:23.244: INFO: (3) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 23.009633ms)
Apr 17 21:42:23.246: INFO: (3) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 24.272609ms)
Apr 17 21:42:23.248: INFO: (3) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 26.301287ms)
Apr 17 21:42:23.251: INFO: (3) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 29.079242ms)
Apr 17 21:42:23.255: INFO: (3) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 32.691655ms)
Apr 17 21:42:23.255: INFO: (3) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 32.899745ms)
Apr 17 21:42:23.255: INFO: (3) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 32.982133ms)
Apr 17 21:42:23.255: INFO: (3) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 33.223836ms)
Apr 17 21:42:23.255: INFO: (3) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 33.10315ms)
Apr 17 21:42:23.255: INFO: (3) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 33.22637ms)
Apr 17 21:42:23.255: INFO: (3) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 33.385561ms)
Apr 17 21:42:23.257: INFO: (3) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 35.061068ms)
Apr 17 21:42:23.261: INFO: (3) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 39.304667ms)
Apr 17 21:42:23.262: INFO: (3) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 39.844901ms)
Apr 17 21:42:23.262: INFO: (3) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 40.201917ms)
Apr 17 21:42:23.265: INFO: (3) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 42.945242ms)
Apr 17 21:42:23.290: INFO: (4) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 25.421307ms)
Apr 17 21:42:23.293: INFO: (4) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 27.27595ms)
Apr 17 21:42:23.294: INFO: (4) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 27.837631ms)
Apr 17 21:42:23.294: INFO: (4) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 28.009871ms)
Apr 17 21:42:23.297: INFO: (4) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 31.152978ms)
Apr 17 21:42:23.297: INFO: (4) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 31.220896ms)
Apr 17 21:42:23.297: INFO: (4) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 31.190009ms)
Apr 17 21:42:23.297: INFO: (4) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 31.14307ms)
Apr 17 21:42:23.297: INFO: (4) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 31.213407ms)
Apr 17 21:42:23.297: INFO: (4) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 31.391812ms)
Apr 17 21:42:23.300: INFO: (4) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 34.398416ms)
Apr 17 21:42:23.302: INFO: (4) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 36.319752ms)
Apr 17 21:42:23.302: INFO: (4) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 36.893206ms)
Apr 17 21:42:23.304: INFO: (4) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 38.609712ms)
Apr 17 21:42:23.306: INFO: (4) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 39.683112ms)
Apr 17 21:42:23.306: INFO: (4) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 40.273003ms)
Apr 17 21:42:23.326: INFO: (5) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 20.120297ms)
Apr 17 21:42:23.332: INFO: (5) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 25.266344ms)
Apr 17 21:42:23.335: INFO: (5) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 28.622893ms)
Apr 17 21:42:23.337: INFO: (5) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 30.094034ms)
Apr 17 21:42:23.339: INFO: (5) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 32.559658ms)
Apr 17 21:42:23.340: INFO: (5) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 33.828487ms)
Apr 17 21:42:23.341: INFO: (5) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 34.719272ms)
Apr 17 21:42:23.343: INFO: (5) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 36.077084ms)
Apr 17 21:42:23.343: INFO: (5) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 36.169442ms)
Apr 17 21:42:23.343: INFO: (5) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 36.609885ms)
Apr 17 21:42:23.345: INFO: (5) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 38.474986ms)
Apr 17 21:42:23.345: INFO: (5) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 38.549477ms)
Apr 17 21:42:23.350: INFO: (5) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 43.741243ms)
Apr 17 21:42:23.352: INFO: (5) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 45.074492ms)
Apr 17 21:42:23.354: INFO: (5) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 47.337066ms)
Apr 17 21:42:23.354: INFO: (5) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 47.75815ms)
Apr 17 21:42:23.376: INFO: (6) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 22.037788ms)
Apr 17 21:42:23.381: INFO: (6) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 26.316174ms)
Apr 17 21:42:23.384: INFO: (6) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 28.390498ms)
Apr 17 21:42:23.385: INFO: (6) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 29.459781ms)
Apr 17 21:42:23.388: INFO: (6) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 32.602571ms)
Apr 17 21:42:23.390: INFO: (6) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 34.190841ms)
Apr 17 21:42:23.393: INFO: (6) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 36.866697ms)
Apr 17 21:42:23.393: INFO: (6) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 37.66456ms)
Apr 17 21:42:23.395: INFO: (6) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 39.776242ms)
Apr 17 21:42:23.403: INFO: (6) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 46.646935ms)
Apr 17 21:42:23.403: INFO: (6) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 46.793705ms)
Apr 17 21:42:23.405: INFO: (6) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 48.581197ms)
Apr 17 21:42:23.406: INFO: (6) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 49.932944ms)
Apr 17 21:42:23.412: INFO: (6) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 55.446395ms)
Apr 17 21:42:23.412: INFO: (6) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 55.652367ms)
Apr 17 21:42:23.412: INFO: (6) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 56.138808ms)
Apr 17 21:42:23.465: INFO: (7) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 52.609827ms)
Apr 17 21:42:23.466: INFO: (7) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 53.012951ms)
Apr 17 21:42:23.470: INFO: (7) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 57.205531ms)
Apr 17 21:42:23.471: INFO: (7) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 58.047948ms)
Apr 17 21:42:23.481: INFO: (7) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 68.317108ms)
Apr 17 21:42:23.487: INFO: (7) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 73.886695ms)
Apr 17 21:42:23.489: INFO: (7) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 75.944117ms)
Apr 17 21:42:23.491: INFO: (7) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 78.02665ms)
Apr 17 21:42:23.491: INFO: (7) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 78.373245ms)
Apr 17 21:42:23.491: INFO: (7) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 78.163344ms)
Apr 17 21:42:23.491: INFO: (7) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 78.446102ms)
Apr 17 21:42:23.495: INFO: (7) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 82.890335ms)
Apr 17 21:42:23.497: INFO: (7) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 83.843518ms)
Apr 17 21:42:23.499: INFO: (7) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 86.211551ms)
Apr 17 21:42:23.499: INFO: (7) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 86.15403ms)
Apr 17 21:42:23.501: INFO: (7) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 88.325142ms)
Apr 17 21:42:23.546: INFO: (8) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 44.524604ms)
Apr 17 21:42:23.569: INFO: (8) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 67.535036ms)
Apr 17 21:42:23.569: INFO: (8) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 67.493629ms)
Apr 17 21:42:23.569: INFO: (8) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 68.020926ms)
Apr 17 21:42:23.569: INFO: (8) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 67.797049ms)
Apr 17 21:42:23.569: INFO: (8) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 67.640869ms)
Apr 17 21:42:23.584: INFO: (8) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 82.33664ms)
Apr 17 21:42:23.586: INFO: (8) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 84.299235ms)
Apr 17 21:42:23.588: INFO: (8) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 86.812745ms)
Apr 17 21:42:23.590: INFO: (8) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 88.233556ms)
Apr 17 21:42:23.598: INFO: (8) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 96.718469ms)
Apr 17 21:42:23.599: INFO: (8) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 97.395913ms)
Apr 17 21:42:23.602: INFO: (8) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 100.906199ms)
Apr 17 21:42:23.603: INFO: (8) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 101.070169ms)
Apr 17 21:42:23.603: INFO: (8) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 101.357464ms)
Apr 17 21:42:23.603: INFO: (8) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 101.641822ms)
Apr 17 21:42:23.628: INFO: (9) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 25.045438ms)
Apr 17 21:42:23.628: INFO: (9) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 25.427678ms)
Apr 17 21:42:23.632: INFO: (9) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 27.920836ms)
Apr 17 21:42:23.640: INFO: (9) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 36.425664ms)
Apr 17 21:42:23.640: INFO: (9) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 36.471671ms)
Apr 17 21:42:23.640: INFO: (9) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 36.531953ms)
Apr 17 21:42:23.640: INFO: (9) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 36.628151ms)
Apr 17 21:42:23.640: INFO: (9) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 36.703409ms)
Apr 17 21:42:23.640: INFO: (9) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 36.633161ms)
Apr 17 21:42:23.640: INFO: (9) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 36.621065ms)
Apr 17 21:42:23.646: INFO: (9) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 42.217177ms)
Apr 17 21:42:23.646: INFO: (9) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 42.33943ms)
Apr 17 21:42:23.649: INFO: (9) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 45.434135ms)
Apr 17 21:42:23.649: INFO: (9) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 45.676614ms)
Apr 17 21:42:23.655: INFO: (9) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 51.629846ms)
Apr 17 21:42:23.656: INFO: (9) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 51.968599ms)
Apr 17 21:42:23.675: INFO: (10) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 18.917928ms)
Apr 17 21:42:23.676: INFO: (10) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 20.084891ms)
Apr 17 21:42:23.676: INFO: (10) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 20.542082ms)
Apr 17 21:42:23.677: INFO: (10) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 20.384123ms)
Apr 17 21:42:23.680: INFO: (10) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 23.63747ms)
Apr 17 21:42:23.680: INFO: (10) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 23.646177ms)
Apr 17 21:42:23.680: INFO: (10) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 23.134928ms)
Apr 17 21:42:23.683: INFO: (10) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 26.176924ms)
Apr 17 21:42:23.684: INFO: (10) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 26.504787ms)
Apr 17 21:42:23.687: INFO: (10) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 30.861325ms)
Apr 17 21:42:23.687: INFO: (10) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 30.664763ms)
Apr 17 21:42:23.692: INFO: (10) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 34.931147ms)
Apr 17 21:42:23.696: INFO: (10) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 38.956229ms)
Apr 17 21:42:23.696: INFO: (10) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 38.867576ms)
Apr 17 21:42:23.696: INFO: (10) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 38.851363ms)
Apr 17 21:42:23.696: INFO: (10) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 38.844878ms)
Apr 17 21:42:23.726: INFO: (11) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 29.236354ms)
Apr 17 21:42:23.726: INFO: (11) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 30.314194ms)
Apr 17 21:42:23.732: INFO: (11) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 35.673745ms)
Apr 17 21:42:23.736: INFO: (11) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 39.421817ms)
Apr 17 21:42:23.738: INFO: (11) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 41.235734ms)
Apr 17 21:42:23.738: INFO: (11) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 40.744011ms)
Apr 17 21:42:23.738: INFO: (11) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 41.183179ms)
Apr 17 21:42:23.740: INFO: (11) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 42.578298ms)
Apr 17 21:42:23.740: INFO: (11) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 42.661724ms)
Apr 17 21:42:23.740: INFO: (11) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 42.88246ms)
Apr 17 21:42:23.742: INFO: (11) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 44.414734ms)
Apr 17 21:42:23.757: INFO: (11) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 60.11994ms)
Apr 17 21:42:23.757: INFO: (11) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 60.279488ms)
Apr 17 21:42:23.757: INFO: (11) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 60.310062ms)
Apr 17 21:42:23.758: INFO: (11) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 60.36343ms)
Apr 17 21:42:23.758: INFO: (11) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 60.328523ms)
Apr 17 21:42:23.784: INFO: (12) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 26.356443ms)
Apr 17 21:42:23.784: INFO: (12) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 26.182522ms)
Apr 17 21:42:23.789: INFO: (12) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 30.88047ms)
Apr 17 21:42:23.794: INFO: (12) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 35.928301ms)
Apr 17 21:42:23.796: INFO: (12) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 38.087624ms)
Apr 17 21:42:23.799: INFO: (12) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 40.588739ms)
Apr 17 21:42:23.800: INFO: (12) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 42.027372ms)
Apr 17 21:42:23.800: INFO: (12) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 41.984757ms)
Apr 17 21:42:23.803: INFO: (12) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 44.275774ms)
Apr 17 21:42:23.803: INFO: (12) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 44.529036ms)
Apr 17 21:42:23.822: INFO: (12) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 63.513197ms)
Apr 17 21:42:23.825: INFO: (12) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 66.721413ms)
Apr 17 21:42:23.826: INFO: (12) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 67.61758ms)
Apr 17 21:42:23.826: INFO: (12) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 67.596339ms)
Apr 17 21:42:23.828: INFO: (12) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 70.072072ms)
Apr 17 21:42:23.828: INFO: (12) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 70.274576ms)
Apr 17 21:42:23.849: INFO: (13) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 20.303197ms)
Apr 17 21:42:23.858: INFO: (13) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 28.865224ms)
Apr 17 21:42:23.859: INFO: (13) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 30.45281ms)
Apr 17 21:42:23.864: INFO: (13) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 34.532285ms)
Apr 17 21:42:23.867: INFO: (13) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 38.204805ms)
Apr 17 21:42:23.867: INFO: (13) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 36.94103ms)
Apr 17 21:42:23.867: INFO: (13) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 38.432829ms)
Apr 17 21:42:23.867: INFO: (13) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 38.147596ms)
Apr 17 21:42:23.868: INFO: (13) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 38.728247ms)
Apr 17 21:42:23.868: INFO: (13) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 38.875595ms)
Apr 17 21:42:23.874: INFO: (13) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 44.565696ms)
Apr 17 21:42:23.891: INFO: (13) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 61.910865ms)
Apr 17 21:42:23.891: INFO: (13) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 61.990176ms)
Apr 17 21:42:23.895: INFO: (13) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 65.744288ms)
Apr 17 21:42:23.895: INFO: (13) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 65.689004ms)
Apr 17 21:42:23.895: INFO: (13) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 66.158643ms)
Apr 17 21:42:23.919: INFO: (14) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 23.621775ms)
Apr 17 21:42:23.919: INFO: (14) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 23.526995ms)
Apr 17 21:42:23.919: INFO: (14) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 24.177431ms)
Apr 17 21:42:23.925: INFO: (14) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 29.723293ms)
Apr 17 21:42:23.927: INFO: (14) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 31.268225ms)
Apr 17 21:42:23.932: INFO: (14) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 36.131788ms)
Apr 17 21:42:23.932: INFO: (14) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 36.082305ms)
Apr 17 21:42:23.932: INFO: (14) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 36.385589ms)
Apr 17 21:42:23.932: INFO: (14) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 36.240266ms)
Apr 17 21:42:23.936: INFO: (14) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 40.118941ms)
Apr 17 21:42:23.936: INFO: (14) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 39.960413ms)
Apr 17 21:42:23.936: INFO: (14) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 39.920627ms)
Apr 17 21:42:23.945: INFO: (14) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 49.153375ms)
Apr 17 21:42:23.945: INFO: (14) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 49.558466ms)
Apr 17 21:42:23.945: INFO: (14) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 49.423436ms)
Apr 17 21:42:23.949: INFO: (14) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 52.876779ms)
Apr 17 21:42:23.967: INFO: (15) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 17.917952ms)
Apr 17 21:42:23.969: INFO: (15) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 19.9804ms)
Apr 17 21:42:23.974: INFO: (15) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 23.8482ms)
Apr 17 21:42:23.974: INFO: (15) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 24.863948ms)
Apr 17 21:42:23.978: INFO: (15) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 26.937961ms)
Apr 17 21:42:23.979: INFO: (15) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 28.604998ms)
Apr 17 21:42:23.979: INFO: (15) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 29.035621ms)
Apr 17 21:42:23.979: INFO: (15) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 27.158126ms)
Apr 17 21:42:23.979: INFO: (15) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 29.845089ms)
Apr 17 21:42:23.979: INFO: (15) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 27.26176ms)
Apr 17 21:42:23.979: INFO: (15) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 27.959553ms)
Apr 17 21:42:23.984: INFO: (15) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 33.342492ms)
Apr 17 21:42:23.989: INFO: (15) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 38.204407ms)
Apr 17 21:42:23.991: INFO: (15) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 40.305741ms)
Apr 17 21:42:23.993: INFO: (15) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 42.423438ms)
Apr 17 21:42:23.996: INFO: (15) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 45.939817ms)
Apr 17 21:42:24.024: INFO: (16) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 27.808662ms)
Apr 17 21:42:24.029: INFO: (16) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 30.708218ms)
Apr 17 21:42:24.050: INFO: (16) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 52.890747ms)
Apr 17 21:42:24.050: INFO: (16) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 50.990518ms)
Apr 17 21:42:24.050: INFO: (16) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 51.779919ms)
Apr 17 21:42:24.050: INFO: (16) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 51.32854ms)
Apr 17 21:42:24.050: INFO: (16) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 51.97747ms)
Apr 17 21:42:24.050: INFO: (16) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 52.15319ms)
Apr 17 21:42:24.050: INFO: (16) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 51.514049ms)
Apr 17 21:42:24.050: INFO: (16) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 52.337556ms)
Apr 17 21:42:24.050: INFO: (16) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 52.87424ms)
Apr 17 21:42:24.051: INFO: (16) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 52.292838ms)
Apr 17 21:42:24.053: INFO: (16) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 56.318856ms)
Apr 17 21:42:24.054: INFO: (16) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 54.775152ms)
Apr 17 21:42:24.055: INFO: (16) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 56.857837ms)
Apr 17 21:42:24.056: INFO: (16) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 59.073177ms)
Apr 17 21:42:24.077: INFO: (17) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 19.549929ms)
Apr 17 21:42:24.077: INFO: (17) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 19.896079ms)
Apr 17 21:42:24.081: INFO: (17) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 23.191329ms)
Apr 17 21:42:24.081: INFO: (17) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 24.27155ms)
Apr 17 21:42:24.089: INFO: (17) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 32.081245ms)
Apr 17 21:42:24.090: INFO: (17) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 31.992764ms)
Apr 17 21:42:24.090: INFO: (17) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 32.250978ms)
Apr 17 21:42:24.090: INFO: (17) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 33.132356ms)
Apr 17 21:42:24.090: INFO: (17) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 32.943878ms)
Apr 17 21:42:24.090: INFO: (17) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 32.411365ms)
Apr 17 21:42:24.091: INFO: (17) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 33.516349ms)
Apr 17 21:42:24.095: INFO: (17) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 37.200381ms)
Apr 17 21:42:24.100: INFO: (17) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 42.66648ms)
Apr 17 21:42:24.101: INFO: (17) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 43.482408ms)
Apr 17 21:42:24.101: INFO: (17) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 43.822239ms)
Apr 17 21:42:24.104: INFO: (17) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 46.095552ms)
Apr 17 21:42:24.126: INFO: (18) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 21.840673ms)
Apr 17 21:42:24.126: INFO: (18) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 22.416624ms)
Apr 17 21:42:24.148: INFO: (18) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 43.754201ms)
Apr 17 21:42:24.149: INFO: (18) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 44.36544ms)
Apr 17 21:42:24.152: INFO: (18) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 47.727761ms)
Apr 17 21:42:24.153: INFO: (18) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 49.091635ms)
Apr 17 21:42:24.153: INFO: (18) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 49.065969ms)
Apr 17 21:42:24.156: INFO: (18) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 51.424366ms)
Apr 17 21:42:24.156: INFO: (18) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 51.582024ms)
Apr 17 21:42:24.156: INFO: (18) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 51.756024ms)
Apr 17 21:42:24.156: INFO: (18) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 51.830133ms)
Apr 17 21:42:24.157: INFO: (18) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 51.815984ms)
Apr 17 21:42:24.159: INFO: (18) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 54.53055ms)
Apr 17 21:42:24.160: INFO: (18) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 55.419939ms)
Apr 17 21:42:24.163: INFO: (18) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 59.173961ms)
Apr 17 21:42:24.164: INFO: (18) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 58.80783ms)
Apr 17 21:42:24.190: INFO: (19) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 26.070972ms)
Apr 17 21:42:24.190: INFO: (19) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch/proxy/rewriteme">test</a> (200; 26.026437ms)
Apr 17 21:42:24.225: INFO: (19) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:443/proxy/tlsrewritem... (200; 60.744833ms)
Apr 17 21:42:24.225: INFO: (19) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:462/proxy/: tls qux (200; 61.106766ms)
Apr 17 21:42:24.228: INFO: (19) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:162/proxy/: bar (200; 63.641111ms)
Apr 17 21:42:24.228: INFO: (19) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">... (200; 63.731205ms)
Apr 17 21:42:24.228: INFO: (19) /api/v1/namespaces/proxy-9028/pods/https:proxy-service-kdlk6-wwrch:460/proxy/: tls baz (200; 63.78942ms)
Apr 17 21:42:24.228: INFO: (19) /api/v1/namespaces/proxy-9028/pods/http:proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 63.829832ms)
Apr 17 21:42:24.228: INFO: (19) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:160/proxy/: foo (200; 64.015321ms)
Apr 17 21:42:24.228: INFO: (19) /api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/: <a href="/api/v1/namespaces/proxy-9028/pods/proxy-service-kdlk6-wwrch:1080/proxy/rewriteme">test<... (200; 63.928793ms)
Apr 17 21:42:24.230: INFO: (19) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname2/proxy/: tls qux (200; 65.415655ms)
Apr 17 21:42:24.232: INFO: (19) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname2/proxy/: bar (200; 67.416441ms)
Apr 17 21:42:24.232: INFO: (19) /api/v1/namespaces/proxy-9028/services/https:proxy-service-kdlk6:tlsportname1/proxy/: tls baz (200; 67.436689ms)
Apr 17 21:42:24.237: INFO: (19) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname1/proxy/: foo (200; 72.823699ms)
Apr 17 21:42:24.237: INFO: (19) /api/v1/namespaces/proxy-9028/services/proxy-service-kdlk6:portname1/proxy/: foo (200; 72.930603ms)
Apr 17 21:42:24.238: INFO: (19) /api/v1/namespaces/proxy-9028/services/http:proxy-service-kdlk6:portname2/proxy/: bar (200; 73.971252ms)
STEP: deleting ReplicationController proxy-service-kdlk6 in namespace proxy-9028, will wait for the garbage collector to delete the pods
Apr 17 21:42:24.344: INFO: Deleting ReplicationController proxy-service-kdlk6 took: 38.55239ms
Apr 17 21:42:24.946: INFO: Terminating ReplicationController proxy-service-kdlk6 pods took: 601.924799ms
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:42:32.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9028" for this suite.
Apr 17 21:42:40.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:42:41.894: INFO: namespace proxy-9028 deletion completed in 9.411984288s

â€¢ [SLOW TEST:23.136 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:42:41.896: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3589
I0417 21:42:42.062636      22 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3589, replica count: 1
I0417 21:42:43.114177      22 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0417 21:42:44.114440      22 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 17 21:42:44.250: INFO: Created: latency-svc-7hb4d
Apr 17 21:42:44.277: INFO: Got endpoints: latency-svc-7hb4d [62.989494ms]
Apr 17 21:42:44.313: INFO: Created: latency-svc-7zpbh
Apr 17 21:42:44.335: INFO: Got endpoints: latency-svc-7zpbh [57.439931ms]
Apr 17 21:42:44.336: INFO: Created: latency-svc-2n8ld
Apr 17 21:42:44.372: INFO: Got endpoints: latency-svc-2n8ld [93.773674ms]
Apr 17 21:42:44.373: INFO: Created: latency-svc-47kjk
Apr 17 21:42:44.379: INFO: Got endpoints: latency-svc-47kjk [100.647884ms]
Apr 17 21:42:44.394: INFO: Created: latency-svc-58j5d
Apr 17 21:42:44.414: INFO: Got endpoints: latency-svc-58j5d [134.989483ms]
Apr 17 21:42:44.417: INFO: Created: latency-svc-gjzz4
Apr 17 21:42:44.431: INFO: Got endpoints: latency-svc-gjzz4 [150.710131ms]
Apr 17 21:42:44.441: INFO: Created: latency-svc-q9948
Apr 17 21:42:44.456: INFO: Got endpoints: latency-svc-q9948 [176.62311ms]
Apr 17 21:42:44.458: INFO: Created: latency-svc-8rxt7
Apr 17 21:42:44.473: INFO: Got endpoints: latency-svc-8rxt7 [189.952174ms]
Apr 17 21:42:44.475: INFO: Created: latency-svc-cdsm2
Apr 17 21:42:44.490: INFO: Got endpoints: latency-svc-cdsm2 [209.498214ms]
Apr 17 21:42:44.503: INFO: Created: latency-svc-cwgcj
Apr 17 21:42:44.508: INFO: Got endpoints: latency-svc-cwgcj [227.235321ms]
Apr 17 21:42:44.511: INFO: Created: latency-svc-xnpdz
Apr 17 21:42:44.527: INFO: Got endpoints: latency-svc-xnpdz [245.260675ms]
Apr 17 21:42:44.531: INFO: Created: latency-svc-wtp62
Apr 17 21:42:44.547: INFO: Got endpoints: latency-svc-wtp62 [266.846229ms]
Apr 17 21:42:44.561: INFO: Created: latency-svc-kbw4t
Apr 17 21:42:44.569: INFO: Got endpoints: latency-svc-kbw4t [59.2917ms]
Apr 17 21:42:44.572: INFO: Created: latency-svc-xsgj4
Apr 17 21:42:44.587: INFO: Got endpoints: latency-svc-xsgj4 [303.010909ms]
Apr 17 21:42:44.596: INFO: Created: latency-svc-96tzh
Apr 17 21:42:44.606: INFO: Got endpoints: latency-svc-96tzh [322.903508ms]
Apr 17 21:42:44.609: INFO: Created: latency-svc-mx9pj
Apr 17 21:42:44.621: INFO: Got endpoints: latency-svc-mx9pj [338.944998ms]
Apr 17 21:42:44.625: INFO: Created: latency-svc-c7xsc
Apr 17 21:42:44.642: INFO: Got endpoints: latency-svc-c7xsc [358.174673ms]
Apr 17 21:42:44.644: INFO: Created: latency-svc-jspbt
Apr 17 21:42:44.657: INFO: Got endpoints: latency-svc-jspbt [321.146752ms]
Apr 17 21:42:44.658: INFO: Created: latency-svc-b2kmd
Apr 17 21:42:44.672: INFO: Got endpoints: latency-svc-b2kmd [300.041215ms]
Apr 17 21:42:44.684: INFO: Created: latency-svc-7q47c
Apr 17 21:42:44.699: INFO: Got endpoints: latency-svc-7q47c [319.998929ms]
Apr 17 21:42:44.719: INFO: Created: latency-svc-jtvzh
Apr 17 21:42:44.732: INFO: Got endpoints: latency-svc-jtvzh [317.63177ms]
Apr 17 21:42:44.737: INFO: Created: latency-svc-hhdvv
Apr 17 21:42:44.752: INFO: Got endpoints: latency-svc-hhdvv [321.045089ms]
Apr 17 21:42:44.757: INFO: Created: latency-svc-tlvhn
Apr 17 21:42:44.772: INFO: Created: latency-svc-n2j2l
Apr 17 21:42:44.786: INFO: Created: latency-svc-jbhbg
Apr 17 21:42:44.788: INFO: Got endpoints: latency-svc-tlvhn [331.41095ms]
Apr 17 21:42:44.788: INFO: Got endpoints: latency-svc-n2j2l [315.726524ms]
Apr 17 21:42:44.802: INFO: Got endpoints: latency-svc-jbhbg [311.10709ms]
Apr 17 21:42:44.803: INFO: Created: latency-svc-r4kzz
Apr 17 21:42:44.821: INFO: Created: latency-svc-gfd5z
Apr 17 21:42:44.830: INFO: Got endpoints: latency-svc-r4kzz [302.86547ms]
Apr 17 21:42:44.836: INFO: Created: latency-svc-7w6sc
Apr 17 21:42:44.843: INFO: Got endpoints: latency-svc-gfd5z [295.339398ms]
Apr 17 21:42:44.849: INFO: Got endpoints: latency-svc-7w6sc [280.261126ms]
Apr 17 21:42:44.852: INFO: Created: latency-svc-v7dtf
Apr 17 21:42:44.869: INFO: Got endpoints: latency-svc-v7dtf [281.740413ms]
Apr 17 21:42:44.869: INFO: Created: latency-svc-dqhkg
Apr 17 21:42:44.888: INFO: Created: latency-svc-t9c8v
Apr 17 21:42:44.893: INFO: Got endpoints: latency-svc-dqhkg [286.262526ms]
Apr 17 21:42:44.904: INFO: Created: latency-svc-p2hq4
Apr 17 21:42:44.905: INFO: Got endpoints: latency-svc-t9c8v [283.987375ms]
Apr 17 21:42:44.920: INFO: Got endpoints: latency-svc-p2hq4 [277.617718ms]
Apr 17 21:42:44.924: INFO: Created: latency-svc-ds688
Apr 17 21:42:44.935: INFO: Got endpoints: latency-svc-ds688 [277.850766ms]
Apr 17 21:42:44.944: INFO: Created: latency-svc-p6mnw
Apr 17 21:42:44.957: INFO: Got endpoints: latency-svc-p6mnw [285.016282ms]
Apr 17 21:42:44.965: INFO: Created: latency-svc-7x9l4
Apr 17 21:42:44.979: INFO: Created: latency-svc-zzpdq
Apr 17 21:42:44.980: INFO: Got endpoints: latency-svc-7x9l4 [280.603543ms]
Apr 17 21:42:44.991: INFO: Got endpoints: latency-svc-zzpdq [259.094445ms]
Apr 17 21:42:44.998: INFO: Created: latency-svc-6zm8d
Apr 17 21:42:45.012: INFO: Got endpoints: latency-svc-6zm8d [260.275669ms]
Apr 17 21:42:45.021: INFO: Created: latency-svc-wt9ng
Apr 17 21:42:45.035: INFO: Created: latency-svc-qjd7s
Apr 17 21:42:45.037: INFO: Got endpoints: latency-svc-wt9ng [249.051378ms]
Apr 17 21:42:45.051: INFO: Got endpoints: latency-svc-qjd7s [262.75463ms]
Apr 17 21:42:45.063: INFO: Created: latency-svc-9sss9
Apr 17 21:42:45.075: INFO: Created: latency-svc-mjghw
Apr 17 21:42:45.078: INFO: Got endpoints: latency-svc-9sss9 [276.023475ms]
Apr 17 21:42:45.091: INFO: Got endpoints: latency-svc-mjghw [260.846651ms]
Apr 17 21:42:45.093: INFO: Created: latency-svc-665qh
Apr 17 21:42:45.113: INFO: Got endpoints: latency-svc-665qh [270.398358ms]
Apr 17 21:42:45.119: INFO: Created: latency-svc-568ml
Apr 17 21:42:45.133: INFO: Created: latency-svc-r5qxq
Apr 17 21:42:45.143: INFO: Got endpoints: latency-svc-568ml [293.670808ms]
Apr 17 21:42:45.152: INFO: Created: latency-svc-8pv55
Apr 17 21:42:45.161: INFO: Got endpoints: latency-svc-r5qxq [292.733116ms]
Apr 17 21:42:45.167: INFO: Got endpoints: latency-svc-8pv55 [274.515994ms]
Apr 17 21:42:45.169: INFO: Created: latency-svc-dfm68
Apr 17 21:42:45.187: INFO: Got endpoints: latency-svc-dfm68 [281.509092ms]
Apr 17 21:42:45.193: INFO: Created: latency-svc-dv9ck
Apr 17 21:42:45.213: INFO: Created: latency-svc-dbn9z
Apr 17 21:42:45.213: INFO: Got endpoints: latency-svc-dv9ck [291.94476ms]
Apr 17 21:42:45.230: INFO: Created: latency-svc-t6qtq
Apr 17 21:42:45.237: INFO: Got endpoints: latency-svc-dbn9z [301.199279ms]
Apr 17 21:42:45.246: INFO: Created: latency-svc-sqfnk
Apr 17 21:42:45.267: INFO: Created: latency-svc-zs92k
Apr 17 21:42:45.271: INFO: Got endpoints: latency-svc-t6qtq [312.972435ms]
Apr 17 21:42:45.274: INFO: Got endpoints: latency-svc-sqfnk [293.794725ms]
Apr 17 21:42:45.287: INFO: Got endpoints: latency-svc-zs92k [294.550613ms]
Apr 17 21:42:45.292: INFO: Created: latency-svc-khtkl
Apr 17 21:42:45.295: INFO: Got endpoints: latency-svc-khtkl [282.914037ms]
Apr 17 21:42:45.307: INFO: Created: latency-svc-td6p9
Apr 17 21:42:45.321: INFO: Got endpoints: latency-svc-td6p9 [283.975173ms]
Apr 17 21:42:45.326: INFO: Created: latency-svc-nwqsq
Apr 17 21:42:45.341: INFO: Got endpoints: latency-svc-nwqsq [289.846675ms]
Apr 17 21:42:45.349: INFO: Created: latency-svc-jr5jf
Apr 17 21:42:45.368: INFO: Got endpoints: latency-svc-jr5jf [290.060391ms]
Apr 17 21:42:45.379: INFO: Created: latency-svc-xphd4
Apr 17 21:42:45.389: INFO: Got endpoints: latency-svc-xphd4 [298.216439ms]
Apr 17 21:42:45.392: INFO: Created: latency-svc-dbm2j
Apr 17 21:42:45.407: INFO: Created: latency-svc-29b6r
Apr 17 21:42:45.408: INFO: Got endpoints: latency-svc-dbm2j [294.230668ms]
Apr 17 21:42:45.421: INFO: Got endpoints: latency-svc-29b6r [278.290905ms]
Apr 17 21:42:45.422: INFO: Created: latency-svc-nzspc
Apr 17 21:42:45.438: INFO: Got endpoints: latency-svc-nzspc [275.935143ms]
Apr 17 21:42:45.444: INFO: Created: latency-svc-slmtc
Apr 17 21:42:45.453: INFO: Got endpoints: latency-svc-slmtc [285.276955ms]
Apr 17 21:42:45.460: INFO: Created: latency-svc-8hssd
Apr 17 21:42:45.476: INFO: Got endpoints: latency-svc-8hssd [289.163479ms]
Apr 17 21:42:45.477: INFO: Created: latency-svc-vq75g
Apr 17 21:42:45.487: INFO: Got endpoints: latency-svc-vq75g [273.666892ms]
Apr 17 21:42:45.492: INFO: Created: latency-svc-w5cqw
Apr 17 21:42:45.516: INFO: Got endpoints: latency-svc-w5cqw [278.426889ms]
Apr 17 21:42:45.527: INFO: Created: latency-svc-hhtx8
Apr 17 21:42:45.532: INFO: Got endpoints: latency-svc-hhtx8 [261.197414ms]
Apr 17 21:42:45.535: INFO: Created: latency-svc-6bq24
Apr 17 21:42:45.550: INFO: Created: latency-svc-2k4zw
Apr 17 21:42:45.551: INFO: Got endpoints: latency-svc-6bq24 [276.721292ms]
Apr 17 21:42:45.563: INFO: Got endpoints: latency-svc-2k4zw [276.135826ms]
Apr 17 21:42:45.567: INFO: Created: latency-svc-dhs2j
Apr 17 21:42:45.580: INFO: Created: latency-svc-2tk58
Apr 17 21:42:45.583: INFO: Got endpoints: latency-svc-dhs2j [287.575778ms]
Apr 17 21:42:45.596: INFO: Got endpoints: latency-svc-2tk58 [274.500537ms]
Apr 17 21:42:45.596: INFO: Created: latency-svc-ls9rh
Apr 17 21:42:45.610: INFO: Got endpoints: latency-svc-ls9rh [268.59024ms]
Apr 17 21:42:45.624: INFO: Created: latency-svc-hxm2x
Apr 17 21:42:45.631: INFO: Created: latency-svc-b6sfh
Apr 17 21:42:45.634: INFO: Got endpoints: latency-svc-hxm2x [266.030716ms]
Apr 17 21:42:45.648: INFO: Got endpoints: latency-svc-b6sfh [258.541441ms]
Apr 17 21:42:45.655: INFO: Created: latency-svc-558t6
Apr 17 21:42:45.667: INFO: Got endpoints: latency-svc-558t6 [259.507551ms]
Apr 17 21:42:45.681: INFO: Created: latency-svc-6568w
Apr 17 21:42:45.695: INFO: Got endpoints: latency-svc-6568w [273.733317ms]
Apr 17 21:42:45.710: INFO: Created: latency-svc-n9dc7
Apr 17 21:42:45.771: INFO: Created: latency-svc-zgr5r
Apr 17 21:42:45.772: INFO: Created: latency-svc-lbrcc
Apr 17 21:42:45.772: INFO: Created: latency-svc-ksprc
Apr 17 21:42:45.772: INFO: Got endpoints: latency-svc-ksprc [295.635309ms]
Apr 17 21:42:45.773: INFO: Got endpoints: latency-svc-n9dc7 [335.057978ms]
Apr 17 21:42:45.773: INFO: Got endpoints: latency-svc-lbrcc [320.591789ms]
Apr 17 21:42:45.783: INFO: Got endpoints: latency-svc-zgr5r [295.445591ms]
Apr 17 21:42:45.789: INFO: Created: latency-svc-wb555
Apr 17 21:42:45.806: INFO: Got endpoints: latency-svc-wb555 [290.538073ms]
Apr 17 21:42:45.811: INFO: Created: latency-svc-hwrn4
Apr 17 21:42:45.822: INFO: Got endpoints: latency-svc-hwrn4 [289.544533ms]
Apr 17 21:42:45.838: INFO: Created: latency-svc-h5bv5
Apr 17 21:42:45.849: INFO: Got endpoints: latency-svc-h5bv5 [298.096083ms]
Apr 17 21:42:45.850: INFO: Created: latency-svc-xsql7
Apr 17 21:42:45.866: INFO: Got endpoints: latency-svc-xsql7 [302.610747ms]
Apr 17 21:42:45.867: INFO: Created: latency-svc-92cjm
Apr 17 21:42:45.879: INFO: Got endpoints: latency-svc-92cjm [296.59828ms]
Apr 17 21:42:45.888: INFO: Created: latency-svc-dvnn8
Apr 17 21:42:45.899: INFO: Got endpoints: latency-svc-dvnn8 [302.993628ms]
Apr 17 21:42:45.901: INFO: Created: latency-svc-8gsfb
Apr 17 21:42:45.915: INFO: Got endpoints: latency-svc-8gsfb [304.609078ms]
Apr 17 21:42:45.919: INFO: Created: latency-svc-xnp7h
Apr 17 21:42:45.934: INFO: Got endpoints: latency-svc-xnp7h [300.177571ms]
Apr 17 21:42:45.940: INFO: Created: latency-svc-v2m2g
Apr 17 21:42:45.952: INFO: Got endpoints: latency-svc-v2m2g [304.08805ms]
Apr 17 21:42:45.954: INFO: Created: latency-svc-txvl7
Apr 17 21:42:45.967: INFO: Got endpoints: latency-svc-txvl7 [299.466071ms]
Apr 17 21:42:45.975: INFO: Created: latency-svc-pmt88
Apr 17 21:42:45.994: INFO: Got endpoints: latency-svc-pmt88 [298.851147ms]
Apr 17 21:42:45.996: INFO: Created: latency-svc-qnv4w
Apr 17 21:42:46.011: INFO: Created: latency-svc-xwbkz
Apr 17 21:42:46.013: INFO: Got endpoints: latency-svc-qnv4w [241.051939ms]
Apr 17 21:42:46.028: INFO: Got endpoints: latency-svc-xwbkz [254.981834ms]
Apr 17 21:42:46.029: INFO: Created: latency-svc-rggjm
Apr 17 21:42:46.044: INFO: Got endpoints: latency-svc-rggjm [270.44863ms]
Apr 17 21:42:46.045: INFO: Created: latency-svc-wjk2w
Apr 17 21:42:46.067: INFO: Got endpoints: latency-svc-wjk2w [284.33796ms]
Apr 17 21:42:46.070: INFO: Created: latency-svc-vj7mr
Apr 17 21:42:46.080: INFO: Created: latency-svc-jl6z8
Apr 17 21:42:46.080: INFO: Got endpoints: latency-svc-vj7mr [273.841399ms]
Apr 17 21:42:46.094: INFO: Created: latency-svc-p2cr5
Apr 17 21:42:46.101: INFO: Got endpoints: latency-svc-jl6z8 [279.332035ms]
Apr 17 21:42:46.111: INFO: Created: latency-svc-dm7sw
Apr 17 21:42:46.114: INFO: Got endpoints: latency-svc-p2cr5 [264.244827ms]
Apr 17 21:42:46.125: INFO: Got endpoints: latency-svc-dm7sw [258.734599ms]
Apr 17 21:42:46.137: INFO: Created: latency-svc-lh742
Apr 17 21:42:46.151: INFO: Got endpoints: latency-svc-lh742 [271.485696ms]
Apr 17 21:42:46.154: INFO: Created: latency-svc-mjjvm
Apr 17 21:42:46.175: INFO: Created: latency-svc-p48l9
Apr 17 21:42:46.175: INFO: Got endpoints: latency-svc-mjjvm [275.21403ms]
Apr 17 21:42:46.191: INFO: Got endpoints: latency-svc-p48l9 [276.548762ms]
Apr 17 21:42:46.218: INFO: Created: latency-svc-d8spl
Apr 17 21:42:46.230: INFO: Got endpoints: latency-svc-d8spl [295.784386ms]
Apr 17 21:42:46.231: INFO: Created: latency-svc-4zlh2
Apr 17 21:42:46.247: INFO: Got endpoints: latency-svc-4zlh2 [294.595649ms]
Apr 17 21:42:46.260: INFO: Created: latency-svc-76xm6
Apr 17 21:42:46.271: INFO: Created: latency-svc-vf9rd
Apr 17 21:42:46.274: INFO: Got endpoints: latency-svc-76xm6 [307.438699ms]
Apr 17 21:42:46.285: INFO: Created: latency-svc-78nhj
Apr 17 21:42:46.288: INFO: Got endpoints: latency-svc-vf9rd [293.652728ms]
Apr 17 21:42:46.298: INFO: Created: latency-svc-btrlt
Apr 17 21:42:46.300: INFO: Got endpoints: latency-svc-78nhj [287.496048ms]
Apr 17 21:42:46.312: INFO: Got endpoints: latency-svc-btrlt [284.569761ms]
Apr 17 21:42:46.317: INFO: Created: latency-svc-5rgtq
Apr 17 21:42:46.334: INFO: Got endpoints: latency-svc-5rgtq [290.199702ms]
Apr 17 21:42:46.334: INFO: Created: latency-svc-mfw2b
Apr 17 21:42:46.349: INFO: Got endpoints: latency-svc-mfw2b [281.631526ms]
Apr 17 21:42:46.351: INFO: Created: latency-svc-7gnxd
Apr 17 21:42:46.369: INFO: Got endpoints: latency-svc-7gnxd [288.372221ms]
Apr 17 21:42:46.370: INFO: Created: latency-svc-vhml9
Apr 17 21:42:46.387: INFO: Created: latency-svc-vk8ld
Apr 17 21:42:46.392: INFO: Got endpoints: latency-svc-vhml9 [291.12867ms]
Apr 17 21:42:46.401: INFO: Got endpoints: latency-svc-vk8ld [286.873445ms]
Apr 17 21:42:46.415: INFO: Created: latency-svc-vjkhx
Apr 17 21:42:46.428: INFO: Got endpoints: latency-svc-vjkhx [303.230947ms]
Apr 17 21:42:46.430: INFO: Created: latency-svc-7p8b7
Apr 17 21:42:46.447: INFO: Created: latency-svc-s6q8z
Apr 17 21:42:46.448: INFO: Got endpoints: latency-svc-7p8b7 [296.370857ms]
Apr 17 21:42:46.463: INFO: Got endpoints: latency-svc-s6q8z [288.695083ms]
Apr 17 21:42:46.470: INFO: Created: latency-svc-ck5t8
Apr 17 21:42:46.485: INFO: Got endpoints: latency-svc-ck5t8 [293.826652ms]
Apr 17 21:42:46.486: INFO: Created: latency-svc-rfglb
Apr 17 21:42:46.501: INFO: Got endpoints: latency-svc-rfglb [270.545759ms]
Apr 17 21:42:46.506: INFO: Created: latency-svc-9t54j
Apr 17 21:42:46.517: INFO: Got endpoints: latency-svc-9t54j [270.435664ms]
Apr 17 21:42:46.523: INFO: Created: latency-svc-w4hg2
Apr 17 21:42:46.543: INFO: Got endpoints: latency-svc-w4hg2 [268.828374ms]
Apr 17 21:42:46.543: INFO: Created: latency-svc-zzqnr
Apr 17 21:42:46.556: INFO: Got endpoints: latency-svc-zzqnr [267.727893ms]
Apr 17 21:42:46.581: INFO: Created: latency-svc-qq5kt
Apr 17 21:42:46.593: INFO: Got endpoints: latency-svc-qq5kt [292.348037ms]
Apr 17 21:42:46.593: INFO: Created: latency-svc-nhmnj
Apr 17 21:42:46.608: INFO: Got endpoints: latency-svc-nhmnj [295.839074ms]
Apr 17 21:42:46.617: INFO: Created: latency-svc-cmtds
Apr 17 21:42:46.640: INFO: Got endpoints: latency-svc-cmtds [305.381921ms]
Apr 17 21:42:46.640: INFO: Created: latency-svc-2tr4k
Apr 17 21:42:46.651: INFO: Got endpoints: latency-svc-2tr4k [301.774996ms]
Apr 17 21:42:46.660: INFO: Created: latency-svc-kp55x
Apr 17 21:42:46.674: INFO: Created: latency-svc-pjd87
Apr 17 21:42:46.676: INFO: Got endpoints: latency-svc-kp55x [307.411202ms]
Apr 17 21:42:46.689: INFO: Got endpoints: latency-svc-pjd87 [296.469505ms]
Apr 17 21:42:46.697: INFO: Created: latency-svc-h9tpf
Apr 17 21:42:46.708: INFO: Created: latency-svc-fcq6p
Apr 17 21:42:46.710: INFO: Got endpoints: latency-svc-h9tpf [309.564937ms]
Apr 17 21:42:46.722: INFO: Got endpoints: latency-svc-fcq6p [293.578664ms]
Apr 17 21:42:46.723: INFO: Created: latency-svc-wnwdz
Apr 17 21:42:46.738: INFO: Got endpoints: latency-svc-wnwdz [290.237821ms]
Apr 17 21:42:46.741: INFO: Created: latency-svc-p2zzq
Apr 17 21:42:46.757: INFO: Created: latency-svc-72kml
Apr 17 21:42:46.758: INFO: Got endpoints: latency-svc-p2zzq [295.006918ms]
Apr 17 21:42:46.778: INFO: Got endpoints: latency-svc-72kml [292.51355ms]
Apr 17 21:42:46.785: INFO: Created: latency-svc-gcd2q
Apr 17 21:42:46.798: INFO: Got endpoints: latency-svc-gcd2q [297.095354ms]
Apr 17 21:42:46.802: INFO: Created: latency-svc-jqpsf
Apr 17 21:42:46.818: INFO: Got endpoints: latency-svc-jqpsf [300.969197ms]
Apr 17 21:42:46.823: INFO: Created: latency-svc-84vfh
Apr 17 21:42:46.844: INFO: Created: latency-svc-7b9vj
Apr 17 21:42:46.847: INFO: Got endpoints: latency-svc-84vfh [303.759821ms]
Apr 17 21:42:46.860: INFO: Created: latency-svc-7ft54
Apr 17 21:42:46.870: INFO: Got endpoints: latency-svc-7b9vj [313.679168ms]
Apr 17 21:42:46.878: INFO: Created: latency-svc-nhx9b
Apr 17 21:42:46.879: INFO: Got endpoints: latency-svc-7ft54 [285.663573ms]
Apr 17 21:42:46.900: INFO: Got endpoints: latency-svc-nhx9b [291.438111ms]
Apr 17 21:42:46.902: INFO: Created: latency-svc-nsgn9
Apr 17 21:42:46.915: INFO: Got endpoints: latency-svc-nsgn9 [274.960224ms]
Apr 17 21:42:46.947: INFO: Created: latency-svc-t4p97
Apr 17 21:42:46.950: INFO: Got endpoints: latency-svc-t4p97 [298.780049ms]
Apr 17 21:42:46.972: INFO: Created: latency-svc-j2xj6
Apr 17 21:42:46.989: INFO: Created: latency-svc-2qc95
Apr 17 21:42:47.003: INFO: Got endpoints: latency-svc-j2xj6 [326.842521ms]
Apr 17 21:42:47.006: INFO: Created: latency-svc-p8ghz
Apr 17 21:42:47.012: INFO: Got endpoints: latency-svc-2qc95 [323.133203ms]
Apr 17 21:42:47.022: INFO: Got endpoints: latency-svc-p8ghz [311.732973ms]
Apr 17 21:42:47.024: INFO: Created: latency-svc-gsjqn
Apr 17 21:42:47.039: INFO: Got endpoints: latency-svc-gsjqn [316.840701ms]
Apr 17 21:42:47.046: INFO: Created: latency-svc-kphhp
Apr 17 21:42:47.062: INFO: Created: latency-svc-zhbfj
Apr 17 21:42:47.062: INFO: Got endpoints: latency-svc-kphhp [324.595979ms]
Apr 17 21:42:47.075: INFO: Got endpoints: latency-svc-zhbfj [316.488848ms]
Apr 17 21:42:47.087: INFO: Created: latency-svc-tbwlh
Apr 17 21:42:47.103: INFO: Got endpoints: latency-svc-tbwlh [324.857167ms]
Apr 17 21:42:47.108: INFO: Created: latency-svc-hpzvr
Apr 17 21:42:47.125: INFO: Got endpoints: latency-svc-hpzvr [327.002092ms]
Apr 17 21:42:47.125: INFO: Created: latency-svc-gmtj9
Apr 17 21:42:47.143: INFO: Got endpoints: latency-svc-gmtj9 [324.092645ms]
Apr 17 21:42:47.161: INFO: Created: latency-svc-k95mr
Apr 17 21:42:47.162: INFO: Got endpoints: latency-svc-k95mr [314.36773ms]
Apr 17 21:42:47.183: INFO: Created: latency-svc-ls76l
Apr 17 21:42:47.199: INFO: Got endpoints: latency-svc-ls76l [329.411358ms]
Apr 17 21:42:47.208: INFO: Created: latency-svc-vqsg7
Apr 17 21:42:47.223: INFO: Got endpoints: latency-svc-vqsg7 [344.493621ms]
Apr 17 21:42:47.236: INFO: Created: latency-svc-5t9ng
Apr 17 21:42:47.247: INFO: Got endpoints: latency-svc-5t9ng [347.086514ms]
Apr 17 21:42:47.248: INFO: Created: latency-svc-b84jr
Apr 17 21:42:47.263: INFO: Got endpoints: latency-svc-b84jr [348.683105ms]
Apr 17 21:42:47.270: INFO: Created: latency-svc-7dppf
Apr 17 21:42:47.284: INFO: Got endpoints: latency-svc-7dppf [334.308796ms]
Apr 17 21:42:47.289: INFO: Created: latency-svc-7g84z
Apr 17 21:42:47.306: INFO: Got endpoints: latency-svc-7g84z [302.040988ms]
Apr 17 21:42:47.310: INFO: Created: latency-svc-gtjmc
Apr 17 21:42:47.325: INFO: Got endpoints: latency-svc-gtjmc [312.83535ms]
Apr 17 21:42:47.355: INFO: Created: latency-svc-r8v5q
Apr 17 21:42:47.355: INFO: Got endpoints: latency-svc-r8v5q [332.901787ms]
Apr 17 21:42:47.367: INFO: Created: latency-svc-ns8kp
Apr 17 21:42:47.380: INFO: Created: latency-svc-gzzzr
Apr 17 21:42:47.380: INFO: Got endpoints: latency-svc-ns8kp [341.313584ms]
Apr 17 21:42:47.393: INFO: Got endpoints: latency-svc-gzzzr [330.774086ms]
Apr 17 21:42:47.400: INFO: Created: latency-svc-sw8n2
Apr 17 21:42:47.414: INFO: Got endpoints: latency-svc-sw8n2 [338.988482ms]
Apr 17 21:42:47.422: INFO: Created: latency-svc-sxtmt
Apr 17 21:42:47.432: INFO: Got endpoints: latency-svc-sxtmt [328.944034ms]
Apr 17 21:42:47.443: INFO: Created: latency-svc-lkzgx
Apr 17 21:42:47.455: INFO: Got endpoints: latency-svc-lkzgx [330.055048ms]
Apr 17 21:42:47.467: INFO: Created: latency-svc-cdkbz
Apr 17 21:42:47.481: INFO: Created: latency-svc-f9rwn
Apr 17 21:42:47.481: INFO: Got endpoints: latency-svc-cdkbz [338.683237ms]
Apr 17 21:42:47.494: INFO: Got endpoints: latency-svc-f9rwn [332.575807ms]
Apr 17 21:42:47.498: INFO: Created: latency-svc-44bph
Apr 17 21:42:47.512: INFO: Got endpoints: latency-svc-44bph [312.420934ms]
Apr 17 21:42:47.518: INFO: Created: latency-svc-h4qgf
Apr 17 21:42:47.534: INFO: Got endpoints: latency-svc-h4qgf [310.496485ms]
Apr 17 21:42:47.544: INFO: Created: latency-svc-kq6mn
Apr 17 21:42:47.554: INFO: Got endpoints: latency-svc-kq6mn [306.925907ms]
Apr 17 21:42:47.558: INFO: Created: latency-svc-fm8tk
Apr 17 21:42:47.573: INFO: Got endpoints: latency-svc-fm8tk [309.528997ms]
Apr 17 21:42:47.589: INFO: Created: latency-svc-gvmkj
Apr 17 21:42:47.601: INFO: Got endpoints: latency-svc-gvmkj [316.659917ms]
Apr 17 21:42:47.605: INFO: Created: latency-svc-zkwtv
Apr 17 21:42:47.613: INFO: Got endpoints: latency-svc-zkwtv [307.438457ms]
Apr 17 21:42:47.620: INFO: Created: latency-svc-fd9ct
Apr 17 21:42:47.637: INFO: Got endpoints: latency-svc-fd9ct [311.424262ms]
Apr 17 21:42:47.647: INFO: Created: latency-svc-qwz2t
Apr 17 21:42:47.664: INFO: Got endpoints: latency-svc-qwz2t [308.451739ms]
Apr 17 21:42:47.678: INFO: Created: latency-svc-ggvt8
Apr 17 21:42:47.695: INFO: Got endpoints: latency-svc-ggvt8 [314.688684ms]
Apr 17 21:42:47.704: INFO: Created: latency-svc-st69k
Apr 17 21:42:47.714: INFO: Got endpoints: latency-svc-st69k [320.606663ms]
Apr 17 21:42:47.718: INFO: Created: latency-svc-pqkd6
Apr 17 21:42:47.731: INFO: Got endpoints: latency-svc-pqkd6 [316.598349ms]
Apr 17 21:42:47.738: INFO: Created: latency-svc-7n5d9
Apr 17 21:42:47.753: INFO: Got endpoints: latency-svc-7n5d9 [321.506003ms]
Apr 17 21:42:47.793: INFO: Created: latency-svc-2qcc8
Apr 17 21:42:47.800: INFO: Got endpoints: latency-svc-2qcc8 [344.627525ms]
Apr 17 21:42:47.819: INFO: Created: latency-svc-7rqjp
Apr 17 21:42:47.835: INFO: Got endpoints: latency-svc-7rqjp [353.542214ms]
Apr 17 21:42:47.839: INFO: Created: latency-svc-rswfz
Apr 17 21:42:47.857: INFO: Got endpoints: latency-svc-rswfz [362.808633ms]
Apr 17 21:42:47.866: INFO: Created: latency-svc-ttwb9
Apr 17 21:42:47.881: INFO: Got endpoints: latency-svc-ttwb9 [368.640147ms]
Apr 17 21:42:47.885: INFO: Created: latency-svc-kcvv4
Apr 17 21:42:47.899: INFO: Got endpoints: latency-svc-kcvv4 [365.339087ms]
Apr 17 21:42:47.903: INFO: Created: latency-svc-6dv8k
Apr 17 21:42:47.919: INFO: Got endpoints: latency-svc-6dv8k [364.668795ms]
Apr 17 21:42:47.924: INFO: Created: latency-svc-74vdq
Apr 17 21:42:47.938: INFO: Got endpoints: latency-svc-74vdq [364.988072ms]
Apr 17 21:42:47.940: INFO: Created: latency-svc-kzdm8
Apr 17 21:42:47.956: INFO: Created: latency-svc-mmnvg
Apr 17 21:42:47.958: INFO: Got endpoints: latency-svc-kzdm8 [356.775165ms]
Apr 17 21:42:47.974: INFO: Got endpoints: latency-svc-mmnvg [360.403878ms]
Apr 17 21:42:47.976: INFO: Created: latency-svc-4dgl5
Apr 17 21:42:47.990: INFO: Got endpoints: latency-svc-4dgl5 [352.808951ms]
Apr 17 21:42:47.991: INFO: Created: latency-svc-k9879
Apr 17 21:42:48.006: INFO: Got endpoints: latency-svc-k9879 [342.465364ms]
Apr 17 21:42:48.014: INFO: Created: latency-svc-x7j4s
Apr 17 21:42:48.034: INFO: Created: latency-svc-zpqkp
Apr 17 21:42:48.044: INFO: Got endpoints: latency-svc-x7j4s [348.166053ms]
Apr 17 21:42:48.056: INFO: Created: latency-svc-6nt6v
Apr 17 21:42:48.056: INFO: Got endpoints: latency-svc-zpqkp [342.08838ms]
Apr 17 21:42:48.068: INFO: Got endpoints: latency-svc-6nt6v [337.111571ms]
Apr 17 21:42:48.081: INFO: Created: latency-svc-6pwrw
Apr 17 21:42:48.093: INFO: Created: latency-svc-j5r4w
Apr 17 21:42:48.095: INFO: Got endpoints: latency-svc-6pwrw [341.59689ms]
Apr 17 21:42:48.109: INFO: Got endpoints: latency-svc-j5r4w [308.473026ms]
Apr 17 21:42:48.111: INFO: Created: latency-svc-97wk9
Apr 17 21:42:48.124: INFO: Got endpoints: latency-svc-97wk9 [288.397219ms]
Apr 17 21:42:48.128: INFO: Created: latency-svc-q4qn8
Apr 17 21:42:48.141: INFO: Got endpoints: latency-svc-q4qn8 [284.064844ms]
Apr 17 21:42:48.146: INFO: Created: latency-svc-9zw5w
Apr 17 21:42:48.162: INFO: Got endpoints: latency-svc-9zw5w [280.502912ms]
Apr 17 21:42:48.163: INFO: Created: latency-svc-kqr7t
Apr 17 21:42:48.179: INFO: Got endpoints: latency-svc-kqr7t [279.771178ms]
Apr 17 21:42:48.183: INFO: Created: latency-svc-r5bbw
Apr 17 21:42:48.205: INFO: Got endpoints: latency-svc-r5bbw [285.799914ms]
Apr 17 21:42:48.206: INFO: Created: latency-svc-b49lp
Apr 17 21:42:48.222: INFO: Created: latency-svc-lk948
Apr 17 21:42:48.237: INFO: Got endpoints: latency-svc-b49lp [298.372792ms]
Apr 17 21:42:48.244: INFO: Created: latency-svc-7z879
Apr 17 21:42:48.262: INFO: Got endpoints: latency-svc-lk948 [303.658653ms]
Apr 17 21:42:48.262: INFO: Created: latency-svc-rvxwr
Apr 17 21:42:48.266: INFO: Got endpoints: latency-svc-7z879 [292.529435ms]
Apr 17 21:42:48.279: INFO: Created: latency-svc-nrkf7
Apr 17 21:42:48.280: INFO: Got endpoints: latency-svc-rvxwr [289.898498ms]
Apr 17 21:42:48.294: INFO: Got endpoints: latency-svc-nrkf7 [287.728665ms]
Apr 17 21:42:48.298: INFO: Created: latency-svc-h7js5
Apr 17 21:42:48.313: INFO: Got endpoints: latency-svc-h7js5 [269.214735ms]
Apr 17 21:42:48.313: INFO: Latencies: [57.439931ms 59.2917ms 93.773674ms 100.647884ms 134.989483ms 150.710131ms 176.62311ms 189.952174ms 209.498214ms 227.235321ms 241.051939ms 245.260675ms 249.051378ms 254.981834ms 258.541441ms 258.734599ms 259.094445ms 259.507551ms 260.275669ms 260.846651ms 261.197414ms 262.75463ms 264.244827ms 266.030716ms 266.846229ms 267.727893ms 268.59024ms 268.828374ms 269.214735ms 270.398358ms 270.435664ms 270.44863ms 270.545759ms 271.485696ms 273.666892ms 273.733317ms 273.841399ms 274.500537ms 274.515994ms 274.960224ms 275.21403ms 275.935143ms 276.023475ms 276.135826ms 276.548762ms 276.721292ms 277.617718ms 277.850766ms 278.290905ms 278.426889ms 279.332035ms 279.771178ms 280.261126ms 280.502912ms 280.603543ms 281.509092ms 281.631526ms 281.740413ms 282.914037ms 283.975173ms 283.987375ms 284.064844ms 284.33796ms 284.569761ms 285.016282ms 285.276955ms 285.663573ms 285.799914ms 286.262526ms 286.873445ms 287.496048ms 287.575778ms 287.728665ms 288.372221ms 288.397219ms 288.695083ms 289.163479ms 289.544533ms 289.846675ms 289.898498ms 290.060391ms 290.199702ms 290.237821ms 290.538073ms 291.12867ms 291.438111ms 291.94476ms 292.348037ms 292.51355ms 292.529435ms 292.733116ms 293.578664ms 293.652728ms 293.670808ms 293.794725ms 293.826652ms 294.230668ms 294.550613ms 294.595649ms 295.006918ms 295.339398ms 295.445591ms 295.635309ms 295.784386ms 295.839074ms 296.370857ms 296.469505ms 296.59828ms 297.095354ms 298.096083ms 298.216439ms 298.372792ms 298.780049ms 298.851147ms 299.466071ms 300.041215ms 300.177571ms 300.969197ms 301.199279ms 301.774996ms 302.040988ms 302.610747ms 302.86547ms 302.993628ms 303.010909ms 303.230947ms 303.658653ms 303.759821ms 304.08805ms 304.609078ms 305.381921ms 306.925907ms 307.411202ms 307.438457ms 307.438699ms 308.451739ms 308.473026ms 309.528997ms 309.564937ms 310.496485ms 311.10709ms 311.424262ms 311.732973ms 312.420934ms 312.83535ms 312.972435ms 313.679168ms 314.36773ms 314.688684ms 315.726524ms 316.488848ms 316.598349ms 316.659917ms 316.840701ms 317.63177ms 319.998929ms 320.591789ms 320.606663ms 321.045089ms 321.146752ms 321.506003ms 322.903508ms 323.133203ms 324.092645ms 324.595979ms 324.857167ms 326.842521ms 327.002092ms 328.944034ms 329.411358ms 330.055048ms 330.774086ms 331.41095ms 332.575807ms 332.901787ms 334.308796ms 335.057978ms 337.111571ms 338.683237ms 338.944998ms 338.988482ms 341.313584ms 341.59689ms 342.08838ms 342.465364ms 344.493621ms 344.627525ms 347.086514ms 348.166053ms 348.683105ms 352.808951ms 353.542214ms 356.775165ms 358.174673ms 360.403878ms 362.808633ms 364.668795ms 364.988072ms 365.339087ms 368.640147ms]
Apr 17 21:42:48.313: INFO: 50 %ile: 295.339398ms
Apr 17 21:42:48.313: INFO: 90 %ile: 338.988482ms
Apr 17 21:42:48.313: INFO: 99 %ile: 365.339087ms
Apr 17 21:42:48.313: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:42:48.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3589" for this suite.
Apr 17 21:43:12.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:43:14.481: INFO: namespace svc-latency-3589 deletion completed in 26.134596402s

â€¢ [SLOW TEST:32.586 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:43:14.482: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 17 21:43:14.712: INFO: Waiting up to 5m0s for pod "pod-bbf267b3-4810-4ae6-a42a-6485ffb6b3ab" in namespace "emptydir-2624" to be "success or failure"
Apr 17 21:43:14.725: INFO: Pod "pod-bbf267b3-4810-4ae6-a42a-6485ffb6b3ab": Phase="Pending", Reason="", readiness=false. Elapsed: 12.613203ms
Apr 17 21:43:16.736: INFO: Pod "pod-bbf267b3-4810-4ae6-a42a-6485ffb6b3ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023584303s
Apr 17 21:43:18.751: INFO: Pod "pod-bbf267b3-4810-4ae6-a42a-6485ffb6b3ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038353172s
STEP: Saw pod success
Apr 17 21:43:18.751: INFO: Pod "pod-bbf267b3-4810-4ae6-a42a-6485ffb6b3ab" satisfied condition "success or failure"
Apr 17 21:43:18.765: INFO: Trying to get logs from node 10.72.119.74 pod pod-bbf267b3-4810-4ae6-a42a-6485ffb6b3ab container test-container: <nil>
STEP: delete the pod
Apr 17 21:43:18.868: INFO: Waiting for pod pod-bbf267b3-4810-4ae6-a42a-6485ffb6b3ab to disappear
Apr 17 21:43:18.880: INFO: Pod pod-bbf267b3-4810-4ae6-a42a-6485ffb6b3ab no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:43:18.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2624" for this suite.
Apr 17 21:43:26.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:43:29.075: INFO: namespace emptydir-2624 deletion completed in 10.165494405s

â€¢ [SLOW TEST:14.594 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:43:29.075: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Apr 17 21:43:29.291: INFO: Waiting up to 5m0s for pod "var-expansion-c152628d-7a9c-4081-9db0-add795e22625" in namespace "var-expansion-3491" to be "success or failure"
Apr 17 21:43:29.311: INFO: Pod "var-expansion-c152628d-7a9c-4081-9db0-add795e22625": Phase="Pending", Reason="", readiness=false. Elapsed: 19.487135ms
Apr 17 21:43:31.329: INFO: Pod "var-expansion-c152628d-7a9c-4081-9db0-add795e22625": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.037395761s
STEP: Saw pod success
Apr 17 21:43:31.329: INFO: Pod "var-expansion-c152628d-7a9c-4081-9db0-add795e22625" satisfied condition "success or failure"
Apr 17 21:43:31.341: INFO: Trying to get logs from node 10.72.119.74 pod var-expansion-c152628d-7a9c-4081-9db0-add795e22625 container dapi-container: <nil>
STEP: delete the pod
Apr 17 21:43:31.417: INFO: Waiting for pod var-expansion-c152628d-7a9c-4081-9db0-add795e22625 to disappear
Apr 17 21:43:31.429: INFO: Pod var-expansion-c152628d-7a9c-4081-9db0-add795e22625 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:43:31.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3491" for this suite.
Apr 17 21:43:39.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:43:41.632: INFO: namespace var-expansion-3491 deletion completed in 10.161733456s

â€¢ [SLOW TEST:12.556 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:43:41.634: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:43:41.776: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:43:47.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4365" for this suite.
Apr 17 21:43:55.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:43:58.087: INFO: namespace custom-resource-definition-4365 deletion completed in 10.201858291s

â€¢ [SLOW TEST:16.454 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:43:58.088: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Apr 17 21:43:58.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 create -f - --namespace=kubectl-4966'
Apr 17 21:43:58.774: INFO: stderr: ""
Apr 17 21:43:58.774: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 17 21:43:59.786: INFO: Selector matched 1 pods for map[app:redis]
Apr 17 21:43:59.786: INFO: Found 0 / 1
Apr 17 21:44:00.787: INFO: Selector matched 1 pods for map[app:redis]
Apr 17 21:44:00.787: INFO: Found 0 / 1
Apr 17 21:44:01.789: INFO: Selector matched 1 pods for map[app:redis]
Apr 17 21:44:01.789: INFO: Found 1 / 1
Apr 17 21:44:01.789: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr 17 21:44:01.800: INFO: Selector matched 1 pods for map[app:redis]
Apr 17 21:44:01.800: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 17 21:44:01.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 patch pod redis-master-tr2dt --namespace=kubectl-4966 -p {"metadata":{"annotations":{"x":"y"}}}'
Apr 17 21:44:01.997: INFO: stderr: ""
Apr 17 21:44:01.997: INFO: stdout: "pod/redis-master-tr2dt patched\n"
STEP: checking annotations
Apr 17 21:44:02.009: INFO: Selector matched 1 pods for map[app:redis]
Apr 17 21:44:02.010: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:44:02.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4966" for this suite.
Apr 17 21:44:16.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:44:18.258: INFO: namespace kubectl-4966 deletion completed in 16.205078373s

â€¢ [SLOW TEST:20.171 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:44:18.259: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-5195
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5195 to expose endpoints map[]
Apr 17 21:44:18.491: INFO: successfully validated that service endpoint-test2 in namespace services-5195 exposes endpoints map[] (26.550785ms elapsed)
STEP: Creating pod pod1 in namespace services-5195
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5195 to expose endpoints map[pod1:[80]]
Apr 17 21:44:20.629: INFO: successfully validated that service endpoint-test2 in namespace services-5195 exposes endpoints map[pod1:[80]] (2.096627664s elapsed)
STEP: Creating pod pod2 in namespace services-5195
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5195 to expose endpoints map[pod1:[80] pod2:[80]]
Apr 17 21:44:22.831: INFO: successfully validated that service endpoint-test2 in namespace services-5195 exposes endpoints map[pod1:[80] pod2:[80]] (2.127452837s elapsed)
STEP: Deleting pod pod1 in namespace services-5195
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5195 to expose endpoints map[pod2:[80]]
Apr 17 21:44:23.911: INFO: successfully validated that service endpoint-test2 in namespace services-5195 exposes endpoints map[pod2:[80]] (1.059724119s elapsed)
STEP: Deleting pod pod2 in namespace services-5195
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5195 to expose endpoints map[]
Apr 17 21:44:24.969: INFO: successfully validated that service endpoint-test2 in namespace services-5195 exposes endpoints map[] (1.03841092s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:44:25.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5195" for this suite.
Apr 17 21:44:41.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:44:43.322: INFO: namespace services-5195 deletion completed in 18.233558208s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:25.063 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:44:43.322: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:44:43.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2823" for this suite.
Apr 17 21:44:51.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:44:53.819: INFO: namespace resourcequota-2823 deletion completed in 10.202798173s

â€¢ [SLOW TEST:10.496 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:44:53.819: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Apr 17 21:44:53.971: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 17 21:44:54.058: INFO: Waiting for terminating namespaces to be deleted...
Apr 17 21:44:54.081: INFO: 
Logging pods the kubelet thinks is on node 10.72.119.72 before test
Apr 17 21:44:54.248: INFO: sonobuoy from sonobuoy started at 2020-04-17 20:47:07 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 17 21:44:54.249: INFO: configmap-cabundle-injector-5cf6d9695-kfct9 from openshift-service-ca started at 2020-04-17 19:11:11 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Apr 17 21:44:54.249: INFO: console-operator-db5d785db-r4qh6 from openshift-console-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container console-operator ready: true, restart count 1
Apr 17 21:44:54.249: INFO: multus-wlttp from openshift-multus started at 2020-04-17 19:09:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container kube-multus ready: true, restart count 0
Apr 17 21:44:54.249: INFO: openshift-kube-proxy-bkf2z from openshift-kube-proxy started at 2020-04-17 19:10:02 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:44:54.249: INFO: cluster-image-registry-operator-7857d56744-c2pch from openshift-image-registry started at 2020-04-17 19:10:34 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Apr 17 21:44:54.249: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Apr 17 21:44:54.249: INFO: ibm-cloud-provider-ip-158-176-122-172-6f57dbd74b-cdqd6 from ibm-system started at 2020-04-17 19:13:49 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container ibm-cloud-provider-ip-158-176-122-172 ready: true, restart count 0
Apr 17 21:44:54.249: INFO: network-operator-64f597f5d-8tg4w from openshift-network-operator started at 2020-04-17 19:09:28 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container network-operator ready: true, restart count 0
Apr 17 21:44:54.249: INFO: catalog-operator-5665d988d5-kvkfk from openshift-operator-lifecycle-manager started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container catalog-operator ready: true, restart count 0
Apr 17 21:44:54.249: INFO: dns-operator-74d97d58d-56295 from openshift-dns-operator started at 2020-04-17 19:10:34 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container dns-operator ready: true, restart count 0
Apr 17 21:44:54.249: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:44:54.249: INFO: openshift-service-catalog-controller-manager-operator-f954mtxlp from openshift-service-catalog-controller-manager-operator started at 2020-04-17 19:10:35 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container operator ready: true, restart count 1
Apr 17 21:44:54.249: INFO: kube-state-metrics-5bc9b987bc-hvx9k from openshift-monitoring started at 2020-04-17 19:11:29 +0000 UTC (3 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr 17 21:44:54.249: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr 17 21:44:54.249: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 17 21:44:54.249: INFO: calico-typha-76b588567c-d6d9k from calico-system started at 2020-04-17 19:09:39 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container calico-typha ready: true, restart count 1
Apr 17 21:44:54.249: INFO: cluster-monitoring-operator-668c7b8d6d-khmrg from openshift-monitoring started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Apr 17 21:44:54.249: INFO: ibmcloud-block-storage-plugin-75f7cd767-lcw2w from kube-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Apr 17 21:44:54.249: INFO: downloads-64bb8b89c9-jxkw4 from openshift-console started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container download-server ready: true, restart count 0
Apr 17 21:44:54.249: INFO: service-serving-cert-signer-5968cc5d5c-m6hb2 from openshift-service-ca started at 2020-04-17 19:11:10 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Apr 17 21:44:54.249: INFO: apiservice-cabundle-injector-7bf8cddb9-kxhvx from openshift-service-ca started at 2020-04-17 19:11:11 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Apr 17 21:44:54.249: INFO: ibmcloud-block-storage-driver-g7lth from kube-system started at 2020-04-17 19:09:28 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Apr 17 21:44:54.249: INFO: service-ca-operator-5d59f48888-hk9zs from openshift-service-ca-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container operator ready: true, restart count 0
Apr 17 21:44:54.249: INFO: olm-operator-7bd9dc9457-2wn6n from openshift-operator-lifecycle-manager started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container olm-operator ready: true, restart count 0
Apr 17 21:44:54.249: INFO: calico-kube-controllers-84d976f9ff-g9864 from calico-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr 17 21:44:54.249: INFO: downloads-64bb8b89c9-sccw2 from openshift-console started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container download-server ready: true, restart count 0
Apr 17 21:44:54.249: INFO: tuned-xrlm4 from openshift-cluster-node-tuning-operator started at 2020-04-17 19:11:12 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container tuned ready: true, restart count 0
Apr 17 21:44:54.249: INFO: ibm-keepalived-watcher-58jb7 from kube-system started at 2020-04-17 19:09:24 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr 17 21:44:54.249: INFO: cluster-node-tuning-operator-77bdbd4f-79qvb from openshift-cluster-node-tuning-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Apr 17 21:44:54.249: INFO: openshift-service-catalog-apiserver-operator-76969db7f5-wpk2m from openshift-service-catalog-apiserver-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container operator ready: true, restart count 1
Apr 17 21:44:54.249: INFO: node-exporter-5qztm from openshift-monitoring started at 2020-04-17 19:11:29 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:44:54.249: INFO: 	Container node-exporter ready: true, restart count 0
Apr 17 21:44:54.249: INFO: dns-default-ffsfn from openshift-dns started at 2020-04-17 19:12:00 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container dns ready: true, restart count 0
Apr 17 21:44:54.249: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr 17 21:44:54.249: INFO: packageserver-6bfd584bf6-xvcxz from openshift-operator-lifecycle-manager started at 2020-04-17 19:53:44 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container packageserver ready: true, restart count 0
Apr 17 21:44:54.249: INFO: ibm-master-proxy-static-10.72.119.72 from kube-system started at 2020-04-17 19:09:18 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr 17 21:44:54.249: INFO: 	Container pause ready: true, restart count 0
Apr 17 21:44:54.249: INFO: calico-node-tcc87 from calico-system started at 2020-04-17 19:09:40 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:44:54.249: INFO: cluster-storage-operator-56475d49d7-fsvdb from openshift-cluster-storage-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Apr 17 21:44:54.249: INFO: marketplace-operator-699fb8f5d-5nrcq from openshift-marketplace started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container marketplace-operator ready: true, restart count 0
Apr 17 21:44:54.249: INFO: ibm-file-plugin-7cbd86d68f-9tm42 from kube-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Apr 17 21:44:54.249: INFO: ibm-storage-watcher-d9c7cf586-f72vk from kube-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Apr 17 21:44:54.249: INFO: openshift-state-metrics-6c465bc47f-p9qr6 from openshift-monitoring started at 2020-04-17 19:11:29 +0000 UTC (3 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr 17 21:44:54.249: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr 17 21:44:54.249: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Apr 17 21:44:54.249: INFO: node-ca-x5gt7 from openshift-image-registry started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container node-ca ready: true, restart count 0
Apr 17 21:44:54.249: INFO: tigera-operator-df8f4c87c-c7sjz from tigera-operator started at 2020-04-17 19:09:24 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container tigera-operator ready: true, restart count 0
Apr 17 21:44:54.249: INFO: sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-l8m9b from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:44:54.249: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:44:54.249: INFO: multus-admission-controller-2jxfm from openshift-multus started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr 17 21:44:54.249: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-04-17 19:16:48 +0000 UTC (3 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container alertmanager ready: true, restart count 0
Apr 17 21:44:54.249: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr 17 21:44:54.249: INFO: 	Container config-reloader ready: true, restart count 0
Apr 17 21:44:54.249: INFO: ingress-operator-66cf4674d8-467pw from openshift-ingress-operator started at 2020-04-17 19:10:34 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.249: INFO: 	Container ingress-operator ready: true, restart count 0
Apr 17 21:44:54.249: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:44:54.249: INFO: 
Logging pods the kubelet thinks is on node 10.72.119.74 before test
Apr 17 21:44:54.315: INFO: calico-node-xm4xf from calico-system started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.315: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:44:54.315: INFO: router-default-8779c94d4-drwq9 from openshift-ingress started at 2020-04-17 19:12:06 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.315: INFO: 	Container router ready: true, restart count 0
Apr 17 21:44:54.315: INFO: tuned-tsvx9 from openshift-cluster-node-tuning-operator started at 2020-04-17 19:11:12 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.315: INFO: 	Container tuned ready: true, restart count 0
Apr 17 21:44:54.315: INFO: multus-s62pc from openshift-multus started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.315: INFO: 	Container kube-multus ready: true, restart count 0
Apr 17 21:44:54.315: INFO: node-ca-5dp5t from openshift-image-registry started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.315: INFO: 	Container node-ca ready: true, restart count 0
Apr 17 21:44:54.316: INFO: ibm-master-proxy-static-10.72.119.74 from kube-system started at 2020-04-17 19:10:42 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.316: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr 17 21:44:54.316: INFO: 	Container pause ready: true, restart count 0
Apr 17 21:44:54.316: INFO: node-exporter-949xg from openshift-monitoring started at 2020-04-17 19:11:30 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.316: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:44:54.316: INFO: 	Container node-exporter ready: true, restart count 0
Apr 17 21:44:54.316: INFO: dns-default-kvkts from openshift-dns started at 2020-04-17 19:12:00 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.316: INFO: 	Container dns ready: true, restart count 0
Apr 17 21:44:54.316: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr 17 21:44:54.316: INFO: vpn-5d7df69b48-txcqf from kube-system started at 2020-04-17 19:28:14 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.316: INFO: 	Container vpn ready: true, restart count 0
Apr 17 21:44:54.316: INFO: prometheus-adapter-5cd8cd848d-vtjmp from openshift-monitoring started at 2020-04-17 19:16:43 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.316: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr 17 21:44:54.316: INFO: grafana-6b4f8c85c5-ggf9s from openshift-monitoring started at 2020-04-17 19:16:49 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.316: INFO: 	Container grafana ready: true, restart count 0
Apr 17 21:44:54.316: INFO: 	Container grafana-proxy ready: true, restart count 0
Apr 17 21:44:54.316: INFO: sonobuoy-e2e-job-793c74f2197d4b50 from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.316: INFO: 	Container e2e ready: true, restart count 0
Apr 17 21:44:54.316: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:44:54.316: INFO: console-5b98d99db9-cmb7l from openshift-console started at 2020-04-17 19:12:41 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.316: INFO: 	Container console ready: true, restart count 0
Apr 17 21:44:54.316: INFO: ibm-keepalived-watcher-hm7n9 from kube-system started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.316: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr 17 21:44:54.316: INFO: ibmcloud-block-storage-driver-6j6qk from kube-system started at 2020-04-17 19:10:53 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.316: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Apr 17 21:44:54.317: INFO: calico-typha-76b588567c-khgqc from calico-system started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.317: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 21:44:54.317: INFO: sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-tg59r from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.317: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:44:54.317: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:44:54.317: INFO: packageserver-6bfd584bf6-8l96s from openshift-operator-lifecycle-manager started at 2020-04-17 19:53:34 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.317: INFO: 	Container packageserver ready: true, restart count 0
Apr 17 21:44:54.317: INFO: openshift-kube-proxy-gsgx2 from openshift-kube-proxy started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.317: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:44:54.317: INFO: multus-admission-controller-px85k from openshift-multus started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.317: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr 17 21:44:54.317: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-04-17 19:17:02 +0000 UTC (3 container statuses recorded)
Apr 17 21:44:54.317: INFO: 	Container alertmanager ready: true, restart count 0
Apr 17 21:44:54.317: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr 17 21:44:54.317: INFO: 	Container config-reloader ready: true, restart count 0
Apr 17 21:44:54.317: INFO: thanos-querier-5c84fd9f68-rxgq7 from openshift-monitoring started at 2020-04-17 19:17:38 +0000 UTC (4 container statuses recorded)
Apr 17 21:44:54.317: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:44:54.317: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr 17 21:44:54.317: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 21:44:54.317: INFO: 	Container thanos-querier ready: true, restart count 0
Apr 17 21:44:54.317: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-04-17 19:17:57 +0000 UTC (7 container statuses recorded)
Apr 17 21:44:54.317: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:44:54.317: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 21:44:54.317: INFO: 	Container prometheus ready: true, restart count 1
Apr 17 21:44:54.317: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Apr 17 21:44:54.318: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr 17 21:44:54.318: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Apr 17 21:44:54.318: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr 17 21:44:54.318: INFO: 
Logging pods the kubelet thinks is on node 10.72.119.98 before test
Apr 17 21:44:54.445: INFO: telemeter-client-5f4f6fb5fc-rwg74 from openshift-monitoring started at 2020-04-17 19:16:40 +0000 UTC (3 container statuses recorded)
Apr 17 21:44:54.445: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:44:54.445: INFO: 	Container reload ready: true, restart count 0
Apr 17 21:44:54.445: INFO: 	Container telemeter-client ready: true, restart count 0
Apr 17 21:44:54.445: INFO: sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-kbntn from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.445: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:44:54.445: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:44:54.445: INFO: calico-typha-76b588567c-kfx7z from calico-system started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.445: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 21:44:54.445: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-04-17 19:17:09 +0000 UTC (3 container statuses recorded)
Apr 17 21:44:54.445: INFO: 	Container alertmanager ready: true, restart count 0
Apr 17 21:44:54.445: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr 17 21:44:54.445: INFO: 	Container config-reloader ready: true, restart count 0
Apr 17 21:44:54.445: INFO: thanos-querier-5c84fd9f68-vbdc4 from openshift-monitoring started at 2020-04-17 19:17:49 +0000 UTC (4 container statuses recorded)
Apr 17 21:44:54.445: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:44:54.445: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr 17 21:44:54.446: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 21:44:54.446: INFO: 	Container thanos-querier ready: true, restart count 0
Apr 17 21:44:54.446: INFO: ibm-keepalived-watcher-h2bpf from kube-system started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.446: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr 17 21:44:54.446: INFO: multus-lddbc from openshift-multus started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.446: INFO: 	Container kube-multus ready: true, restart count 0
Apr 17 21:44:54.446: INFO: image-registry-5655cc46bf-gzrxj from openshift-image-registry started at 2020-04-17 19:14:35 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.446: INFO: 	Container registry ready: true, restart count 0
Apr 17 21:44:54.446: INFO: router-default-8779c94d4-mdtbq from openshift-ingress started at 2020-04-17 19:12:06 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.446: INFO: 	Container router ready: true, restart count 0
Apr 17 21:44:54.446: INFO: certified-operators-586b58fc67-nk9d4 from openshift-marketplace started at 2020-04-17 19:12:23 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.446: INFO: 	Container certified-operators ready: true, restart count 0
Apr 17 21:44:54.446: INFO: prometheus-adapter-5cd8cd848d-xspwf from openshift-monitoring started at 2020-04-17 19:16:43 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.446: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr 17 21:44:54.446: INFO: calico-node-6sr5x from calico-system started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.446: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:44:54.446: INFO: openshift-kube-proxy-7dshq from openshift-kube-proxy started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.446: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:44:54.446: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-04-17 19:14:09 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.446: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Apr 17 21:44:54.446: INFO: registry-pvc-permissions-d9dpb from openshift-image-registry started at 2020-04-17 19:14:35 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.446: INFO: 	Container pvc-permissions ready: false, restart count 0
Apr 17 21:44:54.446: INFO: tuned-sz2br from openshift-cluster-node-tuning-operator started at 2020-04-17 19:11:12 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.446: INFO: 	Container tuned ready: true, restart count 0
Apr 17 21:44:54.446: INFO: dns-default-z6b7d from openshift-dns started at 2020-04-17 19:12:00 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.446: INFO: 	Container dns ready: true, restart count 0
Apr 17 21:44:54.446: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr 17 21:44:54.446: INFO: ibm-cloud-provider-ip-158-176-122-172-6f57dbd74b-bpkpw from ibm-system started at 2020-04-17 19:13:51 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.446: INFO: 	Container ibm-cloud-provider-ip-158-176-122-172 ready: true, restart count 0
Apr 17 21:44:54.446: INFO: console-5b98d99db9-2z7vw from openshift-console started at 2020-04-17 19:12:51 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.446: INFO: 	Container console ready: true, restart count 0
Apr 17 21:44:54.447: INFO: ibmcloud-block-storage-driver-6jgj5 from kube-system started at 2020-04-17 19:10:47 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.447: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Apr 17 21:44:54.447: INFO: node-ca-wr66h from openshift-image-registry started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.447: INFO: 	Container node-ca ready: true, restart count 0
Apr 17 21:44:54.447: INFO: community-operators-5574589d55-swkx4 from openshift-marketplace started at 2020-04-17 19:12:21 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.447: INFO: 	Container community-operators ready: true, restart count 0
Apr 17 21:44:54.447: INFO: redhat-operators-7b557fbbc4-pq5mj from openshift-marketplace started at 2020-04-17 19:12:24 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.447: INFO: 	Container redhat-operators ready: true, restart count 0
Apr 17 21:44:54.447: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-04-17 19:17:47 +0000 UTC (7 container statuses recorded)
Apr 17 21:44:54.447: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:44:54.447: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 21:44:54.447: INFO: 	Container prometheus ready: true, restart count 1
Apr 17 21:44:54.447: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Apr 17 21:44:54.447: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr 17 21:44:54.447: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Apr 17 21:44:54.447: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr 17 21:44:54.447: INFO: cluster-samples-operator-6d4b68977f-5q62h from openshift-cluster-samples-operator started at 2020-04-17 19:11:57 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.447: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Apr 17 21:44:54.447: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Apr 17 21:44:54.447: INFO: multus-admission-controller-72gwb from openshift-multus started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.447: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr 17 21:44:54.447: INFO: prometheus-operator-75c7889c9b-47f2p from openshift-monitoring started at 2020-04-17 19:16:29 +0000 UTC (1 container statuses recorded)
Apr 17 21:44:54.447: INFO: 	Container prometheus-operator ready: true, restart count 0
Apr 17 21:44:54.447: INFO: ibm-master-proxy-static-10.72.119.98 from kube-system started at 2020-04-17 19:10:37 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.447: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr 17 21:44:54.447: INFO: 	Container pause ready: true, restart count 0
Apr 17 21:44:54.447: INFO: node-exporter-lh6vl from openshift-monitoring started at 2020-04-17 19:11:30 +0000 UTC (2 container statuses recorded)
Apr 17 21:44:54.447: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 21:44:54.447: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node 10.72.119.72
STEP: verifying the node has the label node 10.72.119.74
STEP: verifying the node has the label node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod calico-kube-controllers-84d976f9ff-g9864 requesting resource cpu=0m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod calico-node-6sr5x requesting resource cpu=0m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod calico-node-tcc87 requesting resource cpu=0m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod calico-node-xm4xf requesting resource cpu=0m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod calico-typha-76b588567c-d6d9k requesting resource cpu=0m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod calico-typha-76b588567c-kfx7z requesting resource cpu=0m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod calico-typha-76b588567c-khgqc requesting resource cpu=0m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod ibm-cloud-provider-ip-158-176-122-172-6f57dbd74b-bpkpw requesting resource cpu=5m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod ibm-cloud-provider-ip-158-176-122-172-6f57dbd74b-cdqd6 requesting resource cpu=5m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod ibm-file-plugin-7cbd86d68f-9tm42 requesting resource cpu=50m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod ibm-keepalived-watcher-58jb7 requesting resource cpu=5m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod ibm-keepalived-watcher-h2bpf requesting resource cpu=5m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod ibm-keepalived-watcher-hm7n9 requesting resource cpu=5m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod ibm-master-proxy-static-10.72.119.72 requesting resource cpu=25m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod ibm-master-proxy-static-10.72.119.74 requesting resource cpu=25m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod ibm-master-proxy-static-10.72.119.98 requesting resource cpu=25m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod ibm-storage-watcher-d9c7cf586-f72vk requesting resource cpu=50m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod ibmcloud-block-storage-driver-6j6qk requesting resource cpu=25m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod ibmcloud-block-storage-driver-6jgj5 requesting resource cpu=25m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod ibmcloud-block-storage-driver-g7lth requesting resource cpu=25m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod ibmcloud-block-storage-plugin-75f7cd767-lcw2w requesting resource cpu=50m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod vpn-5d7df69b48-txcqf requesting resource cpu=5m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod cluster-node-tuning-operator-77bdbd4f-79qvb requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod tuned-sz2br requesting resource cpu=10m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod tuned-tsvx9 requesting resource cpu=10m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod tuned-xrlm4 requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod cluster-samples-operator-6d4b68977f-5q62h requesting resource cpu=20m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod cluster-storage-operator-56475d49d7-fsvdb requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod console-operator-db5d785db-r4qh6 requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod console-5b98d99db9-2z7vw requesting resource cpu=10m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod console-5b98d99db9-cmb7l requesting resource cpu=10m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod downloads-64bb8b89c9-jxkw4 requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod downloads-64bb8b89c9-sccw2 requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod dns-operator-74d97d58d-56295 requesting resource cpu=20m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod dns-default-ffsfn requesting resource cpu=110m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod dns-default-kvkts requesting resource cpu=110m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod dns-default-z6b7d requesting resource cpu=110m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod cluster-image-registry-operator-7857d56744-c2pch requesting resource cpu=20m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod image-registry-5655cc46bf-gzrxj requesting resource cpu=100m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod node-ca-5dp5t requesting resource cpu=10m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod node-ca-wr66h requesting resource cpu=10m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod node-ca-x5gt7 requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod ingress-operator-66cf4674d8-467pw requesting resource cpu=20m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod router-default-8779c94d4-drwq9 requesting resource cpu=100m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod router-default-8779c94d4-mdtbq requesting resource cpu=100m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod openshift-kube-proxy-7dshq requesting resource cpu=0m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod openshift-kube-proxy-bkf2z requesting resource cpu=0m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod openshift-kube-proxy-gsgx2 requesting resource cpu=0m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod certified-operators-586b58fc67-nk9d4 requesting resource cpu=10m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod community-operators-5574589d55-swkx4 requesting resource cpu=10m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod marketplace-operator-699fb8f5d-5nrcq requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod redhat-operators-7b557fbbc4-pq5mj requesting resource cpu=10m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod alertmanager-main-0 requesting resource cpu=110m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod alertmanager-main-1 requesting resource cpu=110m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod alertmanager-main-2 requesting resource cpu=110m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod cluster-monitoring-operator-668c7b8d6d-khmrg requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod grafana-6b4f8c85c5-ggf9s requesting resource cpu=110m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod kube-state-metrics-5bc9b987bc-hvx9k requesting resource cpu=30m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod node-exporter-5qztm requesting resource cpu=112m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod node-exporter-949xg requesting resource cpu=112m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod node-exporter-lh6vl requesting resource cpu=112m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod openshift-state-metrics-6c465bc47f-p9qr6 requesting resource cpu=120m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod prometheus-adapter-5cd8cd848d-vtjmp requesting resource cpu=10m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod prometheus-adapter-5cd8cd848d-xspwf requesting resource cpu=10m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod prometheus-k8s-0 requesting resource cpu=480m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod prometheus-k8s-1 requesting resource cpu=480m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod prometheus-operator-75c7889c9b-47f2p requesting resource cpu=10m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod telemeter-client-5f4f6fb5fc-rwg74 requesting resource cpu=10m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod thanos-querier-5c84fd9f68-rxgq7 requesting resource cpu=40m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod thanos-querier-5c84fd9f68-vbdc4 requesting resource cpu=40m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod multus-admission-controller-2jxfm requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod multus-admission-controller-72gwb requesting resource cpu=10m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod multus-admission-controller-px85k requesting resource cpu=10m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod multus-lddbc requesting resource cpu=10m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod multus-s62pc requesting resource cpu=10m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod multus-wlttp requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod network-operator-64f597f5d-8tg4w requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod catalog-operator-5665d988d5-kvkfk requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod olm-operator-7bd9dc9457-2wn6n requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod packageserver-6bfd584bf6-8l96s requesting resource cpu=10m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod packageserver-6bfd584bf6-xvcxz requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod service-ca-operator-5d59f48888-hk9zs requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod apiservice-cabundle-injector-7bf8cddb9-kxhvx requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod configmap-cabundle-injector-5cf6d9695-kfct9 requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod service-serving-cert-signer-5968cc5d5c-m6hb2 requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod openshift-service-catalog-apiserver-operator-76969db7f5-wpk2m requesting resource cpu=0m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod openshift-service-catalog-controller-manager-operator-f954mtxlp requesting resource cpu=10m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod sonobuoy-e2e-job-793c74f2197d4b50 requesting resource cpu=0m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-kbntn requesting resource cpu=0m on Node 10.72.119.98
Apr 17 21:44:54.733: INFO: Pod sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-l8m9b requesting resource cpu=0m on Node 10.72.119.72
Apr 17 21:44:54.733: INFO: Pod sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-tg59r requesting resource cpu=0m on Node 10.72.119.74
Apr 17 21:44:54.733: INFO: Pod tigera-operator-df8f4c87c-c7sjz requesting resource cpu=0m on Node 10.72.119.72
STEP: Starting Pods to consume most of the cluster CPU.
Apr 17 21:44:54.733: INFO: Creating a pod which consumes cpu=2070m on Node 10.72.119.72
Apr 17 21:44:54.790: INFO: Creating a pod which consumes cpu=1902m on Node 10.72.119.74
Apr 17 21:44:54.824: INFO: Creating a pod which consumes cpu=1867m on Node 10.72.119.98
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-133abb1a-1a5e-4d63-a2ef-6da50f2554b3.1606b983e7f5676f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8319/filler-pod-133abb1a-1a5e-4d63-a2ef-6da50f2554b3 to 10.72.119.98]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-133abb1a-1a5e-4d63-a2ef-6da50f2554b3.1606b98428129e3a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-133abb1a-1a5e-4d63-a2ef-6da50f2554b3.1606b984356af8c7], Reason = [Created], Message = [Created container filler-pod-133abb1a-1a5e-4d63-a2ef-6da50f2554b3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-133abb1a-1a5e-4d63-a2ef-6da50f2554b3.1606b984385f33fa], Reason = [Started], Message = [Started container filler-pod-133abb1a-1a5e-4d63-a2ef-6da50f2554b3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b5f956c-9e86-4252-a3a9-0189f5a89fb5.1606b983e35cf39e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8319/filler-pod-4b5f956c-9e86-4252-a3a9-0189f5a89fb5 to 10.72.119.72]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b5f956c-9e86-4252-a3a9-0189f5a89fb5.1606b98423349b12], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b5f956c-9e86-4252-a3a9-0189f5a89fb5.1606b98431e37efb], Reason = [Created], Message = [Created container filler-pod-4b5f956c-9e86-4252-a3a9-0189f5a89fb5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4b5f956c-9e86-4252-a3a9-0189f5a89fb5.1606b984347772aa], Reason = [Started], Message = [Started container filler-pod-4b5f956c-9e86-4252-a3a9-0189f5a89fb5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b8753dfc-9c87-4477-9178-bb90210d0e9f.1606b983e588077e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8319/filler-pod-b8753dfc-9c87-4477-9178-bb90210d0e9f to 10.72.119.74]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b8753dfc-9c87-4477-9178-bb90210d0e9f.1606b984235014be], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b8753dfc-9c87-4477-9178-bb90210d0e9f.1606b984302076d9], Reason = [Created], Message = [Created container filler-pod-b8753dfc-9c87-4477-9178-bb90210d0e9f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b8753dfc-9c87-4477-9178-bb90210d0e9f.1606b98432c16942], Reason = [Started], Message = [Started container filler-pod-b8753dfc-9c87-4477-9178-bb90210d0e9f]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1606b984dd263bef], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.72.119.72
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.72.119.74
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.72.119.98
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:45:00.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8319" for this suite.
Apr 17 21:45:08.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:45:10.352: INFO: namespace sched-pred-8319 deletion completed in 10.133602245s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:16.533 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:45:10.352: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 21:45:10.555: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c9d2a890-83cb-48dc-80fb-b531cc317117" in namespace "downward-api-3810" to be "success or failure"
Apr 17 21:45:10.566: INFO: Pod "downwardapi-volume-c9d2a890-83cb-48dc-80fb-b531cc317117": Phase="Pending", Reason="", readiness=false. Elapsed: 11.200949ms
Apr 17 21:45:12.578: INFO: Pod "downwardapi-volume-c9d2a890-83cb-48dc-80fb-b531cc317117": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023405616s
STEP: Saw pod success
Apr 17 21:45:12.578: INFO: Pod "downwardapi-volume-c9d2a890-83cb-48dc-80fb-b531cc317117" satisfied condition "success or failure"
Apr 17 21:45:12.588: INFO: Trying to get logs from node 10.72.119.74 pod downwardapi-volume-c9d2a890-83cb-48dc-80fb-b531cc317117 container client-container: <nil>
STEP: delete the pod
Apr 17 21:45:12.667: INFO: Waiting for pod downwardapi-volume-c9d2a890-83cb-48dc-80fb-b531cc317117 to disappear
Apr 17 21:45:12.678: INFO: Pod downwardapi-volume-c9d2a890-83cb-48dc-80fb-b531cc317117 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:45:12.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3810" for this suite.
Apr 17 21:45:20.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:45:22.949: INFO: namespace downward-api-3810 deletion completed in 10.248060372s

â€¢ [SLOW TEST:12.597 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:45:22.949: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-rq6w
STEP: Creating a pod to test atomic-volume-subpath
Apr 17 21:45:23.207: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-rq6w" in namespace "subpath-9628" to be "success or failure"
Apr 17 21:45:23.218: INFO: Pod "pod-subpath-test-secret-rq6w": Phase="Pending", Reason="", readiness=false. Elapsed: 10.388611ms
Apr 17 21:45:25.229: INFO: Pod "pod-subpath-test-secret-rq6w": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021749749s
Apr 17 21:45:27.240: INFO: Pod "pod-subpath-test-secret-rq6w": Phase="Running", Reason="", readiness=true. Elapsed: 4.03298031s
Apr 17 21:45:29.254: INFO: Pod "pod-subpath-test-secret-rq6w": Phase="Running", Reason="", readiness=true. Elapsed: 6.046607961s
Apr 17 21:45:31.266: INFO: Pod "pod-subpath-test-secret-rq6w": Phase="Running", Reason="", readiness=true. Elapsed: 8.059001234s
Apr 17 21:45:33.278: INFO: Pod "pod-subpath-test-secret-rq6w": Phase="Running", Reason="", readiness=true. Elapsed: 10.070420586s
Apr 17 21:45:35.300: INFO: Pod "pod-subpath-test-secret-rq6w": Phase="Running", Reason="", readiness=true. Elapsed: 12.093076417s
Apr 17 21:45:37.312: INFO: Pod "pod-subpath-test-secret-rq6w": Phase="Running", Reason="", readiness=true. Elapsed: 14.104631667s
Apr 17 21:45:39.337: INFO: Pod "pod-subpath-test-secret-rq6w": Phase="Running", Reason="", readiness=true. Elapsed: 16.129386411s
Apr 17 21:45:41.349: INFO: Pod "pod-subpath-test-secret-rq6w": Phase="Running", Reason="", readiness=true. Elapsed: 18.141703475s
Apr 17 21:45:43.361: INFO: Pod "pod-subpath-test-secret-rq6w": Phase="Running", Reason="", readiness=true. Elapsed: 20.153550435s
Apr 17 21:45:45.374: INFO: Pod "pod-subpath-test-secret-rq6w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.166702114s
STEP: Saw pod success
Apr 17 21:45:45.374: INFO: Pod "pod-subpath-test-secret-rq6w" satisfied condition "success or failure"
Apr 17 21:45:45.388: INFO: Trying to get logs from node 10.72.119.74 pod pod-subpath-test-secret-rq6w container test-container-subpath-secret-rq6w: <nil>
STEP: delete the pod
Apr 17 21:45:45.458: INFO: Waiting for pod pod-subpath-test-secret-rq6w to disappear
Apr 17 21:45:45.469: INFO: Pod pod-subpath-test-secret-rq6w no longer exists
STEP: Deleting pod pod-subpath-test-secret-rq6w
Apr 17 21:45:45.469: INFO: Deleting pod "pod-subpath-test-secret-rq6w" in namespace "subpath-9628"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:45:45.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9628" for this suite.
Apr 17 21:45:53.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:45:55.804: INFO: namespace subpath-9628 deletion completed in 10.290246519s

â€¢ [SLOW TEST:32.855 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:45:55.806: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-6090/configmap-test-cb61003e-583a-4ecd-bc14-693055a64b22
STEP: Creating a pod to test consume configMaps
Apr 17 21:45:56.029: INFO: Waiting up to 5m0s for pod "pod-configmaps-49bef6e2-750c-4894-a4c7-e76e68e12605" in namespace "configmap-6090" to be "success or failure"
Apr 17 21:45:56.041: INFO: Pod "pod-configmaps-49bef6e2-750c-4894-a4c7-e76e68e12605": Phase="Pending", Reason="", readiness=false. Elapsed: 12.27822ms
Apr 17 21:45:58.057: INFO: Pod "pod-configmaps-49bef6e2-750c-4894-a4c7-e76e68e12605": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028151657s
Apr 17 21:46:00.069: INFO: Pod "pod-configmaps-49bef6e2-750c-4894-a4c7-e76e68e12605": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040833344s
STEP: Saw pod success
Apr 17 21:46:00.069: INFO: Pod "pod-configmaps-49bef6e2-750c-4894-a4c7-e76e68e12605" satisfied condition "success or failure"
Apr 17 21:46:00.083: INFO: Trying to get logs from node 10.72.119.74 pod pod-configmaps-49bef6e2-750c-4894-a4c7-e76e68e12605 container env-test: <nil>
STEP: delete the pod
Apr 17 21:46:00.159: INFO: Waiting for pod pod-configmaps-49bef6e2-750c-4894-a4c7-e76e68e12605 to disappear
Apr 17 21:46:00.170: INFO: Pod pod-configmaps-49bef6e2-750c-4894-a4c7-e76e68e12605 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:46:00.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6090" for this suite.
Apr 17 21:46:08.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:46:10.464: INFO: namespace configmap-6090 deletion completed in 10.246998441s

â€¢ [SLOW TEST:14.659 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:46:10.467: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:46:12.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6460" for this suite.
Apr 17 21:46:50.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:46:53.080: INFO: namespace kubelet-test-6460 deletion completed in 40.254275262s

â€¢ [SLOW TEST:42.613 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:46:53.082: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Apr 17 21:46:53.239: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:47:44.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9812" for this suite.
Apr 17 21:47:52.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:47:54.954: INFO: namespace crd-publish-openapi-9812 deletion completed in 10.238182834s

â€¢ [SLOW TEST:61.872 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:47:54.956: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr 17 21:47:55.148: INFO: Waiting up to 5m0s for pod "pod-607ff4cd-3b7d-413c-b87a-1ee9dc2c1f9c" in namespace "emptydir-6886" to be "success or failure"
Apr 17 21:47:55.162: INFO: Pod "pod-607ff4cd-3b7d-413c-b87a-1ee9dc2c1f9c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.611214ms
Apr 17 21:47:57.177: INFO: Pod "pod-607ff4cd-3b7d-413c-b87a-1ee9dc2c1f9c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029115689s
STEP: Saw pod success
Apr 17 21:47:57.177: INFO: Pod "pod-607ff4cd-3b7d-413c-b87a-1ee9dc2c1f9c" satisfied condition "success or failure"
Apr 17 21:47:57.190: INFO: Trying to get logs from node 10.72.119.74 pod pod-607ff4cd-3b7d-413c-b87a-1ee9dc2c1f9c container test-container: <nil>
STEP: delete the pod
Apr 17 21:47:57.279: INFO: Waiting for pod pod-607ff4cd-3b7d-413c-b87a-1ee9dc2c1f9c to disappear
Apr 17 21:47:57.293: INFO: Pod pod-607ff4cd-3b7d-413c-b87a-1ee9dc2c1f9c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:47:57.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6886" for this suite.
Apr 17 21:48:05.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:48:07.525: INFO: namespace emptydir-6886 deletion completed in 10.204945754s

â€¢ [SLOW TEST:12.570 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:48:07.525: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Apr 17 21:48:07.653: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:48:11.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1445" for this suite.
Apr 17 21:48:19.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:48:22.032: INFO: namespace init-container-1445 deletion completed in 10.198559241s

â€¢ [SLOW TEST:14.506 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:48:22.032: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 17 21:48:22.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-2214'
Apr 17 21:48:22.545: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 17 21:48:22.545: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Apr 17 21:48:22.573: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-8p7ml]
Apr 17 21:48:22.573: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-8p7ml" in namespace "kubectl-2214" to be "running and ready"
Apr 17 21:48:22.588: INFO: Pod "e2e-test-httpd-rc-8p7ml": Phase="Pending", Reason="", readiness=false. Elapsed: 14.55057ms
Apr 17 21:48:24.601: INFO: Pod "e2e-test-httpd-rc-8p7ml": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027522948s
Apr 17 21:48:26.612: INFO: Pod "e2e-test-httpd-rc-8p7ml": Phase="Running", Reason="", readiness=true. Elapsed: 4.03873668s
Apr 17 21:48:26.612: INFO: Pod "e2e-test-httpd-rc-8p7ml" satisfied condition "running and ready"
Apr 17 21:48:26.612: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-8p7ml]
Apr 17 21:48:26.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 logs rc/e2e-test-httpd-rc --namespace=kubectl-2214'
Apr 17 21:48:26.854: INFO: stderr: ""
Apr 17 21:48:26.854: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.194.122. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 172.30.194.122. Set the 'ServerName' directive globally to suppress this message\n[Fri Apr 17 21:48:23.903333 2020] [mpm_event:notice] [pid 1:tid 140584116902760] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Fri Apr 17 21:48:23.903458 2020] [core:notice] [pid 1:tid 140584116902760] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Apr 17 21:48:26.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete rc e2e-test-httpd-rc --namespace=kubectl-2214'
Apr 17 21:48:27.061: INFO: stderr: ""
Apr 17 21:48:27.061: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:48:27.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2214" for this suite.
Apr 17 21:48:35.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:48:37.343: INFO: namespace kubectl-2214 deletion completed in 10.252082082s

â€¢ [SLOW TEST:15.311 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:48:37.345: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-9ffd8774-b756-493b-91aa-60ba9c4ec6ee in namespace container-probe-3492
Apr 17 21:48:39.599: INFO: Started pod liveness-9ffd8774-b756-493b-91aa-60ba9c4ec6ee in namespace container-probe-3492
STEP: checking the pod's current state and verifying that restartCount is present
Apr 17 21:48:39.610: INFO: Initial restart count of pod liveness-9ffd8774-b756-493b-91aa-60ba9c4ec6ee is 0
Apr 17 21:48:55.718: INFO: Restart count of pod container-probe-3492/liveness-9ffd8774-b756-493b-91aa-60ba9c4ec6ee is now 1 (16.107888993s elapsed)
Apr 17 21:49:15.855: INFO: Restart count of pod container-probe-3492/liveness-9ffd8774-b756-493b-91aa-60ba9c4ec6ee is now 2 (36.24503783s elapsed)
Apr 17 21:49:36.002: INFO: Restart count of pod container-probe-3492/liveness-9ffd8774-b756-493b-91aa-60ba9c4ec6ee is now 3 (56.391368974s elapsed)
Apr 17 21:49:56.126: INFO: Restart count of pod container-probe-3492/liveness-9ffd8774-b756-493b-91aa-60ba9c4ec6ee is now 4 (1m16.515671375s elapsed)
Apr 17 21:51:06.686: INFO: Restart count of pod container-probe-3492/liveness-9ffd8774-b756-493b-91aa-60ba9c4ec6ee is now 5 (2m27.07628668s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:51:06.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3492" for this suite.
Apr 17 21:51:14.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:51:16.969: INFO: namespace container-probe-3492 deletion completed in 10.201855643s

â€¢ [SLOW TEST:159.624 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:51:16.969: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Apr 17 21:51:17.138: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Apr 17 21:51:54.418: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:52:04.722: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:52:49.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4373" for this suite.
Apr 17 21:52:57.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:53:00.015: INFO: namespace crd-publish-openapi-4373 deletion completed in 10.291943954s

â€¢ [SLOW TEST:103.046 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:53:00.016: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-e8d859c6-cd53-45d1-895c-13be782e5b34
STEP: Creating a pod to test consume secrets
Apr 17 21:53:00.227: INFO: Waiting up to 5m0s for pod "pod-secrets-eede222a-8de0-46b2-9d40-1fe4d672a1dd" in namespace "secrets-6291" to be "success or failure"
Apr 17 21:53:00.243: INFO: Pod "pod-secrets-eede222a-8de0-46b2-9d40-1fe4d672a1dd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.227764ms
Apr 17 21:53:02.260: INFO: Pod "pod-secrets-eede222a-8de0-46b2-9d40-1fe4d672a1dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032552903s
STEP: Saw pod success
Apr 17 21:53:02.260: INFO: Pod "pod-secrets-eede222a-8de0-46b2-9d40-1fe4d672a1dd" satisfied condition "success or failure"
Apr 17 21:53:02.272: INFO: Trying to get logs from node 10.72.119.74 pod pod-secrets-eede222a-8de0-46b2-9d40-1fe4d672a1dd container secret-volume-test: <nil>
STEP: delete the pod
Apr 17 21:53:02.380: INFO: Waiting for pod pod-secrets-eede222a-8de0-46b2-9d40-1fe4d672a1dd to disappear
Apr 17 21:53:02.407: INFO: Pod pod-secrets-eede222a-8de0-46b2-9d40-1fe4d672a1dd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:53:02.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6291" for this suite.
Apr 17 21:53:10.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:53:12.773: INFO: namespace secrets-6291 deletion completed in 10.337232607s

â€¢ [SLOW TEST:12.757 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:53:12.773: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Apr 17 21:53:12.920: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 17 21:54:13.102: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:54:13.118: INFO: Starting informer...
STEP: Starting pods...
Apr 17 21:54:13.393: INFO: Pod1 is running on 10.72.119.74. Tainting Node
Apr 17 21:54:15.676: INFO: Pod2 is running on 10.72.119.74. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Apr 17 21:54:32.527: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Apr 17 21:54:52.501: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:54:52.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7089" for this suite.
Apr 17 21:55:00.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:55:02.826: INFO: namespace taint-multiple-pods-7089 deletion completed in 10.249115978s

â€¢ [SLOW TEST:110.053 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:55:02.827: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:56:03.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-283" for this suite.
Apr 17 21:56:35.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:56:37.231: INFO: namespace container-probe-283 deletion completed in 34.151745339s

â€¢ [SLOW TEST:94.405 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:56:37.234: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-fa0cebbf-1bde-47a9-a5ee-1dedff4d84ed
STEP: Creating a pod to test consume configMaps
Apr 17 21:56:37.430: INFO: Waiting up to 5m0s for pod "pod-configmaps-69a5422d-7c57-41a5-b425-cc4a325eca8d" in namespace "configmap-4641" to be "success or failure"
Apr 17 21:56:37.442: INFO: Pod "pod-configmaps-69a5422d-7c57-41a5-b425-cc4a325eca8d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.755417ms
Apr 17 21:56:39.460: INFO: Pod "pod-configmaps-69a5422d-7c57-41a5-b425-cc4a325eca8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029219297s
Apr 17 21:56:41.472: INFO: Pod "pod-configmaps-69a5422d-7c57-41a5-b425-cc4a325eca8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041294303s
STEP: Saw pod success
Apr 17 21:56:41.472: INFO: Pod "pod-configmaps-69a5422d-7c57-41a5-b425-cc4a325eca8d" satisfied condition "success or failure"
Apr 17 21:56:41.483: INFO: Trying to get logs from node 10.72.119.74 pod pod-configmaps-69a5422d-7c57-41a5-b425-cc4a325eca8d container configmap-volume-test: <nil>
STEP: delete the pod
Apr 17 21:56:41.586: INFO: Waiting for pod pod-configmaps-69a5422d-7c57-41a5-b425-cc4a325eca8d to disappear
Apr 17 21:56:41.598: INFO: Pod pod-configmaps-69a5422d-7c57-41a5-b425-cc4a325eca8d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:56:41.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4641" for this suite.
Apr 17 21:56:49.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:56:51.957: INFO: namespace configmap-4641 deletion completed in 10.331724208s

â€¢ [SLOW TEST:14.723 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:56:51.957: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 21:56:52.151: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-78e16caa-6ffb-44a9-ba69-4becb2e50b84
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-78e16caa-6ffb-44a9-ba69-4becb2e50b84
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:56:56.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6255" for this suite.
Apr 17 21:57:10.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:57:12.724: INFO: namespace configmap-6255 deletion completed in 16.3072312s

â€¢ [SLOW TEST:20.767 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:57:12.724: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:57:24.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1930" for this suite.
Apr 17 21:57:32.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:57:34.539: INFO: namespace resourcequota-1930 deletion completed in 10.388908109s

â€¢ [SLOW TEST:21.815 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:57:34.540: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-9786
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 17 21:57:34.670: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 17 21:57:59.101: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.194.88 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9786 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 21:57:59.102: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:58:00.343: INFO: Found all expected endpoints: [netserver-0]
Apr 17 21:58:00.356: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.232.219 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9786 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 21:58:00.356: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:58:01.675: INFO: Found all expected endpoints: [netserver-1]
Apr 17 21:58:01.691: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.53.59 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9786 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 21:58:01.691: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 21:58:02.880: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:58:02.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9786" for this suite.
Apr 17 21:58:10.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:58:13.125: INFO: namespace pod-network-test-9786 deletion completed in 10.216101489s

â€¢ [SLOW TEST:38.586 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:58:13.126: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-37ae964b-6d97-41e9-b448-47150a596e56
STEP: Creating a pod to test consume configMaps
Apr 17 21:58:13.335: INFO: Waiting up to 5m0s for pod "pod-configmaps-f37884ce-d61a-47fb-9470-eea48842a7a2" in namespace "configmap-2826" to be "success or failure"
Apr 17 21:58:13.345: INFO: Pod "pod-configmaps-f37884ce-d61a-47fb-9470-eea48842a7a2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.377478ms
Apr 17 21:58:15.357: INFO: Pod "pod-configmaps-f37884ce-d61a-47fb-9470-eea48842a7a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021700505s
STEP: Saw pod success
Apr 17 21:58:15.357: INFO: Pod "pod-configmaps-f37884ce-d61a-47fb-9470-eea48842a7a2" satisfied condition "success or failure"
Apr 17 21:58:15.368: INFO: Trying to get logs from node 10.72.119.74 pod pod-configmaps-f37884ce-d61a-47fb-9470-eea48842a7a2 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 17 21:58:15.439: INFO: Waiting for pod pod-configmaps-f37884ce-d61a-47fb-9470-eea48842a7a2 to disappear
Apr 17 21:58:15.454: INFO: Pod pod-configmaps-f37884ce-d61a-47fb-9470-eea48842a7a2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:58:15.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2826" for this suite.
Apr 17 21:58:23.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:58:25.878: INFO: namespace configmap-2826 deletion completed in 10.390166953s

â€¢ [SLOW TEST:12.753 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:58:25.880: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Apr 17 21:58:26.038: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:59:22.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-288" for this suite.
Apr 17 21:59:30.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:59:33.425: INFO: namespace crd-publish-openapi-288 deletion completed in 10.511074511s

â€¢ [SLOW TEST:67.545 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:59:33.427: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:59:39.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9716" for this suite.
Apr 17 21:59:47.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 21:59:49.626: INFO: namespace watch-9716 deletion completed in 10.468784261s

â€¢ [SLOW TEST:16.199 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 21:59:49.626: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-164
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-164
I0417 21:59:49.876354      22 runners.go:184] Created replication controller with name: externalname-service, namespace: services-164, replica count: 2
Apr 17 21:59:52.927: INFO: Creating new exec pod
I0417 21:59:52.927366      22 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 17 21:59:55.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=services-164 execpod8kjtd -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Apr 17 21:59:56.529: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 17 21:59:56.529: INFO: stdout: ""
Apr 17 21:59:56.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=services-164 execpod8kjtd -- /bin/sh -x -c nc -zv -t -w 2 172.21.98.200 80'
Apr 17 21:59:56.837: INFO: stderr: "+ nc -zv -t -w 2 172.21.98.200 80\nConnection to 172.21.98.200 80 port [tcp/http] succeeded!\n"
Apr 17 21:59:56.837: INFO: stdout: ""
Apr 17 21:59:56.837: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 21:59:56.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-164" for this suite.
Apr 17 22:00:06.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:00:09.194: INFO: namespace services-164 deletion completed in 12.266074994s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:19.568 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:00:09.198: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2902
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-2902
STEP: Creating statefulset with conflicting port in namespace statefulset-2902
STEP: Waiting until pod test-pod will start running in namespace statefulset-2902
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2902
Apr 17 22:00:13.524: INFO: Observed stateful pod in namespace: statefulset-2902, name: ss-0, uid: f1221f2d-9acd-4b71-ae95-0c838036528e, status phase: Pending. Waiting for statefulset controller to delete.
Apr 17 22:00:13.529: INFO: Observed stateful pod in namespace: statefulset-2902, name: ss-0, uid: f1221f2d-9acd-4b71-ae95-0c838036528e, status phase: Failed. Waiting for statefulset controller to delete.
Apr 17 22:00:13.543: INFO: Observed stateful pod in namespace: statefulset-2902, name: ss-0, uid: f1221f2d-9acd-4b71-ae95-0c838036528e, status phase: Failed. Waiting for statefulset controller to delete.
Apr 17 22:00:13.552: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2902
STEP: Removing pod with conflicting port in namespace statefulset-2902
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2902 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Apr 17 22:00:17.623: INFO: Deleting all statefulset in ns statefulset-2902
Apr 17 22:00:17.639: INFO: Scaling statefulset ss to 0
Apr 17 22:00:27.701: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 22:00:27.719: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:00:27.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2902" for this suite.
Apr 17 22:00:37.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:00:40.243: INFO: namespace statefulset-2902 deletion completed in 12.428528089s

â€¢ [SLOW TEST:31.046 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:00:40.244: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:00:40.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4769" for this suite.
Apr 17 22:00:54.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:00:56.870: INFO: namespace kubelet-test-4769 deletion completed in 16.325847327s

â€¢ [SLOW TEST:16.627 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:00:56.870: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 17 22:00:57.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7998'
Apr 17 22:00:57.232: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 17 22:00:57.232: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Apr 17 22:00:59.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete deployment e2e-test-httpd-deployment --namespace=kubectl-7998'
Apr 17 22:00:59.434: INFO: stderr: ""
Apr 17 22:00:59.434: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:00:59.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7998" for this suite.
Apr 17 22:01:15.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:01:17.633: INFO: namespace kubectl-7998 deletion completed in 18.159774107s

â€¢ [SLOW TEST:20.762 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:01:17.639: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:01:17.847: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Apr 17 22:01:20.015: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:01:20.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8600" for this suite.
Apr 17 22:01:28.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:01:30.314: INFO: namespace replication-controller-8600 deletion completed in 10.24433179s

â€¢ [SLOW TEST:12.676 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:01:30.315: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:01:30.471: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Apr 17 22:01:40.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-3725 create -f -'
Apr 17 22:01:41.541: INFO: stderr: ""
Apr 17 22:01:41.541: INFO: stdout: "e2e-test-crd-publish-openapi-5070-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 17 22:01:41.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-3725 delete e2e-test-crd-publish-openapi-5070-crds test-foo'
Apr 17 22:01:41.756: INFO: stderr: ""
Apr 17 22:01:41.756: INFO: stdout: "e2e-test-crd-publish-openapi-5070-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Apr 17 22:01:41.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-3725 apply -f -'
Apr 17 22:01:42.312: INFO: stderr: ""
Apr 17 22:01:42.312: INFO: stdout: "e2e-test-crd-publish-openapi-5070-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 17 22:01:42.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-3725 delete e2e-test-crd-publish-openapi-5070-crds test-foo'
Apr 17 22:01:42.498: INFO: stderr: ""
Apr 17 22:01:42.498: INFO: stdout: "e2e-test-crd-publish-openapi-5070-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Apr 17 22:01:42.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-3725 create -f -'
Apr 17 22:01:42.826: INFO: rc: 1
Apr 17 22:01:42.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-3725 apply -f -'
Apr 17 22:01:43.320: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Apr 17 22:01:43.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-3725 create -f -'
Apr 17 22:01:43.611: INFO: rc: 1
Apr 17 22:01:43.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-3725 apply -f -'
Apr 17 22:01:43.919: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Apr 17 22:01:43.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 explain e2e-test-crd-publish-openapi-5070-crds'
Apr 17 22:01:44.509: INFO: stderr: ""
Apr 17 22:01:44.509: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5070-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Apr 17 22:01:44.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 explain e2e-test-crd-publish-openapi-5070-crds.metadata'
Apr 17 22:01:45.050: INFO: stderr: ""
Apr 17 22:01:45.050: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5070-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Apr 17 22:01:45.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 explain e2e-test-crd-publish-openapi-5070-crds.spec'
Apr 17 22:01:45.377: INFO: stderr: ""
Apr 17 22:01:45.377: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5070-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Apr 17 22:01:45.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 explain e2e-test-crd-publish-openapi-5070-crds.spec.bars'
Apr 17 22:01:46.086: INFO: stderr: ""
Apr 17 22:01:46.086: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5070-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Apr 17 22:01:46.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 explain e2e-test-crd-publish-openapi-5070-crds.spec.bars2'
Apr 17 22:01:46.636: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:01:57.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3725" for this suite.
Apr 17 22:02:05.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:02:07.843: INFO: namespace crd-publish-openapi-3725 deletion completed in 10.417113613s

â€¢ [SLOW TEST:37.529 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:02:07.844: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 17 22:02:08.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-8128'
Apr 17 22:02:08.184: INFO: stderr: ""
Apr 17 22:02:08.184: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Apr 17 22:02:08.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete pods e2e-test-httpd-pod --namespace=kubectl-8128'
Apr 17 22:02:12.420: INFO: stderr: ""
Apr 17 22:02:12.420: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:02:12.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8128" for this suite.
Apr 17 22:02:20.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:02:22.637: INFO: namespace kubectl-8128 deletion completed in 10.197879697s

â€¢ [SLOW TEST:14.793 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:02:22.637: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3932
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3932
STEP: creating replication controller externalsvc in namespace services-3932
I0417 22:02:22.969448      22 runners.go:184] Created replication controller with name: externalsvc, namespace: services-3932, replica count: 2
I0417 22:02:26.020021      22 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Apr 17 22:02:26.103: INFO: Creating new exec pod
Apr 17 22:02:28.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=services-3932 execpodgclbh -- /bin/sh -x -c nslookup clusterip-service'
Apr 17 22:02:28.543: INFO: stderr: "+ nslookup clusterip-service\n"
Apr 17 22:02:28.543: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-3932.svc.cluster.local\tcanonical name = externalsvc.services-3932.svc.cluster.local.\nName:\texternalsvc.services-3932.svc.cluster.local\nAddress: 172.21.113.250\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3932, will wait for the garbage collector to delete the pods
Apr 17 22:02:28.649: INFO: Deleting ReplicationController externalsvc took: 39.363835ms
Apr 17 22:02:29.250: INFO: Terminating ReplicationController externalsvc pods took: 601.211095ms
Apr 17 22:02:42.524: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:02:42.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3932" for this suite.
Apr 17 22:02:50.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:02:52.896: INFO: namespace services-3932 deletion completed in 10.295965554s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:30.259 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:02:52.900: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:02:55.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-408" for this suite.
Apr 17 22:03:03.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:03:05.575: INFO: namespace emptydir-wrapper-408 deletion completed in 10.224208019s

â€¢ [SLOW TEST:12.675 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:03:05.575: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 22:03:05.789: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a53759d0-bb53-4dad-a7ce-378494b850a8" in namespace "downward-api-3636" to be "success or failure"
Apr 17 22:03:05.800: INFO: Pod "downwardapi-volume-a53759d0-bb53-4dad-a7ce-378494b850a8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.346905ms
Apr 17 22:03:07.812: INFO: Pod "downwardapi-volume-a53759d0-bb53-4dad-a7ce-378494b850a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022979185s
STEP: Saw pod success
Apr 17 22:03:07.812: INFO: Pod "downwardapi-volume-a53759d0-bb53-4dad-a7ce-378494b850a8" satisfied condition "success or failure"
Apr 17 22:03:07.823: INFO: Trying to get logs from node 10.72.119.74 pod downwardapi-volume-a53759d0-bb53-4dad-a7ce-378494b850a8 container client-container: <nil>
STEP: delete the pod
Apr 17 22:03:07.939: INFO: Waiting for pod downwardapi-volume-a53759d0-bb53-4dad-a7ce-378494b850a8 to disappear
Apr 17 22:03:07.950: INFO: Pod downwardapi-volume-a53759d0-bb53-4dad-a7ce-378494b850a8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:03:07.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3636" for this suite.
Apr 17 22:03:16.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:03:18.158: INFO: namespace downward-api-3636 deletion completed in 10.173973226s

â€¢ [SLOW TEST:12.584 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:03:18.159: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-a0771d47-0ecc-4d67-bb32-a14cf7282937
STEP: Creating a pod to test consume configMaps
Apr 17 22:03:18.392: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-082c3d64-38e3-469e-8289-46706333aedd" in namespace "projected-9262" to be "success or failure"
Apr 17 22:03:18.408: INFO: Pod "pod-projected-configmaps-082c3d64-38e3-469e-8289-46706333aedd": Phase="Pending", Reason="", readiness=false. Elapsed: 15.109207ms
Apr 17 22:03:20.425: INFO: Pod "pod-projected-configmaps-082c3d64-38e3-469e-8289-46706333aedd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032939692s
Apr 17 22:03:22.438: INFO: Pod "pod-projected-configmaps-082c3d64-38e3-469e-8289-46706333aedd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045244861s
STEP: Saw pod success
Apr 17 22:03:22.438: INFO: Pod "pod-projected-configmaps-082c3d64-38e3-469e-8289-46706333aedd" satisfied condition "success or failure"
Apr 17 22:03:22.450: INFO: Trying to get logs from node 10.72.119.74 pod pod-projected-configmaps-082c3d64-38e3-469e-8289-46706333aedd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 17 22:03:22.512: INFO: Waiting for pod pod-projected-configmaps-082c3d64-38e3-469e-8289-46706333aedd to disappear
Apr 17 22:03:22.524: INFO: Pod pod-projected-configmaps-082c3d64-38e3-469e-8289-46706333aedd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:03:22.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9262" for this suite.
Apr 17 22:03:30.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:03:32.798: INFO: namespace projected-9262 deletion completed in 10.23573514s

â€¢ [SLOW TEST:14.640 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:03:32.799: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Apr 17 22:03:35.640: INFO: Successfully updated pod "annotationupdate0fa1eb5e-30db-463c-8d07-a09bcad57d06"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:03:37.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1947" for this suite.
Apr 17 22:03:51.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:03:53.869: INFO: namespace projected-1947 deletion completed in 16.150634748s

â€¢ [SLOW TEST:21.071 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:03:53.869: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Apr 17 22:03:54.140: INFO: Waiting up to 5m0s for pod "downward-api-fab5fa61-feca-4696-9fb3-f29816e49b95" in namespace "downward-api-1388" to be "success or failure"
Apr 17 22:03:54.154: INFO: Pod "downward-api-fab5fa61-feca-4696-9fb3-f29816e49b95": Phase="Pending", Reason="", readiness=false. Elapsed: 13.925359ms
Apr 17 22:03:56.167: INFO: Pod "downward-api-fab5fa61-feca-4696-9fb3-f29816e49b95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026489683s
STEP: Saw pod success
Apr 17 22:03:56.167: INFO: Pod "downward-api-fab5fa61-feca-4696-9fb3-f29816e49b95" satisfied condition "success or failure"
Apr 17 22:03:56.185: INFO: Trying to get logs from node 10.72.119.74 pod downward-api-fab5fa61-feca-4696-9fb3-f29816e49b95 container dapi-container: <nil>
STEP: delete the pod
Apr 17 22:03:56.271: INFO: Waiting for pod downward-api-fab5fa61-feca-4696-9fb3-f29816e49b95 to disappear
Apr 17 22:03:56.287: INFO: Pod downward-api-fab5fa61-feca-4696-9fb3-f29816e49b95 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:03:56.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1388" for this suite.
Apr 17 22:04:04.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:04:06.611: INFO: namespace downward-api-1388 deletion completed in 10.282793006s

â€¢ [SLOW TEST:12.741 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:04:06.613: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:04:06.742: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr 17 22:04:06.792: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 17 22:04:11.804: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 17 22:04:11.804: INFO: Creating deployment "test-rolling-update-deployment"
Apr 17 22:04:11.824: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr 17 22:04:11.860: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr 17 22:04:13.904: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr 17 22:04:13.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722757851, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722757851, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722757853, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722757851, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-55d946486\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:04:15.930: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Apr 17 22:04:15.978: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5964 /apis/apps/v1/namespaces/deployment-5964/deployments/test-rolling-update-deployment 2ef7b60c-0334-4087-aebf-533ca95daa5c 75782 1 2020-04-17 22:04:11 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00248ee18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-04-17 22:04:11 +0000 UTC,LastTransitionTime:2020-04-17 22:04:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2020-04-17 22:04:13 +0000 UTC,LastTransitionTime:2020-04-17 22:04:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 17 22:04:16.000: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-5964 /apis/apps/v1/namespaces/deployment-5964/replicasets/test-rolling-update-deployment-55d946486 1fb200c1-e47a-4ca1-b3fc-d683087408de 75770 1 2020-04-17 22:04:11 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 2ef7b60c-0334-4087-aebf-533ca95daa5c 0xc00248f300 0xc00248f301}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00248f368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 17 22:04:16.000: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr 17 22:04:16.000: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5964 /apis/apps/v1/namespaces/deployment-5964/replicasets/test-rolling-update-controller 5c257972-35ed-4bdf-a0fd-76dea788a2c0 75779 2 2020-04-17 22:04:06 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 2ef7b60c-0334-4087-aebf-533ca95daa5c 0xc00248f237 0xc00248f238}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00248f298 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 17 22:04:16.016: INFO: Pod "test-rolling-update-deployment-55d946486-vtmtq" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-vtmtq test-rolling-update-deployment-55d946486- deployment-5964 /api/v1/namespaces/deployment-5964/pods/test-rolling-update-deployment-55d946486-vtmtq bcc7975d-f6c5-49e0-9cbb-fe0175a67153 75769 0 2020-04-17 22:04:11 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[cni.projectcalico.org/podIP:172.30.194.96/32 cni.projectcalico.org/podIPs:172.30.194.96/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.194.96"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 1fb200c1-e47a-4ca1-b3fc-d683087408de 0xc003426e80 0xc003426e81}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-ph7n7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-ph7n7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-ph7n7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-c5w6m,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:04:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:04:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:04:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:04:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.74,PodIP:172.30.194.96,StartTime:2020-04-17 22:04:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-17 22:04:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://5068df145a8b3c40ebd6681adf431e9400a22f765059b0099c091ed909a3a3d2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.194.96,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:04:16.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5964" for this suite.
Apr 17 22:04:24.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:04:26.277: INFO: namespace deployment-5964 deletion completed in 10.222826044s

â€¢ [SLOW TEST:19.664 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:04:26.278: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 22:04:26.493: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4392de4-038d-46bb-ab5d-d657f9f9a099" in namespace "downward-api-8820" to be "success or failure"
Apr 17 22:04:26.510: INFO: Pod "downwardapi-volume-e4392de4-038d-46bb-ab5d-d657f9f9a099": Phase="Pending", Reason="", readiness=false. Elapsed: 17.280799ms
Apr 17 22:04:28.522: INFO: Pod "downwardapi-volume-e4392de4-038d-46bb-ab5d-d657f9f9a099": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029159154s
STEP: Saw pod success
Apr 17 22:04:28.522: INFO: Pod "downwardapi-volume-e4392de4-038d-46bb-ab5d-d657f9f9a099" satisfied condition "success or failure"
Apr 17 22:04:28.540: INFO: Trying to get logs from node 10.72.119.74 pod downwardapi-volume-e4392de4-038d-46bb-ab5d-d657f9f9a099 container client-container: <nil>
STEP: delete the pod
Apr 17 22:04:28.616: INFO: Waiting for pod downwardapi-volume-e4392de4-038d-46bb-ab5d-d657f9f9a099 to disappear
Apr 17 22:04:28.630: INFO: Pod downwardapi-volume-e4392de4-038d-46bb-ab5d-d657f9f9a099 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:04:28.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8820" for this suite.
Apr 17 22:04:36.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:04:38.909: INFO: namespace downward-api-8820 deletion completed in 10.231515127s

â€¢ [SLOW TEST:12.631 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:04:38.909: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-688aed27-17e3-40cd-8ac0-367c0dc185dc
STEP: Creating a pod to test consume secrets
Apr 17 22:04:39.115: INFO: Waiting up to 5m0s for pod "pod-secrets-fdee9a96-82a4-412a-9306-d5540f54cb0a" in namespace "secrets-9246" to be "success or failure"
Apr 17 22:04:39.135: INFO: Pod "pod-secrets-fdee9a96-82a4-412a-9306-d5540f54cb0a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.198304ms
Apr 17 22:04:41.148: INFO: Pod "pod-secrets-fdee9a96-82a4-412a-9306-d5540f54cb0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032278227s
STEP: Saw pod success
Apr 17 22:04:41.148: INFO: Pod "pod-secrets-fdee9a96-82a4-412a-9306-d5540f54cb0a" satisfied condition "success or failure"
Apr 17 22:04:41.159: INFO: Trying to get logs from node 10.72.119.74 pod pod-secrets-fdee9a96-82a4-412a-9306-d5540f54cb0a container secret-volume-test: <nil>
STEP: delete the pod
Apr 17 22:04:41.237: INFO: Waiting for pod pod-secrets-fdee9a96-82a4-412a-9306-d5540f54cb0a to disappear
Apr 17 22:04:41.248: INFO: Pod pod-secrets-fdee9a96-82a4-412a-9306-d5540f54cb0a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:04:41.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9246" for this suite.
Apr 17 22:04:49.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:04:51.507: INFO: namespace secrets-9246 deletion completed in 10.217622118s

â€¢ [SLOW TEST:12.598 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:04:51.507: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Apr 17 22:04:51.666: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 22:05:02.467: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:05:46.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9094" for this suite.
Apr 17 22:05:54.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:05:57.174: INFO: namespace crd-publish-openapi-9094 deletion completed in 10.330934629s

â€¢ [SLOW TEST:65.666 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:05:57.175: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9007.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9007.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9007.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9007.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 17 22:06:11.484: INFO: DNS probes using dns-test-cbe0602a-5ca0-42b0-bbdd-96d57416640e succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9007.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9007.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9007.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9007.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 17 22:06:15.956: INFO: DNS probes using dns-test-cf2fc1f6-3a92-44ac-8a8b-6017ac10bdaa succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9007.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9007.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9007.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9007.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 17 22:06:20.176: INFO: DNS probes using dns-test-5eec4938-998b-4ec1-aeb8-6c0e9ded3b8f succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:06:20.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9007" for this suite.
Apr 17 22:06:30.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:06:32.583: INFO: namespace dns-9007 deletion completed in 12.27642855s

â€¢ [SLOW TEST:35.408 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:06:32.585: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 22:06:33.327: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 17 22:06:35.368: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722757993, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722757993, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722757993, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722757993, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 22:06:38.423: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:06:38.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9728" for this suite.
Apr 17 22:06:46.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:06:49.068: INFO: namespace webhook-9728 deletion completed in 10.32694409s
STEP: Destroying namespace "webhook-9728-markers" for this suite.
Apr 17 22:06:57.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:06:59.332: INFO: namespace webhook-9728-markers deletion completed in 10.263589894s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:26.831 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:06:59.417: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Apr 17 22:06:59.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=kubectl-3563 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Apr 17 22:07:01.762: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Apr 17 22:07:01.762: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:07:03.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3563" for this suite.
Apr 17 22:07:15.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:07:18.121: INFO: namespace kubectl-3563 deletion completed in 14.267402175s

â€¢ [SLOW TEST:18.704 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:07:18.121: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Apr 17 22:07:18.305: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-9561" to be "success or failure"
Apr 17 22:07:18.317: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 11.24914ms
Apr 17 22:07:20.333: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027943259s
Apr 17 22:07:22.346: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040347449s
STEP: Saw pod success
Apr 17 22:07:22.346: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Apr 17 22:07:22.366: INFO: Trying to get logs from node 10.72.119.74 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Apr 17 22:07:22.487: INFO: Waiting for pod pod-host-path-test to disappear
Apr 17 22:07:22.498: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:07:22.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-9561" for this suite.
Apr 17 22:07:30.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:07:32.897: INFO: namespace hostpath-9561 deletion completed in 10.354687296s

â€¢ [SLOW TEST:14.776 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:07:32.898: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Apr 17 22:07:33.061: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Apr 17 22:07:33.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 create -f - --namespace=kubectl-2465'
Apr 17 22:07:33.588: INFO: stderr: ""
Apr 17 22:07:33.588: INFO: stdout: "service/redis-slave created\n"
Apr 17 22:07:33.588: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Apr 17 22:07:33.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 create -f - --namespace=kubectl-2465'
Apr 17 22:07:34.210: INFO: stderr: ""
Apr 17 22:07:34.210: INFO: stdout: "service/redis-master created\n"
Apr 17 22:07:34.210: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr 17 22:07:34.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 create -f - --namespace=kubectl-2465'
Apr 17 22:07:34.770: INFO: stderr: ""
Apr 17 22:07:34.770: INFO: stdout: "service/frontend created\n"
Apr 17 22:07:34.771: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Apr 17 22:07:34.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 create -f - --namespace=kubectl-2465'
Apr 17 22:07:35.298: INFO: stderr: ""
Apr 17 22:07:35.298: INFO: stdout: "deployment.apps/frontend created\n"
Apr 17 22:07:35.298: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 17 22:07:35.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 create -f - --namespace=kubectl-2465'
Apr 17 22:07:35.840: INFO: stderr: ""
Apr 17 22:07:35.840: INFO: stdout: "deployment.apps/redis-master created\n"
Apr 17 22:07:35.840: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Apr 17 22:07:35.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 create -f - --namespace=kubectl-2465'
Apr 17 22:07:36.203: INFO: stderr: ""
Apr 17 22:07:36.203: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Apr 17 22:07:36.203: INFO: Waiting for all frontend pods to be Running.
Apr 17 22:07:56.254: INFO: Waiting for frontend to serve content.
Apr 17 22:07:56.300: INFO: Trying to add a new entry to the guestbook.
Apr 17 22:07:56.335: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Apr 17 22:07:56.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete --grace-period=0 --force -f - --namespace=kubectl-2465'
Apr 17 22:07:56.579: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 22:07:56.579: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Apr 17 22:07:56.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete --grace-period=0 --force -f - --namespace=kubectl-2465'
Apr 17 22:07:56.790: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 22:07:56.790: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr 17 22:07:56.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete --grace-period=0 --force -f - --namespace=kubectl-2465'
Apr 17 22:07:57.001: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 22:07:57.001: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 17 22:07:57.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete --grace-period=0 --force -f - --namespace=kubectl-2465'
Apr 17 22:07:57.196: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 22:07:57.196: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr 17 22:07:57.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete --grace-period=0 --force -f - --namespace=kubectl-2465'
Apr 17 22:07:57.370: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 22:07:57.370: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr 17 22:07:57.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete --grace-period=0 --force -f - --namespace=kubectl-2465'
Apr 17 22:07:57.552: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 22:07:57.552: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:07:57.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2465" for this suite.
Apr 17 22:08:13.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:08:16.036: INFO: namespace kubectl-2465 deletion completed in 18.453192544s

â€¢ [SLOW TEST:43.139 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:08:16.038: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Apr 17 22:08:16.187: INFO: namespace kubectl-4540
Apr 17 22:08:16.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 create -f - --namespace=kubectl-4540'
Apr 17 22:08:16.917: INFO: stderr: ""
Apr 17 22:08:16.917: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr 17 22:08:17.933: INFO: Selector matched 1 pods for map[app:redis]
Apr 17 22:08:17.933: INFO: Found 0 / 1
Apr 17 22:08:18.932: INFO: Selector matched 1 pods for map[app:redis]
Apr 17 22:08:18.932: INFO: Found 1 / 1
Apr 17 22:08:18.932: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 17 22:08:18.944: INFO: Selector matched 1 pods for map[app:redis]
Apr 17 22:08:18.944: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 17 22:08:18.944: INFO: wait on redis-master startup in kubectl-4540 
Apr 17 22:08:18.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 logs redis-master-n6l28 redis-master --namespace=kubectl-4540'
Apr 17 22:08:19.122: INFO: stderr: ""
Apr 17 22:08:19.122: INFO: stdout: "1:C 17 Apr 2020 22:08:18.208 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 17 Apr 2020 22:08:18.208 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 17 Apr 2020 22:08:18.208 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 17 Apr 2020 22:08:18.211 * Running mode=standalone, port=6379.\n1:M 17 Apr 2020 22:08:18.211 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 17 Apr 2020 22:08:18.211 # Server initialized\n1:M 17 Apr 2020 22:08:18.211 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 17 Apr 2020 22:08:18.212 * Ready to accept connections\n"
STEP: exposing RC
Apr 17 22:08:19.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-4540'
Apr 17 22:08:19.295: INFO: stderr: ""
Apr 17 22:08:19.295: INFO: stdout: "service/rm2 exposed\n"
Apr 17 22:08:19.314: INFO: Service rm2 in namespace kubectl-4540 found.
STEP: exposing service
Apr 17 22:08:21.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-4540'
Apr 17 22:08:21.552: INFO: stderr: ""
Apr 17 22:08:21.552: INFO: stdout: "service/rm3 exposed\n"
Apr 17 22:08:21.585: INFO: Service rm3 in namespace kubectl-4540 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:08:23.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4540" for this suite.
Apr 17 22:08:37.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:08:39.833: INFO: namespace kubectl-4540 deletion completed in 16.158082864s

â€¢ [SLOW TEST:23.796 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:08:39.834: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-0ffafb0c-1deb-4871-bcca-78d7bb1842ab
STEP: Creating a pod to test consume configMaps
Apr 17 22:08:40.063: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-398f82fa-f16e-4e93-a6d6-3fad45873a5b" in namespace "projected-2580" to be "success or failure"
Apr 17 22:08:40.078: INFO: Pod "pod-projected-configmaps-398f82fa-f16e-4e93-a6d6-3fad45873a5b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.736615ms
Apr 17 22:08:42.090: INFO: Pod "pod-projected-configmaps-398f82fa-f16e-4e93-a6d6-3fad45873a5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027375118s
STEP: Saw pod success
Apr 17 22:08:42.090: INFO: Pod "pod-projected-configmaps-398f82fa-f16e-4e93-a6d6-3fad45873a5b" satisfied condition "success or failure"
Apr 17 22:08:42.102: INFO: Trying to get logs from node 10.72.119.74 pod pod-projected-configmaps-398f82fa-f16e-4e93-a6d6-3fad45873a5b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 17 22:08:42.167: INFO: Waiting for pod pod-projected-configmaps-398f82fa-f16e-4e93-a6d6-3fad45873a5b to disappear
Apr 17 22:08:42.177: INFO: Pod pod-projected-configmaps-398f82fa-f16e-4e93-a6d6-3fad45873a5b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:08:42.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2580" for this suite.
Apr 17 22:08:50.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:08:52.402: INFO: namespace projected-2580 deletion completed in 10.196007207s

â€¢ [SLOW TEST:12.568 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:08:52.403: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9287.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9287.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9287.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9287.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9287.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9287.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 17 22:08:56.824: INFO: DNS probes using dns-9287/dns-test-2ceac2a4-0e0d-4d90-8002-227b54faece2 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:08:56.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9287" for this suite.
Apr 17 22:09:04.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:09:07.058: INFO: namespace dns-9287 deletion completed in 10.1676464s

â€¢ [SLOW TEST:14.656 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:09:07.059: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7015
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-7015
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7015
Apr 17 22:09:07.272: INFO: Found 0 stateful pods, waiting for 1
Apr 17 22:09:17.285: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr 17 22:09:17.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 22:09:17.631: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 22:09:17.631: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 22:09:17.631: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 22:09:17.643: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 17 22:09:27.656: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 17 22:09:27.656: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 22:09:27.730: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Apr 17 22:09:27.730: INFO: ss-0  10.72.119.74  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:18 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  }]
Apr 17 22:09:27.730: INFO: 
Apr 17 22:09:27.730: INFO: StatefulSet ss has not reached scale 3, at 1
Apr 17 22:09:28.746: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.986926388s
Apr 17 22:09:29.761: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.971648803s
Apr 17 22:09:30.775: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.955998838s
Apr 17 22:09:31.789: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.942707295s
Apr 17 22:09:32.802: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.928305177s
Apr 17 22:09:33.818: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.914946437s
Apr 17 22:09:34.832: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.899712001s
Apr 17 22:09:35.846: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.885638245s
Apr 17 22:09:36.858: INFO: Verifying statefulset ss doesn't scale past 3 for another 871.421917ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7015
Apr 17 22:09:37.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:09:38.189: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 17 22:09:38.189: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 22:09:38.189: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 17 22:09:38.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:09:38.574: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 17 22:09:38.574: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 22:09:38.574: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 17 22:09:38.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:09:38.969: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 17 22:09:38.969: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 22:09:38.969: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 17 22:09:38.982: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 22:09:38.982: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 22:09:38.982: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr 17 22:09:38.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 22:09:39.412: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 22:09:39.412: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 22:09:39.412: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 22:09:39.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 22:09:39.821: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 22:09:39.821: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 22:09:39.821: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 22:09:39.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 22:09:40.171: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 22:09:40.171: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 22:09:40.171: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 22:09:40.171: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 22:09:40.186: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr 17 22:09:50.216: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 17 22:09:50.217: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 17 22:09:50.217: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 17 22:09:50.269: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Apr 17 22:09:50.269: INFO: ss-0  10.72.119.74  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  }]
Apr 17 22:09:50.269: INFO: ss-1  10.72.119.98  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:50.269: INFO: ss-2  10.72.119.74  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:50.269: INFO: 
Apr 17 22:09:50.269: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 17 22:09:51.282: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Apr 17 22:09:51.282: INFO: ss-0  10.72.119.74  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  }]
Apr 17 22:09:51.282: INFO: ss-1  10.72.119.98  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:51.282: INFO: ss-2  10.72.119.74  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:51.282: INFO: 
Apr 17 22:09:51.282: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 17 22:09:52.295: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Apr 17 22:09:52.295: INFO: ss-0  10.72.119.74  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  }]
Apr 17 22:09:52.295: INFO: ss-1  10.72.119.98  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:52.295: INFO: ss-2  10.72.119.74  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:52.295: INFO: 
Apr 17 22:09:52.295: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 17 22:09:53.311: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Apr 17 22:09:53.311: INFO: ss-0  10.72.119.74  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  }]
Apr 17 22:09:53.311: INFO: ss-1  10.72.119.98  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:53.311: INFO: ss-2  10.72.119.74  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:53.311: INFO: 
Apr 17 22:09:53.311: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 17 22:09:54.324: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Apr 17 22:09:54.324: INFO: ss-0  10.72.119.74  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  }]
Apr 17 22:09:54.324: INFO: ss-1  10.72.119.98  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:54.324: INFO: ss-2  10.72.119.74  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:54.324: INFO: 
Apr 17 22:09:54.324: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 17 22:09:55.339: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Apr 17 22:09:55.339: INFO: ss-0  10.72.119.74  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  }]
Apr 17 22:09:55.339: INFO: ss-1  10.72.119.98  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:55.339: INFO: ss-2  10.72.119.74  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:55.339: INFO: 
Apr 17 22:09:55.339: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 17 22:09:56.351: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Apr 17 22:09:56.351: INFO: ss-0  10.72.119.74  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  }]
Apr 17 22:09:56.351: INFO: ss-1  10.72.119.98  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:56.351: INFO: ss-2  10.72.119.74  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:56.351: INFO: 
Apr 17 22:09:56.351: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 17 22:09:57.364: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Apr 17 22:09:57.364: INFO: ss-0  10.72.119.74  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  }]
Apr 17 22:09:57.364: INFO: ss-2  10.72.119.74  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:57.364: INFO: 
Apr 17 22:09:57.364: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 17 22:09:58.378: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Apr 17 22:09:58.378: INFO: ss-0  10.72.119.74  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  }]
Apr 17 22:09:58.378: INFO: ss-2  10.72.119.74  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:58.378: INFO: 
Apr 17 22:09:58.378: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 17 22:09:59.391: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Apr 17 22:09:59.391: INFO: ss-0  10.72.119.74  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:07 +0000 UTC  }]
Apr 17 22:09:59.391: INFO: ss-2  10.72.119.74  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2020-04-17 22:09:27 +0000 UTC  }]
Apr 17 22:09:59.391: INFO: 
Apr 17 22:09:59.391: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7015
Apr 17 22:10:00.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:10:00.604: INFO: rc: 1
Apr 17 22:10:00.604: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc0068ea990 exit status 1 <nil> <nil> true [0xc00225edd8 0xc00225ef68 0xc00225efa0] [0xc00225edd8 0xc00225ef68 0xc00225efa0] [0xc00225ef18 0xc00225ef80] [0x10efce0 0x10efce0] 0xc002def080 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Apr 17 22:10:10.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:10:10.762: INFO: rc: 1
Apr 17 22:10:10.762: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0056996e0 exit status 1 <nil> <nil> true [0xc002b441b0 0xc002b44200 0xc002b44228] [0xc002b441b0 0xc002b44200 0xc002b44228] [0xc002b441f0 0xc002b44220] [0x10efce0 0x10efce0] 0xc0033878c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:10:20.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:10:20.900: INFO: rc: 1
Apr 17 22:10:20.900: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc005abac00 exit status 1 <nil> <nil> true [0xc00348c280 0xc00348c328 0xc00348c3e8] [0xc00348c280 0xc00348c328 0xc00348c3e8] [0xc00348c308 0xc00348c3d0] [0x10efce0 0x10efce0] 0xc003b4b380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:10:30.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:10:31.038: INFO: rc: 1
Apr 17 22:10:31.038: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0068ead20 exit status 1 <nil> <nil> true [0xc00225f018 0xc00225f080 0xc00225f0e8] [0xc00225f018 0xc00225f080 0xc00225f0e8] [0xc00225f070 0xc00225f0c8] [0x10efce0 0x10efce0] 0xc0029cca80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:10:41.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:10:41.201: INFO: rc: 1
Apr 17 22:10:41.201: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc005699b00 exit status 1 <nil> <nil> true [0xc002b44230 0xc002b44250 0xc002b44288] [0xc002b44230 0xc002b44250 0xc002b44288] [0xc002b44240 0xc002b44278] [0x10efce0 0x10efce0] 0xc002f8a8a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:10:51.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:10:51.354: INFO: rc: 1
Apr 17 22:10:51.354: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc005699e90 exit status 1 <nil> <nil> true [0xc002b44298 0xc002b442c8 0xc002b442e8] [0xc002b44298 0xc002b442c8 0xc002b442e8] [0xc002b442b8 0xc002b442e0] [0x10efce0 0x10efce0] 0xc002be40c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:11:01.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:11:01.496: INFO: rc: 1
Apr 17 22:11:01.496: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0069bc240 exit status 1 <nil> <nil> true [0xc002b442f0 0xc002b44308 0xc002b44328] [0xc002b442f0 0xc002b44308 0xc002b44328] [0xc002b44300 0xc002b44320] [0x10efce0 0x10efce0] 0xc002be5aa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:11:11.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:11:11.650: INFO: rc: 1
Apr 17 22:11:11.650: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0069bc5d0 exit status 1 <nil> <nil> true [0xc002b44330 0xc002b44358 0xc002b44388] [0xc002b44330 0xc002b44358 0xc002b44388] [0xc002b44348 0xc002b44378] [0x10efce0 0x10efce0] 0xc002b8aa80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:11:21.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:11:21.804: INFO: rc: 1
Apr 17 22:11:21.804: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0069bc960 exit status 1 <nil> <nil> true [0xc002b44398 0xc002b443b0 0xc002b443c8] [0xc002b44398 0xc002b443b0 0xc002b443c8] [0xc002b443a8 0xc002b443c0] [0x10efce0 0x10efce0] 0xc002b8bc80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:11:31.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:11:31.954: INFO: rc: 1
Apr 17 22:11:31.954: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0069bccc0 exit status 1 <nil> <nil> true [0xc002b443d0 0xc002b443f0 0xc002b44418] [0xc002b443d0 0xc002b443f0 0xc002b44418] [0xc002b443e8 0xc002b44408] [0x10efce0 0x10efce0] 0xc00248c780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:11:41.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:11:42.294: INFO: rc: 1
Apr 17 22:11:42.295: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0068eb0e0 exit status 1 <nil> <nil> true [0xc00225f0f0 0xc00225f168 0xc00225f188] [0xc00225f0f0 0xc00225f168 0xc00225f188] [0xc00225f108 0xc00225f178] [0x10efce0 0x10efce0] 0xc002894d80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:11:52.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:11:52.442: INFO: rc: 1
Apr 17 22:11:52.442: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0056983c0 exit status 1 <nil> <nil> true [0xc002b44038 0xc002b44098 0xc002b44100] [0xc002b44038 0xc002b44098 0xc002b44100] [0xc002b44088 0xc002b440f8] [0x10efce0 0x10efce0] 0xc002b8aa80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:12:02.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:12:02.594: INFO: rc: 1
Apr 17 22:12:02.594: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c42330 exit status 1 <nil> <nil> true [0xc00225e010 0xc00225e2c0 0xc00225e5b8] [0xc00225e010 0xc00225e2c0 0xc00225e5b8] [0xc00225e228 0xc00225e4c0] [0x10efce0 0x10efce0] 0xc002be5380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:12:12.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:12:12.742: INFO: rc: 1
Apr 17 22:12:12.742: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c426c0 exit status 1 <nil> <nil> true [0xc00225e638 0xc00225e870 0xc00225e960] [0xc00225e638 0xc00225e870 0xc00225e960] [0xc00225e790 0xc00225e938] [0x10efce0 0x10efce0] 0xc002f8ae40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:12:22.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:12:22.905: INFO: rc: 1
Apr 17 22:12:22.905: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0056987e0 exit status 1 <nil> <nil> true [0xc002b44108 0xc002b44138 0xc002b44170] [0xc002b44108 0xc002b44138 0xc002b44170] [0xc002b44128 0xc002b44158] [0x10efce0 0x10efce0] 0xc002b8bc80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:12:32.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:12:33.053: INFO: rc: 1
Apr 17 22:12:33.053: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc005698b70 exit status 1 <nil> <nil> true [0xc002b44178 0xc002b44190 0xc002b441b0] [0xc002b44178 0xc002b44190 0xc002b441b0] [0xc002b44188 0xc002b441a8] [0x10efce0 0x10efce0] 0xc0029cda40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:12:43.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:12:43.193: INFO: rc: 1
Apr 17 22:12:43.193: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00b88a420 exit status 1 <nil> <nil> true [0xc00348c000 0xc00348c038 0xc00348c070] [0xc00348c000 0xc00348c038 0xc00348c070] [0xc00348c028 0xc00348c060] [0x10efce0 0x10efce0] 0xc003386c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:12:53.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:12:53.334: INFO: rc: 1
Apr 17 22:12:53.334: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00b88a810 exit status 1 <nil> <nil> true [0xc00348c080 0xc00348c0d0 0xc00348c158] [0xc00348c080 0xc00348c0d0 0xc00348c158] [0xc00348c0a8 0xc00348c128] [0x10efce0 0x10efce0] 0xc0033879e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:13:03.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:13:03.509: INFO: rc: 1
Apr 17 22:13:03.509: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00b88aba0 exit status 1 <nil> <nil> true [0xc00348c180 0xc00348c208 0xc00348c280] [0xc00348c180 0xc00348c208 0xc00348c280] [0xc00348c1e0 0xc00348c258] [0x10efce0 0x10efce0] 0xc0028ce600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:13:13.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:13:13.655: INFO: rc: 1
Apr 17 22:13:13.655: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00b88af90 exit status 1 <nil> <nil> true [0xc00348c2e8 0xc00348c350 0xc00348c3f8] [0xc00348c2e8 0xc00348c350 0xc00348c3f8] [0xc00348c328 0xc00348c3e8] [0x10efce0 0x10efce0] 0xc002f7c600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:13:23.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:13:23.814: INFO: rc: 1
Apr 17 22:13:23.814: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc005698f30 exit status 1 <nil> <nil> true [0xc002b441e0 0xc002b44210 0xc002b44230] [0xc002b441e0 0xc002b44210 0xc002b44230] [0xc002b44200 0xc002b44228] [0x10efce0 0x10efce0] 0xc002deef60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:13:33.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:13:33.960: INFO: rc: 1
Apr 17 22:13:33.961: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0056992c0 exit status 1 <nil> <nil> true [0xc002b44238 0xc002b44268 0xc002b44298] [0xc002b44238 0xc002b44268 0xc002b44298] [0xc002b44250 0xc002b44288] [0x10efce0 0x10efce0] 0xc00248c000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:13:43.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:13:44.091: INFO: rc: 1
Apr 17 22:13:44.091: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc005699650 exit status 1 <nil> <nil> true [0xc002b442a8 0xc002b442d8 0xc002b442f0] [0xc002b442a8 0xc002b442d8 0xc002b442f0] [0xc002b442c8 0xc002b442e8] [0x10efce0 0x10efce0] 0xc00248cf00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:13:54.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:13:54.249: INFO: rc: 1
Apr 17 22:13:54.249: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00b88a450 exit status 1 <nil> <nil> true [0xc00348c018 0xc00348c048 0xc00348c080] [0xc00348c018 0xc00348c048 0xc00348c080] [0xc00348c038 0xc00348c070] [0x10efce0 0x10efce0] 0xc002f7cb40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:14:04.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:14:04.380: INFO: rc: 1
Apr 17 22:14:04.380: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c42360 exit status 1 <nil> <nil> true [0xc002b44028 0xc002b44088 0xc002b440f8] [0xc002b44028 0xc002b44088 0xc002b440f8] [0xc002b44048 0xc002b440d0] [0x10efce0 0x10efce0] 0xc00235f260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:14:14.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:14:14.519: INFO: rc: 1
Apr 17 22:14:14.519: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c42720 exit status 1 <nil> <nil> true [0xc002b44100 0xc002b44128 0xc002b44158] [0xc002b44100 0xc002b44128 0xc002b44158] [0xc002b44118 0xc002b44148] [0x10efce0 0x10efce0] 0xc002deede0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:14:24.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:14:24.676: INFO: rc: 1
Apr 17 22:14:24.676: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00b88a840 exit status 1 <nil> <nil> true [0xc00348c098 0xc00348c0f8 0xc00348c180] [0xc00348c098 0xc00348c0f8 0xc00348c180] [0xc00348c0d0 0xc00348c158] [0x10efce0 0x10efce0] 0xc002f7d8c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:14:34.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:14:34.836: INFO: rc: 1
Apr 17 22:14:34.836: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001c42ab0 exit status 1 <nil> <nil> true [0xc002b44170 0xc002b44188 0xc002b441a8] [0xc002b44170 0xc002b44188 0xc002b441a8] [0xc002b44180 0xc002b44198] [0x10efce0 0x10efce0] 0xc002defbc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:14:44.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:14:44.993: INFO: rc: 1
Apr 17 22:14:44.993: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00ae15380 exit status 1 <nil> <nil> true [0xc00225e5b8 0xc00225e790 0xc00225e938] [0xc00225e5b8 0xc00225e790 0xc00225e938] [0xc00225e740 0xc00225e8e0] [0x10efce0 0x10efce0] 0xc003387320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:14:54.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:14:55.137: INFO: rc: 1
Apr 17 22:14:55.137: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc005698450 exit status 1 <nil> <nil> true [0xc00133a040 0xc00133a120 0xc00133a188] [0xc00133a040 0xc00133a120 0xc00133a188] [0xc00133a0f8 0xc00133a170] [0x10efce0 0x10efce0] 0xc002be5380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Apr 17 22:15:05.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-7015 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:15:05.279: INFO: rc: 1
Apr 17 22:15:05.279: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Apr 17 22:15:05.279: INFO: Scaling statefulset ss to 0
Apr 17 22:15:05.336: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Apr 17 22:15:05.356: INFO: Deleting all statefulset in ns statefulset-7015
Apr 17 22:15:05.374: INFO: Scaling statefulset ss to 0
Apr 17 22:15:05.423: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 22:15:05.440: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:15:05.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7015" for this suite.
Apr 17 22:15:13.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:15:15.763: INFO: namespace statefulset-7015 deletion completed in 10.228292655s

â€¢ [SLOW TEST:368.704 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:15:15.763: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-c2604403-1989-4fd0-884c-82e784c0fd5c
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:15:15.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-173" for this suite.
Apr 17 22:15:24.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:15:26.149: INFO: namespace secrets-173 deletion completed in 10.168689058s

â€¢ [SLOW TEST:10.385 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:15:26.149: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:15:26.293: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:15:26.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8290" for this suite.
Apr 17 22:15:35.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:15:37.216: INFO: namespace custom-resource-definition-8290 deletion completed in 10.247919818s

â€¢ [SLOW TEST:11.067 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:15:37.217: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 17 22:15:37.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-1277'
Apr 17 22:15:37.569: INFO: stderr: ""
Apr 17 22:15:37.569: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Apr 17 22:15:42.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pod e2e-test-httpd-pod --namespace=kubectl-1277 -o json'
Apr 17 22:15:42.762: INFO: stderr: ""
Apr 17 22:15:42.762: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.30.194.126/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.194.126/32\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.194.126\\\"\\n    ],\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2020-04-17T22:15:37Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1277\",\n        \"resourceVersion\": \"80190\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1277/pods/e2e-test-httpd-pod\",\n        \"uid\": \"9c720d6c-55ec-4e6a-9ecd-2035eab8c45f\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-9bnss\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-4w7jn\"\n            }\n        ],\n        \"nodeName\": \"10.72.119.74\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c59,c39\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-9bnss\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-9bnss\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-04-17T22:15:37Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-04-17T22:15:39Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-04-17T22:15:39Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2020-04-17T22:15:37Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://fc804bdce7d5ead723c10fc380cee7b58dbeda27ab06f7709673a71194cb9292\",\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2020-04-17T22:15:38Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.72.119.74\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.194.126\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.194.126\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2020-04-17T22:15:37Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr 17 22:15:42.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 replace -f - --namespace=kubectl-1277'
Apr 17 22:15:43.531: INFO: stderr: ""
Apr 17 22:15:43.531: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Apr 17 22:15:43.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete pods e2e-test-httpd-pod --namespace=kubectl-1277'
Apr 17 22:15:45.127: INFO: stderr: ""
Apr 17 22:15:45.127: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:15:45.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1277" for this suite.
Apr 17 22:15:53.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:15:55.539: INFO: namespace kubectl-1277 deletion completed in 10.389135563s

â€¢ [SLOW TEST:18.322 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:15:55.539: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 22:15:55.771: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c92b8de-4602-415f-8850-3362fa10ad44" in namespace "projected-2954" to be "success or failure"
Apr 17 22:15:55.783: INFO: Pod "downwardapi-volume-7c92b8de-4602-415f-8850-3362fa10ad44": Phase="Pending", Reason="", readiness=false. Elapsed: 12.222001ms
Apr 17 22:15:57.797: INFO: Pod "downwardapi-volume-7c92b8de-4602-415f-8850-3362fa10ad44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026473916s
STEP: Saw pod success
Apr 17 22:15:57.797: INFO: Pod "downwardapi-volume-7c92b8de-4602-415f-8850-3362fa10ad44" satisfied condition "success or failure"
Apr 17 22:15:57.810: INFO: Trying to get logs from node 10.72.119.74 pod downwardapi-volume-7c92b8de-4602-415f-8850-3362fa10ad44 container client-container: <nil>
STEP: delete the pod
Apr 17 22:15:57.924: INFO: Waiting for pod downwardapi-volume-7c92b8de-4602-415f-8850-3362fa10ad44 to disappear
Apr 17 22:15:57.936: INFO: Pod downwardapi-volume-7c92b8de-4602-415f-8850-3362fa10ad44 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:15:57.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2954" for this suite.
Apr 17 22:16:06.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:16:08.132: INFO: namespace projected-2954 deletion completed in 10.174649368s

â€¢ [SLOW TEST:12.593 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:16:08.137: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Apr 17 22:16:12.895: INFO: Successfully updated pod "adopt-release-l4qrq"
STEP: Checking that the Job readopts the Pod
Apr 17 22:16:12.896: INFO: Waiting up to 15m0s for pod "adopt-release-l4qrq" in namespace "job-4140" to be "adopted"
Apr 17 22:16:12.907: INFO: Pod "adopt-release-l4qrq": Phase="Running", Reason="", readiness=true. Elapsed: 11.577106ms
Apr 17 22:16:14.921: INFO: Pod "adopt-release-l4qrq": Phase="Running", Reason="", readiness=true. Elapsed: 2.02550489s
Apr 17 22:16:14.921: INFO: Pod "adopt-release-l4qrq" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Apr 17 22:16:15.477: INFO: Successfully updated pod "adopt-release-l4qrq"
STEP: Checking that the Job releases the Pod
Apr 17 22:16:15.477: INFO: Waiting up to 15m0s for pod "adopt-release-l4qrq" in namespace "job-4140" to be "released"
Apr 17 22:16:15.491: INFO: Pod "adopt-release-l4qrq": Phase="Running", Reason="", readiness=true. Elapsed: 13.260583ms
Apr 17 22:16:15.491: INFO: Pod "adopt-release-l4qrq" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:16:15.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4140" for this suite.
Apr 17 22:17:05.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:17:07.928: INFO: namespace job-4140 deletion completed in 52.414664731s

â€¢ [SLOW TEST:59.791 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:17:07.929: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Apr 17 22:17:10.751: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5002 pod-service-account-ec0cddfd-e159-4957-b673-12b31021ce4e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Apr 17 22:17:11.158: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5002 pod-service-account-ec0cddfd-e159-4957-b673-12b31021ce4e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Apr 17 22:17:11.536: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5002 pod-service-account-ec0cddfd-e159-4957-b673-12b31021ce4e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:17:11.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5002" for this suite.
Apr 17 22:17:19.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:17:22.166: INFO: namespace svcaccounts-5002 deletion completed in 10.24296325s

â€¢ [SLOW TEST:14.237 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:17:22.166: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Apr 17 22:17:22.314: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Apr 17 22:17:22.899: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Apr 17 22:17:25.108: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758643, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:17:27.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758643, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:17:29.123: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758643, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:17:31.136: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758643, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:17:33.122: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758643, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:17:35.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758643, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:17:37.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758643, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758642, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:17:39.341: INFO: Waited 196.581957ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:17:41.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-505" for this suite.
Apr 17 22:17:49.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:17:51.946: INFO: namespace aggregator-505 deletion completed in 10.404836402s

â€¢ [SLOW TEST:29.780 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:17:51.947: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:17:56.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2277" for this suite.
Apr 17 22:18:08.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:18:10.627: INFO: namespace containers-2277 deletion completed in 14.2306986s

â€¢ [SLOW TEST:18.681 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:18:10.628: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-7741
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7741
STEP: Deleting pre-stop pod
Apr 17 22:18:19.969: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:18:19.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7741" for this suite.
Apr 17 22:18:52.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:18:54.141: INFO: namespace prestop-7741 deletion completed in 34.119471896s

â€¢ [SLOW TEST:43.513 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:18:54.142: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:19:02.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-979" for this suite.
Apr 17 22:19:10.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:19:12.554: INFO: namespace job-979 deletion completed in 10.199585752s

â€¢ [SLOW TEST:18.412 seconds]
[sig-apps] Job
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:19:12.554: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 22:19:12.853: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f7cb1c0a-ee90-46ef-a336-a17344d9d2ce" in namespace "downward-api-1115" to be "success or failure"
Apr 17 22:19:12.865: INFO: Pod "downwardapi-volume-f7cb1c0a-ee90-46ef-a336-a17344d9d2ce": Phase="Pending", Reason="", readiness=false. Elapsed: 12.16912ms
Apr 17 22:19:14.877: INFO: Pod "downwardapi-volume-f7cb1c0a-ee90-46ef-a336-a17344d9d2ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023557824s
STEP: Saw pod success
Apr 17 22:19:14.877: INFO: Pod "downwardapi-volume-f7cb1c0a-ee90-46ef-a336-a17344d9d2ce" satisfied condition "success or failure"
Apr 17 22:19:14.888: INFO: Trying to get logs from node 10.72.119.74 pod downwardapi-volume-f7cb1c0a-ee90-46ef-a336-a17344d9d2ce container client-container: <nil>
STEP: delete the pod
Apr 17 22:19:14.958: INFO: Waiting for pod downwardapi-volume-f7cb1c0a-ee90-46ef-a336-a17344d9d2ce to disappear
Apr 17 22:19:14.973: INFO: Pod downwardapi-volume-f7cb1c0a-ee90-46ef-a336-a17344d9d2ce no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:19:14.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1115" for this suite.
Apr 17 22:19:23.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:19:25.212: INFO: namespace downward-api-1115 deletion completed in 10.206980046s

â€¢ [SLOW TEST:12.658 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:19:25.213: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Apr 17 22:19:25.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-7413'
Apr 17 22:19:25.482: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr 17 22:19:25.482: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Apr 17 22:19:29.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete deployment e2e-test-httpd-deployment --namespace=kubectl-7413'
Apr 17 22:19:29.708: INFO: stderr: ""
Apr 17 22:19:29.708: INFO: stdout: "deployment.extensions \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:19:29.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7413" for this suite.
Apr 17 22:19:37.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:19:39.951: INFO: namespace kubectl-7413 deletion completed in 10.214225641s

â€¢ [SLOW TEST:14.738 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:19:39.952: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:19:40.091: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 17 22:19:50.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-7691 create -f -'
Apr 17 22:19:51.149: INFO: stderr: ""
Apr 17 22:19:51.150: INFO: stdout: "e2e-test-crd-publish-openapi-2328-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 17 22:19:51.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-7691 delete e2e-test-crd-publish-openapi-2328-crds test-cr'
Apr 17 22:19:51.322: INFO: stderr: ""
Apr 17 22:19:51.322: INFO: stdout: "e2e-test-crd-publish-openapi-2328-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Apr 17 22:19:51.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-7691 apply -f -'
Apr 17 22:19:52.072: INFO: stderr: ""
Apr 17 22:19:52.072: INFO: stdout: "e2e-test-crd-publish-openapi-2328-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 17 22:19:52.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-7691 delete e2e-test-crd-publish-openapi-2328-crds test-cr'
Apr 17 22:19:52.262: INFO: stderr: ""
Apr 17 22:19:52.262: INFO: stdout: "e2e-test-crd-publish-openapi-2328-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Apr 17 22:19:52.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 explain e2e-test-crd-publish-openapi-2328-crds'
Apr 17 22:19:52.768: INFO: stderr: ""
Apr 17 22:19:52.768: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-2328-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:20:03.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7691" for this suite.
Apr 17 22:20:11.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:20:13.737: INFO: namespace crd-publish-openapi-7691 deletion completed in 10.240991443s

â€¢ [SLOW TEST:33.786 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:20:13.739: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 17 22:20:14.042: INFO: Number of nodes with available pods: 0
Apr 17 22:20:14.042: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 22:20:15.076: INFO: Number of nodes with available pods: 0
Apr 17 22:20:15.076: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 22:20:16.080: INFO: Number of nodes with available pods: 1
Apr 17 22:20:16.080: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 22:20:17.083: INFO: Number of nodes with available pods: 3
Apr 17 22:20:17.083: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr 17 22:20:17.169: INFO: Number of nodes with available pods: 2
Apr 17 22:20:17.169: INFO: Node 10.72.119.74 is running more than one daemon pod
Apr 17 22:20:18.200: INFO: Number of nodes with available pods: 2
Apr 17 22:20:18.200: INFO: Node 10.72.119.74 is running more than one daemon pod
Apr 17 22:20:19.205: INFO: Number of nodes with available pods: 2
Apr 17 22:20:19.205: INFO: Node 10.72.119.74 is running more than one daemon pod
Apr 17 22:20:20.207: INFO: Number of nodes with available pods: 2
Apr 17 22:20:20.207: INFO: Node 10.72.119.74 is running more than one daemon pod
Apr 17 22:20:21.213: INFO: Number of nodes with available pods: 2
Apr 17 22:20:21.213: INFO: Node 10.72.119.74 is running more than one daemon pod
Apr 17 22:20:22.215: INFO: Number of nodes with available pods: 2
Apr 17 22:20:22.215: INFO: Node 10.72.119.74 is running more than one daemon pod
Apr 17 22:20:23.199: INFO: Number of nodes with available pods: 2
Apr 17 22:20:23.199: INFO: Node 10.72.119.74 is running more than one daemon pod
Apr 17 22:20:24.200: INFO: Number of nodes with available pods: 3
Apr 17 22:20:24.200: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2262, will wait for the garbage collector to delete the pods
Apr 17 22:20:24.302: INFO: Deleting DaemonSet.extensions daemon-set took: 24.615159ms
Apr 17 22:20:25.002: INFO: Terminating DaemonSet.extensions daemon-set pods took: 700.331493ms
Apr 17 22:20:37.914: INFO: Number of nodes with available pods: 0
Apr 17 22:20:37.914: INFO: Number of running nodes: 0, number of available pods: 0
Apr 17 22:20:37.925: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2262/daemonsets","resourceVersion":"82516"},"items":null}

Apr 17 22:20:37.937: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2262/pods","resourceVersion":"82516"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:20:37.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2262" for this suite.
Apr 17 22:20:46.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:20:48.244: INFO: namespace daemonsets-2262 deletion completed in 10.232973423s

â€¢ [SLOW TEST:34.505 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:20:48.248: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:20:48.406: INFO: Creating deployment "test-recreate-deployment"
Apr 17 22:20:48.447: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr 17 22:20:48.494: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Apr 17 22:20:50.524: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr 17 22:20:50.540: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr 17 22:20:50.582: INFO: Updating deployment test-recreate-deployment
Apr 17 22:20:50.582: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Apr 17 22:20:50.819: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-2472 /apis/apps/v1/namespaces/deployment-2472/deployments/test-recreate-deployment bb1d39ac-0bd5-4f9b-940b-25e40d9b3a1d 82687 2 2020-04-17 22:20:48 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc009858ad8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-04-17 22:20:50 +0000 UTC,LastTransitionTime:2020-04-17 22:20:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2020-04-17 22:20:50 +0000 UTC,LastTransitionTime:2020-04-17 22:20:48 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Apr 17 22:20:50.837: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-2472 /apis/apps/v1/namespaces/deployment-2472/replicasets/test-recreate-deployment-5f94c574ff cfaf1d21-f283-4124-a011-ba740e5acbd1 82684 1 2020-04-17 22:20:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment bb1d39ac-0bd5-4f9b-940b-25e40d9b3a1d 0xc009858ea7 0xc009858ea8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc009858f08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 17 22:20:50.837: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr 17 22:20:50.838: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-2472 /apis/apps/v1/namespaces/deployment-2472/replicasets/test-recreate-deployment-68fc85c7bb e6f0006d-245d-44c5-966d-d9c81c35351e 82675 2 2020-04-17 22:20:48 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment bb1d39ac-0bd5-4f9b-940b-25e40d9b3a1d 0xc009858f77 0xc009858f78}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc009858fd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 17 22:20:50.850: INFO: Pod "test-recreate-deployment-5f94c574ff-dp5zr" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-dp5zr test-recreate-deployment-5f94c574ff- deployment-2472 /api/v1/namespaces/deployment-2472/pods/test-recreate-deployment-5f94c574ff-dp5zr efeeb83a-c34d-46ec-b494-931c915787c5 82686 0 2020-04-17 22:20:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff cfaf1d21-f283-4124-a011-ba740e5acbd1 0xc009859477 0xc009859478}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-75zn8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-75zn8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-75zn8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-987c8,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:20:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:20:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:20:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:20:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.74,PodIP:,StartTime:2020-04-17 22:20:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:20:50.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2472" for this suite.
Apr 17 22:20:58.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:21:01.240: INFO: namespace deployment-2472 deletion completed in 10.333655962s

â€¢ [SLOW TEST:12.992 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:21:01.240: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Apr 17 22:21:01.419: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:21:12.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-153" for this suite.
Apr 17 22:21:20.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:21:22.883: INFO: namespace pods-153 deletion completed in 10.444093091s

â€¢ [SLOW TEST:21.644 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:21:22.884: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 22:21:23.518: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 17 22:21:25.560: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758883, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758883, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758883, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722758883, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 22:21:28.610: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:21:28.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9481" for this suite.
Apr 17 22:21:36.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:21:39.088: INFO: namespace webhook-9481 deletion completed in 10.186793304s
STEP: Destroying namespace "webhook-9481-markers" for this suite.
Apr 17 22:21:47.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:21:49.290: INFO: namespace webhook-9481-markers deletion completed in 10.201767255s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:26.492 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:21:49.377: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 22:21:49.603: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ceffc214-a607-4b0d-87a9-b01fa46976dc" in namespace "downward-api-750" to be "success or failure"
Apr 17 22:21:49.620: INFO: Pod "downwardapi-volume-ceffc214-a607-4b0d-87a9-b01fa46976dc": Phase="Pending", Reason="", readiness=false. Elapsed: 17.057516ms
Apr 17 22:21:51.635: INFO: Pod "downwardapi-volume-ceffc214-a607-4b0d-87a9-b01fa46976dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032339437s
STEP: Saw pod success
Apr 17 22:21:51.635: INFO: Pod "downwardapi-volume-ceffc214-a607-4b0d-87a9-b01fa46976dc" satisfied condition "success or failure"
Apr 17 22:21:51.649: INFO: Trying to get logs from node 10.72.119.74 pod downwardapi-volume-ceffc214-a607-4b0d-87a9-b01fa46976dc container client-container: <nil>
STEP: delete the pod
Apr 17 22:21:51.769: INFO: Waiting for pod downwardapi-volume-ceffc214-a607-4b0d-87a9-b01fa46976dc to disappear
Apr 17 22:21:51.782: INFO: Pod downwardapi-volume-ceffc214-a607-4b0d-87a9-b01fa46976dc no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:21:51.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-750" for this suite.
Apr 17 22:21:59.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:22:02.185: INFO: namespace downward-api-750 deletion completed in 10.369307182s

â€¢ [SLOW TEST:12.808 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:22:02.187: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:22:02.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1964" for this suite.
Apr 17 22:22:10.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:22:12.671: INFO: namespace services-1964 deletion completed in 10.216758764s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:10.484 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:22:12.672: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-5943
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5943
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5943
Apr 17 22:22:12.887: INFO: Found 0 stateful pods, waiting for 1
Apr 17 22:22:22.899: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr 17 22:22:22.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-5943 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 22:22:23.273: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 22:22:23.273: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 22:22:23.273: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 22:22:23.288: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 17 22:22:33.315: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 17 22:22:33.315: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 22:22:33.604: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999997717s
Apr 17 22:22:34.617: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.988199334s
Apr 17 22:22:35.629: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.975730834s
Apr 17 22:22:36.641: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.963205303s
Apr 17 22:22:37.653: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.951226148s
Apr 17 22:22:38.670: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.939065759s
Apr 17 22:22:39.685: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.921943755s
Apr 17 22:22:40.698: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.90770491s
Apr 17 22:22:41.710: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.894497122s
Apr 17 22:22:42.724: INFO: Verifying statefulset ss doesn't scale past 1 for another 882.843722ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5943
Apr 17 22:22:43.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-5943 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:22:44.071: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 17 22:22:44.071: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 22:22:44.071: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 17 22:22:44.083: INFO: Found 1 stateful pods, waiting for 3
Apr 17 22:22:54.104: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 22:22:54.104: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 22:22:54.104: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr 17 22:22:54.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-5943 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 22:22:54.484: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 22:22:54.484: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 22:22:54.484: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 22:22:54.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-5943 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 22:22:54.867: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 22:22:54.867: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 22:22:54.867: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 22:22:54.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-5943 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 22:22:55.247: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 22:22:55.247: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 22:22:55.247: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 22:22:55.247: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 22:22:55.263: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr 17 22:23:05.296: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 17 22:23:05.296: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 17 22:23:05.296: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 17 22:23:05.352: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999997903s
Apr 17 22:23:06.367: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.980786995s
Apr 17 22:23:07.385: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.965279778s
Apr 17 22:23:08.402: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.947816791s
Apr 17 22:23:09.419: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.930873388s
Apr 17 22:23:10.436: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.913217544s
Apr 17 22:23:11.455: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.896568383s
Apr 17 22:23:12.471: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.877876185s
Apr 17 22:23:13.490: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.861905319s
Apr 17 22:23:14.506: INFO: Verifying statefulset ss doesn't scale past 3 for another 842.412527ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5943
Apr 17 22:23:15.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-5943 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:23:15.902: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 17 22:23:15.902: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 22:23:15.902: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 17 22:23:15.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-5943 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:23:16.287: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 17 22:23:16.287: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 22:23:16.287: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 17 22:23:16.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 exec --namespace=statefulset-5943 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:23:16.619: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 17 22:23:16.619: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 22:23:16.619: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 17 22:23:16.619: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Apr 17 22:23:36.692: INFO: Deleting all statefulset in ns statefulset-5943
Apr 17 22:23:36.708: INFO: Scaling statefulset ss to 0
Apr 17 22:23:36.753: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 22:23:36.768: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:23:36.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5943" for this suite.
Apr 17 22:23:44.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:23:47.093: INFO: namespace statefulset-5943 deletion completed in 10.22940032s

â€¢ [SLOW TEST:94.422 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:23:47.094: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr 17 22:23:47.359: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-342 /api/v1/namespaces/watch-342/configmaps/e2e-watch-test-label-changed 4207a942-d04c-41fa-8c1e-3be8781f4e64 84076 0 2020-04-17 22:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 17 22:23:47.359: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-342 /api/v1/namespaces/watch-342/configmaps/e2e-watch-test-label-changed 4207a942-d04c-41fa-8c1e-3be8781f4e64 84082 0 2020-04-17 22:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr 17 22:23:47.359: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-342 /api/v1/namespaces/watch-342/configmaps/e2e-watch-test-label-changed 4207a942-d04c-41fa-8c1e-3be8781f4e64 84084 0 2020-04-17 22:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr 17 22:23:57.489: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-342 /api/v1/namespaces/watch-342/configmaps/e2e-watch-test-label-changed 4207a942-d04c-41fa-8c1e-3be8781f4e64 84124 0 2020-04-17 22:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 17 22:23:57.489: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-342 /api/v1/namespaces/watch-342/configmaps/e2e-watch-test-label-changed 4207a942-d04c-41fa-8c1e-3be8781f4e64 84125 0 2020-04-17 22:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Apr 17 22:23:57.489: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-342 /api/v1/namespaces/watch-342/configmaps/e2e-watch-test-label-changed 4207a942-d04c-41fa-8c1e-3be8781f4e64 84126 0 2020-04-17 22:23:47 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:23:57.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-342" for this suite.
Apr 17 22:24:05.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:24:07.738: INFO: namespace watch-342 deletion completed in 10.213049323s

â€¢ [SLOW TEST:20.644 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:24:07.739: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:24:07.975: INFO: (0) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 57.685894ms)
Apr 17 22:24:08.000: INFO: (1) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 25.287761ms)
Apr 17 22:24:08.025: INFO: (2) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 24.278878ms)
Apr 17 22:24:08.050: INFO: (3) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 24.820178ms)
Apr 17 22:24:08.073: INFO: (4) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.934593ms)
Apr 17 22:24:08.094: INFO: (5) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.336036ms)
Apr 17 22:24:08.118: INFO: (6) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 24.159953ms)
Apr 17 22:24:08.143: INFO: (7) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 24.759663ms)
Apr 17 22:24:08.167: INFO: (8) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 24.277948ms)
Apr 17 22:24:08.197: INFO: (9) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 29.242366ms)
Apr 17 22:24:08.218: INFO: (10) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.328128ms)
Apr 17 22:24:08.240: INFO: (11) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.169267ms)
Apr 17 22:24:08.281: INFO: (12) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 40.819257ms)
Apr 17 22:24:08.309: INFO: (13) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 28.506895ms)
Apr 17 22:24:08.336: INFO: (14) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 26.576944ms)
Apr 17 22:24:08.361: INFO: (15) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 25.078183ms)
Apr 17 22:24:08.386: INFO: (16) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 25.06874ms)
Apr 17 22:24:08.410: INFO: (17) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 23.389002ms)
Apr 17 22:24:08.446: INFO: (18) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 36.278614ms)
Apr 17 22:24:08.471: INFO: (19) /api/v1/nodes/10.72.119.72/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 25.316811ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:24:08.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7843" for this suite.
Apr 17 22:24:16.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:24:17.828: INFO: namespace proxy-7843 deletion completed in 9.327157032s

â€¢ [SLOW TEST:10.088 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:24:17.829: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:24:35.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3931" for this suite.
Apr 17 22:24:43.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:24:45.435: INFO: namespace resourcequota-3931 deletion completed in 10.255038503s

â€¢ [SLOW TEST:27.606 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:24:45.435: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-845a2bed-3e31-43bf-83de-9ef6efa6116b
STEP: Creating a pod to test consume secrets
Apr 17 22:24:45.641: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0ad023f7-5ff9-4bc8-964b-de3e9bdfbd79" in namespace "projected-8421" to be "success or failure"
Apr 17 22:24:45.653: INFO: Pod "pod-projected-secrets-0ad023f7-5ff9-4bc8-964b-de3e9bdfbd79": Phase="Pending", Reason="", readiness=false. Elapsed: 12.179503ms
Apr 17 22:24:47.671: INFO: Pod "pod-projected-secrets-0ad023f7-5ff9-4bc8-964b-de3e9bdfbd79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029674502s
STEP: Saw pod success
Apr 17 22:24:47.671: INFO: Pod "pod-projected-secrets-0ad023f7-5ff9-4bc8-964b-de3e9bdfbd79" satisfied condition "success or failure"
Apr 17 22:24:47.686: INFO: Trying to get logs from node 10.72.119.74 pod pod-projected-secrets-0ad023f7-5ff9-4bc8-964b-de3e9bdfbd79 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 17 22:24:47.775: INFO: Waiting for pod pod-projected-secrets-0ad023f7-5ff9-4bc8-964b-de3e9bdfbd79 to disappear
Apr 17 22:24:47.786: INFO: Pod pod-projected-secrets-0ad023f7-5ff9-4bc8-964b-de3e9bdfbd79 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:24:47.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8421" for this suite.
Apr 17 22:24:55.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:24:58.131: INFO: namespace projected-8421 deletion completed in 10.310155102s

â€¢ [SLOW TEST:12.696 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:24:58.136: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:25:14.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5509" for this suite.
Apr 17 22:25:22.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:25:24.730: INFO: namespace resourcequota-5509 deletion completed in 10.261915346s

â€¢ [SLOW TEST:26.594 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:25:24.730: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Apr 17 22:25:31.082: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:25:31.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0417 22:25:31.081990      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3044" for this suite.
Apr 17 22:25:41.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:25:43.301: INFO: namespace gc-3044 deletion completed in 12.192690034s

â€¢ [SLOW TEST:18.571 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:25:43.303: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Apr 17 22:25:43.491: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-531062789 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:25:43.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7822" for this suite.
Apr 17 22:25:51.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:25:53.971: INFO: namespace kubectl-7822 deletion completed in 10.343050649s

â€¢ [SLOW TEST:10.668 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:25:53.971: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-5f282a7c-3f00-4d91-b725-792800e3c221
STEP: Creating a pod to test consume configMaps
Apr 17 22:25:54.213: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5ae558a4-ffa6-4e0e-8f2e-d5660dd83ae6" in namespace "projected-2259" to be "success or failure"
Apr 17 22:25:54.225: INFO: Pod "pod-projected-configmaps-5ae558a4-ffa6-4e0e-8f2e-d5660dd83ae6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.734471ms
Apr 17 22:25:56.237: INFO: Pod "pod-projected-configmaps-5ae558a4-ffa6-4e0e-8f2e-d5660dd83ae6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023161763s
STEP: Saw pod success
Apr 17 22:25:56.237: INFO: Pod "pod-projected-configmaps-5ae558a4-ffa6-4e0e-8f2e-d5660dd83ae6" satisfied condition "success or failure"
Apr 17 22:25:56.247: INFO: Trying to get logs from node 10.72.119.74 pod pod-projected-configmaps-5ae558a4-ffa6-4e0e-8f2e-d5660dd83ae6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 17 22:25:56.304: INFO: Waiting for pod pod-projected-configmaps-5ae558a4-ffa6-4e0e-8f2e-d5660dd83ae6 to disappear
Apr 17 22:25:56.320: INFO: Pod pod-projected-configmaps-5ae558a4-ffa6-4e0e-8f2e-d5660dd83ae6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:25:56.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2259" for this suite.
Apr 17 22:26:04.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:26:06.500: INFO: namespace projected-2259 deletion completed in 10.154489235s

â€¢ [SLOW TEST:12.529 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:26:06.500: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Apr 17 22:26:07.293: INFO: created pod pod-service-account-defaultsa
Apr 17 22:26:07.293: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr 17 22:26:07.329: INFO: created pod pod-service-account-mountsa
Apr 17 22:26:07.329: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr 17 22:26:07.369: INFO: created pod pod-service-account-nomountsa
Apr 17 22:26:07.369: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr 17 22:26:07.405: INFO: created pod pod-service-account-defaultsa-mountspec
Apr 17 22:26:07.405: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr 17 22:26:07.445: INFO: created pod pod-service-account-mountsa-mountspec
Apr 17 22:26:07.445: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr 17 22:26:07.489: INFO: created pod pod-service-account-nomountsa-mountspec
Apr 17 22:26:07.489: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr 17 22:26:07.532: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr 17 22:26:07.532: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr 17 22:26:07.571: INFO: created pod pod-service-account-mountsa-nomountspec
Apr 17 22:26:07.571: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr 17 22:26:07.605: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr 17 22:26:07.605: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:26:07.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8084" for this suite.
Apr 17 22:26:15.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:26:17.897: INFO: namespace svcaccounts-8084 deletion completed in 10.263294177s

â€¢ [SLOW TEST:11.397 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:26:17.897: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:26:18.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4645" for this suite.
Apr 17 22:26:50.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:26:52.443: INFO: namespace pods-4645 deletion completed in 34.312552097s

â€¢ [SLOW TEST:34.546 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:26:52.448: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Apr 17 22:26:52.595: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 17 22:26:52.686: INFO: Waiting for terminating namespaces to be deleted...
Apr 17 22:26:52.713: INFO: 
Logging pods the kubelet thinks is on node 10.72.119.72 before test
Apr 17 22:26:52.876: INFO: sonobuoy from sonobuoy started at 2020-04-17 20:47:07 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 17 22:26:52.876: INFO: configmap-cabundle-injector-5cf6d9695-kfct9 from openshift-service-ca started at 2020-04-17 19:11:11 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Apr 17 22:26:52.876: INFO: router-default-8779c94d4-nbbtf from openshift-ingress started at 2020-04-17 21:54:15 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container router ready: true, restart count 0
Apr 17 22:26:52.876: INFO: console-operator-db5d785db-r4qh6 from openshift-console-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container console-operator ready: true, restart count 1
Apr 17 22:26:52.876: INFO: multus-wlttp from openshift-multus started at 2020-04-17 19:09:55 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container kube-multus ready: true, restart count 0
Apr 17 22:26:52.876: INFO: openshift-kube-proxy-bkf2z from openshift-kube-proxy started at 2020-04-17 19:10:02 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 22:26:52.876: INFO: cluster-image-registry-operator-7857d56744-c2pch from openshift-image-registry started at 2020-04-17 19:10:34 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Apr 17 22:26:52.876: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Apr 17 22:26:52.876: INFO: thanos-querier-5c84fd9f68-4k28v from openshift-monitoring started at 2020-04-17 21:54:16 +0000 UTC (4 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:26:52.876: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr 17 22:26:52.876: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 22:26:52.876: INFO: 	Container thanos-querier ready: true, restart count 0
Apr 17 22:26:52.876: INFO: ibm-cloud-provider-ip-158-176-122-172-6f57dbd74b-cdqd6 from ibm-system started at 2020-04-17 19:13:49 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container ibm-cloud-provider-ip-158-176-122-172 ready: true, restart count 0
Apr 17 22:26:52.876: INFO: network-operator-64f597f5d-8tg4w from openshift-network-operator started at 2020-04-17 19:09:28 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container network-operator ready: true, restart count 0
Apr 17 22:26:52.876: INFO: catalog-operator-5665d988d5-kvkfk from openshift-operator-lifecycle-manager started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container catalog-operator ready: true, restart count 0
Apr 17 22:26:52.876: INFO: dns-operator-74d97d58d-56295 from openshift-dns-operator started at 2020-04-17 19:10:34 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container dns-operator ready: true, restart count 0
Apr 17 22:26:52.876: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:26:52.876: INFO: openshift-service-catalog-controller-manager-operator-f954mtxlp from openshift-service-catalog-controller-manager-operator started at 2020-04-17 19:10:35 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container operator ready: true, restart count 1
Apr 17 22:26:52.876: INFO: kube-state-metrics-5bc9b987bc-hvx9k from openshift-monitoring started at 2020-04-17 19:11:29 +0000 UTC (3 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr 17 22:26:52.876: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr 17 22:26:52.876: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 17 22:26:52.876: INFO: prometheus-adapter-5cd8cd848d-j9qgt from openshift-monitoring started at 2020-04-17 21:54:15 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr 17 22:26:52.876: INFO: calico-typha-76b588567c-d6d9k from calico-system started at 2020-04-17 19:09:39 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container calico-typha ready: true, restart count 1
Apr 17 22:26:52.876: INFO: cluster-monitoring-operator-668c7b8d6d-khmrg from openshift-monitoring started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Apr 17 22:26:52.876: INFO: ibmcloud-block-storage-plugin-75f7cd767-lcw2w from kube-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Apr 17 22:26:52.876: INFO: downloads-64bb8b89c9-jxkw4 from openshift-console started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container download-server ready: true, restart count 0
Apr 17 22:26:52.876: INFO: service-serving-cert-signer-5968cc5d5c-m6hb2 from openshift-service-ca started at 2020-04-17 19:11:10 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Apr 17 22:26:52.876: INFO: apiservice-cabundle-injector-7bf8cddb9-kxhvx from openshift-service-ca started at 2020-04-17 19:11:11 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Apr 17 22:26:52.876: INFO: ibmcloud-block-storage-driver-g7lth from kube-system started at 2020-04-17 19:09:28 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Apr 17 22:26:52.876: INFO: service-ca-operator-5d59f48888-hk9zs from openshift-service-ca-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container operator ready: true, restart count 0
Apr 17 22:26:52.876: INFO: olm-operator-7bd9dc9457-2wn6n from openshift-operator-lifecycle-manager started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container olm-operator ready: true, restart count 0
Apr 17 22:26:52.876: INFO: calico-kube-controllers-84d976f9ff-g9864 from calico-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr 17 22:26:52.876: INFO: downloads-64bb8b89c9-sccw2 from openshift-console started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container download-server ready: true, restart count 0
Apr 17 22:26:52.876: INFO: tuned-xrlm4 from openshift-cluster-node-tuning-operator started at 2020-04-17 19:11:12 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container tuned ready: true, restart count 0
Apr 17 22:26:52.876: INFO: console-5b98d99db9-lb2kc from openshift-console started at 2020-04-17 21:54:15 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container console ready: true, restart count 0
Apr 17 22:26:52.876: INFO: ibm-keepalived-watcher-58jb7 from kube-system started at 2020-04-17 19:09:24 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr 17 22:26:52.876: INFO: cluster-node-tuning-operator-77bdbd4f-79qvb from openshift-cluster-node-tuning-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.876: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Apr 17 22:26:52.877: INFO: openshift-service-catalog-apiserver-operator-76969db7f5-wpk2m from openshift-service-catalog-apiserver-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container operator ready: true, restart count 1
Apr 17 22:26:52.877: INFO: node-exporter-5qztm from openshift-monitoring started at 2020-04-17 19:11:29 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:26:52.877: INFO: 	Container node-exporter ready: true, restart count 0
Apr 17 22:26:52.877: INFO: dns-default-ffsfn from openshift-dns started at 2020-04-17 19:12:00 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container dns ready: true, restart count 0
Apr 17 22:26:52.877: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr 17 22:26:52.877: INFO: packageserver-6cfdb6b4bd-8kbgz from openshift-operator-lifecycle-manager started at 2020-04-17 21:54:17 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container packageserver ready: true, restart count 0
Apr 17 22:26:52.877: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-04-17 21:54:32 +0000 UTC (7 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:26:52.877: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 22:26:52.877: INFO: 	Container prometheus ready: true, restart count 1
Apr 17 22:26:52.877: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Apr 17 22:26:52.877: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr 17 22:26:52.877: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Apr 17 22:26:52.877: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr 17 22:26:52.877: INFO: ibm-master-proxy-static-10.72.119.72 from kube-system started at 2020-04-17 19:09:18 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr 17 22:26:52.877: INFO: 	Container pause ready: true, restart count 0
Apr 17 22:26:52.877: INFO: calico-node-tcc87 from calico-system started at 2020-04-17 19:09:40 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 22:26:52.877: INFO: cluster-storage-operator-56475d49d7-fsvdb from openshift-cluster-storage-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Apr 17 22:26:52.877: INFO: marketplace-operator-699fb8f5d-5nrcq from openshift-marketplace started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container marketplace-operator ready: true, restart count 0
Apr 17 22:26:52.877: INFO: ibm-file-plugin-7cbd86d68f-9tm42 from kube-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Apr 17 22:26:52.877: INFO: ibm-storage-watcher-d9c7cf586-f72vk from kube-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Apr 17 22:26:52.877: INFO: openshift-state-metrics-6c465bc47f-p9qr6 from openshift-monitoring started at 2020-04-17 19:11:29 +0000 UTC (3 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr 17 22:26:52.877: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr 17 22:26:52.877: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Apr 17 22:26:52.877: INFO: node-ca-x5gt7 from openshift-image-registry started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container node-ca ready: true, restart count 0
Apr 17 22:26:52.877: INFO: tigera-operator-df8f4c87c-c7sjz from tigera-operator started at 2020-04-17 19:09:24 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container tigera-operator ready: true, restart count 0
Apr 17 22:26:52.877: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-04-17 21:54:22 +0000 UTC (3 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container alertmanager ready: true, restart count 0
Apr 17 22:26:52.877: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr 17 22:26:52.877: INFO: 	Container config-reloader ready: true, restart count 0
Apr 17 22:26:52.877: INFO: sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-l8m9b from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 17 22:26:52.877: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 22:26:52.877: INFO: multus-admission-controller-2jxfm from openshift-multus started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr 17 22:26:52.877: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-04-17 19:16:48 +0000 UTC (3 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container alertmanager ready: true, restart count 0
Apr 17 22:26:52.877: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr 17 22:26:52.877: INFO: 	Container config-reloader ready: true, restart count 0
Apr 17 22:26:52.877: INFO: ingress-operator-66cf4674d8-467pw from openshift-ingress-operator started at 2020-04-17 19:10:34 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:52.877: INFO: 	Container ingress-operator ready: true, restart count 0
Apr 17 22:26:52.877: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:26:52.877: INFO: 
Logging pods the kubelet thinks is on node 10.72.119.74 before test
Apr 17 22:26:52.942: INFO: tuned-tsvx9 from openshift-cluster-node-tuning-operator started at 2020-04-17 19:11:12 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.942: INFO: 	Container tuned ready: true, restart count 0
Apr 17 22:26:52.942: INFO: multus-s62pc from openshift-multus started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.942: INFO: 	Container kube-multus ready: true, restart count 0
Apr 17 22:26:52.942: INFO: certified-operators-78496d489f-xfk5t from openshift-marketplace started at 2020-04-17 22:12:09 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.942: INFO: 	Container certified-operators ready: true, restart count 0
Apr 17 22:26:52.942: INFO: node-exporter-949xg from openshift-monitoring started at 2020-04-17 19:11:30 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:52.942: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:26:52.942: INFO: 	Container node-exporter ready: true, restart count 0
Apr 17 22:26:52.942: INFO: dns-default-sdtp9 from openshift-dns started at 2020-04-17 21:54:52 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:52.942: INFO: 	Container dns ready: true, restart count 0
Apr 17 22:26:52.942: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr 17 22:26:52.942: INFO: ibm-master-proxy-static-10.72.119.74 from kube-system started at 2020-04-17 19:10:42 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:52.942: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr 17 22:26:52.942: INFO: 	Container pause ready: true, restart count 0
Apr 17 22:26:52.942: INFO: sonobuoy-e2e-job-793c74f2197d4b50 from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:52.942: INFO: 	Container e2e ready: true, restart count 0
Apr 17 22:26:52.942: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 22:26:52.942: INFO: ibmcloud-block-storage-driver-6j6qk from kube-system started at 2020-04-17 19:10:53 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.942: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Apr 17 22:26:52.942: INFO: calico-typha-76b588567c-khgqc from calico-system started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.942: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 22:26:52.942: INFO: sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-tg59r from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:52.942: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 17 22:26:52.942: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 22:26:52.942: INFO: ibm-keepalived-watcher-hm7n9 from kube-system started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.942: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr 17 22:26:52.942: INFO: node-ca-8cpqb from openshift-image-registry started at 2020-04-17 21:54:52 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.942: INFO: 	Container node-ca ready: true, restart count 0
Apr 17 22:26:52.942: INFO: openshift-kube-proxy-gsgx2 from openshift-kube-proxy started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.942: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 22:26:52.942: INFO: multus-admission-controller-dj76f from openshift-multus started at 2020-04-17 21:54:52 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.942: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr 17 22:26:52.942: INFO: calico-node-xm4xf from calico-system started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:52.942: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 22:26:52.942: INFO: 
Logging pods the kubelet thinks is on node 10.72.119.98 before test
Apr 17 22:26:53.064: INFO: ibm-master-proxy-static-10.72.119.98 from kube-system started at 2020-04-17 19:10:37 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container pause ready: true, restart count 0
Apr 17 22:26:53.064: INFO: node-exporter-lh6vl from openshift-monitoring started at 2020-04-17 19:11:30 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container node-exporter ready: true, restart count 0
Apr 17 22:26:53.064: INFO: prometheus-operator-75c7889c9b-47f2p from openshift-monitoring started at 2020-04-17 19:16:29 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container prometheus-operator ready: true, restart count 0
Apr 17 22:26:53.064: INFO: telemeter-client-5f4f6fb5fc-rwg74 from openshift-monitoring started at 2020-04-17 19:16:40 +0000 UTC (3 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container reload ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container telemeter-client ready: true, restart count 0
Apr 17 22:26:53.064: INFO: sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-kbntn from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 17 22:26:53.064: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 22:26:53.064: INFO: ibm-keepalived-watcher-h2bpf from kube-system started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr 17 22:26:53.064: INFO: multus-lddbc from openshift-multus started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container kube-multus ready: true, restart count 0
Apr 17 22:26:53.064: INFO: calico-typha-76b588567c-kfx7z from calico-system started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 22:26:53.064: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-04-17 19:17:09 +0000 UTC (3 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container alertmanager ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container config-reloader ready: true, restart count 0
Apr 17 22:26:53.064: INFO: thanos-querier-5c84fd9f68-vbdc4 from openshift-monitoring started at 2020-04-17 19:17:49 +0000 UTC (4 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container thanos-querier ready: true, restart count 0
Apr 17 22:26:53.064: INFO: router-default-8779c94d4-mdtbq from openshift-ingress started at 2020-04-17 19:12:06 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container router ready: true, restart count 0
Apr 17 22:26:53.064: INFO: image-registry-5655cc46bf-gzrxj from openshift-image-registry started at 2020-04-17 19:14:35 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container registry ready: true, restart count 0
Apr 17 22:26:53.064: INFO: grafana-6b4f8c85c5-5n72k from openshift-monitoring started at 2020-04-17 21:54:16 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container grafana ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container grafana-proxy ready: true, restart count 0
Apr 17 22:26:53.064: INFO: calico-node-6sr5x from calico-system started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 22:26:53.064: INFO: openshift-kube-proxy-7dshq from openshift-kube-proxy started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 22:26:53.064: INFO: prometheus-adapter-5cd8cd848d-xspwf from openshift-monitoring started at 2020-04-17 19:16:43 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr 17 22:26:53.064: INFO: packageserver-6cfdb6b4bd-lnzh2 from openshift-operator-lifecycle-manager started at 2020-04-17 21:54:28 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container packageserver ready: true, restart count 0
Apr 17 22:26:53.064: INFO: tuned-sz2br from openshift-cluster-node-tuning-operator started at 2020-04-17 19:11:12 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container tuned ready: true, restart count 0
Apr 17 22:26:53.064: INFO: dns-default-z6b7d from openshift-dns started at 2020-04-17 19:12:00 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container dns ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr 17 22:26:53.064: INFO: vpn-5d7df69b48-5mp4n from kube-system started at 2020-04-17 21:54:15 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container vpn ready: true, restart count 0
Apr 17 22:26:53.064: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-04-17 19:14:09 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Apr 17 22:26:53.064: INFO: registry-pvc-permissions-d9dpb from openshift-image-registry started at 2020-04-17 19:14:35 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container pvc-permissions ready: false, restart count 0
Apr 17 22:26:53.064: INFO: ibmcloud-block-storage-driver-6jgj5 from kube-system started at 2020-04-17 19:10:47 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Apr 17 22:26:53.064: INFO: node-ca-wr66h from openshift-image-registry started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container node-ca ready: true, restart count 0
Apr 17 22:26:53.064: INFO: ibm-cloud-provider-ip-158-176-122-172-6f57dbd74b-bpkpw from ibm-system started at 2020-04-17 19:13:51 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container ibm-cloud-provider-ip-158-176-122-172 ready: true, restart count 0
Apr 17 22:26:53.064: INFO: console-5b98d99db9-2z7vw from openshift-console started at 2020-04-17 19:12:51 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container console ready: true, restart count 0
Apr 17 22:26:53.064: INFO: cluster-samples-operator-6d4b68977f-5q62h from openshift-cluster-samples-operator started at 2020-04-17 19:11:57 +0000 UTC (2 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Apr 17 22:26:53.064: INFO: multus-admission-controller-72gwb from openshift-multus started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr 17 22:26:53.064: INFO: community-operators-5574589d55-swkx4 from openshift-marketplace started at 2020-04-17 19:12:21 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container community-operators ready: true, restart count 0
Apr 17 22:26:53.064: INFO: redhat-operators-7b557fbbc4-pq5mj from openshift-marketplace started at 2020-04-17 19:12:24 +0000 UTC (1 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container redhat-operators ready: true, restart count 0
Apr 17 22:26:53.064: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-04-17 19:17:47 +0000 UTC (7 container statuses recorded)
Apr 17 22:26:53.064: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container prometheus ready: true, restart count 1
Apr 17 22:26:53.064: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Apr 17 22:26:53.064: INFO: 	Container thanos-sidecar ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8c626918-d8d5-4c87-b10e-d72573c260ff 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-8c626918-d8d5-4c87-b10e-d72573c260ff off the node 10.72.119.74
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8c626918-d8d5-4c87-b10e-d72573c260ff
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:26:59.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5809" for this suite.
Apr 17 22:27:15.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:27:17.552: INFO: namespace sched-pred-5809 deletion completed in 18.178090698s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:25.104 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:27:17.553: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 17 22:27:20.416: INFO: Successfully updated pod "pod-update-e9e10d55-b63a-415b-869f-0cf296b3cedd"
STEP: verifying the updated pod is in kubernetes
Apr 17 22:27:20.441: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:27:20.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2089" for this suite.
Apr 17 22:27:34.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:27:36.759: INFO: namespace pods-2089 deletion completed in 16.297582197s

â€¢ [SLOW TEST:19.206 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:27:36.760: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr 17 22:27:41.075: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 17 22:27:41.087: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 17 22:27:43.087: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 17 22:27:43.108: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 17 22:27:45.087: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 17 22:27:45.100: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:27:45.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5689" for this suite.
Apr 17 22:27:59.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:28:01.351: INFO: namespace container-lifecycle-hook-5689 deletion completed in 16.198504235s

â€¢ [SLOW TEST:24.592 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:28:01.352: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr 17 22:28:01.675: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4593 /api/v1/namespaces/watch-4593/configmaps/e2e-watch-test-resource-version d7bbe798-a654-4ae2-b5c9-7eb13f22ae5d 86463 0 2020-04-17 22:28:01 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 17 22:28:01.675: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4593 /api/v1/namespaces/watch-4593/configmaps/e2e-watch-test-resource-version d7bbe798-a654-4ae2-b5c9-7eb13f22ae5d 86464 0 2020-04-17 22:28:01 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:28:01.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4593" for this suite.
Apr 17 22:28:09.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:28:11.916: INFO: namespace watch-4593 deletion completed in 10.221372981s

â€¢ [SLOW TEST:10.564 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:28:11.917: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Apr 17 22:28:12.108: INFO: Waiting up to 5m0s for pod "client-containers-d093e967-9867-466b-9953-b5306031dd67" in namespace "containers-9411" to be "success or failure"
Apr 17 22:28:12.119: INFO: Pod "client-containers-d093e967-9867-466b-9953-b5306031dd67": Phase="Pending", Reason="", readiness=false. Elapsed: 11.327769ms
Apr 17 22:28:14.135: INFO: Pod "client-containers-d093e967-9867-466b-9953-b5306031dd67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027089448s
STEP: Saw pod success
Apr 17 22:28:14.135: INFO: Pod "client-containers-d093e967-9867-466b-9953-b5306031dd67" satisfied condition "success or failure"
Apr 17 22:28:14.146: INFO: Trying to get logs from node 10.72.119.74 pod client-containers-d093e967-9867-466b-9953-b5306031dd67 container test-container: <nil>
STEP: delete the pod
Apr 17 22:28:14.214: INFO: Waiting for pod client-containers-d093e967-9867-466b-9953-b5306031dd67 to disappear
Apr 17 22:28:14.225: INFO: Pod client-containers-d093e967-9867-466b-9953-b5306031dd67 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:28:14.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9411" for this suite.
Apr 17 22:28:22.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:28:24.505: INFO: namespace containers-9411 deletion completed in 10.261491728s

â€¢ [SLOW TEST:12.588 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:28:24.505: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Apr 17 22:28:55.380: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0417 22:28:55.380679      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:28:55.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7436" for this suite.
Apr 17 22:29:03.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:29:05.839: INFO: namespace gc-7436 deletion completed in 10.438140313s

â€¢ [SLOW TEST:41.334 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:29:05.839: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Apr 17 22:29:06.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 api-versions'
Apr 17 22:29:06.137: INFO: stderr: ""
Apr 17 22:29:06.137: INFO: stdout: "admissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps.openshift.io/v1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1beta1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\npolicy/v1beta1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsecurity.openshift.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:29:06.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5237" for this suite.
Apr 17 22:29:14.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:29:16.360: INFO: namespace kubectl-5237 deletion completed in 10.193786391s

â€¢ [SLOW TEST:10.521 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:29:16.366: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Apr 17 22:29:16.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-8696 -- logs-generator --log-lines-total 100 --run-duration 20s'
Apr 17 22:29:16.714: INFO: stderr: ""
Apr 17 22:29:16.714: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Apr 17 22:29:16.714: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Apr 17 22:29:16.714: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8696" to be "running and ready, or succeeded"
Apr 17 22:29:16.727: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.401899ms
Apr 17 22:29:18.738: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024183188s
Apr 17 22:29:20.756: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.042325135s
Apr 17 22:29:20.757: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Apr 17 22:29:20.757: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Apr 17 22:29:20.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 logs logs-generator logs-generator --namespace=kubectl-8696'
Apr 17 22:29:20.954: INFO: stderr: ""
Apr 17 22:29:20.954: INFO: stdout: "I0417 22:29:18.021025       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/kt5q 264\nI0417 22:29:18.221213       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/tth 215\nI0417 22:29:18.421242       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/57vh 526\nI0417 22:29:18.621258       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/kcj 365\nI0417 22:29:18.821249       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/cnd 227\nI0417 22:29:19.021273       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/7c69 351\nI0417 22:29:19.221262       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/tslt 423\nI0417 22:29:19.421290       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/zb4d 321\nI0417 22:29:19.621223       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/k2v 284\nI0417 22:29:19.821262       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/vwh 359\nI0417 22:29:20.021288       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/s7n 235\nI0417 22:29:20.221278       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/m2d 466\nI0417 22:29:20.421267       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/v8c 562\nI0417 22:29:20.621272       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/rqx 314\nI0417 22:29:20.821257       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/9t9 428\n"
STEP: limiting log lines
Apr 17 22:29:20.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 logs logs-generator logs-generator --namespace=kubectl-8696 --tail=1'
Apr 17 22:29:21.136: INFO: stderr: ""
Apr 17 22:29:21.136: INFO: stdout: "I0417 22:29:21.021270       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/7z74 416\n"
STEP: limiting log bytes
Apr 17 22:29:21.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 logs logs-generator logs-generator --namespace=kubectl-8696 --limit-bytes=1'
Apr 17 22:29:21.321: INFO: stderr: ""
Apr 17 22:29:21.321: INFO: stdout: "I"
STEP: exposing timestamps
Apr 17 22:29:21.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 logs logs-generator logs-generator --namespace=kubectl-8696 --tail=1 --timestamps'
Apr 17 22:29:21.469: INFO: stderr: ""
Apr 17 22:29:21.469: INFO: stdout: "2020-04-17T17:29:21.421366571-05:00 I0417 22:29:21.421264       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/vl9 424\n"
STEP: restricting to a time range
Apr 17 22:29:23.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 logs logs-generator logs-generator --namespace=kubectl-8696 --since=1s'
Apr 17 22:29:24.130: INFO: stderr: ""
Apr 17 22:29:24.130: INFO: stdout: "I0417 22:29:23.221301       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/5qm7 422\nI0417 22:29:23.421277       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/hgzl 510\nI0417 22:29:23.621280       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/t5dr 328\nI0417 22:29:23.821264       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/mnt 583\nI0417 22:29:24.021253       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/hbr 428\n"
Apr 17 22:29:24.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 logs logs-generator logs-generator --namespace=kubectl-8696 --since=24h'
Apr 17 22:29:24.308: INFO: stderr: ""
Apr 17 22:29:24.308: INFO: stdout: "I0417 22:29:18.021025       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/kt5q 264\nI0417 22:29:18.221213       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/tth 215\nI0417 22:29:18.421242       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/57vh 526\nI0417 22:29:18.621258       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/kcj 365\nI0417 22:29:18.821249       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/cnd 227\nI0417 22:29:19.021273       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/7c69 351\nI0417 22:29:19.221262       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/tslt 423\nI0417 22:29:19.421290       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/zb4d 321\nI0417 22:29:19.621223       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/k2v 284\nI0417 22:29:19.821262       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/vwh 359\nI0417 22:29:20.021288       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/s7n 235\nI0417 22:29:20.221278       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/m2d 466\nI0417 22:29:20.421267       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/v8c 562\nI0417 22:29:20.621272       1 logs_generator.go:76] 13 POST /api/v1/namespaces/ns/pods/rqx 314\nI0417 22:29:20.821257       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/9t9 428\nI0417 22:29:21.021270       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/7z74 416\nI0417 22:29:21.221281       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/qtw 387\nI0417 22:29:21.421264       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/vl9 424\nI0417 22:29:21.621276       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/mhd 486\nI0417 22:29:21.821290       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/l2c4 584\nI0417 22:29:22.021263       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/cgpt 311\nI0417 22:29:22.221250       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/bqt 522\nI0417 22:29:22.421274       1 logs_generator.go:76] 22 GET /api/v1/namespaces/default/pods/qm5 497\nI0417 22:29:22.621289       1 logs_generator.go:76] 23 GET /api/v1/namespaces/kube-system/pods/zqm 452\nI0417 22:29:22.821262       1 logs_generator.go:76] 24 GET /api/v1/namespaces/ns/pods/dxtf 476\nI0417 22:29:23.021252       1 logs_generator.go:76] 25 POST /api/v1/namespaces/default/pods/wmq 395\nI0417 22:29:23.221301       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/ns/pods/5qm7 422\nI0417 22:29:23.421277       1 logs_generator.go:76] 27 PUT /api/v1/namespaces/default/pods/hgzl 510\nI0417 22:29:23.621280       1 logs_generator.go:76] 28 POST /api/v1/namespaces/kube-system/pods/t5dr 328\nI0417 22:29:23.821264       1 logs_generator.go:76] 29 POST /api/v1/namespaces/ns/pods/mnt 583\nI0417 22:29:24.021253       1 logs_generator.go:76] 30 PUT /api/v1/namespaces/kube-system/pods/hbr 428\nI0417 22:29:24.221273       1 logs_generator.go:76] 31 GET /api/v1/namespaces/kube-system/pods/tg6 372\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Apr 17 22:29:24.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete pod logs-generator --namespace=kubectl-8696'
Apr 17 22:29:26.809: INFO: stderr: ""
Apr 17 22:29:26.809: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:29:26.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8696" for this suite.
Apr 17 22:29:34.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:29:37.041: INFO: namespace kubectl-8696 deletion completed in 10.211252618s

â€¢ [SLOW TEST:20.674 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:29:37.041: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-84ca3a94-a07e-4c78-bde1-0ac589f42926
STEP: Creating a pod to test consume configMaps
Apr 17 22:29:37.257: INFO: Waiting up to 5m0s for pod "pod-configmaps-a5bd4939-e5f8-4a9a-a49a-f8cad8a6c6fd" in namespace "configmap-3714" to be "success or failure"
Apr 17 22:29:37.268: INFO: Pod "pod-configmaps-a5bd4939-e5f8-4a9a-a49a-f8cad8a6c6fd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.691951ms
Apr 17 22:29:39.280: INFO: Pod "pod-configmaps-a5bd4939-e5f8-4a9a-a49a-f8cad8a6c6fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022750177s
STEP: Saw pod success
Apr 17 22:29:39.280: INFO: Pod "pod-configmaps-a5bd4939-e5f8-4a9a-a49a-f8cad8a6c6fd" satisfied condition "success or failure"
Apr 17 22:29:39.309: INFO: Trying to get logs from node 10.72.119.74 pod pod-configmaps-a5bd4939-e5f8-4a9a-a49a-f8cad8a6c6fd container configmap-volume-test: <nil>
STEP: delete the pod
Apr 17 22:29:39.369: INFO: Waiting for pod pod-configmaps-a5bd4939-e5f8-4a9a-a49a-f8cad8a6c6fd to disappear
Apr 17 22:29:39.385: INFO: Pod pod-configmaps-a5bd4939-e5f8-4a9a-a49a-f8cad8a6c6fd no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:29:39.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3714" for this suite.
Apr 17 22:29:47.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:29:49.620: INFO: namespace configmap-3714 deletion completed in 10.214220253s

â€¢ [SLOW TEST:12.579 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:29:49.621: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-e4bce276-bc1e-4992-8a66-539ef87015db
STEP: Creating a pod to test consume configMaps
Apr 17 22:29:49.839: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e4466fab-28b7-4d35-93ca-08ff79d9a0dd" in namespace "projected-6464" to be "success or failure"
Apr 17 22:29:49.855: INFO: Pod "pod-projected-configmaps-e4466fab-28b7-4d35-93ca-08ff79d9a0dd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.407631ms
Apr 17 22:29:51.867: INFO: Pod "pod-projected-configmaps-e4466fab-28b7-4d35-93ca-08ff79d9a0dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02834924s
STEP: Saw pod success
Apr 17 22:29:51.867: INFO: Pod "pod-projected-configmaps-e4466fab-28b7-4d35-93ca-08ff79d9a0dd" satisfied condition "success or failure"
Apr 17 22:29:51.879: INFO: Trying to get logs from node 10.72.119.74 pod pod-projected-configmaps-e4466fab-28b7-4d35-93ca-08ff79d9a0dd container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 17 22:29:51.948: INFO: Waiting for pod pod-projected-configmaps-e4466fab-28b7-4d35-93ca-08ff79d9a0dd to disappear
Apr 17 22:29:51.962: INFO: Pod pod-projected-configmaps-e4466fab-28b7-4d35-93ca-08ff79d9a0dd no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:29:51.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6464" for this suite.
Apr 17 22:30:00.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:30:02.190: INFO: namespace projected-6464 deletion completed in 10.191626542s

â€¢ [SLOW TEST:12.569 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:30:02.190: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 17 22:30:02.415: INFO: Waiting up to 5m0s for pod "pod-b6a9ceb5-4b6c-4489-a7bc-6b97445fc9d1" in namespace "emptydir-1362" to be "success or failure"
Apr 17 22:30:02.430: INFO: Pod "pod-b6a9ceb5-4b6c-4489-a7bc-6b97445fc9d1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.323607ms
Apr 17 22:30:04.442: INFO: Pod "pod-b6a9ceb5-4b6c-4489-a7bc-6b97445fc9d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027501509s
STEP: Saw pod success
Apr 17 22:30:04.443: INFO: Pod "pod-b6a9ceb5-4b6c-4489-a7bc-6b97445fc9d1" satisfied condition "success or failure"
Apr 17 22:30:04.453: INFO: Trying to get logs from node 10.72.119.74 pod pod-b6a9ceb5-4b6c-4489-a7bc-6b97445fc9d1 container test-container: <nil>
STEP: delete the pod
Apr 17 22:30:04.512: INFO: Waiting for pod pod-b6a9ceb5-4b6c-4489-a7bc-6b97445fc9d1 to disappear
Apr 17 22:30:04.523: INFO: Pod pod-b6a9ceb5-4b6c-4489-a7bc-6b97445fc9d1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:30:04.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1362" for this suite.
Apr 17 22:30:12.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:30:14.994: INFO: namespace emptydir-1362 deletion completed in 10.445545195s

â€¢ [SLOW TEST:12.804 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:30:14.994: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Apr 17 22:30:15.133: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-531062789 proxy --unix-socket=/tmp/kubectl-proxy-unix936343924/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:30:15.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5347" for this suite.
Apr 17 22:30:23.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:30:25.507: INFO: namespace kubectl-5347 deletion completed in 10.248353167s

â€¢ [SLOW TEST:10.513 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:30:25.510: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:30:25.677: INFO: Creating deployment "webserver-deployment"
Apr 17 22:30:25.705: INFO: Waiting for observed generation 1
Apr 17 22:30:27.745: INFO: Waiting for all required pods to come up
Apr 17 22:30:27.766: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr 17 22:30:29.912: INFO: Waiting for deployment "webserver-deployment" to complete
Apr 17 22:30:29.939: INFO: Updating deployment "webserver-deployment" with a non-existent image
Apr 17 22:30:30.029: INFO: Updating deployment webserver-deployment
Apr 17 22:30:30.029: INFO: Waiting for observed generation 2
Apr 17 22:30:32.072: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr 17 22:30:32.094: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr 17 22:30:32.119: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 17 22:30:32.173: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr 17 22:30:32.174: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr 17 22:30:32.197: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 17 22:30:32.237: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Apr 17 22:30:32.237: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Apr 17 22:30:32.277: INFO: Updating deployment webserver-deployment
Apr 17 22:30:32.277: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Apr 17 22:30:32.318: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr 17 22:30:32.336: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Apr 17 22:30:32.371: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3258 /apis/apps/v1/namespaces/deployment-3258/deployments/webserver-deployment 8c62118a-1c41-4fcb-9cfa-8a116c657ebb 87944 3 2020-04-17 22:30:25 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc008e44e08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2020-04-17 22:30:30 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2020-04-17 22:30:32 +0000 UTC,LastTransitionTime:2020-04-17 22:30:32 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Apr 17 22:30:32.391: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-3258 /apis/apps/v1/namespaces/deployment-3258/replicasets/webserver-deployment-c7997dcc8 7c600697-7452-4f7b-9926-b46663e402bf 87931 3 2020-04-17 22:30:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 8c62118a-1c41-4fcb-9cfa-8a116c657ebb 0xc007071447 0xc007071448}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0070714b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 17 22:30:32.391: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Apr 17 22:30:32.392: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-3258 /apis/apps/v1/namespaces/deployment-3258/replicasets/webserver-deployment-595b5b9587 fe0877a0-b9a7-4486-be4e-2f27414b0aec 87930 3 2020-04-17 22:30:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 8c62118a-1c41-4fcb-9cfa-8a116c657ebb 0xc007071387 0xc007071388}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0070713e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Apr 17 22:30:32.422: INFO: Pod "webserver-deployment-595b5b9587-255r7" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-255r7 webserver-deployment-595b5b9587- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-595b5b9587-255r7 cef050bc-62af-4d3f-b6e3-850be5e58c5d 87954 0 2020-04-17 22:30:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fe0877a0-b9a7-4486-be4e-2f27414b0aec 0xc008e45287 0xc008e45288}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.423: INFO: Pod "webserver-deployment-595b5b9587-2wfd8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2wfd8 webserver-deployment-595b5b9587- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-595b5b9587-2wfd8 20b90e6e-59d4-4996-bca1-fbda6a408ac3 87942 0 2020-04-17 22:30:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fe0877a0-b9a7-4486-be4e-2f27414b0aec 0xc008e453c0 0xc008e453c1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.424: INFO: Pod "webserver-deployment-595b5b9587-4mfws" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4mfws webserver-deployment-595b5b9587- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-595b5b9587-4mfws 44d07013-ed24-4723-9d1b-eff5c786cd9e 87791 0 2020-04-17 22:30:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.194.116/32 cni.projectcalico.org/podIPs:172.30.194.116/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.194.116"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fe0877a0-b9a7-4486-be4e-2f27414b0aec 0xc008e454f0 0xc008e454f1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.74,PodIP:172.30.194.116,StartTime:2020-04-17 22:30:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-17 22:30:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://fffe52ed0b0be507230224447eee978c3efa1a6279fa22a1b9fc2ca3a7b21079,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.194.116,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.426: INFO: Pod "webserver-deployment-595b5b9587-bhtnp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bhtnp webserver-deployment-595b5b9587- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-595b5b9587-bhtnp 909ff76d-493b-41e4-8a73-3223bf90e923 87823 0 2020-04-17 22:30:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.194.127/32 cni.projectcalico.org/podIPs:172.30.194.127/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.194.127"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fe0877a0-b9a7-4486-be4e-2f27414b0aec 0xc008e45687 0xc008e45688}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.74,PodIP:172.30.194.127,StartTime:2020-04-17 22:30:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-17 22:30:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://b49da907be875e9f28c19c2f7e8239d70a33162eaaa3a5178843853fd714afcc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.194.127,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.448: INFO: Pod "webserver-deployment-595b5b9587-gf22k" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-gf22k webserver-deployment-595b5b9587- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-595b5b9587-gf22k 2a2a8d03-8282-459c-9d00-ebb5cbdbb0d0 87956 0 2020-04-17 22:30:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fe0877a0-b9a7-4486-be4e-2f27414b0aec 0xc008e45827 0xc008e45828}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.448: INFO: Pod "webserver-deployment-595b5b9587-h62zh" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-h62zh webserver-deployment-595b5b9587- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-595b5b9587-h62zh 9e5b840c-5e5d-4ce4-92a5-169c3db98469 87805 0 2020-04-17 22:30:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.232.231/32 cni.projectcalico.org/podIPs:172.30.232.231/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.232.231"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fe0877a0-b9a7-4486-be4e-2f27414b0aec 0xc008e45960 0xc008e45961}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.98,PodIP:172.30.232.231,StartTime:2020-04-17 22:30:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-17 22:30:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://c0402faabee4917bbe2eda2c0e05f59f4de7113286923768ecc4b500ad1a0422,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.232.231,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.449: INFO: Pod "webserver-deployment-595b5b9587-hcxgv" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hcxgv webserver-deployment-595b5b9587- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-595b5b9587-hcxgv 50626b64-ac7e-40a1-b759-fc0cd5df0839 87808 0 2020-04-17 22:30:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.232.233/32 cni.projectcalico.org/podIPs:172.30.232.233/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.232.233"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fe0877a0-b9a7-4486-be4e-2f27414b0aec 0xc008e45af7 0xc008e45af8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.98,PodIP:172.30.232.233,StartTime:2020-04-17 22:30:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-17 22:30:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://4882c5f34421a2177678369d6ac9f36cee8614eda93401a4bc3817cbc0de55fc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.232.233,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.449: INFO: Pod "webserver-deployment-595b5b9587-hjbxd" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hjbxd webserver-deployment-595b5b9587- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-595b5b9587-hjbxd 34bee34a-243a-409f-b5f9-5a07f1fa9d20 87803 0 2020-04-17 22:30:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.232.234/32 cni.projectcalico.org/podIPs:172.30.232.234/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.232.234"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fe0877a0-b9a7-4486-be4e-2f27414b0aec 0xc008e45c97 0xc008e45c98}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.98,PodIP:172.30.232.234,StartTime:2020-04-17 22:30:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-17 22:30:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://443e8b81e47881a8e79f34dc3f17fedd4960193b89c4b12e54df6ff888375d9a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.232.234,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.450: INFO: Pod "webserver-deployment-595b5b9587-hkhkp" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-hkhkp webserver-deployment-595b5b9587- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-595b5b9587-hkhkp 790c143c-51d8-4209-b5b8-63aaf553c591 87797 0 2020-04-17 22:30:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.194.115/32 cni.projectcalico.org/podIPs:172.30.194.115/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.194.115"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fe0877a0-b9a7-4486-be4e-2f27414b0aec 0xc008e45e37 0xc008e45e38}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.74,PodIP:172.30.194.115,StartTime:2020-04-17 22:30:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-17 22:30:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://70e7b9ea1d09c21a9e4d17edb0765ef6c7d593c3c01c60bbb425d88f0b45cb79,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.194.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.450: INFO: Pod "webserver-deployment-595b5b9587-jvr4d" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-jvr4d webserver-deployment-595b5b9587- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-595b5b9587-jvr4d 747d0671-034b-4bf3-bb7c-e68255224a6a 87941 0 2020-04-17 22:30:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fe0877a0-b9a7-4486-be4e-2f27414b0aec 0xc008e45fd7 0xc008e45fd8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.450: INFO: Pod "webserver-deployment-595b5b9587-nsc85" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-nsc85 webserver-deployment-595b5b9587- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-595b5b9587-nsc85 2d0375ba-1d6a-47e5-b355-cab801913478 87958 0 2020-04-17 22:30:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fe0877a0-b9a7-4486-be4e-2f27414b0aec 0xc0034e8110 0xc0034e8111}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.450: INFO: Pod "webserver-deployment-595b5b9587-qld64" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-qld64 webserver-deployment-595b5b9587- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-595b5b9587-qld64 5dc2c6b5-3da8-4e31-990e-1543dd447b0a 87794 0 2020-04-17 22:30:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.194.126/32 cni.projectcalico.org/podIPs:172.30.194.126/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.194.126"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fe0877a0-b9a7-4486-be4e-2f27414b0aec 0xc0034e8240 0xc0034e8241}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.74,PodIP:172.30.194.126,StartTime:2020-04-17 22:30:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-17 22:30:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://cb8a8db1b9d0cd624bce7cac52d2d5235965bdc6708c9a5f43283a309171263a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.194.126,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.451: INFO: Pod "webserver-deployment-595b5b9587-r55hw" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-r55hw webserver-deployment-595b5b9587- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-595b5b9587-r55hw 472096a9-cbde-4037-9af6-3815d7f22088 87957 0 2020-04-17 22:30:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fe0877a0-b9a7-4486-be4e-2f27414b0aec 0xc0034e83d7 0xc0034e83d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.451: INFO: Pod "webserver-deployment-595b5b9587-z2vj8" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-z2vj8 webserver-deployment-595b5b9587- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-595b5b9587-z2vj8 1d79a763-d5af-4f73-8aa0-9e623a810132 87943 0 2020-04-17 22:30:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fe0877a0-b9a7-4486-be4e-2f27414b0aec 0xc0034e8510 0xc0034e8511}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:32 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.74,PodIP:,StartTime:2020-04-17 22:30:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.451: INFO: Pod "webserver-deployment-595b5b9587-zjgb2" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-zjgb2 webserver-deployment-595b5b9587- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-595b5b9587-zjgb2 9118f5b7-e821-4575-8dfd-683252616292 87778 0 2020-04-17 22:30:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[cni.projectcalico.org/podIP:172.30.53.31/32 cni.projectcalico.org/podIPs:172.30.53.31/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.53.31"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 fe0877a0-b9a7-4486-be4e-2f27414b0aec 0xc0034e86a7 0xc0034e86a8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.72,PodIP:172.30.53.31,StartTime:2020-04-17 22:30:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-17 22:30:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:docker.io/library/httpd@sha256:6feb0ea7b0967367da66e8d58ba813fde32bdb92f63bfc21a9e170d211539db4,ContainerID:cri-o://b8f86a2b70e1f06617e99a731b9c49b002fc2a52b4efe7f82410029e3b738b9f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.53.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.452: INFO: Pod "webserver-deployment-c7997dcc8-5j8zv" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-5j8zv webserver-deployment-c7997dcc8- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-c7997dcc8-5j8zv 10119d91-6f94-4acc-a442-a8e9cdba36ae 87917 0 2020-04-17 22:30:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.194.124/32 cni.projectcalico.org/podIPs:172.30.194.124/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.194.124"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7c600697-7452-4f7b-9926-b46663e402bf 0xc0034e8847 0xc0034e8848}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.74,PodIP:,StartTime:2020-04-17 22:30:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.453: INFO: Pod "webserver-deployment-c7997dcc8-cpnh8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-cpnh8 webserver-deployment-c7997dcc8- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-c7997dcc8-cpnh8 d276b3fb-7e4d-4ad8-9256-5417ac75861c 87913 0 2020-04-17 22:30:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.194.123/32 cni.projectcalico.org/podIPs:172.30.194.123/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.194.123"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7c600697-7452-4f7b-9926-b46663e402bf 0xc0034e89e7 0xc0034e89e8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.74,PodIP:,StartTime:2020-04-17 22:30:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.453: INFO: Pod "webserver-deployment-c7997dcc8-dvc7p" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-dvc7p webserver-deployment-c7997dcc8- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-c7997dcc8-dvc7p 29e779d7-f9d7-417a-a138-bcbd76efd491 87953 0 2020-04-17 22:30:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7c600697-7452-4f7b-9926-b46663e402bf 0xc0034e8b87 0xc0034e8b88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.455: INFO: Pod "webserver-deployment-c7997dcc8-f64sx" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-f64sx webserver-deployment-c7997dcc8- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-c7997dcc8-f64sx c84237cd-df91-46b9-bc61-ce384f3c5bdf 87905 0 2020-04-17 22:30:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.53.63/32 cni.projectcalico.org/podIPs:172.30.53.63/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.53.63"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7c600697-7452-4f7b-9926-b46663e402bf 0xc0034e8cf0 0xc0034e8cf1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.72,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.72,PodIP:,StartTime:2020-04-17 22:30:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.455: INFO: Pod "webserver-deployment-c7997dcc8-pnt64" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-pnt64 webserver-deployment-c7997dcc8- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-c7997dcc8-pnt64 a7a0d011-0c5d-47f7-b928-45d0e431f059 87924 0 2020-04-17 22:30:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.194.122/32 cni.projectcalico.org/podIPs:172.30.194.122/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.194.122"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7c600697-7452-4f7b-9926-b46663e402bf 0xc0034e8e87 0xc0034e8e88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.74,PodIP:,StartTime:2020-04-17 22:30:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:30:32.456: INFO: Pod "webserver-deployment-c7997dcc8-qnghv" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-qnghv webserver-deployment-c7997dcc8- deployment-3258 /api/v1/namespaces/deployment-3258/pods/webserver-deployment-c7997dcc8-qnghv 58ce74de-3de8-4249-a5ab-c82062fb6039 87908 0 2020-04-17 22:30:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[cni.projectcalico.org/podIP:172.30.232.235/32 cni.projectcalico.org/podIPs:172.30.232.235/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.232.235"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 7c600697-7452-4f7b-9926-b46663e402bf 0xc0034e9027 0xc0034e9028}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6t68z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6t68z,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6t68z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.98,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-7zffx,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 22:30:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.98,PodIP:,StartTime:2020-04-17 22:30:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:30:32.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3258" for this suite.
Apr 17 22:30:44.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:30:47.035: INFO: namespace deployment-3258 deletion completed in 14.531468811s

â€¢ [SLOW TEST:21.525 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:30:47.037: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 22:30:47.284: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a69ef75-449b-4425-ae66-65b172588dc1" in namespace "projected-4995" to be "success or failure"
Apr 17 22:30:47.296: INFO: Pod "downwardapi-volume-5a69ef75-449b-4425-ae66-65b172588dc1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.696684ms
Apr 17 22:30:49.309: INFO: Pod "downwardapi-volume-5a69ef75-449b-4425-ae66-65b172588dc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025615795s
STEP: Saw pod success
Apr 17 22:30:49.310: INFO: Pod "downwardapi-volume-5a69ef75-449b-4425-ae66-65b172588dc1" satisfied condition "success or failure"
Apr 17 22:30:49.322: INFO: Trying to get logs from node 10.72.119.74 pod downwardapi-volume-5a69ef75-449b-4425-ae66-65b172588dc1 container client-container: <nil>
STEP: delete the pod
Apr 17 22:30:49.389: INFO: Waiting for pod downwardapi-volume-5a69ef75-449b-4425-ae66-65b172588dc1 to disappear
Apr 17 22:30:49.405: INFO: Pod downwardapi-volume-5a69ef75-449b-4425-ae66-65b172588dc1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:30:49.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4995" for this suite.
Apr 17 22:30:57.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:30:59.579: INFO: namespace projected-4995 deletion completed in 10.143033849s

â€¢ [SLOW TEST:12.543 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:30:59.580: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:31:25.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3442" for this suite.
Apr 17 22:31:33.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:31:35.940: INFO: namespace container-runtime-3442 deletion completed in 10.378420508s

â€¢ [SLOW TEST:36.359 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:31:35.940: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr 17 22:31:36.185: INFO: Waiting up to 5m0s for pod "pod-093bb74e-5b18-4bda-87bc-ac93cc0168f9" in namespace "emptydir-5540" to be "success or failure"
Apr 17 22:31:36.196: INFO: Pod "pod-093bb74e-5b18-4bda-87bc-ac93cc0168f9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.854577ms
Apr 17 22:31:38.208: INFO: Pod "pod-093bb74e-5b18-4bda-87bc-ac93cc0168f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023270135s
Apr 17 22:31:40.223: INFO: Pod "pod-093bb74e-5b18-4bda-87bc-ac93cc0168f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037470297s
STEP: Saw pod success
Apr 17 22:31:40.223: INFO: Pod "pod-093bb74e-5b18-4bda-87bc-ac93cc0168f9" satisfied condition "success or failure"
Apr 17 22:31:40.236: INFO: Trying to get logs from node 10.72.119.74 pod pod-093bb74e-5b18-4bda-87bc-ac93cc0168f9 container test-container: <nil>
STEP: delete the pod
Apr 17 22:31:40.298: INFO: Waiting for pod pod-093bb74e-5b18-4bda-87bc-ac93cc0168f9 to disappear
Apr 17 22:31:40.309: INFO: Pod pod-093bb74e-5b18-4bda-87bc-ac93cc0168f9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:31:40.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5540" for this suite.
Apr 17 22:31:48.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:31:50.569: INFO: namespace emptydir-5540 deletion completed in 10.23064725s

â€¢ [SLOW TEST:14.630 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:31:50.570: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:31:50.768: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-9bac93a7-337c-4989-8fc7-671af314c972
STEP: Creating configMap with name cm-test-opt-upd-1b0fe77d-85d6-46ef-8b9c-d596bcc1e11a
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-9bac93a7-337c-4989-8fc7-671af314c972
STEP: Updating configmap cm-test-opt-upd-1b0fe77d-85d6-46ef-8b9c-d596bcc1e11a
STEP: Creating configMap with name cm-test-opt-create-e7733f30-de6d-40f8-8e9f-dc828697c668
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:33:14.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4058" for this suite.
Apr 17 22:33:46.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:33:48.740: INFO: namespace projected-4058 deletion completed in 34.219051419s

â€¢ [SLOW TEST:118.170 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:33:48.740: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Apr 17 22:33:48.901: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:33:51.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1733" for this suite.
Apr 17 22:33:59.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:34:01.933: INFO: namespace init-container-1733 deletion completed in 10.109868745s

â€¢ [SLOW TEST:13.193 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:34:01.939: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-81356a48-be63-45de-81bc-2e2284b9fac9
STEP: Creating a pod to test consume secrets
Apr 17 22:34:02.254: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3e551efe-6d0e-4e9b-8892-1424b3b84cfa" in namespace "projected-5532" to be "success or failure"
Apr 17 22:34:02.267: INFO: Pod "pod-projected-secrets-3e551efe-6d0e-4e9b-8892-1424b3b84cfa": Phase="Pending", Reason="", readiness=false. Elapsed: 12.898961ms
Apr 17 22:34:04.283: INFO: Pod "pod-projected-secrets-3e551efe-6d0e-4e9b-8892-1424b3b84cfa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028903004s
STEP: Saw pod success
Apr 17 22:34:04.283: INFO: Pod "pod-projected-secrets-3e551efe-6d0e-4e9b-8892-1424b3b84cfa" satisfied condition "success or failure"
Apr 17 22:34:04.296: INFO: Trying to get logs from node 10.72.119.74 pod pod-projected-secrets-3e551efe-6d0e-4e9b-8892-1424b3b84cfa container secret-volume-test: <nil>
STEP: delete the pod
Apr 17 22:34:04.358: INFO: Waiting for pod pod-projected-secrets-3e551efe-6d0e-4e9b-8892-1424b3b84cfa to disappear
Apr 17 22:34:04.368: INFO: Pod pod-projected-secrets-3e551efe-6d0e-4e9b-8892-1424b3b84cfa no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:34:04.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5532" for this suite.
Apr 17 22:34:12.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:34:14.603: INFO: namespace projected-5532 deletion completed in 10.207264369s

â€¢ [SLOW TEST:12.665 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:34:14.606: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Apr 17 22:34:15.629: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Apr 17 22:34:17.675: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722759655, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722759655, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722759655, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722759655, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 22:34:20.733: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:34:20.749: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:34:22.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8397" for this suite.
Apr 17 22:34:30.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:34:32.452: INFO: namespace crd-webhook-8397 deletion completed in 10.300729912s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:17.941 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:34:32.548: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:34:37.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9903" for this suite.
Apr 17 22:35:09.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:35:12.243: INFO: namespace replication-controller-9903 deletion completed in 34.35433442s

â€¢ [SLOW TEST:39.696 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:35:12.244: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-99wh
STEP: Creating a pod to test atomic-volume-subpath
Apr 17 22:35:12.504: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-99wh" in namespace "subpath-9451" to be "success or failure"
Apr 17 22:35:12.520: INFO: Pod "pod-subpath-test-configmap-99wh": Phase="Pending", Reason="", readiness=false. Elapsed: 15.458052ms
Apr 17 22:35:14.533: INFO: Pod "pod-subpath-test-configmap-99wh": Phase="Running", Reason="", readiness=true. Elapsed: 2.028790789s
Apr 17 22:35:16.548: INFO: Pod "pod-subpath-test-configmap-99wh": Phase="Running", Reason="", readiness=true. Elapsed: 4.043561059s
Apr 17 22:35:18.560: INFO: Pod "pod-subpath-test-configmap-99wh": Phase="Running", Reason="", readiness=true. Elapsed: 6.05596828s
Apr 17 22:35:20.572: INFO: Pod "pod-subpath-test-configmap-99wh": Phase="Running", Reason="", readiness=true. Elapsed: 8.067658897s
Apr 17 22:35:22.585: INFO: Pod "pod-subpath-test-configmap-99wh": Phase="Running", Reason="", readiness=true. Elapsed: 10.080767051s
Apr 17 22:35:24.598: INFO: Pod "pod-subpath-test-configmap-99wh": Phase="Running", Reason="", readiness=true. Elapsed: 12.093424283s
Apr 17 22:35:26.610: INFO: Pod "pod-subpath-test-configmap-99wh": Phase="Running", Reason="", readiness=true. Elapsed: 14.105611342s
Apr 17 22:35:28.621: INFO: Pod "pod-subpath-test-configmap-99wh": Phase="Running", Reason="", readiness=true. Elapsed: 16.11701036s
Apr 17 22:35:30.634: INFO: Pod "pod-subpath-test-configmap-99wh": Phase="Running", Reason="", readiness=true. Elapsed: 18.129497786s
Apr 17 22:35:32.656: INFO: Pod "pod-subpath-test-configmap-99wh": Phase="Running", Reason="", readiness=true. Elapsed: 20.151451479s
Apr 17 22:35:34.668: INFO: Pod "pod-subpath-test-configmap-99wh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.163404962s
STEP: Saw pod success
Apr 17 22:35:34.668: INFO: Pod "pod-subpath-test-configmap-99wh" satisfied condition "success or failure"
Apr 17 22:35:34.679: INFO: Trying to get logs from node 10.72.119.74 pod pod-subpath-test-configmap-99wh container test-container-subpath-configmap-99wh: <nil>
STEP: delete the pod
Apr 17 22:35:34.778: INFO: Waiting for pod pod-subpath-test-configmap-99wh to disappear
Apr 17 22:35:34.794: INFO: Pod pod-subpath-test-configmap-99wh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-99wh
Apr 17 22:35:34.794: INFO: Deleting pod "pod-subpath-test-configmap-99wh" in namespace "subpath-9451"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:35:34.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9451" for this suite.
Apr 17 22:35:42.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:35:45.048: INFO: namespace subpath-9451 deletion completed in 10.21428758s

â€¢ [SLOW TEST:32.804 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:35:45.048: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-79b9d432-24a5-4d5b-81f2-9b37f0e5a268 in namespace container-probe-2123
Apr 17 22:35:47.314: INFO: Started pod busybox-79b9d432-24a5-4d5b-81f2-9b37f0e5a268 in namespace container-probe-2123
STEP: checking the pod's current state and verifying that restartCount is present
Apr 17 22:35:47.339: INFO: Initial restart count of pod busybox-79b9d432-24a5-4d5b-81f2-9b37f0e5a268 is 0
Apr 17 22:36:39.709: INFO: Restart count of pod container-probe-2123/busybox-79b9d432-24a5-4d5b-81f2-9b37f0e5a268 is now 1 (52.370571132s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:36:39.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2123" for this suite.
Apr 17 22:36:47.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:36:50.204: INFO: namespace container-probe-2123 deletion completed in 10.42880012s

â€¢ [SLOW TEST:65.156 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:36:50.204: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:36:50.397: INFO: Creating ReplicaSet my-hostname-basic-6c80ee7c-ed94-4537-9793-98bed6f27fa6
Apr 17 22:36:50.468: INFO: Pod name my-hostname-basic-6c80ee7c-ed94-4537-9793-98bed6f27fa6: Found 0 pods out of 1
Apr 17 22:36:55.484: INFO: Pod name my-hostname-basic-6c80ee7c-ed94-4537-9793-98bed6f27fa6: Found 1 pods out of 1
Apr 17 22:36:55.484: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6c80ee7c-ed94-4537-9793-98bed6f27fa6" is running
Apr 17 22:36:55.498: INFO: Pod "my-hostname-basic-6c80ee7c-ed94-4537-9793-98bed6f27fa6-xfvvj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-17 22:36:50 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-17 22:36:52 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-17 22:36:52 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-04-17 22:36:50 +0000 UTC Reason: Message:}])
Apr 17 22:36:55.498: INFO: Trying to dial the pod
Apr 17 22:37:00.550: INFO: Controller my-hostname-basic-6c80ee7c-ed94-4537-9793-98bed6f27fa6: Got expected result from replica 1 [my-hostname-basic-6c80ee7c-ed94-4537-9793-98bed6f27fa6-xfvvj]: "my-hostname-basic-6c80ee7c-ed94-4537-9793-98bed6f27fa6-xfvvj", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:37:00.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3605" for this suite.
Apr 17 22:37:08.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:37:10.906: INFO: namespace replicaset-3605 deletion completed in 10.312864999s

â€¢ [SLOW TEST:20.702 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:37:10.907: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 22:37:11.902: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 17 22:37:13.963: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722759831, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722759831, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722759831, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722759831, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 22:37:17.011: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:37:17.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8161" for this suite.
Apr 17 22:37:25.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:37:27.631: INFO: namespace webhook-8161 deletion completed in 10.192793455s
STEP: Destroying namespace "webhook-8161-markers" for this suite.
Apr 17 22:37:35.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:37:37.938: INFO: namespace webhook-8161-markers deletion completed in 10.306162692s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:27.103 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:37:38.010: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 22:37:38.187: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4358524d-be5e-40b8-acf8-c6e569b7df2a" in namespace "downward-api-5367" to be "success or failure"
Apr 17 22:37:38.200: INFO: Pod "downwardapi-volume-4358524d-be5e-40b8-acf8-c6e569b7df2a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.289764ms
Apr 17 22:37:40.212: INFO: Pod "downwardapi-volume-4358524d-be5e-40b8-acf8-c6e569b7df2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02460273s
Apr 17 22:37:42.224: INFO: Pod "downwardapi-volume-4358524d-be5e-40b8-acf8-c6e569b7df2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036446339s
STEP: Saw pod success
Apr 17 22:37:42.224: INFO: Pod "downwardapi-volume-4358524d-be5e-40b8-acf8-c6e569b7df2a" satisfied condition "success or failure"
Apr 17 22:37:42.235: INFO: Trying to get logs from node 10.72.119.74 pod downwardapi-volume-4358524d-be5e-40b8-acf8-c6e569b7df2a container client-container: <nil>
STEP: delete the pod
Apr 17 22:37:42.340: INFO: Waiting for pod downwardapi-volume-4358524d-be5e-40b8-acf8-c6e569b7df2a to disappear
Apr 17 22:37:42.352: INFO: Pod downwardapi-volume-4358524d-be5e-40b8-acf8-c6e569b7df2a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:37:42.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5367" for this suite.
Apr 17 22:37:50.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:37:52.694: INFO: namespace downward-api-5367 deletion completed in 10.302951465s

â€¢ [SLOW TEST:14.684 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:37:52.695: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:38:09.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2391" for this suite.
Apr 17 22:38:17.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:38:19.665: INFO: namespace resourcequota-2391 deletion completed in 10.392147314s

â€¢ [SLOW TEST:26.970 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:38:19.665: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:38:19.834: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:38:21.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7251" for this suite.
Apr 17 22:39:02.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:39:04.197: INFO: namespace pods-7251 deletion completed in 42.175277972s

â€¢ [SLOW TEST:44.532 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:39:04.197: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:39:06.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5288" for this suite.
Apr 17 22:39:46.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:39:48.785: INFO: namespace kubelet-test-5288 deletion completed in 42.26760614s

â€¢ [SLOW TEST:44.587 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:39:48.785: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 22:39:49.483: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 17 22:39:51.528: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722759989, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722759989, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722759989, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722759989, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 22:39:54.582: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:39:54.600: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-351-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:39:55.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6972" for this suite.
Apr 17 22:40:04.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:40:06.344: INFO: namespace webhook-6972 deletion completed in 10.374531365s
STEP: Destroying namespace "webhook-6972-markers" for this suite.
Apr 17 22:40:14.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:40:16.825: INFO: namespace webhook-6972-markers deletion completed in 10.481264181s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:28.115 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:40:16.904: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr 17 22:40:17.241: INFO: Waiting up to 5m0s for pod "pod-5a07bd73-51dc-4288-bdae-dc089972937d" in namespace "emptydir-3316" to be "success or failure"
Apr 17 22:40:17.255: INFO: Pod "pod-5a07bd73-51dc-4288-bdae-dc089972937d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.480167ms
Apr 17 22:40:19.269: INFO: Pod "pod-5a07bd73-51dc-4288-bdae-dc089972937d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028220489s
STEP: Saw pod success
Apr 17 22:40:19.270: INFO: Pod "pod-5a07bd73-51dc-4288-bdae-dc089972937d" satisfied condition "success or failure"
Apr 17 22:40:19.284: INFO: Trying to get logs from node 10.72.119.74 pod pod-5a07bd73-51dc-4288-bdae-dc089972937d container test-container: <nil>
STEP: delete the pod
Apr 17 22:40:19.353: INFO: Waiting for pod pod-5a07bd73-51dc-4288-bdae-dc089972937d to disappear
Apr 17 22:40:19.363: INFO: Pod pod-5a07bd73-51dc-4288-bdae-dc089972937d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:40:19.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3316" for this suite.
Apr 17 22:40:27.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:40:29.628: INFO: namespace emptydir-3316 deletion completed in 10.229119793s

â€¢ [SLOW TEST:12.725 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:40:29.632: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr 17 22:40:37.963: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 17 22:40:37.976: INFO: Pod pod-with-poststart-http-hook still exists
Apr 17 22:40:39.976: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 17 22:40:39.989: INFO: Pod pod-with-poststart-http-hook still exists
Apr 17 22:40:41.976: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 17 22:40:41.989: INFO: Pod pod-with-poststart-http-hook still exists
Apr 17 22:40:43.976: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 17 22:40:43.988: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:40:43.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3212" for this suite.
Apr 17 22:41:16.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:41:18.342: INFO: namespace container-lifecycle-hook-3212 deletion completed in 34.329965482s

â€¢ [SLOW TEST:48.710 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:41:18.342: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Apr 17 22:41:21.212: INFO: Successfully updated pod "labelsupdatefc29a281-b216-4547-be7e-1cc351e24b0f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:41:25.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6217" for this suite.
Apr 17 22:41:57.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:41:59.688: INFO: namespace projected-6217 deletion completed in 34.3583395s

â€¢ [SLOW TEST:41.346 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:41:59.688: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr 17 22:41:59.921: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6240 /api/v1/namespaces/watch-6240/configmaps/e2e-watch-test-watch-closed 097fb4ce-1d7a-4a48-bc56-c4f09843d779 92973 0 2020-04-17 22:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr 17 22:41:59.921: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6240 /api/v1/namespaces/watch-6240/configmaps/e2e-watch-test-watch-closed 097fb4ce-1d7a-4a48-bc56-c4f09843d779 92977 0 2020-04-17 22:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr 17 22:41:59.984: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6240 /api/v1/namespaces/watch-6240/configmaps/e2e-watch-test-watch-closed 097fb4ce-1d7a-4a48-bc56-c4f09843d779 92979 0 2020-04-17 22:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr 17 22:41:59.985: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6240 /api/v1/namespaces/watch-6240/configmaps/e2e-watch-test-watch-closed 097fb4ce-1d7a-4a48-bc56-c4f09843d779 92980 0 2020-04-17 22:41:59 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:41:59.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6240" for this suite.
Apr 17 22:42:08.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:42:10.319: INFO: namespace watch-6240 deletion completed in 10.30755612s

â€¢ [SLOW TEST:10.631 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:42:10.319: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:42:44.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9985" for this suite.
Apr 17 22:42:52.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:42:55.150: INFO: namespace namespaces-9985 deletion completed in 10.21933256s
STEP: Destroying namespace "nsdeletetest-48" for this suite.
Apr 17 22:42:55.161: INFO: Namespace nsdeletetest-48 was already deleted
STEP: Destroying namespace "nsdeletetest-2499" for this suite.
Apr 17 22:43:03.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:43:05.360: INFO: namespace nsdeletetest-2499 deletion completed in 10.198324736s

â€¢ [SLOW TEST:55.040 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:43:05.360: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Apr 17 22:43:05.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 create -f - --namespace=kubectl-3479'
Apr 17 22:43:06.410: INFO: stderr: ""
Apr 17 22:43:06.410: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 17 22:43:06.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3479'
Apr 17 22:43:06.546: INFO: stderr: ""
Apr 17 22:43:06.546: INFO: stdout: "update-demo-nautilus-cqf6v update-demo-nautilus-w74zl "
Apr 17 22:43:06.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-cqf6v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3479'
Apr 17 22:43:06.704: INFO: stderr: ""
Apr 17 22:43:06.704: INFO: stdout: ""
Apr 17 22:43:06.704: INFO: update-demo-nautilus-cqf6v is created but not running
Apr 17 22:43:11.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3479'
Apr 17 22:43:11.837: INFO: stderr: ""
Apr 17 22:43:11.837: INFO: stdout: "update-demo-nautilus-cqf6v update-demo-nautilus-w74zl "
Apr 17 22:43:11.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-cqf6v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3479'
Apr 17 22:43:11.973: INFO: stderr: ""
Apr 17 22:43:11.973: INFO: stdout: "true"
Apr 17 22:43:11.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-cqf6v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3479'
Apr 17 22:43:12.137: INFO: stderr: ""
Apr 17 22:43:12.138: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 17 22:43:12.138: INFO: validating pod update-demo-nautilus-cqf6v
Apr 17 22:43:12.180: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 22:43:12.180: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 22:43:12.180: INFO: update-demo-nautilus-cqf6v is verified up and running
Apr 17 22:43:12.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-w74zl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3479'
Apr 17 22:43:12.350: INFO: stderr: ""
Apr 17 22:43:12.350: INFO: stdout: "true"
Apr 17 22:43:12.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-w74zl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3479'
Apr 17 22:43:12.491: INFO: stderr: ""
Apr 17 22:43:12.491: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 17 22:43:12.491: INFO: validating pod update-demo-nautilus-w74zl
Apr 17 22:43:12.517: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 22:43:12.517: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 22:43:12.517: INFO: update-demo-nautilus-w74zl is verified up and running
STEP: rolling-update to new replication controller
Apr 17 22:43:12.522: INFO: scanned /root for discovery docs: <nil>
Apr 17 22:43:12.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-3479'
Apr 17 22:43:37.830: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr 17 22:43:37.830: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 17 22:43:37.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3479'
Apr 17 22:43:37.988: INFO: stderr: ""
Apr 17 22:43:37.988: INFO: stdout: "update-demo-kitten-rhvxq update-demo-kitten-wfn8b "
Apr 17 22:43:37.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-kitten-rhvxq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3479'
Apr 17 22:43:38.120: INFO: stderr: ""
Apr 17 22:43:38.120: INFO: stdout: "true"
Apr 17 22:43:38.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-kitten-rhvxq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3479'
Apr 17 22:43:38.246: INFO: stderr: ""
Apr 17 22:43:38.246: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr 17 22:43:38.246: INFO: validating pod update-demo-kitten-rhvxq
Apr 17 22:43:38.271: INFO: got data: {
  "image": "kitten.jpg"
}

Apr 17 22:43:38.271: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr 17 22:43:38.271: INFO: update-demo-kitten-rhvxq is verified up and running
Apr 17 22:43:38.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-kitten-wfn8b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3479'
Apr 17 22:43:38.430: INFO: stderr: ""
Apr 17 22:43:38.430: INFO: stdout: "true"
Apr 17 22:43:38.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-kitten-wfn8b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3479'
Apr 17 22:43:38.551: INFO: stderr: ""
Apr 17 22:43:38.551: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr 17 22:43:38.551: INFO: validating pod update-demo-kitten-wfn8b
Apr 17 22:43:38.585: INFO: got data: {
  "image": "kitten.jpg"
}

Apr 17 22:43:38.585: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr 17 22:43:38.585: INFO: update-demo-kitten-wfn8b is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:43:38.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3479" for this suite.
Apr 17 22:44:00.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:44:02.805: INFO: namespace kubectl-3479 deletion completed in 24.177643084s

â€¢ [SLOW TEST:57.445 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:44:02.805: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:44:03.076: INFO: (0) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 70.676852ms)
Apr 17 22:44:03.102: INFO: (1) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 25.479482ms)
Apr 17 22:44:03.124: INFO: (2) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.533255ms)
Apr 17 22:44:03.147: INFO: (3) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 22.915765ms)
Apr 17 22:44:03.167: INFO: (4) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.499931ms)
Apr 17 22:44:03.194: INFO: (5) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 26.881299ms)
Apr 17 22:44:03.216: INFO: (6) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.735863ms)
Apr 17 22:44:03.237: INFO: (7) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.772239ms)
Apr 17 22:44:03.273: INFO: (8) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 35.305181ms)
Apr 17 22:44:03.299: INFO: (9) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 26.596127ms)
Apr 17 22:44:03.319: INFO: (10) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.078769ms)
Apr 17 22:44:03.385: INFO: (11) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 65.959128ms)
Apr 17 22:44:03.414: INFO: (12) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 28.26634ms)
Apr 17 22:44:03.435: INFO: (13) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.839985ms)
Apr 17 22:44:03.458: INFO: (14) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 23.507558ms)
Apr 17 22:44:03.483: INFO: (15) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 24.930715ms)
Apr 17 22:44:03.505: INFO: (16) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.714022ms)
Apr 17 22:44:03.527: INFO: (17) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 21.43921ms)
Apr 17 22:44:03.547: INFO: (18) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 20.687216ms)
Apr 17 22:44:03.578: INFO: (19) /api/v1/nodes/10.72.119.72:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="at/">at/</a>
<a href="at-no-rotate/">at-no-rotat... (200; 30.805671ms)
[AfterEach] version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:44:03.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6318" for this suite.
Apr 17 22:44:11.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:44:12.765: INFO: namespace proxy-6318 deletion completed in 9.162493593s

â€¢ [SLOW TEST:9.960 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:44:12.765: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-98961b93-7436-44f5-9261-bbe08c931f0f
STEP: Creating a pod to test consume secrets
Apr 17 22:44:12.984: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ba36be12-945e-4f75-886a-7a93888fd568" in namespace "projected-7719" to be "success or failure"
Apr 17 22:44:12.994: INFO: Pod "pod-projected-secrets-ba36be12-945e-4f75-886a-7a93888fd568": Phase="Pending", Reason="", readiness=false. Elapsed: 10.142521ms
Apr 17 22:44:15.006: INFO: Pod "pod-projected-secrets-ba36be12-945e-4f75-886a-7a93888fd568": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022113767s
STEP: Saw pod success
Apr 17 22:44:15.006: INFO: Pod "pod-projected-secrets-ba36be12-945e-4f75-886a-7a93888fd568" satisfied condition "success or failure"
Apr 17 22:44:15.019: INFO: Trying to get logs from node 10.72.119.74 pod pod-projected-secrets-ba36be12-945e-4f75-886a-7a93888fd568 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr 17 22:44:15.129: INFO: Waiting for pod pod-projected-secrets-ba36be12-945e-4f75-886a-7a93888fd568 to disappear
Apr 17 22:44:15.145: INFO: Pod pod-projected-secrets-ba36be12-945e-4f75-886a-7a93888fd568 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:44:15.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7719" for this suite.
Apr 17 22:44:23.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:44:25.450: INFO: namespace projected-7719 deletion completed in 10.280110548s

â€¢ [SLOW TEST:12.684 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:44:25.450: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-8da3f4bb-e931-4385-820d-0a8c2be60015
STEP: Creating a pod to test consume secrets
Apr 17 22:44:25.672: INFO: Waiting up to 5m0s for pod "pod-secrets-61bd2a64-1dd6-4c9c-aa46-f8c8cd35daf6" in namespace "secrets-395" to be "success or failure"
Apr 17 22:44:25.686: INFO: Pod "pod-secrets-61bd2a64-1dd6-4c9c-aa46-f8c8cd35daf6": Phase="Pending", Reason="", readiness=false. Elapsed: 13.156228ms
Apr 17 22:44:27.697: INFO: Pod "pod-secrets-61bd2a64-1dd6-4c9c-aa46-f8c8cd35daf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024245278s
STEP: Saw pod success
Apr 17 22:44:27.697: INFO: Pod "pod-secrets-61bd2a64-1dd6-4c9c-aa46-f8c8cd35daf6" satisfied condition "success or failure"
Apr 17 22:44:27.708: INFO: Trying to get logs from node 10.72.119.74 pod pod-secrets-61bd2a64-1dd6-4c9c-aa46-f8c8cd35daf6 container secret-volume-test: <nil>
STEP: delete the pod
Apr 17 22:44:27.782: INFO: Waiting for pod pod-secrets-61bd2a64-1dd6-4c9c-aa46-f8c8cd35daf6 to disappear
Apr 17 22:44:27.792: INFO: Pod pod-secrets-61bd2a64-1dd6-4c9c-aa46-f8c8cd35daf6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:44:27.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-395" for this suite.
Apr 17 22:44:35.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:44:37.996: INFO: namespace secrets-395 deletion completed in 10.179367448s

â€¢ [SLOW TEST:12.546 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:44:37.996: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr 17 22:44:38.191: INFO: Waiting up to 5m0s for pod "pod-2d0b3592-b583-4a91-bad9-e76cd87335cb" in namespace "emptydir-2274" to be "success or failure"
Apr 17 22:44:38.206: INFO: Pod "pod-2d0b3592-b583-4a91-bad9-e76cd87335cb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.288895ms
Apr 17 22:44:40.218: INFO: Pod "pod-2d0b3592-b583-4a91-bad9-e76cd87335cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026474753s
STEP: Saw pod success
Apr 17 22:44:40.218: INFO: Pod "pod-2d0b3592-b583-4a91-bad9-e76cd87335cb" satisfied condition "success or failure"
Apr 17 22:44:40.230: INFO: Trying to get logs from node 10.72.119.74 pod pod-2d0b3592-b583-4a91-bad9-e76cd87335cb container test-container: <nil>
STEP: delete the pod
Apr 17 22:44:40.288: INFO: Waiting for pod pod-2d0b3592-b583-4a91-bad9-e76cd87335cb to disappear
Apr 17 22:44:40.299: INFO: Pod pod-2d0b3592-b583-4a91-bad9-e76cd87335cb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:44:40.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2274" for this suite.
Apr 17 22:44:48.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:44:50.472: INFO: namespace emptydir-2274 deletion completed in 10.148644501s

â€¢ [SLOW TEST:12.476 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:44:50.473: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Apr 17 22:44:55.273: INFO: Successfully updated pod "annotationupdate9752bb4d-5f73-4bc1-941a-3ced32fa345f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:44:57.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-456" for this suite.
Apr 17 22:45:29.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:45:31.663: INFO: namespace downward-api-456 deletion completed in 34.307989715s

â€¢ [SLOW TEST:41.190 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:45:31.663: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 22:45:32.584: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 17 22:45:34.631: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722760332, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722760332, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722760332, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722760332, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 22:45:37.681: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:45:50.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6193" for this suite.
Apr 17 22:45:58.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:46:00.644: INFO: namespace webhook-6193 deletion completed in 10.370738805s
STEP: Destroying namespace "webhook-6193-markers" for this suite.
Apr 17 22:46:08.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:46:10.883: INFO: namespace webhook-6193-markers deletion completed in 10.2394247s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:39.300 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:46:10.964: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Apr 17 22:46:11.157: INFO: Waiting up to 5m0s for pod "var-expansion-160179cb-8710-418a-a2bd-41844005d63b" in namespace "var-expansion-3767" to be "success or failure"
Apr 17 22:46:11.169: INFO: Pod "var-expansion-160179cb-8710-418a-a2bd-41844005d63b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.856121ms
Apr 17 22:46:13.182: INFO: Pod "var-expansion-160179cb-8710-418a-a2bd-41844005d63b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024568403s
STEP: Saw pod success
Apr 17 22:46:13.182: INFO: Pod "var-expansion-160179cb-8710-418a-a2bd-41844005d63b" satisfied condition "success or failure"
Apr 17 22:46:13.193: INFO: Trying to get logs from node 10.72.119.74 pod var-expansion-160179cb-8710-418a-a2bd-41844005d63b container dapi-container: <nil>
STEP: delete the pod
Apr 17 22:46:13.259: INFO: Waiting for pod var-expansion-160179cb-8710-418a-a2bd-41844005d63b to disappear
Apr 17 22:46:13.271: INFO: Pod var-expansion-160179cb-8710-418a-a2bd-41844005d63b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:46:13.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3767" for this suite.
Apr 17 22:46:21.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:46:23.513: INFO: namespace var-expansion-3767 deletion completed in 10.197548821s

â€¢ [SLOW TEST:12.550 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:46:23.514: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:46:23.927: INFO: Waiting up to 5m0s for pod "busybox-user-65534-1654b159-fd2e-4ef3-b369-c416826452e7" in namespace "security-context-test-2557" to be "success or failure"
Apr 17 22:46:23.939: INFO: Pod "busybox-user-65534-1654b159-fd2e-4ef3-b369-c416826452e7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.740811ms
Apr 17 22:46:25.950: INFO: Pod "busybox-user-65534-1654b159-fd2e-4ef3-b369-c416826452e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023069162s
Apr 17 22:46:27.962: INFO: Pod "busybox-user-65534-1654b159-fd2e-4ef3-b369-c416826452e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034825012s
Apr 17 22:46:27.962: INFO: Pod "busybox-user-65534-1654b159-fd2e-4ef3-b369-c416826452e7" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:46:27.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2557" for this suite.
Apr 17 22:46:36.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:46:38.231: INFO: namespace security-context-test-2557 deletion completed in 10.239967676s

â€¢ [SLOW TEST:14.718 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:46:38.233: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Apr 17 22:46:38.377: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 17 22:46:38.469: INFO: Waiting for terminating namespaces to be deleted...
Apr 17 22:46:38.500: INFO: 
Logging pods the kubelet thinks is on node 10.72.119.72 before test
Apr 17 22:46:38.665: INFO: olm-operator-7bd9dc9457-2wn6n from openshift-operator-lifecycle-manager started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container olm-operator ready: true, restart count 0
Apr 17 22:46:38.666: INFO: downloads-64bb8b89c9-sccw2 from openshift-console started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container download-server ready: true, restart count 0
Apr 17 22:46:38.666: INFO: tuned-xrlm4 from openshift-cluster-node-tuning-operator started at 2020-04-17 19:11:12 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container tuned ready: true, restart count 0
Apr 17 22:46:38.666: INFO: console-5b98d99db9-lb2kc from openshift-console started at 2020-04-17 21:54:15 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container console ready: true, restart count 0
Apr 17 22:46:38.666: INFO: ibm-master-proxy-static-10.72.119.72 from kube-system started at 2020-04-17 19:09:18 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container pause ready: true, restart count 0
Apr 17 22:46:38.666: INFO: cluster-node-tuning-operator-77bdbd4f-79qvb from openshift-cluster-node-tuning-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Apr 17 22:46:38.666: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-04-17 21:54:22 +0000 UTC (3 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container alertmanager ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container config-reloader ready: true, restart count 0
Apr 17 22:46:38.666: INFO: tigera-operator-df8f4c87c-c7sjz from tigera-operator started at 2020-04-17 19:09:24 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container tigera-operator ready: true, restart count 0
Apr 17 22:46:38.666: INFO: marketplace-operator-699fb8f5d-5nrcq from openshift-marketplace started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container marketplace-operator ready: true, restart count 0
Apr 17 22:46:38.666: INFO: ibm-storage-watcher-d9c7cf586-f72vk from kube-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Apr 17 22:46:38.666: INFO: node-ca-x5gt7 from openshift-image-registry started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container node-ca ready: true, restart count 0
Apr 17 22:46:38.666: INFO: sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-l8m9b from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 17 22:46:38.666: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 22:46:38.666: INFO: multus-admission-controller-2jxfm from openshift-multus started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr 17 22:46:38.666: INFO: console-operator-db5d785db-r4qh6 from openshift-console-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container console-operator ready: true, restart count 1
Apr 17 22:46:38.666: INFO: thanos-querier-5c84fd9f68-4k28v from openshift-monitoring started at 2020-04-17 21:54:16 +0000 UTC (4 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container thanos-querier ready: true, restart count 0
Apr 17 22:46:38.666: INFO: prometheus-adapter-5cd8cd848d-j9qgt from openshift-monitoring started at 2020-04-17 21:54:15 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr 17 22:46:38.666: INFO: calico-typha-76b588567c-d6d9k from calico-system started at 2020-04-17 19:09:39 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container calico-typha ready: true, restart count 1
Apr 17 22:46:38.666: INFO: catalog-operator-5665d988d5-kvkfk from openshift-operator-lifecycle-manager started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container catalog-operator ready: true, restart count 0
Apr 17 22:46:38.666: INFO: dns-operator-74d97d58d-56295 from openshift-dns-operator started at 2020-04-17 19:10:34 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container dns-operator ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:46:38.666: INFO: openshift-service-catalog-controller-manager-operator-f954mtxlp from openshift-service-catalog-controller-manager-operator started at 2020-04-17 19:10:35 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container operator ready: true, restart count 1
Apr 17 22:46:38.666: INFO: kube-state-metrics-5bc9b987bc-hvx9k from openshift-monitoring started at 2020-04-17 19:11:29 +0000 UTC (3 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 17 22:46:38.666: INFO: downloads-64bb8b89c9-jxkw4 from openshift-console started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container download-server ready: true, restart count 0
Apr 17 22:46:38.666: INFO: ibm-keepalived-watcher-58jb7 from kube-system started at 2020-04-17 19:09:24 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr 17 22:46:38.666: INFO: service-ca-operator-5d59f48888-hk9zs from openshift-service-ca-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container operator ready: true, restart count 0
Apr 17 22:46:38.666: INFO: calico-kube-controllers-84d976f9ff-g9864 from calico-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr 17 22:46:38.666: INFO: openshift-service-catalog-apiserver-operator-76969db7f5-wpk2m from openshift-service-catalog-apiserver-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container operator ready: true, restart count 1
Apr 17 22:46:38.666: INFO: node-exporter-5qztm from openshift-monitoring started at 2020-04-17 19:11:29 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container node-exporter ready: true, restart count 0
Apr 17 22:46:38.666: INFO: dns-default-ffsfn from openshift-dns started at 2020-04-17 19:12:00 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container dns ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr 17 22:46:38.666: INFO: packageserver-6cfdb6b4bd-8kbgz from openshift-operator-lifecycle-manager started at 2020-04-17 21:54:17 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container packageserver ready: true, restart count 0
Apr 17 22:46:38.666: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-04-17 21:54:32 +0000 UTC (7 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container prometheus ready: true, restart count 1
Apr 17 22:46:38.666: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr 17 22:46:38.666: INFO: calico-node-tcc87 from calico-system started at 2020-04-17 19:09:40 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 22:46:38.666: INFO: cluster-storage-operator-56475d49d7-fsvdb from openshift-cluster-storage-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Apr 17 22:46:38.666: INFO: ibm-file-plugin-7cbd86d68f-9tm42 from kube-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Apr 17 22:46:38.666: INFO: openshift-state-metrics-6c465bc47f-p9qr6 from openshift-monitoring started at 2020-04-17 19:11:29 +0000 UTC (3 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Apr 17 22:46:38.666: INFO: ingress-operator-66cf4674d8-467pw from openshift-ingress-operator started at 2020-04-17 19:10:34 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container ingress-operator ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:46:38.666: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-04-17 19:16:48 +0000 UTC (3 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container alertmanager ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container config-reloader ready: true, restart count 0
Apr 17 22:46:38.666: INFO: sonobuoy from sonobuoy started at 2020-04-17 20:47:07 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 17 22:46:38.666: INFO: configmap-cabundle-injector-5cf6d9695-kfct9 from openshift-service-ca started at 2020-04-17 19:11:11 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Apr 17 22:46:38.666: INFO: router-default-8779c94d4-nbbtf from openshift-ingress started at 2020-04-17 21:54:15 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container router ready: true, restart count 0
Apr 17 22:46:38.666: INFO: network-operator-64f597f5d-8tg4w from openshift-network-operator started at 2020-04-17 19:09:28 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container network-operator ready: true, restart count 0
Apr 17 22:46:38.666: INFO: multus-wlttp from openshift-multus started at 2020-04-17 19:09:55 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container kube-multus ready: true, restart count 0
Apr 17 22:46:38.666: INFO: openshift-kube-proxy-bkf2z from openshift-kube-proxy started at 2020-04-17 19:10:02 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 22:46:38.666: INFO: cluster-image-registry-operator-7857d56744-c2pch from openshift-image-registry started at 2020-04-17 19:10:34 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Apr 17 22:46:38.666: INFO: ibm-cloud-provider-ip-158-176-122-172-6f57dbd74b-cdqd6 from ibm-system started at 2020-04-17 19:13:49 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container ibm-cloud-provider-ip-158-176-122-172 ready: true, restart count 0
Apr 17 22:46:38.666: INFO: ibmcloud-block-storage-driver-g7lth from kube-system started at 2020-04-17 19:09:28 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Apr 17 22:46:38.666: INFO: cluster-monitoring-operator-668c7b8d6d-khmrg from openshift-monitoring started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Apr 17 22:46:38.666: INFO: ibmcloud-block-storage-plugin-75f7cd767-lcw2w from kube-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Apr 17 22:46:38.666: INFO: service-serving-cert-signer-5968cc5d5c-m6hb2 from openshift-service-ca started at 2020-04-17 19:11:10 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Apr 17 22:46:38.666: INFO: apiservice-cabundle-injector-7bf8cddb9-kxhvx from openshift-service-ca started at 2020-04-17 19:11:11 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.666: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Apr 17 22:46:38.666: INFO: 
Logging pods the kubelet thinks is on node 10.72.119.74 before test
Apr 17 22:46:38.726: INFO: ibm-master-proxy-static-10.72.119.74 from kube-system started at 2020-04-17 19:10:42 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.726: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr 17 22:46:38.726: INFO: 	Container pause ready: true, restart count 0
Apr 17 22:46:38.726: INFO: node-exporter-949xg from openshift-monitoring started at 2020-04-17 19:11:30 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.726: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:46:38.726: INFO: 	Container node-exporter ready: true, restart count 0
Apr 17 22:46:38.726: INFO: dns-default-sdtp9 from openshift-dns started at 2020-04-17 21:54:52 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.726: INFO: 	Container dns ready: true, restart count 0
Apr 17 22:46:38.726: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr 17 22:46:38.726: INFO: sonobuoy-e2e-job-793c74f2197d4b50 from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.726: INFO: 	Container e2e ready: true, restart count 0
Apr 17 22:46:38.726: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 22:46:38.726: INFO: sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-tg59r from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.726: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 17 22:46:38.726: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 22:46:38.726: INFO: ibm-keepalived-watcher-hm7n9 from kube-system started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.726: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr 17 22:46:38.726: INFO: ibmcloud-block-storage-driver-6j6qk from kube-system started at 2020-04-17 19:10:53 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.726: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Apr 17 22:46:38.726: INFO: calico-typha-76b588567c-khgqc from calico-system started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.726: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 22:46:38.726: INFO: openshift-kube-proxy-gsgx2 from openshift-kube-proxy started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.726: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 22:46:38.726: INFO: node-ca-8cpqb from openshift-image-registry started at 2020-04-17 21:54:52 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.726: INFO: 	Container node-ca ready: true, restart count 0
Apr 17 22:46:38.726: INFO: calico-node-xm4xf from calico-system started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.726: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 22:46:38.726: INFO: multus-admission-controller-dj76f from openshift-multus started at 2020-04-17 21:54:52 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.726: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr 17 22:46:38.726: INFO: tuned-tsvx9 from openshift-cluster-node-tuning-operator started at 2020-04-17 19:11:12 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.726: INFO: 	Container tuned ready: true, restart count 0
Apr 17 22:46:38.726: INFO: multus-s62pc from openshift-multus started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.726: INFO: 	Container kube-multus ready: true, restart count 0
Apr 17 22:46:38.726: INFO: certified-operators-78496d489f-xfk5t from openshift-marketplace started at 2020-04-17 22:12:09 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.726: INFO: 	Container certified-operators ready: true, restart count 0
Apr 17 22:46:38.726: INFO: 
Logging pods the kubelet thinks is on node 10.72.119.98 before test
Apr 17 22:46:38.912: INFO: ibmcloud-block-storage-driver-6jgj5 from kube-system started at 2020-04-17 19:10:47 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Apr 17 22:46:38.912: INFO: node-ca-wr66h from openshift-image-registry started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container node-ca ready: true, restart count 0
Apr 17 22:46:38.912: INFO: ibm-cloud-provider-ip-158-176-122-172-6f57dbd74b-bpkpw from ibm-system started at 2020-04-17 19:13:51 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container ibm-cloud-provider-ip-158-176-122-172 ready: true, restart count 0
Apr 17 22:46:38.912: INFO: console-5b98d99db9-2z7vw from openshift-console started at 2020-04-17 19:12:51 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container console ready: true, restart count 0
Apr 17 22:46:38.912: INFO: cluster-samples-operator-6d4b68977f-5q62h from openshift-cluster-samples-operator started at 2020-04-17 19:11:57 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Apr 17 22:46:38.912: INFO: multus-admission-controller-72gwb from openshift-multus started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr 17 22:46:38.912: INFO: community-operators-5574589d55-swkx4 from openshift-marketplace started at 2020-04-17 19:12:21 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container community-operators ready: true, restart count 0
Apr 17 22:46:38.912: INFO: redhat-operators-7b557fbbc4-pq5mj from openshift-marketplace started at 2020-04-17 19:12:24 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container redhat-operators ready: true, restart count 0
Apr 17 22:46:38.912: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-04-17 19:17:47 +0000 UTC (7 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container prometheus ready: true, restart count 1
Apr 17 22:46:38.912: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr 17 22:46:38.912: INFO: ibm-master-proxy-static-10.72.119.98 from kube-system started at 2020-04-17 19:10:37 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container pause ready: true, restart count 0
Apr 17 22:46:38.912: INFO: node-exporter-lh6vl from openshift-monitoring started at 2020-04-17 19:11:30 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container node-exporter ready: true, restart count 0
Apr 17 22:46:38.912: INFO: prometheus-operator-75c7889c9b-47f2p from openshift-monitoring started at 2020-04-17 19:16:29 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container prometheus-operator ready: true, restart count 0
Apr 17 22:46:38.912: INFO: telemeter-client-5f4f6fb5fc-rwg74 from openshift-monitoring started at 2020-04-17 19:16:40 +0000 UTC (3 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container reload ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container telemeter-client ready: true, restart count 0
Apr 17 22:46:38.912: INFO: sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-kbntn from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Apr 17 22:46:38.912: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 22:46:38.912: INFO: ibm-keepalived-watcher-h2bpf from kube-system started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr 17 22:46:38.912: INFO: multus-lddbc from openshift-multus started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container kube-multus ready: true, restart count 0
Apr 17 22:46:38.912: INFO: calico-typha-76b588567c-kfx7z from calico-system started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 22:46:38.912: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-04-17 19:17:09 +0000 UTC (3 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container alertmanager ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container config-reloader ready: true, restart count 0
Apr 17 22:46:38.912: INFO: thanos-querier-5c84fd9f68-vbdc4 from openshift-monitoring started at 2020-04-17 19:17:49 +0000 UTC (4 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container thanos-querier ready: true, restart count 0
Apr 17 22:46:38.912: INFO: router-default-8779c94d4-mdtbq from openshift-ingress started at 2020-04-17 19:12:06 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container router ready: true, restart count 0
Apr 17 22:46:38.912: INFO: image-registry-5655cc46bf-gzrxj from openshift-image-registry started at 2020-04-17 19:14:35 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container registry ready: true, restart count 0
Apr 17 22:46:38.912: INFO: grafana-6b4f8c85c5-5n72k from openshift-monitoring started at 2020-04-17 21:54:16 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container grafana ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container grafana-proxy ready: true, restart count 0
Apr 17 22:46:38.912: INFO: calico-node-6sr5x from calico-system started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 22:46:38.912: INFO: openshift-kube-proxy-7dshq from openshift-kube-proxy started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 22:46:38.912: INFO: prometheus-adapter-5cd8cd848d-xspwf from openshift-monitoring started at 2020-04-17 19:16:43 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr 17 22:46:38.912: INFO: packageserver-6cfdb6b4bd-lnzh2 from openshift-operator-lifecycle-manager started at 2020-04-17 21:54:28 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container packageserver ready: true, restart count 0
Apr 17 22:46:38.912: INFO: tuned-sz2br from openshift-cluster-node-tuning-operator started at 2020-04-17 19:11:12 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container tuned ready: true, restart count 0
Apr 17 22:46:38.912: INFO: dns-default-z6b7d from openshift-dns started at 2020-04-17 19:12:00 +0000 UTC (2 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container dns ready: true, restart count 0
Apr 17 22:46:38.912: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr 17 22:46:38.912: INFO: vpn-5d7df69b48-5mp4n from kube-system started at 2020-04-17 21:54:15 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container vpn ready: true, restart count 0
Apr 17 22:46:38.912: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-04-17 19:14:09 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Apr 17 22:46:38.912: INFO: registry-pvc-permissions-d9dpb from openshift-image-registry started at 2020-04-17 19:14:35 +0000 UTC (1 container statuses recorded)
Apr 17 22:46:38.912: INFO: 	Container pvc-permissions ready: false, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-333abb09-8d59-466f-b6bb-1ade62994d17 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-333abb09-8d59-466f-b6bb-1ade62994d17 off the node 10.72.119.74
STEP: verifying the node doesn't have the label kubernetes.io/e2e-333abb09-8d59-466f-b6bb-1ade62994d17
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:46:51.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9652" for this suite.
Apr 17 22:47:05.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:47:07.635: INFO: namespace sched-pred-9652 deletion completed in 16.216755964s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:29.403 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:47:07.637: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-7e287a01-aafe-43b3-a793-0c3ecbee364d
STEP: Creating a pod to test consume configMaps
Apr 17 22:47:07.852: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a8132e98-ca95-4920-b366-480ff4e044c0" in namespace "projected-2000" to be "success or failure"
Apr 17 22:47:07.864: INFO: Pod "pod-projected-configmaps-a8132e98-ca95-4920-b366-480ff4e044c0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.950338ms
Apr 17 22:47:09.877: INFO: Pod "pod-projected-configmaps-a8132e98-ca95-4920-b366-480ff4e044c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024923958s
STEP: Saw pod success
Apr 17 22:47:09.877: INFO: Pod "pod-projected-configmaps-a8132e98-ca95-4920-b366-480ff4e044c0" satisfied condition "success or failure"
Apr 17 22:47:09.889: INFO: Trying to get logs from node 10.72.119.74 pod pod-projected-configmaps-a8132e98-ca95-4920-b366-480ff4e044c0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr 17 22:47:09.950: INFO: Waiting for pod pod-projected-configmaps-a8132e98-ca95-4920-b366-480ff4e044c0 to disappear
Apr 17 22:47:09.960: INFO: Pod pod-projected-configmaps-a8132e98-ca95-4920-b366-480ff4e044c0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:47:09.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2000" for this suite.
Apr 17 22:47:18.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:47:20.197: INFO: namespace projected-2000 deletion completed in 10.213877859s

â€¢ [SLOW TEST:12.560 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:47:20.197: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:47:20.340: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 17 22:47:31.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-8617 create -f -'
Apr 17 22:47:32.139: INFO: stderr: ""
Apr 17 22:47:32.139: INFO: stdout: "e2e-test-crd-publish-openapi-9774-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 17 22:47:32.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-8617 delete e2e-test-crd-publish-openapi-9774-crds test-cr'
Apr 17 22:47:32.319: INFO: stderr: ""
Apr 17 22:47:32.319: INFO: stdout: "e2e-test-crd-publish-openapi-9774-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Apr 17 22:47:32.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-8617 apply -f -'
Apr 17 22:47:32.665: INFO: stderr: ""
Apr 17 22:47:32.665: INFO: stdout: "e2e-test-crd-publish-openapi-9774-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 17 22:47:32.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-8617 delete e2e-test-crd-publish-openapi-9774-crds test-cr'
Apr 17 22:47:32.836: INFO: stderr: ""
Apr 17 22:47:32.836: INFO: stdout: "e2e-test-crd-publish-openapi-9774-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr 17 22:47:32.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 explain e2e-test-crd-publish-openapi-9774-crds'
Apr 17 22:47:33.456: INFO: stderr: ""
Apr 17 22:47:33.456: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-9774-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:47:43.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8617" for this suite.
Apr 17 22:47:51.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:47:53.466: INFO: namespace crd-publish-openapi-8617 deletion completed in 10.265463856s

â€¢ [SLOW TEST:33.269 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:47:53.466: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 22:47:53.695: INFO: Waiting up to 5m0s for pod "downwardapi-volume-22dcba37-c75b-4dca-baeb-d7a6ff9eac49" in namespace "projected-3226" to be "success or failure"
Apr 17 22:47:53.714: INFO: Pod "downwardapi-volume-22dcba37-c75b-4dca-baeb-d7a6ff9eac49": Phase="Pending", Reason="", readiness=false. Elapsed: 18.609956ms
Apr 17 22:47:55.726: INFO: Pod "downwardapi-volume-22dcba37-c75b-4dca-baeb-d7a6ff9eac49": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.03060417s
STEP: Saw pod success
Apr 17 22:47:55.726: INFO: Pod "downwardapi-volume-22dcba37-c75b-4dca-baeb-d7a6ff9eac49" satisfied condition "success or failure"
Apr 17 22:47:55.738: INFO: Trying to get logs from node 10.72.119.74 pod downwardapi-volume-22dcba37-c75b-4dca-baeb-d7a6ff9eac49 container client-container: <nil>
STEP: delete the pod
Apr 17 22:47:55.797: INFO: Waiting for pod downwardapi-volume-22dcba37-c75b-4dca-baeb-d7a6ff9eac49 to disappear
Apr 17 22:47:55.809: INFO: Pod downwardapi-volume-22dcba37-c75b-4dca-baeb-d7a6ff9eac49 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:47:55.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3226" for this suite.
Apr 17 22:48:03.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:48:05.991: INFO: namespace projected-3226 deletion completed in 10.164071286s

â€¢ [SLOW TEST:12.524 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:48:05.991: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3807.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3807.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3807.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3807.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3807.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3807.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3807.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3807.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3807.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3807.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3807.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3807.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3807.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 62.15.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.15.62_udp@PTR;check="$$(dig +tcp +noall +answer +search 62.15.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.15.62_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3807.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3807.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3807.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3807.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3807.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3807.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3807.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3807.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3807.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3807.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3807.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3807.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3807.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 62.15.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.15.62_udp@PTR;check="$$(dig +tcp +noall +answer +search 62.15.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.15.62_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 17 22:48:10.367: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3807.svc.cluster.local from pod dns-3807/dns-test-210fc8c0-72af-4569-a5f5-e0747ae46f0b: the server could not find the requested resource (get pods dns-test-210fc8c0-72af-4569-a5f5-e0747ae46f0b)
Apr 17 22:48:10.391: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3807.svc.cluster.local from pod dns-3807/dns-test-210fc8c0-72af-4569-a5f5-e0747ae46f0b: the server could not find the requested resource (get pods dns-test-210fc8c0-72af-4569-a5f5-e0747ae46f0b)
Apr 17 22:48:10.411: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3807.svc.cluster.local from pod dns-3807/dns-test-210fc8c0-72af-4569-a5f5-e0747ae46f0b: the server could not find the requested resource (get pods dns-test-210fc8c0-72af-4569-a5f5-e0747ae46f0b)
Apr 17 22:48:10.710: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3807.svc.cluster.local from pod dns-3807/dns-test-210fc8c0-72af-4569-a5f5-e0747ae46f0b: the server could not find the requested resource (get pods dns-test-210fc8c0-72af-4569-a5f5-e0747ae46f0b)
Apr 17 22:48:10.737: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3807.svc.cluster.local from pod dns-3807/dns-test-210fc8c0-72af-4569-a5f5-e0747ae46f0b: the server could not find the requested resource (get pods dns-test-210fc8c0-72af-4569-a5f5-e0747ae46f0b)
Apr 17 22:48:10.873: INFO: Lookups using dns-3807/dns-test-210fc8c0-72af-4569-a5f5-e0747ae46f0b failed for: [wheezy_tcp@dns-test-service.dns-3807.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3807.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3807.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3807.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3807.svc.cluster.local]

Apr 17 22:48:16.293: INFO: DNS probes using dns-3807/dns-test-210fc8c0-72af-4569-a5f5-e0747ae46f0b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:48:16.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3807" for this suite.
Apr 17 22:48:24.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:48:26.684: INFO: namespace dns-3807 deletion completed in 10.176580364s

â€¢ [SLOW TEST:20.693 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:48:26.685: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 22:48:26.866: INFO: Waiting up to 5m0s for pod "downwardapi-volume-756df4ae-9eef-47d9-b176-1f7138a5e671" in namespace "projected-345" to be "success or failure"
Apr 17 22:48:26.883: INFO: Pod "downwardapi-volume-756df4ae-9eef-47d9-b176-1f7138a5e671": Phase="Pending", Reason="", readiness=false. Elapsed: 16.07341ms
Apr 17 22:48:28.896: INFO: Pod "downwardapi-volume-756df4ae-9eef-47d9-b176-1f7138a5e671": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029869554s
STEP: Saw pod success
Apr 17 22:48:28.897: INFO: Pod "downwardapi-volume-756df4ae-9eef-47d9-b176-1f7138a5e671" satisfied condition "success or failure"
Apr 17 22:48:28.915: INFO: Trying to get logs from node 10.72.119.74 pod downwardapi-volume-756df4ae-9eef-47d9-b176-1f7138a5e671 container client-container: <nil>
STEP: delete the pod
Apr 17 22:48:28.980: INFO: Waiting for pod downwardapi-volume-756df4ae-9eef-47d9-b176-1f7138a5e671 to disappear
Apr 17 22:48:28.990: INFO: Pod downwardapi-volume-756df4ae-9eef-47d9-b176-1f7138a5e671 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:48:28.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-345" for this suite.
Apr 17 22:48:37.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:48:39.281: INFO: namespace projected-345 deletion completed in 10.271392076s

â€¢ [SLOW TEST:12.597 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:48:39.281: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr 17 22:48:39.623: INFO: Number of nodes with available pods: 0
Apr 17 22:48:39.623: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 22:48:40.666: INFO: Number of nodes with available pods: 0
Apr 17 22:48:40.666: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 22:48:41.655: INFO: Number of nodes with available pods: 1
Apr 17 22:48:41.655: INFO: Node 10.72.119.72 is running more than one daemon pod
Apr 17 22:48:42.652: INFO: Number of nodes with available pods: 3
Apr 17 22:48:42.652: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr 17 22:48:42.732: INFO: Number of nodes with available pods: 2
Apr 17 22:48:42.732: INFO: Node 10.72.119.74 is running more than one daemon pod
Apr 17 22:48:43.766: INFO: Number of nodes with available pods: 2
Apr 17 22:48:43.766: INFO: Node 10.72.119.74 is running more than one daemon pod
Apr 17 22:48:44.762: INFO: Number of nodes with available pods: 3
Apr 17 22:48:44.762: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6457, will wait for the garbage collector to delete the pods
Apr 17 22:48:44.875: INFO: Deleting DaemonSet.extensions daemon-set took: 28.725363ms
Apr 17 22:48:44.975: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.29745ms
Apr 17 22:48:57.892: INFO: Number of nodes with available pods: 0
Apr 17 22:48:57.892: INFO: Number of running nodes: 0, number of available pods: 0
Apr 17 22:48:57.907: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6457/daemonsets","resourceVersion":"96341"},"items":null}

Apr 17 22:48:57.919: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6457/pods","resourceVersion":"96341"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:48:57.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6457" for this suite.
Apr 17 22:49:06.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:49:08.263: INFO: namespace daemonsets-6457 deletion completed in 10.264616748s

â€¢ [SLOW TEST:28.982 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:49:08.268: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr 17 22:49:08.454: INFO: Waiting up to 5m0s for pod "pod-09cd3ce5-6302-4bb9-93ee-6f953a346834" in namespace "emptydir-9414" to be "success or failure"
Apr 17 22:49:08.465: INFO: Pod "pod-09cd3ce5-6302-4bb9-93ee-6f953a346834": Phase="Pending", Reason="", readiness=false. Elapsed: 11.445258ms
Apr 17 22:49:10.478: INFO: Pod "pod-09cd3ce5-6302-4bb9-93ee-6f953a346834": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024332568s
Apr 17 22:49:12.491: INFO: Pod "pod-09cd3ce5-6302-4bb9-93ee-6f953a346834": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036700302s
STEP: Saw pod success
Apr 17 22:49:12.491: INFO: Pod "pod-09cd3ce5-6302-4bb9-93ee-6f953a346834" satisfied condition "success or failure"
Apr 17 22:49:12.504: INFO: Trying to get logs from node 10.72.119.74 pod pod-09cd3ce5-6302-4bb9-93ee-6f953a346834 container test-container: <nil>
STEP: delete the pod
Apr 17 22:49:12.566: INFO: Waiting for pod pod-09cd3ce5-6302-4bb9-93ee-6f953a346834 to disappear
Apr 17 22:49:12.578: INFO: Pod pod-09cd3ce5-6302-4bb9-93ee-6f953a346834 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:49:12.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9414" for this suite.
Apr 17 22:49:20.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:49:22.857: INFO: namespace emptydir-9414 deletion completed in 10.260407815s

â€¢ [SLOW TEST:14.589 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:49:22.860: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 17 22:49:23.124: INFO: Waiting up to 5m0s for pod "pod-dc7cfe3c-4892-4b00-a46c-63d0d6638ab9" in namespace "emptydir-8864" to be "success or failure"
Apr 17 22:49:23.147: INFO: Pod "pod-dc7cfe3c-4892-4b00-a46c-63d0d6638ab9": Phase="Pending", Reason="", readiness=false. Elapsed: 23.091698ms
Apr 17 22:49:25.159: INFO: Pod "pod-dc7cfe3c-4892-4b00-a46c-63d0d6638ab9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.035490541s
STEP: Saw pod success
Apr 17 22:49:25.159: INFO: Pod "pod-dc7cfe3c-4892-4b00-a46c-63d0d6638ab9" satisfied condition "success or failure"
Apr 17 22:49:25.170: INFO: Trying to get logs from node 10.72.119.74 pod pod-dc7cfe3c-4892-4b00-a46c-63d0d6638ab9 container test-container: <nil>
STEP: delete the pod
Apr 17 22:49:25.240: INFO: Waiting for pod pod-dc7cfe3c-4892-4b00-a46c-63d0d6638ab9 to disappear
Apr 17 22:49:25.252: INFO: Pod pod-dc7cfe3c-4892-4b00-a46c-63d0d6638ab9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:49:25.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8864" for this suite.
Apr 17 22:49:33.316: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:49:35.489: INFO: namespace emptydir-8864 deletion completed in 10.216150133s

â€¢ [SLOW TEST:12.629 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:49:35.489: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr 17 22:49:38.858: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:49:38.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1452" for this suite.
Apr 17 22:50:10.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:50:13.188: INFO: namespace replicaset-1452 deletion completed in 34.237145005s

â€¢ [SLOW TEST:37.699 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:50:13.189: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-7220
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-7220
Apr 17 22:50:13.446: INFO: Found 0 stateful pods, waiting for 1
Apr 17 22:50:23.463: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Apr 17 22:50:23.558: INFO: Deleting all statefulset in ns statefulset-7220
Apr 17 22:50:23.576: INFO: Scaling statefulset ss to 0
Apr 17 22:50:43.648: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 22:50:43.665: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:50:43.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7220" for this suite.
Apr 17 22:50:51.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:50:53.922: INFO: namespace statefulset-7220 deletion completed in 10.163513251s

â€¢ [SLOW TEST:40.734 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:50:53.922: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6387.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6387.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 17 22:50:56.314: INFO: DNS probes using dns-6387/dns-test-15ab7d9d-313a-4b15-8bab-5e24655df296 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:50:56.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6387" for this suite.
Apr 17 22:51:04.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:51:06.560: INFO: namespace dns-6387 deletion completed in 10.18465478s

â€¢ [SLOW TEST:12.638 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:51:06.562: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Apr 17 22:51:06.704: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 17 22:52:06.890: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:52:06.906: INFO: Starting informer...
STEP: Starting pod...
Apr 17 22:52:07.171: INFO: Pod is running on 10.72.119.74. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Apr 17 22:52:07.216: INFO: Pod wasn't evicted. Proceeding
Apr 17 22:52:07.216: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Apr 17 22:53:22.288: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:53:22.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-8479" for this suite.
Apr 17 22:53:36.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:53:38.474: INFO: namespace taint-single-pod-8479 deletion completed in 16.166793427s

â€¢ [SLOW TEST:151.913 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:53:38.476: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9706.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9706.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9706.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9706.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9706.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9706.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 17 22:53:42.931: INFO: DNS probes using dns-9706/dns-test-416692b6-0d24-4fa4-bd89-46208a698967 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:53:43.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9706" for this suite.
Apr 17 22:53:51.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:53:53.273: INFO: namespace dns-9706 deletion completed in 10.197681031s

â€¢ [SLOW TEST:14.797 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:53:53.273: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 22:53:54.700: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 17 22:53:56.741: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722760834, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722760834, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722760834, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722760834, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 22:53:59.785: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:54:00.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2957" for this suite.
Apr 17 22:54:08.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:54:10.901: INFO: namespace webhook-2957 deletion completed in 10.18753899s
STEP: Destroying namespace "webhook-2957-markers" for this suite.
Apr 17 22:54:18.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:54:21.374: INFO: namespace webhook-2957-markers deletion completed in 10.473425694s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:28.180 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:54:21.454: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Apr 17 22:54:31.767: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0417 22:54:31.767724      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 17 22:54:31.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6203" for this suite.
Apr 17 22:54:39.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:54:42.005: INFO: namespace gc-6203 deletion completed in 10.217652049s

â€¢ [SLOW TEST:20.551 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:54:42.006: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-b994df56-c7f9-4e2d-b2ea-bda11a8a7a53 in namespace container-probe-7563
Apr 17 22:54:46.256: INFO: Started pod busybox-b994df56-c7f9-4e2d-b2ea-bda11a8a7a53 in namespace container-probe-7563
STEP: checking the pod's current state and verifying that restartCount is present
Apr 17 22:54:46.267: INFO: Initial restart count of pod busybox-b994df56-c7f9-4e2d-b2ea-bda11a8a7a53 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:58:47.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7563" for this suite.
Apr 17 22:58:56.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:58:58.174: INFO: namespace container-probe-7563 deletion completed in 10.184419283s

â€¢ [SLOW TEST:256.169 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:58:58.178: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 22:59:18.472: INFO: Container started at 2020-04-17 22:58:59 +0000 UTC, pod became ready at 2020-04-17 22:59:18 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 22:59:18.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6291" for this suite.
Apr 17 22:59:50.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 22:59:52.861: INFO: namespace container-probe-6291 deletion completed in 34.36484849s

â€¢ [SLOW TEST:54.684 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 22:59:52.862: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Apr 17 22:59:53.011: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 17 22:59:53.068: INFO: Waiting for terminating namespaces to be deleted...
Apr 17 22:59:53.090: INFO: 
Logging pods the kubelet thinks is on node 10.72.119.72 before test
Apr 17 22:59:53.266: INFO: downloads-64bb8b89c9-sccw2 from openshift-console started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container download-server ready: true, restart count 0
Apr 17 22:59:53.266: INFO: tuned-xrlm4 from openshift-cluster-node-tuning-operator started at 2020-04-17 19:11:12 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container tuned ready: true, restart count 0
Apr 17 22:59:53.266: INFO: console-5b98d99db9-lb2kc from openshift-console started at 2020-04-17 21:54:15 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container console ready: true, restart count 0
Apr 17 22:59:53.266: INFO: olm-operator-7bd9dc9457-2wn6n from openshift-operator-lifecycle-manager started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container olm-operator ready: true, restart count 0
Apr 17 22:59:53.266: INFO: cluster-node-tuning-operator-77bdbd4f-79qvb from openshift-cluster-node-tuning-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Apr 17 22:59:53.266: INFO: ibm-master-proxy-static-10.72.119.72 from kube-system started at 2020-04-17 19:09:18 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container pause ready: true, restart count 0
Apr 17 22:59:53.266: INFO: marketplace-operator-699fb8f5d-5nrcq from openshift-marketplace started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container marketplace-operator ready: true, restart count 0
Apr 17 22:59:53.266: INFO: ibm-storage-watcher-d9c7cf586-f72vk from kube-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Apr 17 22:59:53.266: INFO: node-ca-x5gt7 from openshift-image-registry started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container node-ca ready: true, restart count 0
Apr 17 22:59:53.266: INFO: sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-l8m9b from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Apr 17 22:59:53.266: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 22:59:53.266: INFO: alertmanager-main-1 from openshift-monitoring started at 2020-04-17 21:54:22 +0000 UTC (3 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container alertmanager ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container config-reloader ready: true, restart count 0
Apr 17 22:59:53.266: INFO: tigera-operator-df8f4c87c-c7sjz from tigera-operator started at 2020-04-17 19:09:24 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container tigera-operator ready: true, restart count 0
Apr 17 22:59:53.266: INFO: multus-admission-controller-2jxfm from openshift-multus started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr 17 22:59:53.266: INFO: console-operator-db5d785db-r4qh6 from openshift-console-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container console-operator ready: true, restart count 1
Apr 17 22:59:53.266: INFO: thanos-querier-5c84fd9f68-4k28v from openshift-monitoring started at 2020-04-17 21:54:16 +0000 UTC (4 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container thanos-querier ready: true, restart count 0
Apr 17 22:59:53.266: INFO: catalog-operator-5665d988d5-kvkfk from openshift-operator-lifecycle-manager started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container catalog-operator ready: true, restart count 0
Apr 17 22:59:53.266: INFO: dns-operator-74d97d58d-56295 from openshift-dns-operator started at 2020-04-17 19:10:34 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container dns-operator ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:59:53.266: INFO: openshift-service-catalog-controller-manager-operator-f954mtxlp from openshift-service-catalog-controller-manager-operator started at 2020-04-17 19:10:35 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container operator ready: true, restart count 1
Apr 17 22:59:53.266: INFO: kube-state-metrics-5bc9b987bc-hvx9k from openshift-monitoring started at 2020-04-17 19:11:29 +0000 UTC (3 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container kube-state-metrics ready: true, restart count 0
Apr 17 22:59:53.266: INFO: prometheus-adapter-5cd8cd848d-j9qgt from openshift-monitoring started at 2020-04-17 21:54:15 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr 17 22:59:53.266: INFO: calico-typha-76b588567c-d6d9k from calico-system started at 2020-04-17 19:09:39 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container calico-typha ready: true, restart count 1
Apr 17 22:59:53.266: INFO: downloads-64bb8b89c9-jxkw4 from openshift-console started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container download-server ready: true, restart count 0
Apr 17 22:59:53.266: INFO: service-ca-operator-5d59f48888-hk9zs from openshift-service-ca-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container operator ready: true, restart count 0
Apr 17 22:59:53.266: INFO: calico-kube-controllers-84d976f9ff-g9864 from calico-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Apr 17 22:59:53.266: INFO: ibm-keepalived-watcher-58jb7 from kube-system started at 2020-04-17 19:09:24 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr 17 22:59:53.266: INFO: node-exporter-5qztm from openshift-monitoring started at 2020-04-17 19:11:29 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container node-exporter ready: true, restart count 0
Apr 17 22:59:53.266: INFO: dns-default-ffsfn from openshift-dns started at 2020-04-17 19:12:00 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container dns ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr 17 22:59:53.266: INFO: packageserver-6cfdb6b4bd-8kbgz from openshift-operator-lifecycle-manager started at 2020-04-17 21:54:17 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container packageserver ready: true, restart count 0
Apr 17 22:59:53.266: INFO: prometheus-k8s-0 from openshift-monitoring started at 2020-04-17 21:54:32 +0000 UTC (7 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container prometheus ready: true, restart count 1
Apr 17 22:59:53.266: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr 17 22:59:53.266: INFO: openshift-service-catalog-apiserver-operator-76969db7f5-wpk2m from openshift-service-catalog-apiserver-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container operator ready: true, restart count 1
Apr 17 22:59:53.266: INFO: cluster-storage-operator-56475d49d7-fsvdb from openshift-cluster-storage-operator started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container cluster-storage-operator ready: true, restart count 0
Apr 17 22:59:53.266: INFO: ibm-file-plugin-7cbd86d68f-9tm42 from kube-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Apr 17 22:59:53.266: INFO: openshift-state-metrics-6c465bc47f-p9qr6 from openshift-monitoring started at 2020-04-17 19:11:29 +0000 UTC (3 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Apr 17 22:59:53.266: INFO: calico-node-tcc87 from calico-system started at 2020-04-17 19:09:40 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 22:59:53.266: INFO: alertmanager-main-2 from openshift-monitoring started at 2020-04-17 19:16:48 +0000 UTC (3 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container alertmanager ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container config-reloader ready: true, restart count 0
Apr 17 22:59:53.266: INFO: ingress-operator-66cf4674d8-467pw from openshift-ingress-operator started at 2020-04-17 19:10:34 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container ingress-operator ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:59:53.266: INFO: configmap-cabundle-injector-5cf6d9695-kfct9 from openshift-service-ca started at 2020-04-17 19:11:11 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container configmap-cabundle-injector-controller ready: true, restart count 0
Apr 17 22:59:53.266: INFO: router-default-8779c94d4-nbbtf from openshift-ingress started at 2020-04-17 21:54:15 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container router ready: true, restart count 0
Apr 17 22:59:53.266: INFO: sonobuoy from sonobuoy started at 2020-04-17 20:47:07 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 17 22:59:53.266: INFO: multus-wlttp from openshift-multus started at 2020-04-17 19:09:55 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container kube-multus ready: true, restart count 0
Apr 17 22:59:53.266: INFO: openshift-kube-proxy-bkf2z from openshift-kube-proxy started at 2020-04-17 19:10:02 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 22:59:53.266: INFO: cluster-image-registry-operator-7857d56744-c2pch from openshift-image-registry started at 2020-04-17 19:10:34 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 	Container cluster-image-registry-operator-watch ready: true, restart count 0
Apr 17 22:59:53.266: INFO: ibm-cloud-provider-ip-158-176-122-172-6f57dbd74b-cdqd6 from ibm-system started at 2020-04-17 19:13:49 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container ibm-cloud-provider-ip-158-176-122-172 ready: true, restart count 0
Apr 17 22:59:53.266: INFO: network-operator-64f597f5d-8tg4w from openshift-network-operator started at 2020-04-17 19:09:28 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container network-operator ready: true, restart count 0
Apr 17 22:59:53.266: INFO: cluster-monitoring-operator-668c7b8d6d-khmrg from openshift-monitoring started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Apr 17 22:59:53.266: INFO: ibmcloud-block-storage-plugin-75f7cd767-lcw2w from kube-system started at 2020-04-17 19:10:34 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Apr 17 22:59:53.266: INFO: service-serving-cert-signer-5968cc5d5c-m6hb2 from openshift-service-ca started at 2020-04-17 19:11:10 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container service-serving-cert-signer-controller ready: true, restart count 0
Apr 17 22:59:53.266: INFO: apiservice-cabundle-injector-7bf8cddb9-kxhvx from openshift-service-ca started at 2020-04-17 19:11:11 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container apiservice-cabundle-injector-controller ready: true, restart count 0
Apr 17 22:59:53.266: INFO: ibmcloud-block-storage-driver-g7lth from kube-system started at 2020-04-17 19:09:28 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.266: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Apr 17 22:59:53.266: INFO: 
Logging pods the kubelet thinks is on node 10.72.119.74 before test
Apr 17 22:59:53.412: INFO: node-ca-82xdj from openshift-image-registry started at 2020-04-17 22:52:12 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.412: INFO: 	Container node-ca ready: true, restart count 0
Apr 17 22:59:53.412: INFO: ibm-master-proxy-static-10.72.119.74 from kube-system started at 2020-04-17 19:10:42 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.412: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr 17 22:59:53.412: INFO: 	Container pause ready: true, restart count 0
Apr 17 22:59:53.412: INFO: node-exporter-949xg from openshift-monitoring started at 2020-04-17 19:11:30 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.412: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:59:53.412: INFO: 	Container node-exporter ready: true, restart count 0
Apr 17 22:59:53.412: INFO: sonobuoy-e2e-job-793c74f2197d4b50 from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.412: INFO: 	Container e2e ready: true, restart count 0
Apr 17 22:59:53.412: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 22:59:53.412: INFO: ibm-keepalived-watcher-hm7n9 from kube-system started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.412: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr 17 22:59:53.412: INFO: ibmcloud-block-storage-driver-6j6qk from kube-system started at 2020-04-17 19:10:53 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.412: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Apr 17 22:59:53.412: INFO: calico-typha-76b588567c-khgqc from calico-system started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.412: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 22:59:53.412: INFO: sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-tg59r from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.412: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Apr 17 22:59:53.412: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 22:59:53.412: INFO: openshift-kube-proxy-gsgx2 from openshift-kube-proxy started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.412: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 22:59:53.412: INFO: calico-node-xm4xf from calico-system started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.412: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 22:59:53.412: INFO: multus-admission-controller-hscnd from openshift-multus started at 2020-04-17 22:52:42 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.412: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr 17 22:59:53.412: INFO: tuned-tsvx9 from openshift-cluster-node-tuning-operator started at 2020-04-17 19:11:12 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.412: INFO: 	Container tuned ready: true, restart count 0
Apr 17 22:59:53.412: INFO: certified-operators-78496d489f-qv7t4 from openshift-marketplace started at 2020-04-17 22:52:07 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.412: INFO: 	Container certified-operators ready: true, restart count 0
Apr 17 22:59:53.412: INFO: multus-s62pc from openshift-multus started at 2020-04-17 19:10:48 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.412: INFO: 	Container kube-multus ready: true, restart count 0
Apr 17 22:59:53.412: INFO: dns-default-2kj2m from openshift-dns started at 2020-04-17 22:52:12 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.412: INFO: 	Container dns ready: true, restart count 0
Apr 17 22:59:53.412: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr 17 22:59:53.412: INFO: 
Logging pods the kubelet thinks is on node 10.72.119.98 before test
Apr 17 22:59:53.533: INFO: telemeter-client-5f4f6fb5fc-rwg74 from openshift-monitoring started at 2020-04-17 19:16:40 +0000 UTC (3 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:59:53.533: INFO: 	Container reload ready: true, restart count 0
Apr 17 22:59:53.533: INFO: 	Container telemeter-client ready: true, restart count 0
Apr 17 22:59:53.533: INFO: sonobuoy-systemd-logs-daemon-set-ad50cb2579f2481d-kbntn from sonobuoy started at 2020-04-17 20:47:17 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Apr 17 22:59:53.533: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 22:59:53.533: INFO: alertmanager-main-0 from openshift-monitoring started at 2020-04-17 19:17:09 +0000 UTC (3 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container alertmanager ready: true, restart count 0
Apr 17 22:59:53.533: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Apr 17 22:59:53.533: INFO: 	Container config-reloader ready: true, restart count 0
Apr 17 22:59:53.533: INFO: thanos-querier-5c84fd9f68-vbdc4 from openshift-monitoring started at 2020-04-17 19:17:49 +0000 UTC (4 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:59:53.533: INFO: 	Container oauth-proxy ready: true, restart count 0
Apr 17 22:59:53.533: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 22:59:53.533: INFO: 	Container thanos-querier ready: true, restart count 0
Apr 17 22:59:53.533: INFO: ibm-keepalived-watcher-h2bpf from kube-system started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container keepalived-watcher ready: true, restart count 0
Apr 17 22:59:53.533: INFO: multus-lddbc from openshift-multus started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container kube-multus ready: true, restart count 0
Apr 17 22:59:53.533: INFO: calico-typha-76b588567c-kfx7z from calico-system started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 22:59:53.533: INFO: grafana-6b4f8c85c5-5n72k from openshift-monitoring started at 2020-04-17 21:54:16 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container grafana ready: true, restart count 0
Apr 17 22:59:53.533: INFO: 	Container grafana-proxy ready: true, restart count 0
Apr 17 22:59:53.533: INFO: router-default-8779c94d4-mdtbq from openshift-ingress started at 2020-04-17 19:12:06 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container router ready: true, restart count 0
Apr 17 22:59:53.533: INFO: image-registry-5655cc46bf-gzrxj from openshift-image-registry started at 2020-04-17 19:14:35 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container registry ready: true, restart count 0
Apr 17 22:59:53.533: INFO: packageserver-6cfdb6b4bd-lnzh2 from openshift-operator-lifecycle-manager started at 2020-04-17 21:54:28 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container packageserver ready: true, restart count 0
Apr 17 22:59:53.533: INFO: calico-node-6sr5x from calico-system started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 22:59:53.533: INFO: openshift-kube-proxy-7dshq from openshift-kube-proxy started at 2020-04-17 19:10:40 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 22:59:53.533: INFO: prometheus-adapter-5cd8cd848d-xspwf from openshift-monitoring started at 2020-04-17 19:16:43 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container prometheus-adapter ready: true, restart count 0
Apr 17 22:59:53.533: INFO: test-k8s-e2e-pvg-master-verification from default started at 2020-04-17 19:14:09 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Apr 17 22:59:53.533: INFO: registry-pvc-permissions-d9dpb from openshift-image-registry started at 2020-04-17 19:14:35 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container pvc-permissions ready: false, restart count 0
Apr 17 22:59:53.533: INFO: tuned-sz2br from openshift-cluster-node-tuning-operator started at 2020-04-17 19:11:12 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container tuned ready: true, restart count 0
Apr 17 22:59:53.533: INFO: dns-default-z6b7d from openshift-dns started at 2020-04-17 19:12:00 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container dns ready: true, restart count 0
Apr 17 22:59:53.533: INFO: 	Container dns-node-resolver ready: true, restart count 0
Apr 17 22:59:53.533: INFO: vpn-5d7df69b48-5mp4n from kube-system started at 2020-04-17 21:54:15 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.533: INFO: 	Container vpn ready: true, restart count 0
Apr 17 22:59:53.534: INFO: console-5b98d99db9-2z7vw from openshift-console started at 2020-04-17 19:12:51 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.534: INFO: 	Container console ready: true, restart count 0
Apr 17 22:59:53.534: INFO: ibmcloud-block-storage-driver-6jgj5 from kube-system started at 2020-04-17 19:10:47 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.534: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Apr 17 22:59:53.534: INFO: node-ca-wr66h from openshift-image-registry started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.534: INFO: 	Container node-ca ready: true, restart count 0
Apr 17 22:59:53.534: INFO: ibm-cloud-provider-ip-158-176-122-172-6f57dbd74b-bpkpw from ibm-system started at 2020-04-17 19:13:51 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.534: INFO: 	Container ibm-cloud-provider-ip-158-176-122-172 ready: true, restart count 0
Apr 17 22:59:53.534: INFO: redhat-operators-7b557fbbc4-pq5mj from openshift-marketplace started at 2020-04-17 19:12:24 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.534: INFO: 	Container redhat-operators ready: true, restart count 0
Apr 17 22:59:53.534: INFO: prometheus-k8s-1 from openshift-monitoring started at 2020-04-17 19:17:47 +0000 UTC (7 container statuses recorded)
Apr 17 22:59:53.534: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:59:53.534: INFO: 	Container prom-label-proxy ready: true, restart count 0
Apr 17 22:59:53.534: INFO: 	Container prometheus ready: true, restart count 1
Apr 17 22:59:53.534: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Apr 17 22:59:53.534: INFO: 	Container prometheus-proxy ready: true, restart count 0
Apr 17 22:59:53.534: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Apr 17 22:59:53.534: INFO: 	Container thanos-sidecar ready: true, restart count 0
Apr 17 22:59:53.534: INFO: cluster-samples-operator-6d4b68977f-5q62h from openshift-cluster-samples-operator started at 2020-04-17 19:11:57 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.534: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Apr 17 22:59:53.534: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Apr 17 22:59:53.534: INFO: multus-admission-controller-72gwb from openshift-multus started at 2020-04-17 19:11:57 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.534: INFO: 	Container multus-admission-controller ready: true, restart count 0
Apr 17 22:59:53.534: INFO: community-operators-5574589d55-swkx4 from openshift-marketplace started at 2020-04-17 19:12:21 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.534: INFO: 	Container community-operators ready: true, restart count 0
Apr 17 22:59:53.534: INFO: ibm-master-proxy-static-10.72.119.98 from kube-system started at 2020-04-17 19:10:37 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.534: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Apr 17 22:59:53.534: INFO: 	Container pause ready: true, restart count 0
Apr 17 22:59:53.534: INFO: node-exporter-lh6vl from openshift-monitoring started at 2020-04-17 19:11:30 +0000 UTC (2 container statuses recorded)
Apr 17 22:59:53.534: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Apr 17 22:59:53.534: INFO: 	Container node-exporter ready: true, restart count 0
Apr 17 22:59:53.534: INFO: prometheus-operator-75c7889c9b-47f2p from openshift-monitoring started at 2020-04-17 19:16:29 +0000 UTC (1 container statuses recorded)
Apr 17 22:59:53.534: INFO: 	Container prometheus-operator ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b093200c-a09b-478b-b7b8-b58ed7c33c0a 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-b093200c-a09b-478b-b7b8-b58ed7c33c0a off the node 10.72.119.74
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b093200c-a09b-478b-b7b8-b58ed7c33c0a
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:04:57.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1776" for this suite.
Apr 17 23:05:13.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:05:16.584: INFO: namespace sched-pred-1776 deletion completed in 18.640627984s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

â€¢ [SLOW TEST:323.723 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:05:16.586: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 23:05:16.724: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Creating first CR 
Apr 17 23:05:17.449: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-17T23:05:17Z generation:1 name:name1 resourceVersion:101511 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4f825350-b2bd-49b4-b11e-5685fa85a7d2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Apr 17 23:05:27.471: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-17T23:05:27Z generation:1 name:name2 resourceVersion:101551 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f5a285f5-ab00-4ac8-a7f9-163a555918ce] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Apr 17 23:05:37.489: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-17T23:05:17Z generation:2 name:name1 resourceVersion:101589 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4f825350-b2bd-49b4-b11e-5685fa85a7d2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Apr 17 23:05:47.508: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-17T23:05:27Z generation:2 name:name2 resourceVersion:101625 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f5a285f5-ab00-4ac8-a7f9-163a555918ce] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Apr 17 23:05:57.532: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-17T23:05:17Z generation:2 name:name1 resourceVersion:101665 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:4f825350-b2bd-49b4-b11e-5685fa85a7d2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Apr 17 23:06:07.556: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2020-04-17T23:05:27Z generation:2 name:name2 resourceVersion:101702 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:f5a285f5-ab00-4ac8-a7f9-163a555918ce] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:06:18.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2479" for this suite.
Apr 17 23:06:26.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:06:28.365: INFO: namespace crd-watch-2479 deletion completed in 10.242007917s

â€¢ [SLOW TEST:71.780 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:06:28.366: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 23:06:29.412: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 23:06:32.502: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 23:06:32.519: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7423-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:06:34.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3173" for this suite.
Apr 17 23:06:42.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:06:44.411: INFO: namespace webhook-3173 deletion completed in 10.221296885s
STEP: Destroying namespace "webhook-3173-markers" for this suite.
Apr 17 23:06:52.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:06:54.681: INFO: namespace webhook-3173-markers deletion completed in 10.269582253s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:26.393 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:06:54.759: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Apr 17 23:06:58.062: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:06:58.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1477" for this suite.
Apr 17 23:07:06.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:07:08.350: INFO: namespace container-runtime-1477 deletion completed in 10.227307998s

â€¢ [SLOW TEST:13.590 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:07:08.352: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 23:07:09.144: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 17 23:07:11.195: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761629, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761629, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761629, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761629, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 23:07:14.247: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 23:07:14.263: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:07:15.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-842" for this suite.
Apr 17 23:07:23.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:07:26.218: INFO: namespace webhook-842 deletion completed in 10.410043341s
STEP: Destroying namespace "webhook-842-markers" for this suite.
Apr 17 23:07:34.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:07:36.498: INFO: namespace webhook-842-markers deletion completed in 10.279984678s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:28.224 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:07:36.576: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 23:07:36.773: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr 17 23:07:41.788: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 17 23:07:41.788: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Apr 17 23:07:43.902: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2509 /apis/apps/v1/namespaces/deployment-2509/deployments/test-cleanup-deployment 1b61f00a-f201-4689-b1ac-b45a7c97d4d2 102624 1 2020-04-17 23:07:41 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc003a51ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-04-17 23:07:41 +0000 UTC,LastTransitionTime:2020-04-17 23:07:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-65db99849b" has successfully progressed.,LastUpdateTime:2020-04-17 23:07:43 +0000 UTC,LastTransitionTime:2020-04-17 23:07:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 17 23:07:43.922: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-2509 /apis/apps/v1/namespaces/deployment-2509/replicasets/test-cleanup-deployment-65db99849b 98382967-bd2c-4daa-977b-93a28a302de2 102613 1 2020-04-17 23:07:41 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 1b61f00a-f201-4689-b1ac-b45a7c97d4d2 0xc00072a5b7 0xc00072a5b8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc00072a678 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 17 23:07:43.934: INFO: Pod "test-cleanup-deployment-65db99849b-fxd7x" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-fxd7x test-cleanup-deployment-65db99849b- deployment-2509 /api/v1/namespaces/deployment-2509/pods/test-cleanup-deployment-65db99849b-fxd7x 04fd34a0-8051-4a67-bdaf-8b3f42730afd 102612 0 2020-04-17 23:07:41 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[cni.projectcalico.org/podIP:172.30.194.88/32 cni.projectcalico.org/podIPs:172.30.194.88/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.194.88"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b 98382967-bd2c-4daa-977b-93a28a302de2 0xc0082045f7 0xc0082045f8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-gxvtn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-gxvtn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-gxvtn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-sjbtg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 23:07:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 23:07:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 23:07:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 23:07:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.74,PodIP:172.30.194.88,StartTime:2020-04-17 23:07:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-17 23:07:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://b95358b43110a466d19451c8398a51a4719c925599bc73706bf49c2b89b45e2d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.194.88,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:07:43.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2509" for this suite.
Apr 17 23:07:51.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:07:54.161: INFO: namespace deployment-2509 deletion completed in 10.209796722s

â€¢ [SLOW TEST:17.585 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:07:54.161: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 23:07:54.348: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr 17 23:07:59.364: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr 17 23:07:59.364: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr 17 23:08:01.384: INFO: Creating deployment "test-rollover-deployment"
Apr 17 23:08:01.419: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr 17 23:08:03.444: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr 17 23:08:03.473: INFO: Ensure that both replica sets have 1 created replica
Apr 17 23:08:03.507: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr 17 23:08:03.538: INFO: Updating deployment test-rollover-deployment
Apr 17 23:08:03.538: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr 17 23:08:05.570: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr 17 23:08:05.600: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr 17 23:08:05.632: INFO: all replica sets need to contain the pod-template-hash label
Apr 17 23:08:05.632: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761681, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761685, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761681, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 23:08:07.663: INFO: all replica sets need to contain the pod-template-hash label
Apr 17 23:08:07.663: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761681, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761685, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761681, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 23:08:09.660: INFO: all replica sets need to contain the pod-template-hash label
Apr 17 23:08:09.660: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761681, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761685, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761681, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 23:08:11.660: INFO: all replica sets need to contain the pod-template-hash label
Apr 17 23:08:11.660: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761681, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761685, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761681, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 23:08:13.663: INFO: all replica sets need to contain the pod-template-hash label
Apr 17 23:08:13.663: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761681, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761681, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761685, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722761681, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 23:08:15.688: INFO: 
Apr 17 23:08:15.688: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Apr 17 23:08:15.957: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5763 /apis/apps/v1/namespaces/deployment-5763/deployments/test-rollover-deployment 91b3f0e4-8764-4742-888c-1a9136fff891 102901 2 2020-04-17 23:08:01 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006b25dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-04-17 23:08:01 +0000 UTC,LastTransitionTime:2020-04-17 23:08:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2020-04-17 23:08:15 +0000 UTC,LastTransitionTime:2020-04-17 23:08:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 17 23:08:15.977: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-5763 /apis/apps/v1/namespaces/deployment-5763/replicasets/test-rollover-deployment-7d7dc6548c 61646ac0-ec2a-4e8b-9f9d-8745f81743a3 102890 2 2020-04-17 23:08:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 91b3f0e4-8764-4742-888c-1a9136fff891 0xc007e0bd87 0xc007e0bd88}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc007e0bde8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 17 23:08:15.977: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr 17 23:08:15.977: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5763 /apis/apps/v1/namespaces/deployment-5763/replicasets/test-rollover-controller 971c0c4d-c8e7-4501-9c65-f21fc67eb083 102899 2 2020-04-17 23:07:54 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 91b3f0e4-8764-4742-888c-1a9136fff891 0xc007e0bcb7 0xc007e0bcb8}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc007e0bd18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 17 23:08:15.977: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-5763 /apis/apps/v1/namespaces/deployment-5763/replicasets/test-rollover-deployment-f6c94f66c f2f5f522-baab-444b-ab27-545a3b80ef50 102829 2 2020-04-17 23:08:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 91b3f0e4-8764-4742-888c-1a9136fff891 0xc007e0be50 0xc007e0be51}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc007e0bec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 17 23:08:15.989: INFO: Pod "test-rollover-deployment-7d7dc6548c-7j887" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-7j887 test-rollover-deployment-7d7dc6548c- deployment-5763 /api/v1/namespaces/deployment-5763/pods/test-rollover-deployment-7d7dc6548c-7j887 f6529af1-c643-4592-8395-b04b97aa1ecf 102853 0 2020-04-17 23:08:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[cni.projectcalico.org/podIP:172.30.194.93/32 cni.projectcalico.org/podIPs:172.30.194.93/32 k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.194.93"
    ],
    "dns": {}
}] openshift.io/scc:privileged] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 61646ac0-ec2a-4e8b-9f9d-8745f81743a3 0xc005980437 0xc005980438}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-bsmww,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-bsmww,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-bsmww,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.72.119.74,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-wxhx4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 23:08:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 23:08:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 23:08:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2020-04-17 23:08:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.72.119.74,PodIP:172.30.194.93,StartTime:2020-04-17 23:08:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2020-04-17 23:08:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:docker.io/library/redis:5.0.5-alpine,ImageID:docker.io/library/redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:cri-o://4da460de11597cc665206b0e023059eaa32439e2d5566469d02dd772bb6d08f9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.194.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:08:15.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5763" for this suite.
Apr 17 23:08:24.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:08:26.213: INFO: namespace deployment-5763 deletion completed in 10.19374265s

â€¢ [SLOW TEST:32.052 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:08:26.214: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-712
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 17 23:08:26.392: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 17 23:08:46.843: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.194.97:8080/dial?request=hostName&protocol=http&host=172.30.194.91&port=8080&tries=1'] Namespace:pod-network-test-712 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 23:08:46.843: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 23:08:47.086: INFO: Waiting for endpoints: map[]
Apr 17 23:08:47.100: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.194.97:8080/dial?request=hostName&protocol=http&host=172.30.232.244&port=8080&tries=1'] Namespace:pod-network-test-712 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 23:08:47.100: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 23:08:47.290: INFO: Waiting for endpoints: map[]
Apr 17 23:08:47.303: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.194.97:8080/dial?request=hostName&protocol=http&host=172.30.53.37&port=8080&tries=1'] Namespace:pod-network-test-712 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 23:08:47.303: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 23:08:47.538: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:08:47.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-712" for this suite.
Apr 17 23:08:55.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:08:57.920: INFO: namespace pod-network-test-712 deletion completed in 10.361146057s

â€¢ [SLOW TEST:31.706 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:08:57.920: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 23:08:58.115: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-3085ad60-9f80-4c1b-a987-72eb9f646668" in namespace "security-context-test-9773" to be "success or failure"
Apr 17 23:08:58.133: INFO: Pod "alpine-nnp-false-3085ad60-9f80-4c1b-a987-72eb9f646668": Phase="Pending", Reason="", readiness=false. Elapsed: 17.059774ms
Apr 17 23:09:00.145: INFO: Pod "alpine-nnp-false-3085ad60-9f80-4c1b-a987-72eb9f646668": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029895381s
Apr 17 23:09:02.162: INFO: Pod "alpine-nnp-false-3085ad60-9f80-4c1b-a987-72eb9f646668": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046890387s
Apr 17 23:09:04.176: INFO: Pod "alpine-nnp-false-3085ad60-9f80-4c1b-a987-72eb9f646668": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060026922s
Apr 17 23:09:04.176: INFO: Pod "alpine-nnp-false-3085ad60-9f80-4c1b-a987-72eb9f646668" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:09:04.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9773" for this suite.
Apr 17 23:09:12.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:09:14.470: INFO: namespace security-context-test-9773 deletion completed in 10.192603409s

â€¢ [SLOW TEST:16.551 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:09:14.471: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-2415
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Apr 17 23:09:14.689: INFO: Found 0 stateful pods, waiting for 3
Apr 17 23:09:24.702: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 23:09:24.702: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 23:09:24.702: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Apr 17 23:09:24.800: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr 17 23:09:34.909: INFO: Updating stateful set ss2
Apr 17 23:09:34.945: INFO: Waiting for Pod statefulset-2415/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Apr 17 23:09:45.050: INFO: Found 1 stateful pods, waiting for 3
Apr 17 23:09:55.063: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 23:09:55.063: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 23:09:55.063: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr 17 23:09:55.141: INFO: Updating stateful set ss2
Apr 17 23:09:55.173: INFO: Waiting for Pod statefulset-2415/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Apr 17 23:10:05.253: INFO: Updating stateful set ss2
Apr 17 23:10:05.284: INFO: Waiting for StatefulSet statefulset-2415/ss2 to complete update
Apr 17 23:10:05.284: INFO: Waiting for Pod statefulset-2415/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Apr 17 23:10:15.314: INFO: Deleting all statefulset in ns statefulset-2415
Apr 17 23:10:15.330: INFO: Scaling statefulset ss2 to 0
Apr 17 23:10:45.390: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 23:10:45.406: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:10:45.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2415" for this suite.
Apr 17 23:10:55.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:10:57.755: INFO: namespace statefulset-2415 deletion completed in 12.265038256s

â€¢ [SLOW TEST:103.284 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:10:57.755: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Apr 17 23:10:57.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 create -f - --namespace=kubectl-5596'
Apr 17 23:10:58.639: INFO: stderr: ""
Apr 17 23:10:58.639: INFO: stdout: "pod/pause created\n"
Apr 17 23:10:58.639: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr 17 23:10:58.639: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5596" to be "running and ready"
Apr 17 23:10:58.652: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 13.733539ms
Apr 17 23:11:00.664: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.024808544s
Apr 17 23:11:00.664: INFO: Pod "pause" satisfied condition "running and ready"
Apr 17 23:11:00.664: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Apr 17 23:11:00.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 label pods pause testing-label=testing-label-value --namespace=kubectl-5596'
Apr 17 23:11:00.837: INFO: stderr: ""
Apr 17 23:11:00.837: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr 17 23:11:00.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pod pause -L testing-label --namespace=kubectl-5596'
Apr 17 23:11:00.968: INFO: stderr: ""
Apr 17 23:11:00.968: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr 17 23:11:00.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 label pods pause testing-label- --namespace=kubectl-5596'
Apr 17 23:11:01.167: INFO: stderr: ""
Apr 17 23:11:01.167: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr 17 23:11:01.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pod pause -L testing-label --namespace=kubectl-5596'
Apr 17 23:11:01.317: INFO: stderr: ""
Apr 17 23:11:01.317: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Apr 17 23:11:01.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete --grace-period=0 --force -f - --namespace=kubectl-5596'
Apr 17 23:11:01.473: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 23:11:01.473: INFO: stdout: "pod \"pause\" force deleted\n"
Apr 17 23:11:01.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get rc,svc -l name=pause --no-headers --namespace=kubectl-5596'
Apr 17 23:11:01.638: INFO: stderr: "No resources found in kubectl-5596 namespace.\n"
Apr 17 23:11:01.638: INFO: stdout: ""
Apr 17 23:11:01.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -l name=pause --namespace=kubectl-5596 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 17 23:11:01.772: INFO: stderr: ""
Apr 17 23:11:01.772: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:11:01.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5596" for this suite.
Apr 17 23:11:09.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:11:11.974: INFO: namespace kubectl-5596 deletion completed in 10.175812614s

â€¢ [SLOW TEST:14.219 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:11:11.978: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:11:28.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5344" for this suite.
Apr 17 23:11:36.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:11:38.674: INFO: namespace resourcequota-5344 deletion completed in 10.167387248s

â€¢ [SLOW TEST:26.696 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:11:38.677: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:11:38.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3284" for this suite.
Apr 17 23:11:46.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:11:51.077: INFO: namespace custom-resource-definition-3284 deletion completed in 12.22255953s

â€¢ [SLOW TEST:12.400 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:11:51.078: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 23:11:51.900: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 23:11:55.012: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 23:11:55.041: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9884-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:11:56.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7517" for this suite.
Apr 17 23:12:04.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:12:06.295: INFO: namespace webhook-7517 deletion completed in 10.25408336s
STEP: Destroying namespace "webhook-7517-markers" for this suite.
Apr 17 23:12:14.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:12:16.493: INFO: namespace webhook-7517-markers deletion completed in 10.198616051s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:25.503 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:12:16.580: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 23:12:16.720: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Apr 17 23:12:27.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-1340 create -f -'
Apr 17 23:12:28.341: INFO: stderr: ""
Apr 17 23:12:28.341: INFO: stdout: "e2e-test-crd-publish-openapi-6422-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 17 23:12:28.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-1340 delete e2e-test-crd-publish-openapi-6422-crds test-cr'
Apr 17 23:12:28.502: INFO: stderr: ""
Apr 17 23:12:28.502: INFO: stdout: "e2e-test-crd-publish-openapi-6422-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Apr 17 23:12:28.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-1340 apply -f -'
Apr 17 23:12:29.363: INFO: stderr: ""
Apr 17 23:12:29.363: INFO: stdout: "e2e-test-crd-publish-openapi-6422-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 17 23:12:29.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 --namespace=crd-publish-openapi-1340 delete e2e-test-crd-publish-openapi-6422-crds test-cr'
Apr 17 23:12:29.558: INFO: stderr: ""
Apr 17 23:12:29.558: INFO: stdout: "e2e-test-crd-publish-openapi-6422-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Apr 17 23:12:29.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 explain e2e-test-crd-publish-openapi-6422-crds'
Apr 17 23:12:29.902: INFO: stderr: ""
Apr 17 23:12:29.902: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6422-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:12:40.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1340" for this suite.
Apr 17 23:12:48.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:12:50.497: INFO: namespace crd-publish-openapi-1340 deletion completed in 10.192100394s

â€¢ [SLOW TEST:33.917 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:12:50.497: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-9684
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr 17 23:12:50.626: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr 17 23:13:11.065: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.194.104:8080/dial?request=hostName&protocol=udp&host=172.30.194.100&port=8081&tries=1'] Namespace:pod-network-test-9684 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 23:13:11.065: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 23:13:11.300: INFO: Waiting for endpoints: map[]
Apr 17 23:13:11.315: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.194.104:8080/dial?request=hostName&protocol=udp&host=172.30.232.247&port=8081&tries=1'] Namespace:pod-network-test-9684 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 23:13:11.315: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 23:13:11.544: INFO: Waiting for endpoints: map[]
Apr 17 23:13:11.556: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.194.104:8080/dial?request=hostName&protocol=udp&host=172.30.53.36&port=8081&tries=1'] Namespace:pod-network-test-9684 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr 17 23:13:11.556: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
Apr 17 23:13:11.796: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:13:11.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9684" for this suite.
Apr 17 23:13:19.863: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:13:21.993: INFO: namespace pod-network-test-9684 deletion completed in 10.174953952s

â€¢ [SLOW TEST:31.496 seconds]
[sig-network] Networking
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:13:21.994: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4925.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4925.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4925.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4925.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4925.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4925.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4925.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4925.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4925.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4925.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4925.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4925.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4925.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4925.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4925.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4925.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4925.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4925.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr 17 23:13:26.313: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4925.svc.cluster.local from pod dns-4925/dns-test-a0fc1dd0-9a1d-4861-8ca1-3cc8809575fb: the server could not find the requested resource (get pods dns-test-a0fc1dd0-9a1d-4861-8ca1-3cc8809575fb)
Apr 17 23:13:26.335: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4925.svc.cluster.local from pod dns-4925/dns-test-a0fc1dd0-9a1d-4861-8ca1-3cc8809575fb: the server could not find the requested resource (get pods dns-test-a0fc1dd0-9a1d-4861-8ca1-3cc8809575fb)
Apr 17 23:13:26.378: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4925.svc.cluster.local from pod dns-4925/dns-test-a0fc1dd0-9a1d-4861-8ca1-3cc8809575fb: the server could not find the requested resource (get pods dns-test-a0fc1dd0-9a1d-4861-8ca1-3cc8809575fb)
Apr 17 23:13:26.444: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4925.svc.cluster.local from pod dns-4925/dns-test-a0fc1dd0-9a1d-4861-8ca1-3cc8809575fb: the server could not find the requested resource (get pods dns-test-a0fc1dd0-9a1d-4861-8ca1-3cc8809575fb)
Apr 17 23:13:26.466: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4925.svc.cluster.local from pod dns-4925/dns-test-a0fc1dd0-9a1d-4861-8ca1-3cc8809575fb: the server could not find the requested resource (get pods dns-test-a0fc1dd0-9a1d-4861-8ca1-3cc8809575fb)
Apr 17 23:13:26.546: INFO: Lookups using dns-4925/dns-test-a0fc1dd0-9a1d-4861-8ca1-3cc8809575fb failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4925.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4925.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4925.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4925.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4925.svc.cluster.local]

Apr 17 23:13:31.862: INFO: DNS probes using dns-4925/dns-test-a0fc1dd0-9a1d-4861-8ca1-3cc8809575fb succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:13:31.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4925" for this suite.
Apr 17 23:13:40.034: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:13:42.129: INFO: namespace dns-4925 deletion completed in 10.141680323s

â€¢ [SLOW TEST:20.135 seconds]
[sig-network] DNS
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:13:42.129: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-6739
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6739 to expose endpoints map[]
Apr 17 23:13:42.334: INFO: successfully validated that service multi-endpoint-test in namespace services-6739 exposes endpoints map[] (22.306706ms elapsed)
STEP: Creating pod pod1 in namespace services-6739
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6739 to expose endpoints map[pod1:[100]]
Apr 17 23:13:44.472: INFO: successfully validated that service multi-endpoint-test in namespace services-6739 exposes endpoints map[pod1:[100]] (2.087309169s elapsed)
STEP: Creating pod pod2 in namespace services-6739
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6739 to expose endpoints map[pod1:[100] pod2:[101]]
Apr 17 23:13:47.673: INFO: successfully validated that service multi-endpoint-test in namespace services-6739 exposes endpoints map[pod1:[100] pod2:[101]] (3.164999418s elapsed)
STEP: Deleting pod pod1 in namespace services-6739
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6739 to expose endpoints map[pod2:[101]]
Apr 17 23:13:48.763: INFO: successfully validated that service multi-endpoint-test in namespace services-6739 exposes endpoints map[pod2:[101]] (1.063486198s elapsed)
STEP: Deleting pod pod2 in namespace services-6739
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6739 to expose endpoints map[]
Apr 17 23:13:48.803: INFO: successfully validated that service multi-endpoint-test in namespace services-6739 exposes endpoints map[] (16.879963ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:13:48.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6739" for this suite.
Apr 17 23:14:02.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:14:05.119: INFO: namespace services-6739 deletion completed in 16.190506078s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

â€¢ [SLOW TEST:22.990 seconds]
[sig-network] Services
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:14:05.122: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 23:14:05.800: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 23:14:08.877: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:14:09.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1163" for this suite.
Apr 17 23:14:17.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:14:19.445: INFO: namespace webhook-1163 deletion completed in 10.193606819s
STEP: Destroying namespace "webhook-1163-markers" for this suite.
Apr 17 23:14:27.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:14:29.623: INFO: namespace webhook-1163-markers deletion completed in 10.177914953s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:24.578 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:14:29.700: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:14:43.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1458" for this suite.
Apr 17 23:14:51.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:14:53.370: INFO: namespace resourcequota-1458 deletion completed in 10.226762983s

â€¢ [SLOW TEST:23.670 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:14:53.371: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr 17 23:14:56.170: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1422acfb-f680-44c5-9692-b77333b2c30b"
Apr 17 23:14:56.170: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1422acfb-f680-44c5-9692-b77333b2c30b" in namespace "pods-3600" to be "terminated due to deadline exceeded"
Apr 17 23:14:56.182: INFO: Pod "pod-update-activedeadlineseconds-1422acfb-f680-44c5-9692-b77333b2c30b": Phase="Running", Reason="", readiness=true. Elapsed: 12.604411ms
Apr 17 23:14:58.195: INFO: Pod "pod-update-activedeadlineseconds-1422acfb-f680-44c5-9692-b77333b2c30b": Phase="Running", Reason="", readiness=true. Elapsed: 2.024666226s
Apr 17 23:15:00.206: INFO: Pod "pod-update-activedeadlineseconds-1422acfb-f680-44c5-9692-b77333b2c30b": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.036498144s
Apr 17 23:15:00.207: INFO: Pod "pod-update-activedeadlineseconds-1422acfb-f680-44c5-9692-b77333b2c30b" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:15:00.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3600" for this suite.
Apr 17 23:15:08.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:15:10.456: INFO: namespace pods-3600 deletion completed in 10.199336796s

â€¢ [SLOW TEST:17.085 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:15:10.456: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Apr 17 23:15:10.658: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-c29cf33b-c144-46e1-ad4f-6ec643b949f2
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:15:12.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5927" for this suite.
Apr 17 23:15:26.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:15:29.201: INFO: namespace configmap-5927 deletion completed in 16.326392538s

â€¢ [SLOW TEST:18.745 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:15:29.201: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Apr 17 23:15:39.718: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W0417 23:15:39.718382      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr 17 23:15:39.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9744" for this suite.
Apr 17 23:15:49.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:15:51.935: INFO: namespace gc-9744 deletion completed in 12.194283195s

â€¢ [SLOW TEST:22.734 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:15:51.935: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Apr 17 23:15:52.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 create -f - --namespace=kubectl-7924'
Apr 17 23:15:52.784: INFO: stderr: ""
Apr 17 23:15:52.784: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr 17 23:15:52.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7924'
Apr 17 23:15:53.141: INFO: stderr: ""
Apr 17 23:15:53.141: INFO: stdout: "update-demo-nautilus-t4hhf update-demo-nautilus-xpwsr "
Apr 17 23:15:53.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-t4hhf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7924'
Apr 17 23:15:53.316: INFO: stderr: ""
Apr 17 23:15:53.316: INFO: stdout: ""
Apr 17 23:15:53.316: INFO: update-demo-nautilus-t4hhf is created but not running
Apr 17 23:15:58.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7924'
Apr 17 23:15:58.461: INFO: stderr: ""
Apr 17 23:15:58.461: INFO: stdout: "update-demo-nautilus-t4hhf update-demo-nautilus-xpwsr "
Apr 17 23:15:58.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-t4hhf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7924'
Apr 17 23:15:58.596: INFO: stderr: ""
Apr 17 23:15:58.596: INFO: stdout: "true"
Apr 17 23:15:58.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-t4hhf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7924'
Apr 17 23:15:58.739: INFO: stderr: ""
Apr 17 23:15:58.739: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 17 23:15:58.739: INFO: validating pod update-demo-nautilus-t4hhf
Apr 17 23:15:58.770: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 23:15:58.770: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 23:15:58.770: INFO: update-demo-nautilus-t4hhf is verified up and running
Apr 17 23:15:58.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-xpwsr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7924'
Apr 17 23:15:58.895: INFO: stderr: ""
Apr 17 23:15:58.895: INFO: stdout: "true"
Apr 17 23:15:58.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods update-demo-nautilus-xpwsr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7924'
Apr 17 23:15:59.058: INFO: stderr: ""
Apr 17 23:15:59.059: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr 17 23:15:59.059: INFO: validating pod update-demo-nautilus-xpwsr
Apr 17 23:15:59.087: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 23:15:59.087: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 23:15:59.087: INFO: update-demo-nautilus-xpwsr is verified up and running
STEP: using delete to clean up resources
Apr 17 23:15:59.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 delete --grace-period=0 --force -f - --namespace=kubectl-7924'
Apr 17 23:15:59.227: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 23:15:59.227: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 17 23:15:59.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7924'
Apr 17 23:15:59.383: INFO: stderr: "No resources found in kubectl-7924 namespace.\n"
Apr 17 23:15:59.383: INFO: stdout: ""
Apr 17 23:15:59.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -l name=update-demo --namespace=kubectl-7924 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 17 23:15:59.539: INFO: stderr: ""
Apr 17 23:15:59.539: INFO: stdout: "update-demo-nautilus-t4hhf\nupdate-demo-nautilus-xpwsr\n"
Apr 17 23:16:00.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7924'
Apr 17 23:16:00.219: INFO: stderr: "No resources found in kubectl-7924 namespace.\n"
Apr 17 23:16:00.219: INFO: stdout: ""
Apr 17 23:16:00.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-531062789 get pods -l name=update-demo --namespace=kubectl-7924 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 17 23:16:00.369: INFO: stderr: ""
Apr 17 23:16:00.369: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:16:00.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7924" for this suite.
Apr 17 23:16:08.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:16:10.572: INFO: namespace kubectl-7924 deletion completed in 10.173883833s

â€¢ [SLOW TEST:18.637 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:16:10.572: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:16:17.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6207" for this suite.
Apr 17 23:16:25.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:16:28.023: INFO: namespace resourcequota-6207 deletion completed in 10.2185793s

â€¢ [SLOW TEST:17.451 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:16:28.025: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:16:30.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4670" for this suite.
Apr 17 23:17:12.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:17:14.565: INFO: namespace kubelet-test-4670 deletion completed in 44.202912017s

â€¢ [SLOW TEST:46.540 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:17:14.565: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 23:17:15.236: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 17 23:17:17.283: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722762235, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722762235, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722762235, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722762235, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 23:17:20.377: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:17:20.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-145" for this suite.
Apr 17 23:17:28.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:17:30.945: INFO: namespace webhook-145 deletion completed in 10.250285196s
STEP: Destroying namespace "webhook-145-markers" for this suite.
Apr 17 23:17:38.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:17:41.204: INFO: namespace webhook-145-markers deletion completed in 10.258656324s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:26.715 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:17:41.281: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Apr 17 23:17:41.489: INFO: Waiting up to 5m0s for pod "downward-api-f3e04aa0-40c7-4af4-b4cf-fd4d890205f0" in namespace "downward-api-2752" to be "success or failure"
Apr 17 23:17:41.506: INFO: Pod "downward-api-f3e04aa0-40c7-4af4-b4cf-fd4d890205f0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.023091ms
Apr 17 23:17:43.521: INFO: Pod "downward-api-f3e04aa0-40c7-4af4-b4cf-fd4d890205f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.032518384s
STEP: Saw pod success
Apr 17 23:17:43.521: INFO: Pod "downward-api-f3e04aa0-40c7-4af4-b4cf-fd4d890205f0" satisfied condition "success or failure"
Apr 17 23:17:43.534: INFO: Trying to get logs from node 10.72.119.74 pod downward-api-f3e04aa0-40c7-4af4-b4cf-fd4d890205f0 container dapi-container: <nil>
STEP: delete the pod
Apr 17 23:17:43.603: INFO: Waiting for pod downward-api-f3e04aa0-40c7-4af4-b4cf-fd4d890205f0 to disappear
Apr 17 23:17:43.615: INFO: Pod downward-api-f3e04aa0-40c7-4af4-b4cf-fd4d890205f0 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:17:43.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2752" for this suite.
Apr 17 23:17:51.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:17:53.869: INFO: namespace downward-api-2752 deletion completed in 10.229046705s

â€¢ [SLOW TEST:12.589 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:17:53.870: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr 17 23:17:54.073: INFO: Waiting up to 5m0s for pod "pod-1da2cd98-421c-4d87-b114-11b8d6f10c11" in namespace "emptydir-3318" to be "success or failure"
Apr 17 23:17:54.090: INFO: Pod "pod-1da2cd98-421c-4d87-b114-11b8d6f10c11": Phase="Pending", Reason="", readiness=false. Elapsed: 17.072472ms
Apr 17 23:17:56.101: INFO: Pod "pod-1da2cd98-421c-4d87-b114-11b8d6f10c11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.028631841s
STEP: Saw pod success
Apr 17 23:17:56.101: INFO: Pod "pod-1da2cd98-421c-4d87-b114-11b8d6f10c11" satisfied condition "success or failure"
Apr 17 23:17:56.111: INFO: Trying to get logs from node 10.72.119.74 pod pod-1da2cd98-421c-4d87-b114-11b8d6f10c11 container test-container: <nil>
STEP: delete the pod
Apr 17 23:17:56.174: INFO: Waiting for pod pod-1da2cd98-421c-4d87-b114-11b8d6f10c11 to disappear
Apr 17 23:17:56.186: INFO: Pod pod-1da2cd98-421c-4d87-b114-11b8d6f10c11 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:17:56.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3318" for this suite.
Apr 17 23:18:04.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:18:06.413: INFO: namespace emptydir-3318 deletion completed in 10.195409357s

â€¢ [SLOW TEST:12.543 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:18:06.414: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Apr 17 23:18:07.542: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Apr 17 23:18:09.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722762287, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722762287, loc:(*time.Location)(0x84c02a0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63722762287, loc:(*time.Location)(0x84c02a0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63722762287, loc:(*time.Location)(0x84c02a0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Apr 17 23:18:12.632: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:18:23.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2322" for this suite.
Apr 17 23:18:31.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:18:33.489: INFO: namespace webhook-2322 deletion completed in 10.236033662s
STEP: Destroying namespace "webhook-2322-markers" for this suite.
Apr 17 23:18:41.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:18:43.771: INFO: namespace webhook-2322-markers deletion completed in 10.282453252s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:37.433 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:18:43.847: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Apr 17 23:18:44.035: INFO: Waiting up to 5m0s for pod "client-containers-c5593f02-e25e-4c85-93c2-25a597e9ebe3" in namespace "containers-1024" to be "success or failure"
Apr 17 23:18:44.047: INFO: Pod "client-containers-c5593f02-e25e-4c85-93c2-25a597e9ebe3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02152ms
Apr 17 23:18:46.060: INFO: Pod "client-containers-c5593f02-e25e-4c85-93c2-25a597e9ebe3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024786641s
Apr 17 23:18:48.074: INFO: Pod "client-containers-c5593f02-e25e-4c85-93c2-25a597e9ebe3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038928717s
STEP: Saw pod success
Apr 17 23:18:48.074: INFO: Pod "client-containers-c5593f02-e25e-4c85-93c2-25a597e9ebe3" satisfied condition "success or failure"
Apr 17 23:18:48.086: INFO: Trying to get logs from node 10.72.119.74 pod client-containers-c5593f02-e25e-4c85-93c2-25a597e9ebe3 container test-container: <nil>
STEP: delete the pod
Apr 17 23:18:48.149: INFO: Waiting for pod client-containers-c5593f02-e25e-4c85-93c2-25a597e9ebe3 to disappear
Apr 17 23:18:48.161: INFO: Pod client-containers-c5593f02-e25e-4c85-93c2-25a597e9ebe3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:18:48.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1024" for this suite.
Apr 17 23:18:56.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:18:58.425: INFO: namespace containers-1024 deletion completed in 10.232916746s

â€¢ [SLOW TEST:14.579 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:18:58.426: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Apr 17 23:18:58.639: INFO: Waiting up to 5m0s for pod "downwardapi-volume-73226801-9b26-4fec-a672-1ed5806bd529" in namespace "downward-api-5824" to be "success or failure"
Apr 17 23:18:58.649: INFO: Pod "downwardapi-volume-73226801-9b26-4fec-a672-1ed5806bd529": Phase="Pending", Reason="", readiness=false. Elapsed: 10.787562ms
Apr 17 23:19:00.662: INFO: Pod "downwardapi-volume-73226801-9b26-4fec-a672-1ed5806bd529": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023133187s
STEP: Saw pod success
Apr 17 23:19:00.662: INFO: Pod "downwardapi-volume-73226801-9b26-4fec-a672-1ed5806bd529" satisfied condition "success or failure"
Apr 17 23:19:00.673: INFO: Trying to get logs from node 10.72.119.74 pod downwardapi-volume-73226801-9b26-4fec-a672-1ed5806bd529 container client-container: <nil>
STEP: delete the pod
Apr 17 23:19:00.734: INFO: Waiting for pod downwardapi-volume-73226801-9b26-4fec-a672-1ed5806bd529 to disappear
Apr 17 23:19:00.745: INFO: Pod downwardapi-volume-73226801-9b26-4fec-a672-1ed5806bd529 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:19:00.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5824" for this suite.
Apr 17 23:19:08.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:19:11.000: INFO: namespace downward-api-5824 deletion completed in 10.211171874s

â€¢ [SLOW TEST:12.575 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Apr 17 23:19:11.001: INFO: >>> kubeConfig: /tmp/kubeconfig-531062789
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-f54a76a2-5433-411d-944d-bb263ebe4f43
STEP: Creating a pod to test consume configMaps
Apr 17 23:19:11.226: INFO: Waiting up to 5m0s for pod "pod-configmaps-0da47d33-c6c8-4f66-b41f-62688b6c8288" in namespace "configmap-5535" to be "success or failure"
Apr 17 23:19:11.237: INFO: Pod "pod-configmaps-0da47d33-c6c8-4f66-b41f-62688b6c8288": Phase="Pending", Reason="", readiness=false. Elapsed: 10.919845ms
Apr 17 23:19:13.251: INFO: Pod "pod-configmaps-0da47d33-c6c8-4f66-b41f-62688b6c8288": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025348804s
STEP: Saw pod success
Apr 17 23:19:13.251: INFO: Pod "pod-configmaps-0da47d33-c6c8-4f66-b41f-62688b6c8288" satisfied condition "success or failure"
Apr 17 23:19:13.264: INFO: Trying to get logs from node 10.72.119.74 pod pod-configmaps-0da47d33-c6c8-4f66-b41f-62688b6c8288 container configmap-volume-test: <nil>
STEP: delete the pod
Apr 17 23:19:13.336: INFO: Waiting for pod pod-configmaps-0da47d33-c6c8-4f66-b41f-62688b6c8288 to disappear
Apr 17 23:19:13.347: INFO: Pod pod-configmaps-0da47d33-c6c8-4f66-b41f-62688b6c8288 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Apr 17 23:19:13.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5535" for this suite.
Apr 17 23:19:21.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr 17 23:19:23.617: INFO: namespace configmap-5535 deletion completed in 10.238730362s

â€¢ [SLOW TEST:12.617 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.2-beta.0.19+c97fe5036ef3df/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSApr 17 23:19:23.617: INFO: Running AfterSuite actions on all nodes
Apr 17 23:19:23.617: INFO: Running AfterSuite actions on node 1
Apr 17 23:19:23.617: INFO: Skipping dumping logs from cluster

Ran 276 of 4897 Specs in 9092.056 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4621 Skipped
PASS

Ginkgo ran 1 suite in 2h31m34.023488262s
Test Suite Passed
